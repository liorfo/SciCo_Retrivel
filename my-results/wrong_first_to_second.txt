class: first -> second, base class: no relation, new model class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
The adaptive Parallel Subgradient Projection ( PSP ) technique improves the convergence speed , in noisy environment , of <m> linear-projection-based algorithms </m> ( e.g. , NLMS and APA ) , with low computational complexity .


class: first -> second, base class: no relation, new model class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
We can build a discrete model of the system by using a <m> linear projection operator </m> to approximate the state space projection .


class: first -> second, base class: no relation, new model class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
For preserving the low rank properties the same , we proposed an algorithm , called <m> linear preserving projection </m> based on low rank representations(LLRLPP),to reduce the dimension of data .


class: first -> second, base class: no relation, new model class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
Principal component analysis(PCA ) and Singular value decomposition(SVD ) are two algebra methods used in <m> linear projection analysis </m> to reduce the dimensions in algebra space .


class: first -> second, base class: no relation, new model class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
In this system , sensors take samples while compressing the signal with <m> linear projection operations </m> using the idea of compressive sensing ( CS ) [ 1 ] .


class: first -> second, base class: no relation, new model class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
The monocular vision-based spacecraft relative navigation method is researched , and non-linear equations are established representing relationship between image characteristics and navigation information , based on <m> linear projection principle </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
Specifically , the authors first present a <m> linear projection view </m> to formulate subspace learning and then develop a novel framework , called Latent Subspace Projection Pursuit ( LSPP ) , to estimate the intrinsic dimension , removing corruptions and recovering the subspace structure for observed datasets .


class: first -> second, base class: no relation, new model class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
This model first applies a <m> linear projection </m> to each tuple ( ) and then takes as input the set of projected tuples .


class: first -> second, base class: no relation, new model class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
We use two additional <m> linear layers </m> and principal component analysis to reduce the dimension of both inputs and internal representations and to transmit the essential information .


class: first -> second, base class: no relation, new model class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
The three methods , named Reconstructing DDT and LAT According to Weight , Executing <m> Linear Layer Operations </m> in Minimal Cost and Merging Two 4-bit S-boxes into One 8-bit S-box respectively , can efficiently speed up the search process by reducing the search space as much as possible and reducing the cost of executing linear layer operations .


class: first -> second, base class: no relation, new model class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
The parameters of the <m> linear neural layer </m> are determined by solving a constrained optimization problem in a regularized framework .


class: first -> second, base class: no relation, new model class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
This method is fundamentally different from existing methods because we use the domain features to calculate a <m> linear combination of linear layers </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
<m> Linear layer ( design ) model </m> gave best outcomes ( MSE : 0.000293366 , RMSE : 0.017127919 , R : 0.996479613 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
The process model described is a <m> linear layered model </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
Among them , the chaotic neuron layer realizes data diffusion , the <m> linear neuron layer </m> realizes data confusion , and the two layers are repeated for several times to strengthen the cipher .


class: first -> second, base class: no relation, new model class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .


class: first -> second, base class: no relation, new model class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
However , being a <m> linear model </m> , LR can only ascribe a uniform policy - in this case , “ treat everyone ” .


class: first -> second, base class: no relation, new model class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
Finally , we pass this fixed vector through a <m> linear layer </m> to obtain the prediction whether the question is answerable .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
The adaptive Parallel Subgradient Projection ( PSP ) technique improves the convergence speed , in noisy environment , of <m> linear-projection-based algorithms </m> ( e.g. , NLMS and APA ) , with low computational complexity .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
We can build a discrete model of the system by using a <m> linear projection operator </m> to approximate the state space projection .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
For preserving the low rank properties the same , we proposed an algorithm , called <m> linear preserving projection </m> based on low rank representations(LLRLPP),to reduce the dimension of data .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
Principal component analysis(PCA ) and Singular value decomposition(SVD ) are two algebra methods used in <m> linear projection analysis </m> to reduce the dimensions in algebra space .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
In this system , sensors take samples while compressing the signal with <m> linear projection operations </m> using the idea of compressive sensing ( CS ) [ 1 ] .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
The monocular vision-based spacecraft relative navigation method is researched , and non-linear equations are established representing relationship between image characteristics and navigation information , based on <m> linear projection principle </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
Specifically , the authors first present a <m> linear projection view </m> to formulate subspace learning and then develop a novel framework , called Latent Subspace Projection Pursuit ( LSPP ) , to estimate the intrinsic dimension , removing corruptions and recovering the subspace structure for observed datasets .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
This model first applies a <m> linear projection </m> to each tuple ( ) and then takes as input the set of projected tuples .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
We use two additional <m> linear layers </m> and principal component analysis to reduce the dimension of both inputs and internal representations and to transmit the essential information .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
The three methods , named Reconstructing DDT and LAT According to Weight , Executing <m> Linear Layer Operations </m> in Minimal Cost and Merging Two 4-bit S-boxes into One 8-bit S-box respectively , can efficiently speed up the search process by reducing the search space as much as possible and reducing the cost of executing linear layer operations .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
The parameters of the <m> linear neural layer </m> are determined by solving a constrained optimization problem in a regularized framework .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
This method is fundamentally different from existing methods because we use the domain features to calculate a <m> linear combination of linear layers </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
<m> Linear layer ( design ) model </m> gave best outcomes ( MSE : 0.000293366 , RMSE : 0.017127919 , R : 0.996479613 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
The process model described is a <m> linear layered model </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
Among them , the chaotic neuron layer realizes data diffusion , the <m> linear neuron layer </m> realizes data confusion , and the two layers are repeated for several times to strengthen the cipher .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
However , being a <m> linear model </m> , LR can only ascribe a uniform policy - in this case , “ treat everyone ” .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
Finally , we pass this fixed vector through a <m> linear layer </m> to obtain the prediction whether the question is answerable .


class: first -> second, base class: no relation, new model class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
The adaptive Parallel Subgradient Projection ( PSP ) technique improves the convergence speed , in noisy environment , of <m> linear-projection-based algorithms </m> ( e.g. , NLMS and APA ) , with low computational complexity .


class: first -> second, base class: no relation, new model class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
We can build a discrete model of the system by using a <m> linear projection operator </m> to approximate the state space projection .


class: first -> second, base class: no relation, new model class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
For preserving the low rank properties the same , we proposed an algorithm , called <m> linear preserving projection </m> based on low rank representations(LLRLPP),to reduce the dimension of data .


class: first -> second, base class: no relation, new model class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
Principal component analysis(PCA ) and Singular value decomposition(SVD ) are two algebra methods used in <m> linear projection analysis </m> to reduce the dimensions in algebra space .


class: first -> second, base class: no relation, new model class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
In this system , sensors take samples while compressing the signal with <m> linear projection operations </m> using the idea of compressive sensing ( CS ) [ 1 ] .


class: first -> second, base class: no relation, new model class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
The monocular vision-based spacecraft relative navigation method is researched , and non-linear equations are established representing relationship between image characteristics and navigation information , based on <m> linear projection principle </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
Specifically , the authors first present a <m> linear projection view </m> to formulate subspace learning and then develop a novel framework , called Latent Subspace Projection Pursuit ( LSPP ) , to estimate the intrinsic dimension , removing corruptions and recovering the subspace structure for observed datasets .


class: first -> second, base class: no relation, new model class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
This model first applies a <m> linear projection </m> to each tuple ( ) and then takes as input the set of projected tuples .


class: first -> second, base class: no relation, new model class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
We use two additional <m> linear layers </m> and principal component analysis to reduce the dimension of both inputs and internal representations and to transmit the essential information .


class: first -> second, base class: no relation, new model class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
The three methods , named Reconstructing DDT and LAT According to Weight , Executing <m> Linear Layer Operations </m> in Minimal Cost and Merging Two 4-bit S-boxes into One 8-bit S-box respectively , can efficiently speed up the search process by reducing the search space as much as possible and reducing the cost of executing linear layer operations .


class: first -> second, base class: no relation, new model class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
The parameters of the <m> linear neural layer </m> are determined by solving a constrained optimization problem in a regularized framework .


class: first -> second, base class: no relation, new model class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
This method is fundamentally different from existing methods because we use the domain features to calculate a <m> linear combination of linear layers </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
<m> Linear layer ( design ) model </m> gave best outcomes ( MSE : 0.000293366 , RMSE : 0.017127919 , R : 0.996479613 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
The process model described is a <m> linear layered model </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
Among them , the chaotic neuron layer realizes data diffusion , the <m> linear neuron layer </m> realizes data confusion , and the two layers are repeated for several times to strengthen the cipher .


class: first -> second, base class: no relation, new model class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .


class: first -> second, base class: no relation, new model class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
However , being a <m> linear model </m> , LR can only ascribe a uniform policy - in this case , “ treat everyone ” .


class: first -> second, base class: no relation, new model class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
Finally , we pass this fixed vector through a <m> linear layer </m> to obtain the prediction whether the question is answerable .


class: first -> second, base class: no relation, new model class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
The adaptive Parallel Subgradient Projection ( PSP ) technique improves the convergence speed , in noisy environment , of <m> linear-projection-based algorithms </m> ( e.g. , NLMS and APA ) , with low computational complexity .


class: first -> second, base class: no relation, new model class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
We can build a discrete model of the system by using a <m> linear projection operator </m> to approximate the state space projection .


class: first -> second, base class: no relation, new model class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
For preserving the low rank properties the same , we proposed an algorithm , called <m> linear preserving projection </m> based on low rank representations(LLRLPP),to reduce the dimension of data .


class: first -> second, base class: no relation, new model class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
Principal component analysis(PCA ) and Singular value decomposition(SVD ) are two algebra methods used in <m> linear projection analysis </m> to reduce the dimensions in algebra space .


class: first -> second, base class: no relation, new model class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
In this system , sensors take samples while compressing the signal with <m> linear projection operations </m> using the idea of compressive sensing ( CS ) [ 1 ] .


class: first -> second, base class: no relation, new model class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
The monocular vision-based spacecraft relative navigation method is researched , and non-linear equations are established representing relationship between image characteristics and navigation information , based on <m> linear projection principle </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
Specifically , the authors first present a <m> linear projection view </m> to formulate subspace learning and then develop a novel framework , called Latent Subspace Projection Pursuit ( LSPP ) , to estimate the intrinsic dimension , removing corruptions and recovering the subspace structure for observed datasets .


class: first -> second, base class: no relation, new model class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
This model first applies a <m> linear projection </m> to each tuple ( ) and then takes as input the set of projected tuples .


class: first -> second, base class: no relation, new model class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
We use two additional <m> linear layers </m> and principal component analysis to reduce the dimension of both inputs and internal representations and to transmit the essential information .


class: first -> second, base class: no relation, new model class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
The three methods , named Reconstructing DDT and LAT According to Weight , Executing <m> Linear Layer Operations </m> in Minimal Cost and Merging Two 4-bit S-boxes into One 8-bit S-box respectively , can efficiently speed up the search process by reducing the search space as much as possible and reducing the cost of executing linear layer operations .


class: first -> second, base class: no relation, new model class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
The parameters of the <m> linear neural layer </m> are determined by solving a constrained optimization problem in a regularized framework .


class: first -> second, base class: no relation, new model class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
This method is fundamentally different from existing methods because we use the domain features to calculate a <m> linear combination of linear layers </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
<m> Linear layer ( design ) model </m> gave best outcomes ( MSE : 0.000293366 , RMSE : 0.017127919 , R : 0.996479613 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
The process model described is a <m> linear layered model </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
Among them , the chaotic neuron layer realizes data diffusion , the <m> linear neuron layer </m> realizes data confusion , and the two layers are repeated for several times to strengthen the cipher .


class: first -> second, base class: no relation, new model class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .


class: first -> second, base class: no relation, new model class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
However , being a <m> linear model </m> , LR can only ascribe a uniform policy - in this case , “ treat everyone ” .


class: first -> second, base class: no relation, new model class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
Finally , we pass this fixed vector through a <m> linear layer </m> to obtain the prediction whether the question is answerable .


class: first -> second, base class: no relation, new model class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
The adaptive Parallel Subgradient Projection ( PSP ) technique improves the convergence speed , in noisy environment , of <m> linear-projection-based algorithms </m> ( e.g. , NLMS and APA ) , with low computational complexity .


class: first -> second, base class: no relation, new model class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
We can build a discrete model of the system by using a <m> linear projection operator </m> to approximate the state space projection .


class: first -> second, base class: no relation, new model class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
For preserving the low rank properties the same , we proposed an algorithm , called <m> linear preserving projection </m> based on low rank representations(LLRLPP),to reduce the dimension of data .


class: first -> second, base class: no relation, new model class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
Principal component analysis(PCA ) and Singular value decomposition(SVD ) are two algebra methods used in <m> linear projection analysis </m> to reduce the dimensions in algebra space .


class: first -> second, base class: no relation, new model class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
In this system , sensors take samples while compressing the signal with <m> linear projection operations </m> using the idea of compressive sensing ( CS ) [ 1 ] .


class: first -> second, base class: no relation, new model class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
The monocular vision-based spacecraft relative navigation method is researched , and non-linear equations are established representing relationship between image characteristics and navigation information , based on <m> linear projection principle </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
Specifically , the authors first present a <m> linear projection view </m> to formulate subspace learning and then develop a novel framework , called Latent Subspace Projection Pursuit ( LSPP ) , to estimate the intrinsic dimension , removing corruptions and recovering the subspace structure for observed datasets .


class: first -> second, base class: no relation, new model class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
This model first applies a <m> linear projection </m> to each tuple ( ) and then takes as input the set of projected tuples .


class: first -> second, base class: no relation, new model class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
We use two additional <m> linear layers </m> and principal component analysis to reduce the dimension of both inputs and internal representations and to transmit the essential information .


class: first -> second, base class: no relation, new model class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
The three methods , named Reconstructing DDT and LAT According to Weight , Executing <m> Linear Layer Operations </m> in Minimal Cost and Merging Two 4-bit S-boxes into One 8-bit S-box respectively , can efficiently speed up the search process by reducing the search space as much as possible and reducing the cost of executing linear layer operations .


class: first -> second, base class: no relation, new model class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
The parameters of the <m> linear neural layer </m> are determined by solving a constrained optimization problem in a regularized framework .


class: first -> second, base class: no relation, new model class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
This method is fundamentally different from existing methods because we use the domain features to calculate a <m> linear combination of linear layers </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
<m> Linear layer ( design ) model </m> gave best outcomes ( MSE : 0.000293366 , RMSE : 0.017127919 , R : 0.996479613 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
The process model described is a <m> linear layered model </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
Among them , the chaotic neuron layer realizes data diffusion , the <m> linear neuron layer </m> realizes data confusion , and the two layers are repeated for several times to strengthen the cipher .


class: first -> second, base class: no relation, new model class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .


class: first -> second, base class: no relation, new model class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
However , being a <m> linear model </m> , LR can only ascribe a uniform policy - in this case , “ treat everyone ” .


class: first -> second, base class: no relation, new model class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
Finally , we pass this fixed vector through a <m> linear layer </m> to obtain the prediction whether the question is answerable .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
The adaptive Parallel Subgradient Projection ( PSP ) technique improves the convergence speed , in noisy environment , of <m> linear-projection-based algorithms </m> ( e.g. , NLMS and APA ) , with low computational complexity .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
We can build a discrete model of the system by using a <m> linear projection operator </m> to approximate the state space projection .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
For preserving the low rank properties the same , we proposed an algorithm , called <m> linear preserving projection </m> based on low rank representations(LLRLPP),to reduce the dimension of data .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
Principal component analysis(PCA ) and Singular value decomposition(SVD ) are two algebra methods used in <m> linear projection analysis </m> to reduce the dimensions in algebra space .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
In this system , sensors take samples while compressing the signal with <m> linear projection operations </m> using the idea of compressive sensing ( CS ) [ 1 ] .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
The monocular vision-based spacecraft relative navigation method is researched , and non-linear equations are established representing relationship between image characteristics and navigation information , based on <m> linear projection principle </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
Specifically , the authors first present a <m> linear projection view </m> to formulate subspace learning and then develop a novel framework , called Latent Subspace Projection Pursuit ( LSPP ) , to estimate the intrinsic dimension , removing corruptions and recovering the subspace structure for observed datasets .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
This model first applies a <m> linear projection </m> to each tuple ( ) and then takes as input the set of projected tuples .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
We use two additional <m> linear layers </m> and principal component analysis to reduce the dimension of both inputs and internal representations and to transmit the essential information .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
The three methods , named Reconstructing DDT and LAT According to Weight , Executing <m> Linear Layer Operations </m> in Minimal Cost and Merging Two 4-bit S-boxes into One 8-bit S-box respectively , can efficiently speed up the search process by reducing the search space as much as possible and reducing the cost of executing linear layer operations .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
The parameters of the <m> linear neural layer </m> are determined by solving a constrained optimization problem in a regularized framework .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
This method is fundamentally different from existing methods because we use the domain features to calculate a <m> linear combination of linear layers </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
<m> Linear layer ( design ) model </m> gave best outcomes ( MSE : 0.000293366 , RMSE : 0.017127919 , R : 0.996479613 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
The process model described is a <m> linear layered model </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
Among them , the chaotic neuron layer realizes data diffusion , the <m> linear neuron layer </m> realizes data confusion , and the two layers are repeated for several times to strengthen the cipher .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
However , being a <m> linear model </m> , LR can only ascribe a uniform policy - in this case , “ treat everyone ” .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
Finally , we pass this fixed vector through a <m> linear layer </m> to obtain the prediction whether the question is answerable .


class: first -> second, base class: no relation, new model class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
The adaptive Parallel Subgradient Projection ( PSP ) technique improves the convergence speed , in noisy environment , of <m> linear-projection-based algorithms </m> ( e.g. , NLMS and APA ) , with low computational complexity .


class: first -> second, base class: no relation, new model class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
We can build a discrete model of the system by using a <m> linear projection operator </m> to approximate the state space projection .


class: first -> second, base class: no relation, new model class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
For preserving the low rank properties the same , we proposed an algorithm , called <m> linear preserving projection </m> based on low rank representations(LLRLPP),to reduce the dimension of data .


class: first -> second, base class: no relation, new model class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
Principal component analysis(PCA ) and Singular value decomposition(SVD ) are two algebra methods used in <m> linear projection analysis </m> to reduce the dimensions in algebra space .


class: first -> second, base class: no relation, new model class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
In this system , sensors take samples while compressing the signal with <m> linear projection operations </m> using the idea of compressive sensing ( CS ) [ 1 ] .


class: first -> second, base class: no relation, new model class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
The monocular vision-based spacecraft relative navigation method is researched , and non-linear equations are established representing relationship between image characteristics and navigation information , based on <m> linear projection principle </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
Specifically , the authors first present a <m> linear projection view </m> to formulate subspace learning and then develop a novel framework , called Latent Subspace Projection Pursuit ( LSPP ) , to estimate the intrinsic dimension , removing corruptions and recovering the subspace structure for observed datasets .


class: first -> second, base class: no relation, new model class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
This model first applies a <m> linear projection </m> to each tuple ( ) and then takes as input the set of projected tuples .


class: first -> second, base class: no relation, new model class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
We use two additional <m> linear layers </m> and principal component analysis to reduce the dimension of both inputs and internal representations and to transmit the essential information .


class: first -> second, base class: no relation, new model class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
The three methods , named Reconstructing DDT and LAT According to Weight , Executing <m> Linear Layer Operations </m> in Minimal Cost and Merging Two 4-bit S-boxes into One 8-bit S-box respectively , can efficiently speed up the search process by reducing the search space as much as possible and reducing the cost of executing linear layer operations .


class: first -> second, base class: no relation, new model class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
The parameters of the <m> linear neural layer </m> are determined by solving a constrained optimization problem in a regularized framework .


class: first -> second, base class: no relation, new model class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
This method is fundamentally different from existing methods because we use the domain features to calculate a <m> linear combination of linear layers </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
<m> Linear layer ( design ) model </m> gave best outcomes ( MSE : 0.000293366 , RMSE : 0.017127919 , R : 0.996479613 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
The process model described is a <m> linear layered model </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
Among them , the chaotic neuron layer realizes data diffusion , the <m> linear neuron layer </m> realizes data confusion , and the two layers are repeated for several times to strengthen the cipher .


class: first -> second, base class: no relation, new model class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .


class: first -> second, base class: no relation, new model class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
However , being a <m> linear model </m> , LR can only ascribe a uniform policy - in this case , “ treat everyone ” .


class: first -> second, base class: no relation, new model class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
Finally , we pass this fixed vector through a <m> linear layer </m> to obtain the prediction whether the question is answerable .


class: first -> second, base class: no relation, new model class: no relation
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
The adaptive Parallel Subgradient Projection ( PSP ) technique improves the convergence speed , in noisy environment , of <m> linear-projection-based algorithms </m> ( e.g. , NLMS and APA ) , with low computational complexity .


class: first -> second, base class: no relation, new model class: no relation
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
We can build a discrete model of the system by using a <m> linear projection operator </m> to approximate the state space projection .


class: first -> second, base class: no relation, new model class: no relation
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
For preserving the low rank properties the same , we proposed an algorithm , called <m> linear preserving projection </m> based on low rank representations(LLRLPP),to reduce the dimension of data .


class: first -> second, base class: no relation, new model class: no relation
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
Principal component analysis(PCA ) and Singular value decomposition(SVD ) are two algebra methods used in <m> linear projection analysis </m> to reduce the dimensions in algebra space .


class: first -> second, base class: no relation, new model class: no relation
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
In this system , sensors take samples while compressing the signal with <m> linear projection operations </m> using the idea of compressive sensing ( CS ) [ 1 ] .


class: first -> second, base class: no relation, new model class: no relation
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
The monocular vision-based spacecraft relative navigation method is researched , and non-linear equations are established representing relationship between image characteristics and navigation information , based on <m> linear projection principle </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
Specifically , the authors first present a <m> linear projection view </m> to formulate subspace learning and then develop a novel framework , called Latent Subspace Projection Pursuit ( LSPP ) , to estimate the intrinsic dimension , removing corruptions and recovering the subspace structure for observed datasets .


class: first -> second, base class: no relation, new model class: no relation
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
This model first applies a <m> linear projection </m> to each tuple ( ) and then takes as input the set of projected tuples .


class: first -> second, base class: same cluster, new model class: no relation
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
We use two additional <m> linear layers </m> and principal component analysis to reduce the dimension of both inputs and internal representations and to transmit the essential information .


class: first -> second, base class: same cluster, new model class: no relation
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
The three methods , named Reconstructing DDT and LAT According to Weight , Executing <m> Linear Layer Operations </m> in Minimal Cost and Merging Two 4-bit S-boxes into One 8-bit S-box respectively , can efficiently speed up the search process by reducing the search space as much as possible and reducing the cost of executing linear layer operations .


class: first -> second, base class: same cluster, new model class: no relation
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
This method is fundamentally different from existing methods because we use the domain features to calculate a <m> linear combination of linear layers </m> .


class: first -> second, base class: same cluster, new model class: no relation
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
<m> Linear layer ( design ) model </m> gave best outcomes ( MSE : 0.000293366 , RMSE : 0.017127919 , R : 0.996479613 ) .


class: first -> second, base class: same cluster, new model class: no relation
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
The process model described is a <m> linear layered model </m> .


class: first -> second, base class: same cluster, new model class: no relation
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
However , being a <m> linear model </m> , LR can only ascribe a uniform policy - in this case , “ treat everyone ” .


class: first -> second, base class: same cluster, new model class: no relation
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
Finally , we pass this fixed vector through a <m> linear layer </m> to obtain the prediction whether the question is answerable .


class: first -> second, base class: no relation, new model class: no relation
first:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .
second:
The adaptive Parallel Subgradient Projection ( PSP ) technique improves the convergence speed , in noisy environment , of <m> linear-projection-based algorithms </m> ( e.g. , NLMS and APA ) , with low computational complexity .


class: first -> second, base class: no relation, new model class: no relation
first:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .
second:
We can build a discrete model of the system by using a <m> linear projection operator </m> to approximate the state space projection .


class: first -> second, base class: no relation, new model class: no relation
first:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .
second:
For preserving the low rank properties the same , we proposed an algorithm , called <m> linear preserving projection </m> based on low rank representations(LLRLPP),to reduce the dimension of data .


class: first -> second, base class: no relation, new model class: no relation
first:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .
second:
Principal component analysis(PCA ) and Singular value decomposition(SVD ) are two algebra methods used in <m> linear projection analysis </m> to reduce the dimensions in algebra space .


class: first -> second, base class: no relation, new model class: no relation
first:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .
second:
In this system , sensors take samples while compressing the signal with <m> linear projection operations </m> using the idea of compressive sensing ( CS ) [ 1 ] .


class: first -> second, base class: no relation, new model class: no relation
first:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .
second:
The monocular vision-based spacecraft relative navigation method is researched , and non-linear equations are established representing relationship between image characteristics and navigation information , based on <m> linear projection principle </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .
second:
Specifically , the authors first present a <m> linear projection view </m> to formulate subspace learning and then develop a novel framework , called Latent Subspace Projection Pursuit ( LSPP ) , to estimate the intrinsic dimension , removing corruptions and recovering the subspace structure for observed datasets .


class: first -> second, base class: no relation, new model class: no relation
first:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .
second:
This model first applies a <m> linear projection </m> to each tuple ( ) and then takes as input the set of projected tuples .


class: first -> second, base class: no relation, new model class: no relation
first:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .
second:
We use two additional <m> linear layers </m> and principal component analysis to reduce the dimension of both inputs and internal representations and to transmit the essential information .


class: first -> second, base class: no relation, new model class: no relation
first:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .
second:
The three methods , named Reconstructing DDT and LAT According to Weight , Executing <m> Linear Layer Operations </m> in Minimal Cost and Merging Two 4-bit S-boxes into One 8-bit S-box respectively , can efficiently speed up the search process by reducing the search space as much as possible and reducing the cost of executing linear layer operations .


class: first -> second, base class: no relation, new model class: no relation
first:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .
second:
The parameters of the <m> linear neural layer </m> are determined by solving a constrained optimization problem in a regularized framework .


class: first -> second, base class: no relation, new model class: no relation
first:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .
second:
This method is fundamentally different from existing methods because we use the domain features to calculate a <m> linear combination of linear layers </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .
second:
<m> Linear layer ( design ) model </m> gave best outcomes ( MSE : 0.000293366 , RMSE : 0.017127919 , R : 0.996479613 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .
second:
The process model described is a <m> linear layered model </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .
second:
Among them , the chaotic neuron layer realizes data diffusion , the <m> linear neuron layer </m> realizes data confusion , and the two layers are repeated for several times to strengthen the cipher .


class: first -> second, base class: no relation, new model class: no relation
first:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .
second:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .


class: first -> second, base class: no relation, new model class: no relation
first:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .
second:
However , being a <m> linear model </m> , LR can only ascribe a uniform policy - in this case , “ treat everyone ” .


class: first -> second, base class: no relation, new model class: no relation
first:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .
second:
Finally , we pass this fixed vector through a <m> linear layer </m> to obtain the prediction whether the question is answerable .


class: first -> second, base class: no relation, new model class: no relation
first:
We take the final hidden state as the segment embedding vector , then can be represented as where corresponds to one layer or multiple layers of <m> linear or non - linear transformation </m> .
second:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Two assumptions are made : 1 ) In a new pair of images the deformation is approximately the same size and has only been spatially relocated in the image , and that by a simple <m> affine transformation </m> one can identify the optimal configuration on this new pair of images ; and 2 ) The deformation is of similar size and shape on the original pair of images .
second:
In this paper , we investigate the effects of three DA approaches ; <m> affine transformation-based approach </m> , synthetic image-based approach , and generative adversarial network (GAN)-based approach , and evaluate the effectiveness of DA for pupil center point detection task using an off-the-shelf CNN model .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Two assumptions are made : 1 ) In a new pair of images the deformation is approximately the same size and has only been spatially relocated in the image , and that by a simple <m> affine transformation </m> one can identify the optimal configuration on this new pair of images ; and 2 ) The deformation is of similar size and shape on the original pair of images .
second:
It creates Shearlet functions , which have different characteristics through zooming , shearing translating and other <m> affine transforming methods </m> and enables its capable of optimally sparse representation .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Two assumptions are made : 1 ) In a new pair of images the deformation is approximately the same size and has only been spatially relocated in the image , and that by a simple <m> affine transformation </m> one can identify the optimal configuration on this new pair of images ; and 2 ) The deformation is of similar size and shape on the original pair of images .
second:
First , we can see that LSTM - based leaf transformation has a clear advantage over the <m> affine - transformation - based one </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Sum of Square Difference ( SSD ) and Bilinear interpolation models were used to establish the similarity measure between the images to be registered , resampling of the pixel-values and computation of non-integer coordinates respectively while Random Sampling Consensus ( RANSAC ) algorithm was used to exclude the outliers and to compute the transformation matrix using <m> affine transformation function </m> .
second:
In this paper , we investigate the effects of three DA approaches ; <m> affine transformation-based approach </m> , synthetic image-based approach , and generative adversarial network (GAN)-based approach , and evaluate the effectiveness of DA for pupil center point detection task using an off-the-shelf CNN model .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Sum of Square Difference ( SSD ) and Bilinear interpolation models were used to establish the similarity measure between the images to be registered , resampling of the pixel-values and computation of non-integer coordinates respectively while Random Sampling Consensus ( RANSAC ) algorithm was used to exclude the outliers and to compute the transformation matrix using <m> affine transformation function </m> .
second:
It creates Shearlet functions , which have different characteristics through zooming , shearing translating and other <m> affine transforming methods </m> and enables its capable of optimally sparse representation .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Sum of Square Difference ( SSD ) and Bilinear interpolation models were used to establish the similarity measure between the images to be registered , resampling of the pixel-values and computation of non-integer coordinates respectively while Random Sampling Consensus ( RANSAC ) algorithm was used to exclude the outliers and to compute the transformation matrix using <m> affine transformation function </m> .
second:
First , we can see that LSTM - based leaf transformation has a clear advantage over the <m> affine - transformation - based one </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In this paper , we investigate the effects of three DA approaches ; <m> affine transformation-based approach </m> , synthetic image-based approach , and generative adversarial network (GAN)-based approach , and evaluate the effectiveness of DA for pupil center point detection task using an off-the-shelf CNN model .
second:
While <m> affine image transformation </m> in conventional image processing is a relatively simple task , learning these transformations is an important part of having neural networks ( NNs ) function as generalized image processors .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In this paper , we investigate the effects of three DA approaches ; <m> affine transformation-based approach </m> , synthetic image-based approach , and generative adversarial network (GAN)-based approach , and evaluate the effectiveness of DA for pupil center point detection task using an off-the-shelf CNN model .
second:
This visualisation , combined with an <m> affine transformation process </m> provided a simple , cost-effective way to accurately co-register photographs and MR images of subcutaneous hematomas located on the thigh .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In this paper , we investigate the effects of three DA approaches ; <m> affine transformation-based approach </m> , synthetic image-based approach , and generative adversarial network (GAN)-based approach , and evaluate the effectiveness of DA for pupil center point detection task using an off-the-shelf CNN model .
second:
We have also experimented with more sophisticated data augmentation techniques , including random histogram matching , <m> affine image transforms </m> , and random image filtering , which did not demonstrate any additional improvements .


class: first -> second, base class: same cluster, new model class: same cluster
first:
After that , the pattern elements are extracted with the <m> affine transformation theory </m> and bilinear interpolation algorithm .
second:
It creates Shearlet functions , which have different characteristics through zooming , shearing translating and other <m> affine transforming methods </m> and enables its capable of optimally sparse representation .


class: first -> second, base class: same cluster, new model class: same cluster
first:
After that , the pattern elements are extracted with the <m> affine transformation theory </m> and bilinear interpolation algorithm .
second:
First , we can see that LSTM - based leaf transformation has a clear advantage over the <m> affine - transformation - based one </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The content includes : dividing an image frame including a target object , determining a target object region , using a feature information classifier to scan the determined target object region , determining central point position information of a region that feature information to be positioned occupies in the target object region , performing <m> affine transformation operation </m> on the determined central point position information , and obtaining initial position information corresponding to the region that the feature information to be positioned occupies in the target object region , thereby improving precision of initial positioning ; performing iteration processing on the obtained initial position information , obtaining position information corresponding to the region that the feature information to be positioned occupies in the target object region , integrating the obtained position information corresponding to the region that the feature information to be positioned occupies in the target object region , and obtaining the feature information of the target object , thereby improving precision of positioning the feature information of the target object .
second:
It creates Shearlet functions , which have different characteristics through zooming , shearing translating and other <m> affine transforming methods </m> and enables its capable of optimally sparse representation .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The content includes : dividing an image frame including a target object , determining a target object region , using a feature information classifier to scan the determined target object region , determining central point position information of a region that feature information to be positioned occupies in the target object region , performing <m> affine transformation operation </m> on the determined central point position information , and obtaining initial position information corresponding to the region that the feature information to be positioned occupies in the target object region , thereby improving precision of initial positioning ; performing iteration processing on the obtained initial position information , obtaining position information corresponding to the region that the feature information to be positioned occupies in the target object region , integrating the obtained position information corresponding to the region that the feature information to be positioned occupies in the target object region , and obtaining the feature information of the target object , thereby improving precision of positioning the feature information of the target object .
second:
First , we can see that LSTM - based leaf transformation has a clear advantage over the <m> affine - transformation - based one </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
It creates Shearlet functions , which have different characteristics through zooming , shearing translating and other <m> affine transforming methods </m> and enables its capable of optimally sparse representation .
second:
We have also experimented with more sophisticated data augmentation techniques , including random histogram matching , <m> affine image transforms </m> , and random image filtering , which did not demonstrate any additional improvements .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Specific topics include : Loading images from JAR files Describing shapes and the affine transformation in Java 2D Designing a class library of shapes Designing a component class for interactive drawing Designing a component class for displaying images Printing in Java 2D Implementing pan and zoom Implementing interactive rotate , shear , and flip Manipulating images at the pixel level using the Java 2D API Working with tiles in JAI to render large images Image manipulation in JAI <m> usingaffine transformation </m> , projections , and warping Image analysis in JAI , including edge detection , statistics , and region of interest computation ( ROI ) Remote imaging using RMI and JAI Internet imaging , including the use of Java 2D and JAI with servlets and JSP , the design of Web-based imaging applications , and the Internet Imaging Protocol ( IIP ) Using the new Image I/O framework to read and write images " Putting It All Together " sections help you put vital concepts and techniques into practice with interactive examples using actual applications .
second:
First , we can see that LSTM - based leaf transformation has a clear advantage over the <m> affine - transformation - based one </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Since the textual and visual vectors have different dimensionality and belong to different spaces , we first map them to a mutual space using an <m> affine transformation </m> .
second:
First , we can see that LSTM - based leaf transformation has a clear advantage over the <m> affine - transformation - based one </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Furthermore , the learned <m> affine transform </m> applied to these normalized activations allows the BN transform to represent the identity transformation and preserves the network capacity .
second:
First , we can see that LSTM - based leaf transformation has a clear advantage over the <m> affine - transformation - based one </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The question nodes and edges use vector embeddings implemented as look - up tables , and the scene nodes and edges use <m> affine projections </m> : with the word embedding ( usually pretrained , see supplementary material ) , the embedding of dependencies , and weight matrices , and and biases .
second:
First , we can see that LSTM - based leaf transformation has a clear advantage over the <m> affine - transformation - based one </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .
second:
The adaptive Parallel Subgradient Projection ( PSP ) technique improves the convergence speed , in noisy environment , of <m> linear-projection-based algorithms </m> ( e.g. , NLMS and APA ) , with low computational complexity .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .
second:
We can build a discrete model of the system by using a <m> linear projection operator </m> to approximate the state space projection .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .
second:
Principal component analysis(PCA ) and Singular value decomposition(SVD ) are two algebra methods used in <m> linear projection analysis </m> to reduce the dimensions in algebra space .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .
second:
In this system , sensors take samples while compressing the signal with <m> linear projection operations </m> using the idea of compressive sensing ( CS ) [ 1 ] .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .
second:
The monocular vision-based spacecraft relative navigation method is researched , and non-linear equations are established representing relationship between image characteristics and navigation information , based on <m> linear projection principle </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .
second:
Specifically , the authors first present a <m> linear projection view </m> to formulate subspace learning and then develop a novel framework , called Latent Subspace Projection Pursuit ( LSPP ) , to estimate the intrinsic dimension , removing corruptions and recovering the subspace structure for observed datasets .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .
second:
This model first applies a <m> linear projection </m> to each tuple ( ) and then takes as input the set of projected tuples .


class: first -> second, base class: no relation, new model class: no relation
first:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .
second:
We use two additional <m> linear layers </m> and principal component analysis to reduce the dimension of both inputs and internal representations and to transmit the essential information .


class: first -> second, base class: no relation, new model class: no relation
first:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .
second:
The three methods , named Reconstructing DDT and LAT According to Weight , Executing <m> Linear Layer Operations </m> in Minimal Cost and Merging Two 4-bit S-boxes into One 8-bit S-box respectively , can efficiently speed up the search process by reducing the search space as much as possible and reducing the cost of executing linear layer operations .


class: first -> second, base class: no relation, new model class: no relation
first:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .
second:
The parameters of the <m> linear neural layer </m> are determined by solving a constrained optimization problem in a regularized framework .


class: first -> second, base class: no relation, new model class: no relation
first:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .
second:
This method is fundamentally different from existing methods because we use the domain features to calculate a <m> linear combination of linear layers </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .
second:
<m> Linear layer ( design ) model </m> gave best outcomes ( MSE : 0.000293366 , RMSE : 0.017127919 , R : 0.996479613 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .
second:
The process model described is a <m> linear layered model </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .
second:
Among them , the chaotic neuron layer realizes data diffusion , the <m> linear neuron layer </m> realizes data confusion , and the two layers are repeated for several times to strengthen the cipher .


class: first -> second, base class: no relation, new model class: no relation
first:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .
second:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .


class: first -> second, base class: no relation, new model class: no relation
first:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .
second:
However , being a <m> linear model </m> , LR can only ascribe a uniform policy - in this case , “ treat everyone ” .


class: first -> second, base class: no relation, new model class: no relation
first:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .
second:
Finally , we pass this fixed vector through a <m> linear layer </m> to obtain the prediction whether the question is answerable .


class: first -> second, base class: no relation, new model class: same cluster
first:
The parameters of the <m> linear neural layer </m> are determined by solving a constrained optimization problem in a regularized framework .
second:
Finally , a <m> linear fully connected layer </m> is used to evaluate the saliency of a queried region .


class: first -> second, base class: no relation, new model class: same cluster
first:
The parameters of the <m> linear neural layer </m> are determined by solving a constrained optimization problem in a regularized framework .
second:
Finally , a <m> fully - connected linear layer </m> projects to the output of the network , i.e. , the Q - values .


class: first -> second, base class: no relation, new model class: no relation
first:
Among them , the chaotic neuron layer realizes data diffusion , the <m> linear neuron layer </m> realizes data confusion , and the two layers are repeated for several times to strengthen the cipher .
second:
This amounts to adding a <m> bottleneck linear layer </m> , and brings the CNN Softmax much closer to our best result , as can be seen in Table [ reference ] , where adding a 128 - dim correction halves the gap between regular and the CNN Softmax .


class: first -> second, base class: no relation, new model class: no relation
first:
Among them , the chaotic neuron layer realizes data diffusion , the <m> linear neuron layer </m> realizes data confusion , and the two layers are repeated for several times to strengthen the cipher .
second:
Finally , a <m> fully - connected linear layer </m> projects to the output of the network , i.e. , the Q - values .


class: first -> second, base class: no relation, new model class: no relation
first:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .
second:
Finally , a <m> fully - connected linear layer </m> projects to the output of the network , i.e. , the Q - values .


class: first -> second, base class: no relation, new model class: no relation
first:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .
second:
subsection : <m> Layer - Wise Linear Model </m> A neural network model based on graph convolutions can therefore be built by stacking multiple convolutional layers of the form of Eq .


class: first -> second, base class: no relation, new model class: no relation
first:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .
second:
Considering that a single <m> linear Weight Layer </m> might cause excessive response on some specific dimensions of the part vector , we add a nonlinear function to equalize the response of part feature vector , and the fused feature representation is where the and the are the global and part feature vectors .


class: first -> second, base class: no relation, new model class: no relation
first:
However , being a <m> linear model </m> , LR can only ascribe a uniform policy - in this case , “ treat everyone ” .
second:
This amounts to adding a <m> bottleneck linear layer </m> , and brings the CNN Softmax much closer to our best result , as can be seen in Table [ reference ] , where adding a 128 - dim correction halves the gap between regular and the CNN Softmax .


class: first -> second, base class: no relation, new model class: no relation
first:
However , being a <m> linear model </m> , LR can only ascribe a uniform policy - in this case , “ treat everyone ” .
second:
subsection : <m> Layer - Wise Linear Model </m> A neural network model based on graph convolutions can therefore be built by stacking multiple convolutional layers of the form of Eq .


class: first -> second, base class: no relation, new model class: no relation
first:
However , being a <m> linear model </m> , LR can only ascribe a uniform policy - in this case , “ treat everyone ” .
second:
Considering that a single <m> linear Weight Layer </m> might cause excessive response on some specific dimensions of the part vector , we add a nonlinear function to equalize the response of part feature vector , and the fused feature representation is where the and the are the global and part feature vectors .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : <m> Layer - Wise Linear Model </m> A neural network model based on graph convolutions can therefore be built by stacking multiple convolutional layers of the form of Eq .
second:
Rainbow uses <m> noisy linear layers </m> and ReLU activations throughout the network , whereas Reactor uses standard linear layers and concatenated ReLU activations throughout .


class: first -> second, base class: no relation, new model class: no relation
first:
Considering that a single <m> linear Weight Layer </m> might cause excessive response on some specific dimensions of the part vector , we add a nonlinear function to equalize the response of part feature vector , and the fused feature representation is where the and the are the global and part feature vectors .
second:
Rainbow uses <m> noisy linear layers </m> and ReLU activations throughout the network , whereas Reactor uses standard linear layers and concatenated ReLU activations throughout .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .


class: first -> second, base class: second -> first, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .


class: first -> second, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
The overall best performance was obtained with a normalized correlation metric coupled with <m> HFUS downsampling </m> and a one-plus-one evolutionary optimizer , yielding a mean registration error of 0.05 mm .


class: first -> second, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
More precisely , the contributions achieved during this thesis are : * A feature-preserving filter for the <m> downsampling of scalar volume data </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
We further evaluate three potential defense methods to mitigate such attacks , including adversarial training , <m> audio downsampling </m> , and moving average filtering , which leads to promising directions for further research .


class: first -> second, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
In an MPEG ( Motion Pictures Experts Group ) video coding system , a <m> color component downsampling </m> from 1050 2:1 60 Hz to 525 1:1 30 Hz line scan formats must be carried out on the U and V color component source signals .


class: first -> second, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .


class: first -> second, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: first -> second, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: first -> second, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: first -> second, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: first -> second, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: first -> second, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
The third one is the <m> volumetric downsampling block </m> that is identical to a volumetric max pooling layer .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
To obtain accurate classification results , this paper proposes the Ensemble-Margin Based Random Forests ( EMRFs ) method , which combines RFs and a new <m> subsampling iterative technique </m> making use of computed ensemble margin values .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
<m> Anti mutual-information subsampling </m> does not require to remove from the brain signals the shared variance between aging and fluid intelligence , and hence does not display this pessimistic behavior .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
A <m> subsampling stochastic gradient descent algorithm </m> has been proposed to accelerate the training of recurrent neural network language models .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
Subsampling in CSXI is accomplished by either <m> view angle spectral subsampling </m> , spatial subsampling enabled by block-unblock coded apertures placed at the source or detector side , or both .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
The spatial frequency decomposition is accomplished by an efficient encoding algorithm incorporating a hierarchical cascading Gaussian pyramid algorithm which is an alternating sequence of image output passing through Nyquist low pass spatial filter and <m> subsampling local operators </m> for image encoding .


class: first -> second, base class: no relation, new model class: second -> first
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .


class: first -> second, base class: no relation, new model class: second -> first
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .


class: first -> second, base class: no relation, new model class: second -> first
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .


class: first -> second, base class: second -> first, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
The overall best performance was obtained with a normalized correlation metric coupled with <m> HFUS downsampling </m> and a one-plus-one evolutionary optimizer , yielding a mean registration error of 0.05 mm .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
More precisely , the contributions achieved during this thesis are : * A feature-preserving filter for the <m> downsampling of scalar volume data </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
We further evaluate three potential defense methods to mitigate such attacks , including adversarial training , <m> audio downsampling </m> , and moving average filtering , which leads to promising directions for further research .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
In an MPEG ( Motion Pictures Experts Group ) video coding system , a <m> color component downsampling </m> from 1050 2:1 60 Hz to 525 1:1 30 Hz line scan formats must be carried out on the U and V color component source signals .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
Note that SNIPER also performs <m> adaptive downsampling </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
The third one is the <m> volumetric downsampling block </m> that is identical to a volumetric max pooling layer .


class: first -> second, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
<m> Anti mutual-information subsampling </m> does not require to remove from the brain signals the shared variance between aging and fluid intelligence , and hence does not display this pessimistic behavior .


class: first -> second, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
A <m> subsampling stochastic gradient descent algorithm </m> has been proposed to accelerate the training of recurrent neural network language models .


class: first -> second, base class: second -> first, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .


class: first -> second, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
Subsampling in CSXI is accomplished by either <m> view angle spectral subsampling </m> , spatial subsampling enabled by block-unblock coded apertures placed at the source or detector side , or both .


class: first -> second, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
The spatial frequency decomposition is accomplished by an efficient encoding algorithm incorporating a hierarchical cascading Gaussian pyramid algorithm which is an alternating sequence of image output passing through Nyquist low pass spatial filter and <m> subsampling local operators </m> for image encoding .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .


class: first -> second, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .


class: first -> second, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .


class: first -> second, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
The overall best performance was obtained with a normalized correlation metric coupled with <m> HFUS downsampling </m> and a one-plus-one evolutionary optimizer , yielding a mean registration error of 0.05 mm .


class: first -> second, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
More precisely , the contributions achieved during this thesis are : * A feature-preserving filter for the <m> downsampling of scalar volume data </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
We further evaluate three potential defense methods to mitigate such attacks , including adversarial training , <m> audio downsampling </m> , and moving average filtering , which leads to promising directions for further research .


class: first -> second, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
In an MPEG ( Motion Pictures Experts Group ) video coding system , a <m> color component downsampling </m> from 1050 2:1 60 Hz to 525 1:1 30 Hz line scan formats must be carried out on the U and V color component source signals .


class: first -> second, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .


class: first -> second, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: first -> second, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: first -> second, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: first -> second, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: first -> second, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: first -> second, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
Note that SNIPER also performs <m> adaptive downsampling </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
The third one is the <m> volumetric downsampling block </m> that is identical to a volumetric max pooling layer .


class: first -> second, base class: no relation, new model class: no relation
first:
A <m> subsampling stochastic gradient descent algorithm </m> has been proposed to accelerate the training of recurrent neural network language models .
second:
We change the ( user , item ) entry subsampling strategy in SGD training in the original paper to the <m> user - level subsampling </m> as we did with Mult - vae pr and Mult - dae .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .


class: first -> second, base class: second -> first, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .


class: first -> second, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
The overall best performance was obtained with a normalized correlation metric coupled with <m> HFUS downsampling </m> and a one-plus-one evolutionary optimizer , yielding a mean registration error of 0.05 mm .


class: first -> second, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
More precisely , the contributions achieved during this thesis are : * A feature-preserving filter for the <m> downsampling of scalar volume data </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
We further evaluate three potential defense methods to mitigate such attacks , including adversarial training , <m> audio downsampling </m> , and moving average filtering , which leads to promising directions for further research .


class: first -> second, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
In an MPEG ( Motion Pictures Experts Group ) video coding system , a <m> color component downsampling </m> from 1050 2:1 60 Hz to 525 1:1 30 Hz line scan formats must be carried out on the U and V color component source signals .


class: first -> second, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .


class: first -> second, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: first -> second, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: first -> second, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: first -> second, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: first -> second, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: first -> second, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
The third one is the <m> volumetric downsampling block </m> that is identical to a volumetric max pooling layer .


class: first -> second, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
Then <m> p - norm subsampling </m> ( or pooling ) with pooling size ( or half - length ) and stride applied to the feature map is a 3 - dimensional array with the following entries : where is the function mapping from positions in to positions in respecting the stride , is the order of the p - norm ( for , it becomes the commonly used max pooling ) .


class: first -> second, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
This paper explores the problem <m> oftask-oriented downsampling </m> , which aims to downsample a pointcloud and maintain the performance of subsequent applications asmuch as possible .


class: first -> second, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: first -> second, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
Then <m> p - norm subsampling </m> ( or pooling ) with pooling size ( or half - length ) and stride applied to the feature map is a 3 - dimensional array with the following entries : where is the function mapping from positions in to positions in respecting the stride , is the order of the p - norm ( for , it becomes the commonly used max pooling ) .


class: first -> second, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
This paper explores the problem <m> oftask-oriented downsampling </m> , which aims to downsample a pointcloud and maintain the performance of subsequent applications asmuch as possible .


class: first -> second, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: first -> second, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
Then <m> p - norm subsampling </m> ( or pooling ) with pooling size ( or half - length ) and stride applied to the feature map is a 3 - dimensional array with the following entries : where is the function mapping from positions in to positions in respecting the stride , is the order of the p - norm ( for , it becomes the commonly used max pooling ) .


class: first -> second, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
This paper explores the problem <m> oftask-oriented downsampling </m> , which aims to downsample a pointcloud and maintain the performance of subsequent applications asmuch as possible .


class: first -> second, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: first -> second, base class: second -> first, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .


class: first -> second, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
The overall best performance was obtained with a normalized correlation metric coupled with <m> HFUS downsampling </m> and a one-plus-one evolutionary optimizer , yielding a mean registration error of 0.05 mm .


class: first -> second, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
More precisely , the contributions achieved during this thesis are : * A feature-preserving filter for the <m> downsampling of scalar volume data </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
We further evaluate three potential defense methods to mitigate such attacks , including adversarial training , <m> audio downsampling </m> , and moving average filtering , which leads to promising directions for further research .


class: first -> second, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
In an MPEG ( Motion Pictures Experts Group ) video coding system , a <m> color component downsampling </m> from 1050 2:1 60 Hz to 525 1:1 30 Hz line scan formats must be carried out on the U and V color component source signals .


class: first -> second, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .


class: first -> second, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: first -> second, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: first -> second, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: first -> second, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: first -> second, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: first -> second, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
The third one is the <m> volumetric downsampling block </m> that is identical to a volumetric max pooling layer .


class: first -> second, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
This paper explores the problem <m> oftask-oriented downsampling </m> , which aims to downsample a pointcloud and maintain the performance of subsequent applications asmuch as possible .


class: first -> second, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: first -> second, base class: no relation, new model class: no relation
first:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .
second:
For that , we propose in this paper a new fusion schema ( coupled multiresolution decomposition model ( CMD ) ) allowing the reconstruction of a high-resolution MS given its approximation and details obtained by <m> MTF-tailored downsampling </m> and wavelet decomposition , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .
second:
Multi-resolution analysis is based on <m> downsampling expansion </m> with maximum energy extraction followed by upsampling reconstruction .


class: first -> second, base class: no relation, new model class: no relation
first:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .
second:
In this contribution , we present and conduct an investigation aimed at locating a robust threshold for <m> downsampling BTF images </m> without loosing perceptual quality .


class: first -> second, base class: no relation, new model class: no relation
first:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .
second:
After segmentation , the HR cell image is downsampled by the <m> bicubic downsampling algorithm </m> to obtain a 20 × 20 low resolution ( LR ) cell image .


class: first -> second, base class: no relation, new model class: same cluster
first:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .
second:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .
second:
In an MPEG ( Motion Pictures Experts Group ) video coding system , a <m> color component downsampling </m> from 1050 2:1 60 Hz to 525 1:1 30 Hz line scan formats must be carried out on the U and V color component source signals .


class: first -> second, base class: same cluster, new model class: same cluster
first:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: first -> second, base class: same cluster, new model class: same cluster
first:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: first -> second, base class: same cluster, new model class: same cluster
first:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: first -> second, base class: no relation, new model class: no relation
first:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .
second:
Note that SNIPER also performs <m> adaptive downsampling </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .
second:
The third one is the <m> volumetric downsampling block </m> that is identical to a volumetric max pooling layer .


class: first -> second, base class: no relation, new model class: same cluster
first:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .
second:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .
second:
In an MPEG ( Motion Pictures Experts Group ) video coding system , a <m> color component downsampling </m> from 1050 2:1 60 Hz to 525 1:1 30 Hz line scan formats must be carried out on the U and V color component source signals .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: first -> second, base class: no relation, new model class: no relation
first:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .
second:
Note that SNIPER also performs <m> adaptive downsampling </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .
second:
The third one is the <m> volumetric downsampling block </m> that is identical to a volumetric max pooling layer .


class: first -> second, base class: no relation, new model class: same cluster
first:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .
second:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .
second:
In an MPEG ( Motion Pictures Experts Group ) video coding system , a <m> color component downsampling </m> from 1050 2:1 60 Hz to 525 1:1 30 Hz line scan formats must be carried out on the U and V color component source signals .


class: first -> second, base class: same cluster, new model class: same cluster
first:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: first -> second, base class: same cluster, new model class: same cluster
first:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: first -> second, base class: same cluster, new model class: same cluster
first:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: first -> second, base class: no relation, new model class: no relation
first:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .
second:
Note that SNIPER also performs <m> adaptive downsampling </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .
second:
The third one is the <m> volumetric downsampling block </m> that is identical to a volumetric max pooling layer .


class: first -> second, base class: no relation, new model class: second -> first
first:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .
second:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .


class: first -> second, base class: no relation, new model class: second -> first
first:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: first -> second, base class: no relation, new model class: no relation
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
The overall best performance was obtained with a normalized correlation metric coupled with <m> HFUS downsampling </m> and a one-plus-one evolutionary optimizer , yielding a mean registration error of 0.05 mm .


class: first -> second, base class: no relation, new model class: no relation
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
More precisely , the contributions achieved during this thesis are : * A feature-preserving filter for the <m> downsampling of scalar volume data </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
We further evaluate three potential defense methods to mitigate such attacks , including adversarial training , <m> audio downsampling </m> , and moving average filtering , which leads to promising directions for further research .


class: first -> second, base class: no relation, new model class: no relation
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
In an MPEG ( Motion Pictures Experts Group ) video coding system , a <m> color component downsampling </m> from 1050 2:1 60 Hz to 525 1:1 30 Hz line scan formats must be carried out on the U and V color component source signals .


class: first -> second, base class: second -> first, new model class: no relation
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .


class: first -> second, base class: second -> first, new model class: second -> first
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: first -> second, base class: second -> first, new model class: second -> first
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: first -> second, base class: second -> first, new model class: second -> first
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: first -> second, base class: second -> first, new model class: second -> first
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: first -> second, base class: no relation, new model class: no relation
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: first -> second, base class: no relation, new model class: no relation
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: first -> second, base class: no relation, new model class: no relation
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
Note that SNIPER also performs <m> adaptive downsampling </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
The third one is the <m> volumetric downsampling block </m> that is identical to a volumetric max pooling layer .


class: first -> second, base class: no relation, new model class: no relation
first:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .
second:
More precisely , the contributions achieved during this thesis are : * A feature-preserving filter for the <m> downsampling of scalar volume data </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .
second:
In an MPEG ( Motion Pictures Experts Group ) video coding system , a <m> color component downsampling </m> from 1050 2:1 60 Hz to 525 1:1 30 Hz line scan formats must be carried out on the U and V color component source signals .


class: first -> second, base class: no relation, new model class: same cluster
first:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: first -> second, base class: no relation, new model class: same cluster
first:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: first -> second, base class: no relation, new model class: same cluster
first:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: first -> second, base class: no relation, new model class: no relation
first:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .
second:
Note that SNIPER also performs <m> adaptive downsampling </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .
second:
The third one is the <m> volumetric downsampling block </m> that is identical to a volumetric max pooling layer .


class: first -> second, base class: no relation, new model class: no relation
first:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .
second:
In an MPEG ( Motion Pictures Experts Group ) video coding system , a <m> color component downsampling </m> from 1050 2:1 60 Hz to 525 1:1 30 Hz line scan formats must be carried out on the U and V color component source signals .


class: first -> second, base class: same cluster, new model class: same cluster
first:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: first -> second, base class: same cluster, new model class: same cluster
first:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: first -> second, base class: same cluster, new model class: same cluster
first:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: first -> second, base class: no relation, new model class: no relation
first:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .
second:
Note that SNIPER also performs <m> adaptive downsampling </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .
second:
The third one is the <m> volumetric downsampling block </m> that is identical to a volumetric max pooling layer .


class: first -> second, base class: second -> first, new model class: same cluster
first:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: first -> second, base class: second -> first, new model class: same cluster
first:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: first -> second, base class: second -> first, new model class: same cluster
first:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: first -> second, base class: second -> first, new model class: same cluster
first:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: first -> second, base class: no relation, new model class: no relation
first:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .
second:
Note that SNIPER also performs <m> adaptive downsampling </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .
second:
The third one is the <m> volumetric downsampling block </m> that is identical to a volumetric max pooling layer .


class: first -> second, base class: second -> first, new model class: second -> first
first:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: first -> second, base class: same cluster, new model class: same cluster
first:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: first -> second, base class: same cluster, new model class: same cluster
first:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: first -> second, base class: same cluster, new model class: same cluster
first:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: first -> second, base class: no relation, new model class: no relation
first:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .
second:
Note that SNIPER also performs <m> adaptive downsampling </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .
second:
The third one is the <m> volumetric downsampling block </m> that is identical to a volumetric max pooling layer .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .
second:
Note that SNIPER also performs <m> adaptive downsampling </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .
second:
The third one is the <m> volumetric downsampling block </m> that is identical to a volumetric max pooling layer .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: first -> second, base class: no relation, new model class: no relation
first:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .
second:
Note that SNIPER also performs <m> adaptive downsampling </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .
second:
The third one is the <m> volumetric downsampling block </m> that is identical to a volumetric max pooling layer .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: first -> second, base class: second -> first, new model class: no relation
first:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .
second:
Note that SNIPER also performs <m> adaptive downsampling </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .
second:
The third one is the <m> volumetric downsampling block </m> that is identical to a volumetric max pooling layer .


class: first -> second, base class: no relation, new model class: no relation
first:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘
second:
Note that SNIPER also performs <m> adaptive downsampling </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘
second:
The third one is the <m> volumetric downsampling block </m> that is identical to a volumetric max pooling layer .


class: first -> second, base class: no relation, new model class: same cluster
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: no relation, new model class: same cluster
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: no relation, new model class: same cluster
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: no relation, new model class: same cluster
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: no relation, new model class: same cluster
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: no relation, new model class: same cluster
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: no relation, new model class: same cluster
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: second -> first, new model class: same cluster
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: second -> first, new model class: same cluster
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .


class: first -> second, base class: same cluster, new model class: same cluster
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .


class: first -> second, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: first -> second, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .


class: first -> second, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: first -> second, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: first -> second, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: first -> second, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .


class: first -> second, base class: same cluster, new model class: same cluster
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .


class: first -> second, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: first -> second, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .


class: first -> second, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: first -> second, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: first -> second, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: first -> second, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .


class: first -> second, base class: same cluster, new model class: same cluster
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .


class: first -> second, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: first -> second, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .


class: first -> second, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: first -> second, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: first -> second, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: first -> second, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .


class: first -> second, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: first -> second, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .


class: first -> second, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: first -> second, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: first -> second, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: first -> second, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .


class: first -> second, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: first -> second, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .


class: first -> second, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: first -> second, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: first -> second, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: first -> second, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .


class: first -> second, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: first -> second, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .


class: first -> second, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: first -> second, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: first -> second, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: first -> second, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .


class: first -> second, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: first -> second, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .


class: first -> second, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: first -> second, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: first -> second, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: first -> second, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: no relation, new model class: no relation
first:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .
second:
Popular conversational agents frameworks such as <m> Alexa Skills Kit </m> ( ASK ) and Google Actions ( gActions ) offer unprecedented opportunities for facilitating the development and deployment of voice-enabled AI solutions in various verticals .


class: first -> second, base class: no relation, new model class: no relation
first:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .
second:
For a C-UI we define a taskoid for the creation of state machines for <m> Alexa Controls </m> using the Alexa Gadget API and CLI tools .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .


class: first -> second, base class: no relation, new model class: second -> first
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .


class: first -> second, base class: no relation, new model class: second -> first
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: no relation, new model class: second -> first
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: same cluster, new model class: same cluster
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .


class: first -> second, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: first -> second, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .


class: first -> second, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: first -> second, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: first -> second, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: first -> second, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .


class: first -> second, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .


class: first -> second, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .


class: first -> second, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .


class: first -> second, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .


class: first -> second, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .


class: first -> second, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .


class: first -> second, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: first -> second, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: first -> second, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: first -> second, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: first -> second, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: first -> second, base class: same cluster, new model class: same cluster
first:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: same cluster, new model class: same cluster
first:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: same cluster, new model class: same cluster
first:
We provide a short state of the art on <m> virtual assistant 's technology </m> .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
Moreover , the scheme incorporates several source coding techniques , such as Wyner-Ziv coding , <m> binning </m> and superposition coding .
second:
Our evaluation shows that approximate fixes are competitive with using true fixes for <m> crash bucketing </m> , and significantly outperforms built-in deduplication techniques for three state of the art fuzzers .


class: first -> second, base class: no relation, new model class: no relation
first:
Moreover , the scheme incorporates several source coding techniques , such as Wyner-Ziv coding , <m> binning </m> and superposition coding .
second:
The new algorithm works under the dynamic programming framework and runs in provably linear time for multiple buffer types due to two novel techniques : <m> restrictive cost bucketing </m> and efficient delay update .


class: first -> second, base class: no relation, new model class: no relation
first:
Moreover , the scheme incorporates several source coding techniques , such as Wyner-Ziv coding , <m> binning </m> and superposition coding .
second:
To do so we carry out empirical analysis of the order flows from market and limit order submissions , aggregated from tick-by-tick data via <m> volume-based bucketing </m> , as well as various LOB depth and shape metrics .


class: first -> second, base class: no relation, new model class: no relation
first:
In this study , we propose a method com-bining K-means clustering algorithm and <m> unsupervised binning approaches </m> to improve the performance in metagenome-based disease prediction .
second:
<m> Quantile binning </m> is popular in several fields .


class: first -> second, base class: no relation, new model class: no relation
first:
In this study , we propose a method com-bining K-means clustering algorithm and <m> unsupervised binning approaches </m> to improve the performance in metagenome-based disease prediction .
second:
<m> Sub-pixel binning </m> : Chandra coordinates contain positional accuracy finer than one ACIS pixel ( 0.492 arcsec ) through dither and aspect correction .


class: first -> second, base class: no relation, new model class: no relation
first:
In this study , we propose a method com-bining K-means clustering algorithm and <m> unsupervised binning approaches </m> to improve the performance in metagenome-based disease prediction .
second:
Although conceptually simple , the calculation of radial distribution functions via <m> distance binning </m> requires the evaluation of O ( N 2 ) particle-pair distances where N is the number of particles under consideration .


class: first -> second, base class: no relation, new model class: no relation
first:
In this study , we propose a method com-bining K-means clustering algorithm and <m> unsupervised binning approaches </m> to improve the performance in metagenome-based disease prediction .
second:
To improve its efficiency this paper proposes GbDBSCAN(Grid based DBSCAN).GbDBSCAN adopts <m> gird and data binning technique </m> to query density reachable objects for all objects in dataset .


class: first -> second, base class: no relation, new model class: no relation
first:
The central task of automatic data visualization is , given a dataset , to visualize its compelling stories by transforming the data ( e.g. , selecting attributes , <m> grouping and binning values </m> ) and deciding the right type of visualization ( e.g. , bar or line charts ) .
second:
This paper demonstrates and compares different approaches for incorporating deterministic models of physical parameters into probabilistic models ; <m> parameter range binning </m> , response curves , and integral deterministic models .


class: first -> second, base class: no relation, new model class: no relation
first:
The central task of automatic data visualization is , given a dataset , to visualize its compelling stories by transforming the data ( e.g. , selecting attributes , <m> grouping and binning values </m> ) and deciding the right type of visualization ( e.g. , bar or line charts ) .
second:
Results We present here a <m> scalable pre-assembly binning scheme </m> ( i.e. , operating on unassembled short reads ) enabling latent genome recovery by leveraging sparse dictionary learning and elastic-net regularization , and its use to recover hundreds of metagenome-assembled genomes , including very low-abundance genomes , from a joint analysis of microbiomes from the LifeLines DEEP population cohort ( n = 1,135 , > 1010 reads ) .


class: first -> second, base class: no relation, new model class: no relation
first:
The central task of automatic data visualization is , given a dataset , to visualize its compelling stories by transforming the data ( e.g. , selecting attributes , <m> grouping and binning values </m> ) and deciding the right type of visualization ( e.g. , bar or line charts ) .
second:
Peak picking and separation are evaluated against the conventional methods of <m> mass binning </m> and manually selecting regions of a peak to image on a model data set .


class: first -> second, base class: no relation, new model class: no relation
first:
The central task of automatic data visualization is , given a dataset , to visualize its compelling stories by transforming the data ( e.g. , selecting attributes , <m> grouping and binning values </m> ) and deciding the right type of visualization ( e.g. , bar or line charts ) .
second:
Our evaluation shows that approximate fixes are competitive with using true fixes for <m> crash bucketing </m> , and significantly outperforms built-in deduplication techniques for three state of the art fuzzers .


class: first -> second, base class: no relation, new model class: no relation
first:
The central task of automatic data visualization is , given a dataset , to visualize its compelling stories by transforming the data ( e.g. , selecting attributes , <m> grouping and binning values </m> ) and deciding the right type of visualization ( e.g. , bar or line charts ) .
second:
The new algorithm works under the dynamic programming framework and runs in provably linear time for multiple buffer types due to two novel techniques : <m> restrictive cost bucketing </m> and efficient delay update .


class: first -> second, base class: no relation, new model class: no relation
first:
The central task of automatic data visualization is , given a dataset , to visualize its compelling stories by transforming the data ( e.g. , selecting attributes , <m> grouping and binning values </m> ) and deciding the right type of visualization ( e.g. , bar or line charts ) .
second:
To do so we carry out empirical analysis of the order flows from market and limit order submissions , aggregated from tick-by-tick data via <m> volume-based bucketing </m> , as well as various LOB depth and shape metrics .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a new <m> binning technic </m> for informed data hiding problem .
second:
Our evaluation shows that approximate fixes are competitive with using true fixes for <m> crash bucketing </m> , and significantly outperforms built-in deduplication techniques for three state of the art fuzzers .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a new <m> binning technic </m> for informed data hiding problem .
second:
The new algorithm works under the dynamic programming framework and runs in provably linear time for multiple buffer types due to two novel techniques : <m> restrictive cost bucketing </m> and efficient delay update .


class: first -> second, base class: no relation, new model class: no relation
first:
We describe a new <m> binning technic </m> for informed data hiding problem .
second:
To do so we carry out empirical analysis of the order flows from market and limit order submissions , aggregated from tick-by-tick data via <m> volume-based bucketing </m> , as well as various LOB depth and shape metrics .


class: first -> second, base class: no relation, new model class: no relation
first:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .
second:
Obtaining a coherent binned representation in higher dimensions is challenging because <m> multidimensional binning </m> can suffer from the curse of dimensionality .


class: first -> second, base class: no relation, new model class: no relation
first:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .
second:
Peak picking and separation are evaluated against the conventional methods of <m> mass binning </m> and manually selecting regions of a peak to image on a model data set .


class: first -> second, base class: no relation, new model class: no relation
first:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .
second:
Our evaluation shows that approximate fixes are competitive with using true fixes for <m> crash bucketing </m> , and significantly outperforms built-in deduplication techniques for three state of the art fuzzers .


class: first -> second, base class: no relation, new model class: no relation
first:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .
second:
The new algorithm works under the dynamic programming framework and runs in provably linear time for multiple buffer types due to two novel techniques : <m> restrictive cost bucketing </m> and efficient delay update .


class: first -> second, base class: no relation, new model class: no relation
first:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .
second:
To do so we carry out empirical analysis of the order flows from market and limit order submissions , aggregated from tick-by-tick data via <m> volume-based bucketing </m> , as well as various LOB depth and shape metrics .


class: first -> second, base class: no relation, new model class: no relation
first:
Obtaining a coherent binned representation in higher dimensions is challenging because <m> multidimensional binning </m> can suffer from the curse of dimensionality .
second:
A fast HOTLink ( 400Mbit/s , Cypress Semiconductor inc . ) serial bus is used to transfer the coincidence data to networked data acquisition and image processing computers , and the <m> sinogram binning </m> and image reconstruction can be processed in parallel .


class: first -> second, base class: no relation, new model class: no relation
first:
Obtaining a coherent binned representation in higher dimensions is challenging because <m> multidimensional binning </m> can suffer from the curse of dimensionality .
second:
We establish a new achievable rate region for the ZC , using <m> Marton 's binning technique </m> .


class: first -> second, base class: no relation, new model class: second -> first
first:
Obtaining a coherent binned representation in higher dimensions is challenging because <m> multidimensional binning </m> can suffer from the curse of dimensionality .
second:
The method shows significant improvement over the <m> binning and stacking method </m> when applied to both synthetic as well as real data .


class: first -> second, base class: no relation, new model class: no relation
first:
Obtaining a coherent binned representation in higher dimensions is challenging because <m> multidimensional binning </m> can suffer from the curse of dimensionality .
second:
A feasible distributed data indexing algorithm is proposed for Hadoop data mining , based on <m> ZSCORE binning </m> and inverted indexing and on the Hadoop SequenceFile format .


class: first -> second, base class: no relation, new model class: no relation
first:
This candidate list was then compared with an expert-generated list of marijuana terms to assess the accuracy and efficacy of using <m> word-vector embeddings </m> to search for novel drug terminology .
second:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .


class: first -> second, base class: no relation, new model class: no relation
first:
This candidate list was then compared with an expert-generated list of marijuana terms to assess the accuracy and efficacy of using <m> word-vector embeddings </m> to search for novel drug terminology .
second:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
Recent approaches to word vector representations , e.g. , ‘ w2vec ’ and ‘ <m> GloVe ’ </m> , have been shown to be powerful methods for capturing the semantics and syntax of words in a text .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
First , we propose an online topic clustering algorithm based on keyword frequency to combine the extended TextRank with weighted titles and the improved <m> GloVe methods </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
The deep learning techniques such as long short term memory cell ( LSTM ) with and without word GloVe embeddings , a Convolution neural network ( CNN ) with or without GloVe are used , and <m> GloVe pretrained model </m> is used for classification


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
subsection : Parsing , POS and predicate detection We first report the labeled and unlabeled attachment scores ( LAS , UAS ) of our parsing models on the CoNLL - 2005 and 2012 test sets ( Table [ reference ] ) with <m> GloVe </m> ( ) and ELMo ( ) embeddings .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
We use open - source <m> GloVe vectors </m> trained on Common Crawl 840B with 300 dimensions as fixed word embeddings .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
<m> GloVe vectors </m> were trained on a large corpus of about 6 billion tokens pennington2014glove , and provide an important source of prior knowledge for the model .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
The model exploits the 300 , 000 most frequent <m> pretrained GloVe embeddings </m> and improves them during the training process .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
We use pre - trained <m> word vectors GloVe </m> to obtain the fixed word embedding of each word .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
On the textual side , we worked with <m> GloVe word embeddings </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
<m> GloVe embeddings </m> [ Pennington et al. in : Empirical methods in natural language processing ( EMNLP ) , pp 1532–1543 , 2014 ] have been used to represent text data numerically .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
For the unsupervised classifiers , we implemented several models of CNN and RNN classifiers utilizing <m> GloVe-based word embeddings </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
This model uses <m> GloVe embeddings </m> and the same optimization and procedure described above .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
An initial embedding layer vectorises words in P and H using pretrained <m> GLoVe embeddings </m> , and passing them to a context representation layer , which uses bidirectional LSTMs ( BiLSTMs ) to encode context vectors for each time - step .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
In particular , our experiments utilize the <m> GloVe embeddings </m> trained by pennington2014glove on 6 billion tokens of Wikipedia 2014 and Gigaword 5 .


class: first -> second, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
We choose word embedding size , and use the - dimensional pre - trained <m> GloVe word embeddings </m> for initialization .


class: first -> second, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
The deep learning techniques such as long short term memory cell ( LSTM ) with and without word GloVe embeddings , a Convolution neural network ( CNN ) with or without GloVe are used , and <m> GloVe pretrained model </m> is used for classification


class: first -> second, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
subsection : Parsing , POS and predicate detection We first report the labeled and unlabeled attachment scores ( LAS , UAS ) of our parsing models on the CoNLL - 2005 and 2012 test sets ( Table [ reference ] ) with <m> GloVe </m> ( ) and ELMo ( ) embeddings .


class: first -> second, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
We use open - source <m> GloVe vectors </m> trained on Common Crawl 840B with 300 dimensions as fixed word embeddings .


class: first -> second, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
<m> GloVe vectors </m> were trained on a large corpus of about 6 billion tokens pennington2014glove , and provide an important source of prior knowledge for the model .


class: first -> second, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .


class: first -> second, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .


class: first -> second, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: first -> second, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
The model exploits the 300 , 000 most frequent <m> pretrained GloVe embeddings </m> and improves them during the training process .


class: first -> second, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .


class: first -> second, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .


class: first -> second, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
We use pre - trained <m> word vectors GloVe </m> to obtain the fixed word embedding of each word .


class: first -> second, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
On the textual side , we worked with <m> GloVe word embeddings </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
<m> GloVe embeddings </m> [ Pennington et al. in : Empirical methods in natural language processing ( EMNLP ) , pp 1532–1543 , 2014 ] have been used to represent text data numerically .


class: first -> second, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
For the unsupervised classifiers , we implemented several models of CNN and RNN classifiers utilizing <m> GloVe-based word embeddings </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: first -> second, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: first -> second, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
This model uses <m> GloVe embeddings </m> and the same optimization and procedure described above .


class: first -> second, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
An initial embedding layer vectorises words in P and H using pretrained <m> GLoVe embeddings </m> , and passing them to a context representation layer , which uses bidirectional LSTMs ( BiLSTMs ) to encode context vectors for each time - step .


class: first -> second, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
In particular , our experiments utilize the <m> GloVe embeddings </m> trained by pennington2014glove on 6 billion tokens of Wikipedia 2014 and Gigaword 5 .


class: first -> second, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
We choose word embedding size , and use the - dimensional pre - trained <m> GloVe word embeddings </m> for initialization .


class: first -> second, base class: no relation, new model class: no relation
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: first -> second, base class: no relation, new model class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: first -> second, base class: no relation, new model class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: first -> second, base class: no relation, new model class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: first -> second, base class: no relation, new model class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: first -> second, base class: no relation, new model class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: first -> second, base class: no relation, new model class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: first -> second, base class: no relation, new model class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: first -> second, base class: no relation, new model class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: first -> second, base class: no relation, new model class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .


class: first -> second, base class: same cluster, new model class: same cluster
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: first -> second, base class: same cluster, new model class: same cluster
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: first -> second, base class: same cluster, new model class: same cluster
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: first -> second, base class: no relation, new model class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: first -> second, base class: no relation, new model class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: first -> second, base class: no relation, new model class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: first -> second, base class: no relation, new model class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: first -> second, base class: no relation, new model class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: first -> second, base class: no relation, new model class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: first -> second, base class: no relation, new model class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: first -> second, base class: no relation, new model class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: first -> second, base class: no relation, new model class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: no relation
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation, new model class: same cluster
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .


class: first -> second, base class: no relation, new model class: same cluster
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .


class: first -> second, base class: no relation, new model class: same cluster
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .


class: first -> second, base class: no relation, new model class: same cluster
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .


class: first -> second, base class: no relation, new model class: same cluster
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .


class: first -> second, base class: no relation, new model class: same cluster
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .


class: first -> second, base class: no relation, new model class: same cluster
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .


class: first -> second, base class: no relation, new model class: same cluster
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .


class: first -> second, base class: no relation, new model class: same cluster
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .


class: first -> second, base class: no relation, new model class: same cluster
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: first -> second, base class: no relation, new model class: same cluster
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .


class: first -> second, base class: no relation, new model class: same cluster
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .


class: first -> second, base class: no relation, new model class: same cluster
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .


class: first -> second, base class: no relation, new model class: same cluster
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .


class: first -> second, base class: no relation, new model class: same cluster
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .


class: first -> second, base class: no relation, new model class: same cluster
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .


class: first -> second, base class: no relation, new model class: same cluster
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .


class: first -> second, base class: no relation, new model class: same cluster
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .


class: first -> second, base class: no relation, new model class: same cluster
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .


class: first -> second, base class: no relation, new model class: same cluster
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .


class: first -> second, base class: no relation, new model class: same cluster
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .


class: first -> second, base class: no relation, new model class: same cluster
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .


class: first -> second, base class: no relation, new model class: same cluster
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .


class: first -> second, base class: no relation, new model class: same cluster
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: first -> second, base class: no relation, new model class: same cluster
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .


class: first -> second, base class: no relation, new model class: same cluster
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .


class: first -> second, base class: no relation, new model class: same cluster
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .


class: first -> second, base class: no relation, new model class: same cluster
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .


class: first -> second, base class: no relation, new model class: same cluster
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .


class: first -> second, base class: no relation, new model class: same cluster
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .


class: first -> second, base class: no relation, new model class: same cluster
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .


class: first -> second, base class: no relation, new model class: same cluster
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .


class: first -> second, base class: no relation, new model class: same cluster
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .


class: first -> second, base class: no relation, new model class: same cluster
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .


class: first -> second, base class: no relation, new model class: same cluster
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .


class: first -> second, base class: no relation, new model class: same cluster
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .


class: first -> second, base class: no relation, new model class: same cluster
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .


class: first -> second, base class: no relation, new model class: same cluster
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: first -> second, base class: no relation, new model class: same cluster
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .


class: first -> second, base class: no relation, new model class: same cluster
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .


class: first -> second, base class: no relation, new model class: same cluster
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .


class: first -> second, base class: no relation, new model class: same cluster
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .


class: first -> second, base class: no relation, new model class: same cluster
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .


class: first -> second, base class: no relation, new model class: same cluster
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .


class: first -> second, base class: no relation, new model class: same cluster
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .


class: first -> second, base class: no relation, new model class: same cluster
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .


class: first -> second, base class: no relation, new model class: same cluster
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .


class: first -> second, base class: no relation, new model class: same cluster
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .


class: first -> second, base class: no relation, new model class: same cluster
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .


class: first -> second, base class: no relation, new model class: same cluster
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .


class: first -> second, base class: no relation, new model class: same cluster
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .


class: first -> second, base class: no relation, new model class: same cluster
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: first -> second, base class: no relation, new model class: same cluster
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .


class: first -> second, base class: no relation, new model class: same cluster
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .


class: first -> second, base class: no relation, new model class: same cluster
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .


class: first -> second, base class: no relation, new model class: same cluster
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .


class: first -> second, base class: no relation, new model class: same cluster
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .


class: first -> second, base class: no relation, new model class: same cluster
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .


class: first -> second, base class: no relation, new model class: same cluster
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .


class: first -> second, base class: no relation, new model class: same cluster
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .


class: first -> second, base class: no relation, new model class: same cluster
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .


class: first -> second, base class: no relation, new model class: same cluster
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .


class: first -> second, base class: no relation, new model class: same cluster
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .


class: first -> second, base class: no relation, new model class: same cluster
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .


class: first -> second, base class: no relation, new model class: same cluster
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .


class: first -> second, base class: no relation, new model class: same cluster
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: first -> second, base class: no relation, new model class: same cluster
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .


class: first -> second, base class: no relation, new model class: same cluster
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .


class: first -> second, base class: no relation, new model class: same cluster
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .


class: first -> second, base class: no relation, new model class: same cluster
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .


class: first -> second, base class: no relation, new model class: same cluster
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .


class: first -> second, base class: no relation, new model class: same cluster
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .


class: first -> second, base class: no relation, new model class: same cluster
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .


class: first -> second, base class: no relation, new model class: same cluster
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .


class: first -> second, base class: no relation, new model class: same cluster
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .


class: first -> second, base class: no relation, new model class: same cluster
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .


class: first -> second, base class: no relation, new model class: same cluster
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .


class: first -> second, base class: no relation, new model class: same cluster
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .


class: first -> second, base class: no relation, new model class: same cluster
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .


class: first -> second, base class: no relation, new model class: same cluster
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: first -> second, base class: no relation, new model class: same cluster
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .


class: first -> second, base class: no relation, new model class: same cluster
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .


class: first -> second, base class: no relation, new model class: same cluster
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .


class: first -> second, base class: no relation, new model class: same cluster
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .


class: first -> second, base class: no relation, new model class: no relation
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .


class: first -> second, base class: no relation, new model class: no relation
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .


class: first -> second, base class: no relation, new model class: no relation
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .


class: first -> second, base class: no relation, new model class: no relation
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .


class: first -> second, base class: no relation, new model class: no relation
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .


class: first -> second, base class: no relation, new model class: no relation
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
In our practice , several important techniques are deployed for accelerating online serving of industrial deep networks under the <m> CPU - GPU architecture </m> : i ) request batching which merges adjacent requests from CPU to take advantage of GPU power , ii ) GPU memory optimization which improves the access pattern to reduce wasted transactions in GPU memory , iii ) concurrent kernel computation which allows execution of matrix computations to be processed with multiple CUDA kernels concurrently .


class: first -> second, base class: no relation, new model class: no relation
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
section : Conclusion In this work we presented a new off - policy agent based on <m> Retrace actor - critic architecture </m> and show that it achieves similar performance as the current state - of - the - art while giving significant real - time performance gains .


class: first -> second, base class: no relation, new model class: no relation
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: first -> second, base class: no relation, new model class: no relation
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: first -> second, base class: no relation, new model class: no relation
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .


class: first -> second, base class: no relation, new model class: no relation
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .


class: first -> second, base class: no relation, new model class: no relation
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .


class: first -> second, base class: no relation, new model class: no relation
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .


class: first -> second, base class: no relation, new model class: no relation
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .


class: first -> second, base class: no relation, new model class: no relation
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
In our practice , several important techniques are deployed for accelerating online serving of industrial deep networks under the <m> CPU - GPU architecture </m> : i ) request batching which merges adjacent requests from CPU to take advantage of GPU power , ii ) GPU memory optimization which improves the access pattern to reduce wasted transactions in GPU memory , iii ) concurrent kernel computation which allows execution of matrix computations to be processed with multiple CUDA kernels concurrently .


class: first -> second, base class: no relation, new model class: no relation
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
section : Conclusion In this work we presented a new off - policy agent based on <m> Retrace actor - critic architecture </m> and show that it achieves similar performance as the current state - of - the - art while giving significant real - time performance gains .


class: first -> second, base class: no relation, new model class: no relation
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: first -> second, base class: no relation, new model class: no relation
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: first -> second, base class: no relation, new model class: no relation
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .


class: first -> second, base class: no relation, new model class: no relation
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .


class: first -> second, base class: no relation, new model class: no relation
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .


class: first -> second, base class: no relation, new model class: no relation
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .


class: first -> second, base class: no relation, new model class: no relation
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .


class: first -> second, base class: no relation, new model class: no relation
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
In our practice , several important techniques are deployed for accelerating online serving of industrial deep networks under the <m> CPU - GPU architecture </m> : i ) request batching which merges adjacent requests from CPU to take advantage of GPU power , ii ) GPU memory optimization which improves the access pattern to reduce wasted transactions in GPU memory , iii ) concurrent kernel computation which allows execution of matrix computations to be processed with multiple CUDA kernels concurrently .


class: first -> second, base class: no relation, new model class: no relation
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
section : Conclusion In this work we presented a new off - policy agent based on <m> Retrace actor - critic architecture </m> and show that it achieves similar performance as the current state - of - the - art while giving significant real - time performance gains .


class: first -> second, base class: no relation, new model class: no relation
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: first -> second, base class: no relation, new model class: no relation
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: first -> second, base class: no relation, new model class: no relation
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
In our practice , several important techniques are deployed for accelerating online serving of industrial deep networks under the <m> CPU - GPU architecture </m> : i ) request batching which merges adjacent requests from CPU to take advantage of GPU power , ii ) GPU memory optimization which improves the access pattern to reduce wasted transactions in GPU memory , iii ) concurrent kernel computation which allows execution of matrix computations to be processed with multiple CUDA kernels concurrently .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
section : Conclusion In this work we presented a new off - policy agent based on <m> Retrace actor - critic architecture </m> and show that it achieves similar performance as the current state - of - the - art while giving significant real - time performance gains .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
In our practice , several important techniques are deployed for accelerating online serving of industrial deep networks under the <m> CPU - GPU architecture </m> : i ) request batching which merges adjacent requests from CPU to take advantage of GPU power , ii ) GPU memory optimization which improves the access pattern to reduce wasted transactions in GPU memory , iii ) concurrent kernel computation which allows execution of matrix computations to be processed with multiple CUDA kernels concurrently .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
section : Conclusion In this work we presented a new off - policy agent based on <m> Retrace actor - critic architecture </m> and show that it achieves similar performance as the current state - of - the - art while giving significant real - time performance gains .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
In our practice , several important techniques are deployed for accelerating online serving of industrial deep networks under the <m> CPU - GPU architecture </m> : i ) request batching which merges adjacent requests from CPU to take advantage of GPU power , ii ) GPU memory optimization which improves the access pattern to reduce wasted transactions in GPU memory , iii ) concurrent kernel computation which allows execution of matrix computations to be processed with multiple CUDA kernels concurrently .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
section : Conclusion In this work we presented a new off - policy agent based on <m> Retrace actor - critic architecture </m> and show that it achieves similar performance as the current state - of - the - art while giving significant real - time performance gains .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .


class: first -> second, base class: no relation, new model class: no relation
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .


class: first -> second, base class: no relation, new model class: no relation
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .


class: first -> second, base class: no relation, new model class: no relation
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .


class: first -> second, base class: no relation, new model class: no relation
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .


class: first -> second, base class: no relation, new model class: no relation
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
In our practice , several important techniques are deployed for accelerating online serving of industrial deep networks under the <m> CPU - GPU architecture </m> : i ) request batching which merges adjacent requests from CPU to take advantage of GPU power , ii ) GPU memory optimization which improves the access pattern to reduce wasted transactions in GPU memory , iii ) concurrent kernel computation which allows execution of matrix computations to be processed with multiple CUDA kernels concurrently .


class: first -> second, base class: no relation, new model class: no relation
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
section : Conclusion In this work we presented a new off - policy agent based on <m> Retrace actor - critic architecture </m> and show that it achieves similar performance as the current state - of - the - art while giving significant real - time performance gains .


class: first -> second, base class: no relation, new model class: no relation
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: first -> second, base class: no relation, new model class: no relation
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: first -> second, base class: no relation, new model class: no relation
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
In our practice , several important techniques are deployed for accelerating online serving of industrial deep networks under the <m> CPU - GPU architecture </m> : i ) request batching which merges adjacent requests from CPU to take advantage of GPU power , ii ) GPU memory optimization which improves the access pattern to reduce wasted transactions in GPU memory , iii ) concurrent kernel computation which allows execution of matrix computations to be processed with multiple CUDA kernels concurrently .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
section : Conclusion In this work we presented a new off - policy agent based on <m> Retrace actor - critic architecture </m> and show that it achieves similar performance as the current state - of - the - art while giving significant real - time performance gains .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
In our practice , several important techniques are deployed for accelerating online serving of industrial deep networks under the <m> CPU - GPU architecture </m> : i ) request batching which merges adjacent requests from CPU to take advantage of GPU power , ii ) GPU memory optimization which improves the access pattern to reduce wasted transactions in GPU memory , iii ) concurrent kernel computation which allows execution of matrix computations to be processed with multiple CUDA kernels concurrently .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
section : Conclusion In this work we presented a new off - policy agent based on <m> Retrace actor - critic architecture </m> and show that it achieves similar performance as the current state - of - the - art while giving significant real - time performance gains .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .


class: first -> second, base class: no relation, new model class: no relation
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .


class: first -> second, base class: no relation, new model class: no relation
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .


class: first -> second, base class: no relation, new model class: no relation
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .


class: first -> second, base class: no relation, new model class: no relation
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .


class: first -> second, base class: no relation, new model class: no relation
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
In our practice , several important techniques are deployed for accelerating online serving of industrial deep networks under the <m> CPU - GPU architecture </m> : i ) request batching which merges adjacent requests from CPU to take advantage of GPU power , ii ) GPU memory optimization which improves the access pattern to reduce wasted transactions in GPU memory , iii ) concurrent kernel computation which allows execution of matrix computations to be processed with multiple CUDA kernels concurrently .


class: first -> second, base class: no relation, new model class: no relation
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
section : Conclusion In this work we presented a new off - policy agent based on <m> Retrace actor - critic architecture </m> and show that it achieves similar performance as the current state - of - the - art while giving significant real - time performance gains .


class: first -> second, base class: no relation, new model class: no relation
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: first -> second, base class: no relation, new model class: no relation
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: first -> second, base class: no relation, new model class: no relation
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .


class: first -> second, base class: no relation, new model class: no relation
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .


class: first -> second, base class: no relation, new model class: no relation
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .


class: first -> second, base class: no relation, new model class: no relation
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .


class: first -> second, base class: no relation, new model class: no relation
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .


class: first -> second, base class: no relation, new model class: no relation
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
In our practice , several important techniques are deployed for accelerating online serving of industrial deep networks under the <m> CPU - GPU architecture </m> : i ) request batching which merges adjacent requests from CPU to take advantage of GPU power , ii ) GPU memory optimization which improves the access pattern to reduce wasted transactions in GPU memory , iii ) concurrent kernel computation which allows execution of matrix computations to be processed with multiple CUDA kernels concurrently .


class: first -> second, base class: no relation, new model class: no relation
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
section : Conclusion In this work we presented a new off - policy agent based on <m> Retrace actor - critic architecture </m> and show that it achieves similar performance as the current state - of - the - art while giving significant real - time performance gains .


class: first -> second, base class: no relation, new model class: no relation
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: first -> second, base class: no relation, new model class: no relation
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: first -> second, base class: no relation, new model class: no relation
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .


class: first -> second, base class: no relation, new model class: no relation
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .


class: first -> second, base class: no relation, new model class: no relation
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .


class: first -> second, base class: no relation, new model class: no relation
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .


class: first -> second, base class: no relation, new model class: no relation
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .


class: first -> second, base class: no relation, new model class: no relation
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
In our practice , several important techniques are deployed for accelerating online serving of industrial deep networks under the <m> CPU - GPU architecture </m> : i ) request batching which merges adjacent requests from CPU to take advantage of GPU power , ii ) GPU memory optimization which improves the access pattern to reduce wasted transactions in GPU memory , iii ) concurrent kernel computation which allows execution of matrix computations to be processed with multiple CUDA kernels concurrently .


class: first -> second, base class: no relation, new model class: no relation
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
section : Conclusion In this work we presented a new off - policy agent based on <m> Retrace actor - critic architecture </m> and show that it achieves similar performance as the current state - of - the - art while giving significant real - time performance gains .


class: first -> second, base class: no relation, new model class: no relation
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: first -> second, base class: no relation, new model class: no relation
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: first -> second, base class: no relation, new model class: no relation
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .


class: first -> second, base class: no relation, new model class: no relation
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .


class: first -> second, base class: no relation, new model class: no relation
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .


class: first -> second, base class: no relation, new model class: no relation
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .


class: first -> second, base class: no relation, new model class: no relation
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .


class: first -> second, base class: no relation, new model class: no relation
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
In our practice , several important techniques are deployed for accelerating online serving of industrial deep networks under the <m> CPU - GPU architecture </m> : i ) request batching which merges adjacent requests from CPU to take advantage of GPU power , ii ) GPU memory optimization which improves the access pattern to reduce wasted transactions in GPU memory , iii ) concurrent kernel computation which allows execution of matrix computations to be processed with multiple CUDA kernels concurrently .


class: first -> second, base class: no relation, new model class: no relation
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
section : Conclusion In this work we presented a new off - policy agent based on <m> Retrace actor - critic architecture </m> and show that it achieves similar performance as the current state - of - the - art while giving significant real - time performance gains .


class: first -> second, base class: no relation, new model class: no relation
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: first -> second, base class: no relation, new model class: no relation
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: first -> second, base class: no relation, new model class: no relation
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .


class: first -> second, base class: no relation, new model class: no relation
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .


class: first -> second, base class: no relation, new model class: no relation
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .


class: first -> second, base class: no relation, new model class: no relation
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .


class: first -> second, base class: no relation, new model class: no relation
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .


class: first -> second, base class: no relation, new model class: no relation
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
In our practice , several important techniques are deployed for accelerating online serving of industrial deep networks under the <m> CPU - GPU architecture </m> : i ) request batching which merges adjacent requests from CPU to take advantage of GPU power , ii ) GPU memory optimization which improves the access pattern to reduce wasted transactions in GPU memory , iii ) concurrent kernel computation which allows execution of matrix computations to be processed with multiple CUDA kernels concurrently .


class: first -> second, base class: no relation, new model class: no relation
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
section : Conclusion In this work we presented a new off - policy agent based on <m> Retrace actor - critic architecture </m> and show that it achieves similar performance as the current state - of - the - art while giving significant real - time performance gains .


class: first -> second, base class: no relation, new model class: no relation
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: first -> second, base class: no relation, new model class: no relation
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: first -> second, base class: no relation, new model class: no relation
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .


class: first -> second, base class: no relation, new model class: no relation
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .


class: first -> second, base class: no relation, new model class: no relation
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .


class: first -> second, base class: no relation, new model class: no relation
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .


class: first -> second, base class: no relation, new model class: no relation
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .


class: first -> second, base class: no relation, new model class: no relation
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
In our practice , several important techniques are deployed for accelerating online serving of industrial deep networks under the <m> CPU - GPU architecture </m> : i ) request batching which merges adjacent requests from CPU to take advantage of GPU power , ii ) GPU memory optimization which improves the access pattern to reduce wasted transactions in GPU memory , iii ) concurrent kernel computation which allows execution of matrix computations to be processed with multiple CUDA kernels concurrently .


class: first -> second, base class: no relation, new model class: no relation
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
section : Conclusion In this work we presented a new off - policy agent based on <m> Retrace actor - critic architecture </m> and show that it achieves similar performance as the current state - of - the - art while giving significant real - time performance gains .


class: first -> second, base class: no relation, new model class: no relation
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: first -> second, base class: no relation, new model class: no relation
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: first -> second, base class: no relation, new model class: no relation
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .


class: first -> second, base class: no relation, new model class: no relation
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .


class: first -> second, base class: no relation, new model class: no relation
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .


class: first -> second, base class: no relation, new model class: no relation
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .


class: first -> second, base class: no relation, new model class: no relation
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .


class: first -> second, base class: no relation, new model class: no relation
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
In our practice , several important techniques are deployed for accelerating online serving of industrial deep networks under the <m> CPU - GPU architecture </m> : i ) request batching which merges adjacent requests from CPU to take advantage of GPU power , ii ) GPU memory optimization which improves the access pattern to reduce wasted transactions in GPU memory , iii ) concurrent kernel computation which allows execution of matrix computations to be processed with multiple CUDA kernels concurrently .


class: first -> second, base class: no relation, new model class: no relation
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
section : Conclusion In this work we presented a new off - policy agent based on <m> Retrace actor - critic architecture </m> and show that it achieves similar performance as the current state - of - the - art while giving significant real - time performance gains .


class: first -> second, base class: no relation, new model class: no relation
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: first -> second, base class: no relation, new model class: no relation
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: first -> second, base class: no relation, new model class: no relation
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .
second:
We will compare the following models : Faster R - CNN [ reference ] , R 2 CNN - 1 , R 2 CNN - 2 , <m> R 2 CNN - 3 </m> , R 2 CNN - 4 , and R 2 CNN - 5 .


class: first -> second, base class: no relation, new model class: no relation
first:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .
second:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .


class: first -> second, base class: no relation, new model class: second -> first
first:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .
second:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .


class: first -> second, base class: no relation, new model class: no relation
first:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .
second:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .
second:
We will compare the following models : Faster R - CNN [ reference ] , R 2 CNN - 1 , R 2 CNN - 2 , <m> R 2 CNN - 3 </m> , R 2 CNN - 4 , and R 2 CNN - 5 .


class: first -> second, base class: no relation, new model class: no relation
first:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .
second:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .


class: first -> second, base class: no relation, new model class: second -> first
first:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .
second:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .


class: first -> second, base class: no relation, new model class: no relation
first:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .
second:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .
second:
After that , <m> Conv - LSTM layers </m> with 256 channels are inserted into the single - shot TLL network .


class: first -> second, base class: no relation, new model class: same cluster
first:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .
second:
For RNNs , <m> Convolutional - LSTM </m> ( CLSTM ) also achieved a good performance in HSI classification .


class: first -> second, base class: no relation, new model class: same cluster
first:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .
second:
After that , <m> Conv - LSTM layers </m> with 256 channels are inserted into the single - shot TLL network .


class: first -> second, base class: no relation, new model class: same cluster
first:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .
second:
For RNNs , <m> Convolutional - LSTM </m> ( CLSTM ) also achieved a good performance in HSI classification .


class: first -> second, base class: no relation, new model class: same cluster
first:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .
second:
For RNNs , <m> Convolutional - LSTM </m> ( CLSTM ) also achieved a good performance in HSI classification .


class: first -> second, base class: no relation, new model class: no relation
first:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: first -> second, base class: no relation, new model class: no relation
first:
This system achieves 79.5 % correct prediction using the " hard " CASP 3-class assignment , and 81.4 % with a more lenient assignment , outperforming a sophisticated state-of-the-art <m> predictor </m> ( Porter ) trained in the same experimental conditions .
second:
The <m> fully connected predictor </m> as above can not make full use of such natural geometric statistics , since each point is predicted independently .


class: first -> second, base class: no relation, new model class: no relation
first:
This system achieves 79.5 % correct prediction using the " hard " CASP 3-class assignment , and 81.4 % with a more lenient assignment , outperforming a sophisticated state-of-the-art <m> predictor </m> ( Porter ) trained in the same experimental conditions .
second:
[ reference ] , the model is composed of the following four modules : an encoder stack , a decoder stack , a newly added fertility predictor ( details in [ reference ] ) , and a <m> translation predictor </m> for token decoding .


class: first -> second, base class: no relation, new model class: no relation
first:
This system achieves 79.5 % correct prediction using the " hard " CASP 3-class assignment , and 81.4 % with a more lenient assignment , outperforming a sophisticated state-of-the-art <m> predictor </m> ( Porter ) trained in the same experimental conditions .
second:
A <m> convolutional predictor </m> with kernels is added to each of the 6 layers to detect segments and links .


class: first -> second, base class: no relation, new model class: no relation
first:
This system achieves 79.5 % correct prediction using the " hard " CASP 3-class assignment , and 81.4 % with a more lenient assignment , outperforming a sophisticated state-of-the-art <m> predictor </m> ( Porter ) trained in the same experimental conditions .
second:
ABSTRACT A general approach to building an effective justification process around a multi <m> — attribute decision model </m> is suggested which incorporates factory performance measurements to focus on difficult to quantify strategic and tactical benefits from Computer Integrated Manufacturing .


class: first -> second, base class: no relation, new model class: no relation
first:
This system achieves 79.5 % correct prediction using the " hard " CASP 3-class assignment , and 81.4 % with a more lenient assignment , outperforming a sophisticated state-of-the-art <m> predictor </m> ( Porter ) trained in the same experimental conditions .
second:
First , we propose a <m> fully trainable attribute - based neural network </m> founded upon the CNN + RNN architecture , that can be applied to multiple V2L problems .


class: first -> second, base class: no relation, new model class: no relation
first:
Several studies have also confirmed that this philosophy of making the <m> predictor </m> robust against random and local perturbation is effective in semi - supervised learning .
second:
The <m> fully connected predictor </m> as above can not make full use of such natural geometric statistics , since each point is predicted independently .


class: first -> second, base class: no relation, new model class: no relation
first:
Several studies have also confirmed that this philosophy of making the <m> predictor </m> robust against random and local perturbation is effective in semi - supervised learning .
second:
ABSTRACT A general approach to building an effective justification process around a multi <m> — attribute decision model </m> is suggested which incorporates factory performance measurements to focus on difficult to quantify strategic and tactical benefits from Computer Integrated Manufacturing .


class: first -> second, base class: no relation, new model class: no relation
first:
Several studies have also confirmed that this philosophy of making the <m> predictor </m> robust against random and local perturbation is effective in semi - supervised learning .
second:
First , we propose a <m> fully trainable attribute - based neural network </m> founded upon the CNN + RNN architecture , that can be applied to multiple V2L problems .


class: first -> second, base class: no relation, new model class: no relation
first:
We apply domainadversarial learning , as we consider a <m> descriptor predictor </m> trained with a Siamese - like loss instead of the label predictor trained with a classification loss .
second:
<m> Adversarial attribute loss </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We apply domainadversarial learning , as we consider a <m> descriptor predictor </m> trained with a Siamese - like loss instead of the label predictor trained with a classification loss .
second:
As for in the <m> attribute ration regularization </m> , we set it to be ( i ) for small local attributes ( e . g . , mouth ) , ( ii ) for large local attributes ( e . g . , eyes ) , and ( iii ) for global attributes ( e . g . , gender and age ) .


class: first -> second, base class: no relation, new model class: no relation
first:
We apply domainadversarial learning , as we consider a <m> descriptor predictor </m> trained with a Siamese - like loss instead of the label predictor trained with a classification loss .
second:
There have been works on first learning <m> attribute classifiers </m> and using attribute predictions for face recognition .


class: first -> second, base class: no relation, new model class: no relation
first:
In both approaches , the actual <m> classifier / predictor </m> is learned in a separate step using the feature representation learned by autoencoder ( s ) .
second:
Formally , let denote the encoding of the state , and let be a <m> dynamics predictor </m> parameterized by .


class: first -> second, base class: no relation, new model class: no relation
first:
In both approaches , the actual <m> classifier / predictor </m> is learned in a separate step using the feature representation learned by autoencoder ( s ) .
second:
[ reference ] , the model is composed of the following four modules : an encoder stack , a decoder stack , a newly added fertility predictor ( details in [ reference ] ) , and a <m> translation predictor </m> for token decoding .


class: first -> second, base class: no relation, new model class: no relation
first:
In both approaches , the actual <m> classifier / predictor </m> is learned in a separate step using the feature representation learned by autoencoder ( s ) .
second:
A <m> convolutional predictor </m> with kernels is added to each of the 6 layers to detect segments and links .


class: first -> second, base class: no relation, new model class: no relation
first:
In both approaches , the actual <m> classifier / predictor </m> is learned in a separate step using the feature representation learned by autoencoder ( s ) .
second:
ABSTRACT A general approach to building an effective justification process around a multi <m> — attribute decision model </m> is suggested which incorporates factory performance measurements to focus on difficult to quantify strategic and tactical benefits from Computer Integrated Manufacturing .


class: first -> second, base class: no relation, new model class: no relation
first:
In both approaches , the actual <m> classifier / predictor </m> is learned in a separate step using the feature representation learned by autoencoder ( s ) .
second:
First , we propose a <m> fully trainable attribute - based neural network </m> founded upon the CNN + RNN architecture , that can be applied to multiple V2L problems .


class: first -> second, base class: no relation, new model class: no relation
first:
As in Fig 2 ( top ) , our network has an encoder stage and a <m> predictor stage </m> .
second:
The <m> fully connected predictor </m> as above can not make full use of such natural geometric statistics , since each point is predicted independently .


class: first -> second, base class: no relation, new model class: no relation
first:
As in Fig 2 ( top ) , our network has an encoder stage and a <m> predictor stage </m> .
second:
ABSTRACT A general approach to building an effective justification process around a multi <m> — attribute decision model </m> is suggested which incorporates factory performance measurements to focus on difficult to quantify strategic and tactical benefits from Computer Integrated Manufacturing .


class: first -> second, base class: no relation, new model class: no relation
first:
As in Fig 2 ( top ) , our network has an encoder stage and a <m> predictor stage </m> .
second:
First , we propose a <m> fully trainable attribute - based neural network </m> founded upon the CNN + RNN architecture , that can be applied to multiple V2L problems .


class: first -> second, base class: no relation, new model class: no relation
first:
However , SVM has a weakness for parameter selection or suitable <m> feature </m> .
second:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .


class: first -> second, base class: no relation, new model class: no relation
first:
However , SVM has a weakness for parameter selection or suitable <m> feature </m> .
second:
AttKGCN first builds a <m> directed attribute KG </m> whose nodes denote attributes and edges encode the co-occurrence relationships of different attributes .


class: first -> second, base class: no relation, new model class: no relation
first:
However , SVM has a weakness for parameter selection or suitable <m> feature </m> .
second:
With our GHM - C loss , the huge number of very easy examples are largely down - weighted and the outliers are slightly down - weighted as well , which simultaneously addresses the <m> attribute imbalance problem </m> and the outliers problem .


class: first -> second, base class: second -> first, new model class: no relation
first:
In prediction problems on networks this means that one has to construct a <m> feature vector representation </m> for the nodes and edges .
second:
subsection : Locality Sensitive Hashing ( LSH ) An alternative approach to BASS and DISCO is to use well - established <m> feature generation methods </m> that are agnostic about the type of input they receive .


class: first -> second, base class: second -> first, new model class: no relation
first:
In prediction problems on networks this means that one has to construct a <m> feature vector representation </m> for the nodes and edges .
second:
subsection : Unimodal Feature Extraction For a fair comparison with the state - of - the - art method , conversational memory networks ( CMN ) , we follow identical <m> feature extraction procedures </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In prediction problems on networks this means that one has to construct a <m> feature vector representation </m> for the nodes and edges .
second:
When used as <m> generic feature extractors </m> , feature aggregation techniques designed for hand - crafted features can also work with CNN embeddings and achieve competitive performance .


class: first -> second, base class: no relation, new model class: no relation
first:
In prediction problems on networks this means that one has to construct a <m> feature vector representation </m> for the nodes and edges .
second:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .


class: first -> second, base class: no relation, new model class: no relation
first:
In prediction problems on networks this means that one has to construct a <m> feature vector representation </m> for the nodes and edges .
second:
AttKGCN first builds a <m> directed attribute KG </m> whose nodes denote attributes and edges encode the co-occurrence relationships of different attributes .


class: first -> second, base class: no relation, new model class: no relation
first:
In prediction problems on networks this means that one has to construct a <m> feature vector representation </m> for the nodes and edges .
second:
With our GHM - C loss , the huge number of very easy examples are largely down - weighted and the outliers are slightly down - weighted as well , which simultaneously addresses the <m> attribute imbalance problem </m> and the outliers problem .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : Locality Sensitive Hashing ( LSH ) An alternative approach to BASS and DISCO is to use well - established <m> feature generation methods </m> that are agnostic about the type of input they receive .
second:
subsection : Multi - scale feature maps The main goal of constructing <m> multi - scale feature maps </m> is to add more context without increasing the computational cost .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : Locality Sensitive Hashing ( LSH ) An alternative approach to BASS and DISCO is to use well - established <m> feature generation methods </m> that are agnostic about the type of input they receive .
second:
Fang et al . won the 2015 COCO Captioning Challenge with an approach that is similar to ours in as much as it applies a visual concept ( i.e. , <m> attribute ) detection process </m> before generating sentences .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : Locality Sensitive Hashing ( LSH ) An alternative approach to BASS and DISCO is to use well - established <m> feature generation methods </m> that are agnostic about the type of input they receive .
second:
Since we formulate the <m> attribute prediction </m> as a multi - label problem , our attributes prediction network can be replaced by any other multi - label classification framework and it also can be benefit from the development of the multi - label classification researches .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : Unimodal Feature Extraction For a fair comparison with the state - of - the - art method , conversational memory networks ( CMN ) , we follow identical <m> feature extraction procedures </m> .
second:
Fang et al . won the 2015 COCO Captioning Challenge with an approach that is similar to ours in as much as it applies a visual concept ( i.e. , <m> attribute ) detection process </m> before generating sentences .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : Unimodal Feature Extraction For a fair comparison with the state - of - the - art method , conversational memory networks ( CMN ) , we follow identical <m> feature extraction procedures </m> .
second:
Since we formulate the <m> attribute prediction </m> as a multi - label problem , our attributes prediction network can be replaced by any other multi - label classification framework and it also can be benefit from the development of the multi - label classification researches .


class: first -> second, base class: no relation, new model class: no relation
first:
When used as <m> generic feature extractors </m> , feature aggregation techniques designed for hand - crafted features can also work with CNN embeddings and achieve competitive performance .
second:
Fang et al . won the 2015 COCO Captioning Challenge with an approach that is similar to ours in as much as it applies a visual concept ( i.e. , <m> attribute ) detection process </m> before generating sentences .


class: first -> second, base class: no relation, new model class: no relation
first:
When used as <m> generic feature extractors </m> , feature aggregation techniques designed for hand - crafted features can also work with CNN embeddings and achieve competitive performance .
second:
Since we formulate the <m> attribute prediction </m> as a multi - label problem , our attributes prediction network can be replaced by any other multi - label classification framework and it also can be benefit from the development of the multi - label classification researches .


class: first -> second, base class: no relation, new model class: no relation
first:
This would amount to an additional <m> feature level </m> GAN loss ( see Figure [ reference ] orange portion ) : Taken together , these loss functions form our complete objective : This ultimately corresponds to solving for a target model according to the optimization problem We have introduced a method for unsupervised adaptation which generalizes adversarial objectives to be viewed as operating at the pixel or feature level .
second:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .


class: first -> second, base class: no relation, new model class: no relation
first:
This would amount to an additional <m> feature level </m> GAN loss ( see Figure [ reference ] orange portion ) : Taken together , these loss functions form our complete objective : This ultimately corresponds to solving for a target model according to the optimization problem We have introduced a method for unsupervised adaptation which generalizes adversarial objectives to be viewed as operating at the pixel or feature level .
second:
AttKGCN first builds a <m> directed attribute KG </m> whose nodes denote attributes and edges encode the co-occurrence relationships of different attributes .


class: first -> second, base class: no relation, new model class: no relation
first:
This would amount to an additional <m> feature level </m> GAN loss ( see Figure [ reference ] orange portion ) : Taken together , these loss functions form our complete objective : This ultimately corresponds to solving for a target model according to the optimization problem We have introduced a method for unsupervised adaptation which generalizes adversarial objectives to be viewed as operating at the pixel or feature level .
second:
With our GHM - C loss , the huge number of very easy examples are largely down - weighted and the outliers are slightly down - weighted as well , which simultaneously addresses the <m> attribute imbalance problem </m> and the outliers problem .


class: first -> second, base class: no relation, new model class: no relation
first:
The authors show that if the trail arrival process is modeled by three <m> independent random variables </m> and the decay factor is modeled as a random process , the observed distribution functions for the amplitude and duration of underdense meteor trails can be more accurately synthesized .
second:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .


class: first -> second, base class: no relation, new model class: no relation
first:
The authors show that if the trail arrival process is modeled by three <m> independent random variables </m> and the decay factor is modeled as a random process , the observed distribution functions for the amplitude and duration of underdense meteor trails can be more accurately synthesized .
second:
AttKGCN first builds a <m> directed attribute KG </m> whose nodes denote attributes and edges encode the co-occurrence relationships of different attributes .


class: first -> second, base class: no relation, new model class: no relation
first:
The authors show that if the trail arrival process is modeled by three <m> independent random variables </m> and the decay factor is modeled as a random process , the observed distribution functions for the amplitude and duration of underdense meteor trails can be more accurately synthesized .
second:
With our GHM - C loss , the huge number of very easy examples are largely down - weighted and the outliers are slightly down - weighted as well , which simultaneously addresses the <m> attribute imbalance problem </m> and the outliers problem .


class: first -> second, base class: no relation, new model class: no relation
first:
The empirical results show that both the confirmation data of the <m> independent variable system </m> and the confirmation data of the dependent variable have the satisfying reliability .
second:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .


class: first -> second, base class: no relation, new model class: no relation
first:
The empirical results show that both the confirmation data of the <m> independent variable system </m> and the confirmation data of the dependent variable have the satisfying reliability .
second:
AttKGCN first builds a <m> directed attribute KG </m> whose nodes denote attributes and edges encode the co-occurrence relationships of different attributes .


class: first -> second, base class: no relation, new model class: no relation
first:
The empirical results show that both the confirmation data of the <m> independent variable system </m> and the confirmation data of the dependent variable have the satisfying reliability .
second:
With our GHM - C loss , the huge number of very easy examples are largely down - weighted and the outliers are slightly down - weighted as well , which simultaneously addresses the <m> attribute imbalance problem </m> and the outliers problem .


class: first -> second, base class: no relation, new model class: no relation
first:
We pursue two separate analyses : an <m> analysis ofindependent variables </m> , in which we tie each study to a particular guiding learning theory in an attempt to determine which guiding theory has had the most predictive success ; and an analysis of dependent variables , which enables us to determine which measurement techniques have been most sensitive to the learning benefits of AV technology .
second:
Thus , <m> one variable analysis </m> is used in order to see the value of the central tendency and then tested by using the rate value .


class: first -> second, base class: no relation, new model class: no relation
first:
We pursue two separate analyses : an <m> analysis ofindependent variables </m> , in which we tie each study to a particular guiding learning theory in an attempt to determine which guiding theory has had the most predictive success ; and an analysis of dependent variables , which enables us to determine which measurement techniques have been most sensitive to the learning benefits of AV technology .
second:
The data types of software defect attributes are discussed , and then the basic analysis methods of software defect data , including <m> one-variable data analysis </m> and multiple-variable data analysis , are proposed .


class: first -> second, base class: no relation, new model class: no relation
first:
We pursue two separate analyses : an <m> analysis ofindependent variables </m> , in which we tie each study to a particular guiding learning theory in an attempt to determine which guiding theory has had the most predictive success ; and an analysis of dependent variables , which enables us to determine which measurement techniques have been most sensitive to the learning benefits of AV technology .
second:
The <m> single random variable case </m> is treated here .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Up to date in the area of feature extraction , image feature representation method based on the spatial gradient is still lacking in efficiency especially for the <m> covariate case </m> like carrying bag and wearing a coat .
second:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .


class: first -> second, base class: no relation, new model class: no relation
first:
Up to date in the area of feature extraction , image feature representation method based on the spatial gradient is still lacking in efficiency especially for the <m> covariate case </m> like carrying bag and wearing a coat .
second:
AttKGCN first builds a <m> directed attribute KG </m> whose nodes denote attributes and edges encode the co-occurrence relationships of different attributes .


class: first -> second, base class: no relation, new model class: no relation
first:
Up to date in the area of feature extraction , image feature representation method based on the spatial gradient is still lacking in efficiency especially for the <m> covariate case </m> like carrying bag and wearing a coat .
second:
With our GHM - C loss , the huge number of very easy examples are largely down - weighted and the outliers are slightly down - weighted as well , which simultaneously addresses the <m> attribute imbalance problem </m> and the outliers problem .


class: first -> second, base class: no relation, new model class: no relation
first:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .
second:
Despite well-developed literature the routine practical use of many current status data modeling methods remains infrequent due to the lack of specialized statistical software , the difficulty to assess model goodness-of-fit , as well as the possible loss of information caused by <m> covariate grouping </m> or discretization .


class: first -> second, base class: no relation, new model class: no relation
first:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .
second:
subsection : Batch Normalization Review Batch Normalization has been introduced in as an effective tool to reduce <m> internal covariate shift </m> in deep networks and accelerate the training process .


class: first -> second, base class: no relation, new model class: no relation
first:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .
second:
One of the most widely used approaches to estimating ATE is <m> covariate adjustment </m> , also known as back - door adjustment or the G - computation formula .


class: first -> second, base class: no relation, new model class: no relation
first:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .
second:
Therefore , <m> covariate adjustment methods </m> are the most natural candidates for estimating ITE as well as ATE , using the estimates of .


class: first -> second, base class: no relation, new model class: no relation
first:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .
second:
subsection : Batch Normalization Batch normalization has been recently proposed in the machine learning community and addresses the so - called <m> internal covariate shift problem </m> by normalizing the mean and the variance of each layer ’s pre - activations for each training mini - batch .


class: first -> second, base class: no relation, new model class: no relation
first:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .
second:
This method was proved to be equal to <m> attribute reduction by discernable matrix </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .
second:
The paper introduces a SVD method to reduce dimension of original dataset and makes use of the <m> attribute of LSA technique </m> to combine SVD method with LSA technique , and then presents new methods for dual private protection data mining .


class: first -> second, base class: no relation, new model class: no relation
first:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .
second:
The present paper proposes an organized and smart fault classifier with a combination of reliable preprocessing technique for <m> key attribute selection </m> from the power system recorded waveforms and employs a dependable decision tree based classification algorithm to acquire fault detection precision , even when the power network under consideration is large .


class: first -> second, base class: no relation, new model class: no relation
first:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .
second:
Finally , comparative experiments on different real-life data sets are conducted to demonstrate the effectiveness and efficiency of the proposed incremental algorithms for <m> updating attribute reducts </m> with the variation of multiple objects in incomplete decision systems .


class: first -> second, base class: no relation, new model class: no relation
first:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .
second:
Compared with conventional perceptual loss , ours is more effective in computation , tailored to our specific <m> attribute transfer task </m> , and can serve as a kind of hidden - layer supervision or regularization to ease the training of the DIAT model .


class: first -> second, base class: no relation, new model class: no relation
first:
I should like to express my gratitude to Keith Rapley ( now Innovation Manager ) at British Airways for arranging sponsorship of this work , to Mark Raskino and Neil Morrison for early stimulation of ideas concerning neural networks ; and to Dr Maurice Barr , Louis Busuttil , Paul Summerbell , Dr Himadri Chatterjee and Rupert Blackley for making possible the forecasting and <m> attribute </m> work .
second:
AttKGCN first builds a <m> directed attribute KG </m> whose nodes denote attributes and edges encode the co-occurrence relationships of different attributes .


class: first -> second, base class: no relation, new model class: no relation
first:
Fang et al . won the 2015 COCO Captioning Challenge with an approach that is similar to ours in as much as it applies a visual concept ( i.e. , <m> attribute ) detection process </m> before generating sentences .
second:
As for in the <m> attribute ration regularization </m> , we set it to be ( i ) for small local attributes ( e . g . , mouth ) , ( ii ) for large local attributes ( e . g . , eyes ) , and ( iii ) for global attributes ( e . g . , gender and age ) .


class: first -> second, base class: same cluster, new model class: no relation
first:
Fang et al . won the 2015 COCO Captioning Challenge with an approach that is similar to ours in as much as it applies a visual concept ( i.e. , <m> attribute ) detection process </m> before generating sentences .
second:
There have been works on first learning <m> attribute classifiers </m> and using attribute predictions for face recognition .


class: first -> second, base class: no relation, new model class: same cluster
first:
Since we formulate the <m> attribute prediction </m> as a multi - label problem , our attributes prediction network can be replaced by any other multi - label classification framework and it also can be benefit from the development of the multi - label classification researches .
second:
<m> Adversarial attribute loss </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Since we formulate the <m> attribute prediction </m> as a multi - label problem , our attributes prediction network can be replaced by any other multi - label classification framework and it also can be benefit from the development of the multi - label classification researches .
second:
As for in the <m> attribute ration regularization </m> , we set it to be ( i ) for small local attributes ( e . g . , mouth ) , ( ii ) for large local attributes ( e . g . , eyes ) , and ( iii ) for global attributes ( e . g . , gender and age ) .


class: first -> second, base class: same cluster, new model class: no relation
first:
Since we formulate the <m> attribute prediction </m> as a multi - label problem , our attributes prediction network can be replaced by any other multi - label classification framework and it also can be benefit from the development of the multi - label classification researches .
second:
There have been works on first learning <m> attribute classifiers </m> and using attribute predictions for face recognition .


class: first -> second, base class: no relation, new model class: same cluster
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
They are averaging , Karmarker 's linear programming algorithm , <m> MLP combiner </m> and fuzzy neural network .


class: first -> second, base class: no relation, new model class: same cluster
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
Further experiment shows that using <m> MLP merging </m> and decision , the problem of variation of thresholds can be solved to some extent , and the robustness of the system can be improved effectively .


class: first -> second, base class: no relation, new model class: same cluster
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
They are averaging , Karmarker 's linear programming algorithm , <m> MLP combiner </m> and fuzzy neural network .


class: first -> second, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
We evaluate the proposed learning rule by training <m> deep spiking multi-layer perceptron ( MLP ) </m> and spiking convolutional neural network ( CNN ) on the UCI machine learning and MNIST handwritten digit datasets .


class: first -> second, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
They are averaging , Karmarker 's linear programming algorithm , <m> MLP combiner </m> and fuzzy neural network .


class: first -> second, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
Further experiment shows that using <m> MLP merging </m> and decision , the problem of variation of thresholds can be solved to some extent , and the robustness of the system can be improved effectively .


class: first -> second, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :


class: first -> second, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .


class: first -> second, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
We evaluate the proposed learning rule by training <m> deep spiking multi-layer perceptron ( MLP ) </m> and spiking convolutional neural network ( CNN ) on the UCI machine learning and MNIST handwritten digit datasets .


class: first -> second, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
They are averaging , Karmarker 's linear programming algorithm , <m> MLP combiner </m> and fuzzy neural network .


class: first -> second, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
Further experiment shows that using <m> MLP merging </m> and decision , the problem of variation of thresholds can be solved to some extent , and the robustness of the system can be improved effectively .


class: first -> second, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .


class: first -> second, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :


class: first -> second, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .


class: first -> second, base class: no relation, new model class: same cluster
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
Further experiment shows that using <m> MLP merging </m> and decision , the problem of variation of thresholds can be solved to some extent , and the robustness of the system can be improved effectively .


class: first -> second, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :


class: first -> second, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .


class: first -> second, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .


class: first -> second, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .


class: first -> second, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .


class: first -> second, base class: second -> first, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: first -> second, base class: no relation, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: first -> second, base class: no relation, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: first -> second, base class: no relation, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: first -> second, base class: no relation, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: first -> second, base class: no relation, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: first -> second, base class: no relation, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: first -> second, base class: no relation, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: first -> second, base class: no relation, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: first -> second, base class: no relation, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: first -> second, base class: second -> first, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: first -> second, base class: no relation, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: first -> second, base class: second -> first, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: first -> second, base class: second -> first, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: first -> second, base class: no relation, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: first -> second, base class: no relation, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: first -> second, base class: no relation, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: first -> second, base class: no relation, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: first -> second, base class: no relation, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: first -> second, base class: no relation, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: first -> second, base class: no relation, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: first -> second, base class: no relation, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: first -> second, base class: no relation, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: first -> second, base class: second -> first, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: first -> second, base class: no relation, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: first -> second, base class: no relation, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: first -> second, base class: no relation, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: first -> second, base class: no relation, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: first -> second, base class: no relation, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: first -> second, base class: no relation, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: first -> second, base class: no relation, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: first -> second, base class: no relation, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: first -> second, base class: no relation, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: first -> second, base class: second -> first, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: first -> second, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: first -> second, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: first -> second, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: first -> second, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: first -> second, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: first -> second, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: first -> second, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: first -> second, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: first -> second, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: first -> second, base class: second -> first, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: first -> second, base class: no relation, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: first -> second, base class: no relation, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: first -> second, base class: no relation, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: first -> second, base class: no relation, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: first -> second, base class: no relation, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: first -> second, base class: no relation, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: first -> second, base class: no relation, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: first -> second, base class: no relation, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: first -> second, base class: no relation, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: first -> second, base class: no relation, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: first -> second, base class: second -> first, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: first -> second, base class: no relation, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: first -> second, base class: no relation, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: first -> second, base class: no relation, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: first -> second, base class: no relation, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: first -> second, base class: no relation, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: first -> second, base class: no relation, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: first -> second, base class: no relation, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: first -> second, base class: no relation, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: first -> second, base class: no relation, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: first -> second, base class: no relation, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: first -> second, base class: no relation, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: first -> second, base class: no relation, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: first -> second, base class: no relation, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: first -> second, base class: no relation, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: first -> second, base class: no relation, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: first -> second, base class: no relation, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: first -> second, base class: no relation, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: first -> second, base class: no relation, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: first -> second, base class: no relation, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: first -> second, base class: no relation, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: first -> second, base class: no relation, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: first -> second, base class: no relation, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: first -> second, base class: no relation, new model class: same cluster
first:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: first -> second, base class: no relation, new model class: no relation
first:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: first -> second, base class: no relation, new model class: same cluster
first:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: first -> second, base class: no relation, new model class: no relation
first:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: first -> second, base class: no relation, new model class: no relation
first:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: first -> second, base class: no relation, new model class: no relation
first:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: first -> second, base class: no relation, new model class: no relation
first:
A stochastic resonance behavior is detected for the <m> processing neuron </m> in correspondence with the " ghost " frequencies both in the harmonic and in the anharmonic case .
second:
Recent years have seen substantial effort being put in the development of algorithms for the <m> systematic evaluation and optimization of neuron models </m> with respect to electrophysiological data .


class: first -> second, base class: no relation, new model class: no relation
first:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: first -> second, base class: no relation, new model class: no relation
first:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: first -> second, base class: no relation, new model class: no relation
first:
Results indicate that , the stochastic resonance response of the double layers FHN neuron network is better than the <m> single FHN neuron model </m> , and has better stability , and can be effectively detected for input signal at a wider range of noise intensity .
second:
Recent years have seen substantial effort being put in the development of algorithms for the <m> systematic evaluation and optimization of neuron models </m> with respect to electrophysiological data .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Here we review recent progress in our understanding of how intrinsic correlations arise in simple <m> biophysically justified neuron models </m> .
second:
Recent years have seen substantial effort being put in the development of algorithms for the <m> systematic evaluation and optimization of neuron models </m> with respect to electrophysiological data .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: first -> second, base class: no relation, new model class: no relation
first:
In this algorithm , the <m> aggregation computation of process neuron </m> can be simplified and the complex integration procedure can be avoided , and the training of process neuron networks can equate to the training of common networks , namely the arbitrary function approximating problems are converted to the function optimization problems .
second:
Recent years have seen substantial effort being put in the development of algorithms for the <m> systematic evaluation and optimization of neuron models </m> with respect to electrophysiological data .


class: first -> second, base class: no relation, new model class: no relation
first:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .
second:
In particular , “ transfer ” means that DNN weights trained on large - scale data can be used in other tasks by two <m> light - weight neuron operations </m> : Scaling and Shifting ( SS ) , i.e. .


class: first -> second, base class: no relation, new model class: no relation
first:
We empirically showed that DLDL produces robust and competitive performances than traditional <m> classification or regression deep models </m> on several popular visual recognition tasks .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: first -> second, base class: no relation, new model class: no relation
first:
We evaluate the accessibility of our approach with a <m> user study </m> .
second:
The evaluation is conducted through a <m> user empirical study </m> based on user observation and in-depth interviews .


class: first -> second, base class: no relation, new model class: no relation
first:
We evaluate the accessibility of our approach with a <m> user study </m> .
second:
A standard <m> survey </m> was applied in five experiments .


class: first -> second, base class: no relation, new model class: no relation
first:
We evaluate the accessibility of our approach with a <m> user study </m> .
second:
In the <m> survey and exploration phase </m> , a number of studies have been proposed to carry out mineral potential mapping by integrating geochemical and geophysical explorations , geological maps , fault lines , and topographic information .


class: first -> second, base class: no relation, new model class: no relation
first:
We evaluate the accessibility of our approach with a <m> user study </m> .
second:
The other two research studies in this trilogy used ( 1 ) <m> focus group approach </m> and ( 2 ) survey research to identify such impacting factors .


class: first -> second, base class: no relation, new model class: no relation
first:
We evaluate the accessibility of our approach with a <m> user study </m> .
second:
In this paper , a user study was designed to be conducted to validate behavioral archetypes of these groups through interviews and <m> focus group sessions </m> with different physicians , therapists and caregivers in rehabilitation centers .


class: first -> second, base class: no relation, new model class: no relation
first:
We evaluate the accessibility of our approach with a <m> user study </m> .
second:
The determination of cluster competitive advantages is carried out by the method of <m> focus group conduct </m> among experts , and their quantitative assessment is carried out using the questionnaire method , which gives the assessment of implementation and importance .


class: first -> second, base class: no relation, new model class: no relation
first:
We evaluate the accessibility of our approach with a <m> user study </m> .
second:
The discussion includes a review of the library literature on <m> focus group use </m> , practical aspects of focus group methodology , and the benefits of employing professionals where librarian expertise is low .


class: first -> second, base class: no relation, new model class: no relation
first:
We evaluate the accessibility of our approach with a <m> user study </m> .
second:
The paper finally presents a <m> user experiment </m> , carried out with 30 participants according to a between-subject protocol , to evaluate the usability of ImAtHome and compare it with the official app for home automation recently released by Apple .


class: first -> second, base class: no relation, new model class: no relation
first:
We evaluate the accessibility of our approach with a <m> user study </m> .
second:
This tutorial is for researchers and engineers , working in the field of Virtual Reality ( VR ) and Augmented Reality ( AR ) , who wish to conduct <m> user-based experiments </m> with a specific aim of promoting both traditional quantitative human-subject experiments and qualitative methods for assessing usability .


class: first -> second, base class: no relation, new model class: no relation
first:
We evaluate the accessibility of our approach with a <m> user study </m> .
second:
Keyword Outdoor Mixed-Reality , Augmented-Reality annotation , <m> User experiment 1 </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We evaluate the accessibility of our approach with a <m> user study </m> .
second:
We evaluate four interactive segmentation algorithms using these strategies , and compare the results with our previous <m> user experiment-based evaluation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The usability and ease of learning of this web interface were evaluated in a <m> user-based study </m> , the results of which are also reported .
second:
The evaluation is conducted through a <m> user empirical study </m> based on user observation and in-depth interviews .


class: first -> second, base class: no relation, new model class: no relation
first:
The usability and ease of learning of this web interface were evaluated in a <m> user-based study </m> , the results of which are also reported .
second:
A standard <m> survey </m> was applied in five experiments .


class: first -> second, base class: no relation, new model class: no relation
first:
The usability and ease of learning of this web interface were evaluated in a <m> user-based study </m> , the results of which are also reported .
second:
In the <m> survey and exploration phase </m> , a number of studies have been proposed to carry out mineral potential mapping by integrating geochemical and geophysical explorations , geological maps , fault lines , and topographic information .


class: first -> second, base class: no relation, new model class: no relation
first:
The usability and ease of learning of this web interface were evaluated in a <m> user-based study </m> , the results of which are also reported .
second:
The other two research studies in this trilogy used ( 1 ) <m> focus group approach </m> and ( 2 ) survey research to identify such impacting factors .


class: first -> second, base class: no relation, new model class: no relation
first:
The usability and ease of learning of this web interface were evaluated in a <m> user-based study </m> , the results of which are also reported .
second:
In this paper , a user study was designed to be conducted to validate behavioral archetypes of these groups through interviews and <m> focus group sessions </m> with different physicians , therapists and caregivers in rehabilitation centers .


class: first -> second, base class: no relation, new model class: no relation
first:
The usability and ease of learning of this web interface were evaluated in a <m> user-based study </m> , the results of which are also reported .
second:
The determination of cluster competitive advantages is carried out by the method of <m> focus group conduct </m> among experts , and their quantitative assessment is carried out using the questionnaire method , which gives the assessment of implementation and importance .


class: first -> second, base class: no relation, new model class: no relation
first:
The usability and ease of learning of this web interface were evaluated in a <m> user-based study </m> , the results of which are also reported .
second:
The discussion includes a review of the library literature on <m> focus group use </m> , practical aspects of focus group methodology , and the benefits of employing professionals where librarian expertise is low .


class: first -> second, base class: no relation, new model class: no relation
first:
The usability and ease of learning of this web interface were evaluated in a <m> user-based study </m> , the results of which are also reported .
second:
The paper finally presents a <m> user experiment </m> , carried out with 30 participants according to a between-subject protocol , to evaluate the usability of ImAtHome and compare it with the official app for home automation recently released by Apple .


class: first -> second, base class: no relation, new model class: no relation
first:
The usability and ease of learning of this web interface were evaluated in a <m> user-based study </m> , the results of which are also reported .
second:
This tutorial is for researchers and engineers , working in the field of Virtual Reality ( VR ) and Augmented Reality ( AR ) , who wish to conduct <m> user-based experiments </m> with a specific aim of promoting both traditional quantitative human-subject experiments and qualitative methods for assessing usability .


class: first -> second, base class: no relation, new model class: no relation
first:
The usability and ease of learning of this web interface were evaluated in a <m> user-based study </m> , the results of which are also reported .
second:
Keyword Outdoor Mixed-Reality , Augmented-Reality annotation , <m> User experiment 1 </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The usability and ease of learning of this web interface were evaluated in a <m> user-based study </m> , the results of which are also reported .
second:
We evaluate four interactive segmentation algorithms using these strategies , and compare the results with our previous <m> user experiment-based evaluation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The experimental results demonstrated that our work outperformed the state-of-the-art methods not only in several objective quality measures but also in a <m> user study analysis </m> .
second:
The evaluation is conducted through a <m> user empirical study </m> based on user observation and in-depth interviews .


class: first -> second, base class: no relation, new model class: no relation
first:
The experimental results demonstrated that our work outperformed the state-of-the-art methods not only in several objective quality measures but also in a <m> user study analysis </m> .
second:
A standard <m> survey </m> was applied in five experiments .


class: first -> second, base class: no relation, new model class: no relation
first:
The experimental results demonstrated that our work outperformed the state-of-the-art methods not only in several objective quality measures but also in a <m> user study analysis </m> .
second:
In the <m> survey and exploration phase </m> , a number of studies have been proposed to carry out mineral potential mapping by integrating geochemical and geophysical explorations , geological maps , fault lines , and topographic information .


class: first -> second, base class: no relation, new model class: no relation
first:
The experimental results demonstrated that our work outperformed the state-of-the-art methods not only in several objective quality measures but also in a <m> user study analysis </m> .
second:
The other two research studies in this trilogy used ( 1 ) <m> focus group approach </m> and ( 2 ) survey research to identify such impacting factors .


class: first -> second, base class: no relation, new model class: no relation
first:
The experimental results demonstrated that our work outperformed the state-of-the-art methods not only in several objective quality measures but also in a <m> user study analysis </m> .
second:
In this paper , a user study was designed to be conducted to validate behavioral archetypes of these groups through interviews and <m> focus group sessions </m> with different physicians , therapists and caregivers in rehabilitation centers .


class: first -> second, base class: no relation, new model class: no relation
first:
The experimental results demonstrated that our work outperformed the state-of-the-art methods not only in several objective quality measures but also in a <m> user study analysis </m> .
second:
The determination of cluster competitive advantages is carried out by the method of <m> focus group conduct </m> among experts , and their quantitative assessment is carried out using the questionnaire method , which gives the assessment of implementation and importance .


class: first -> second, base class: no relation, new model class: no relation
first:
The experimental results demonstrated that our work outperformed the state-of-the-art methods not only in several objective quality measures but also in a <m> user study analysis </m> .
second:
The discussion includes a review of the library literature on <m> focus group use </m> , practical aspects of focus group methodology , and the benefits of employing professionals where librarian expertise is low .


class: first -> second, base class: no relation, new model class: no relation
first:
The experimental results demonstrated that our work outperformed the state-of-the-art methods not only in several objective quality measures but also in a <m> user study analysis </m> .
second:
The paper finally presents a <m> user experiment </m> , carried out with 30 participants according to a between-subject protocol , to evaluate the usability of ImAtHome and compare it with the official app for home automation recently released by Apple .


class: first -> second, base class: no relation, new model class: no relation
first:
The experimental results demonstrated that our work outperformed the state-of-the-art methods not only in several objective quality measures but also in a <m> user study analysis </m> .
second:
This tutorial is for researchers and engineers , working in the field of Virtual Reality ( VR ) and Augmented Reality ( AR ) , who wish to conduct <m> user-based experiments </m> with a specific aim of promoting both traditional quantitative human-subject experiments and qualitative methods for assessing usability .


class: first -> second, base class: no relation, new model class: no relation
first:
The experimental results demonstrated that our work outperformed the state-of-the-art methods not only in several objective quality measures but also in a <m> user study analysis </m> .
second:
Keyword Outdoor Mixed-Reality , Augmented-Reality annotation , <m> User experiment 1 </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The experimental results demonstrated that our work outperformed the state-of-the-art methods not only in several objective quality measures but also in a <m> user study analysis </m> .
second:
We evaluate four interactive segmentation algorithms using these strategies , and compare the results with our previous <m> user experiment-based evaluation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The main contributions of this paper are two-fold and materialize into i ) a <m> user research study </m> that covers aspects such as perception , cognition , mental and psychosocial changes that occur with age and ii ) an example-based description of the process of creating personas .
second:
The evaluation is conducted through a <m> user empirical study </m> based on user observation and in-depth interviews .


class: first -> second, base class: no relation, new model class: no relation
first:
The main contributions of this paper are two-fold and materialize into i ) a <m> user research study </m> that covers aspects such as perception , cognition , mental and psychosocial changes that occur with age and ii ) an example-based description of the process of creating personas .
second:
A standard <m> survey </m> was applied in five experiments .


class: first -> second, base class: no relation, new model class: no relation
first:
The main contributions of this paper are two-fold and materialize into i ) a <m> user research study </m> that covers aspects such as perception , cognition , mental and psychosocial changes that occur with age and ii ) an example-based description of the process of creating personas .
second:
In the <m> survey and exploration phase </m> , a number of studies have been proposed to carry out mineral potential mapping by integrating geochemical and geophysical explorations , geological maps , fault lines , and topographic information .


class: first -> second, base class: no relation, new model class: no relation
first:
The main contributions of this paper are two-fold and materialize into i ) a <m> user research study </m> that covers aspects such as perception , cognition , mental and psychosocial changes that occur with age and ii ) an example-based description of the process of creating personas .
second:
The other two research studies in this trilogy used ( 1 ) <m> focus group approach </m> and ( 2 ) survey research to identify such impacting factors .


class: first -> second, base class: no relation, new model class: no relation
first:
The main contributions of this paper are two-fold and materialize into i ) a <m> user research study </m> that covers aspects such as perception , cognition , mental and psychosocial changes that occur with age and ii ) an example-based description of the process of creating personas .
second:
In this paper , a user study was designed to be conducted to validate behavioral archetypes of these groups through interviews and <m> focus group sessions </m> with different physicians , therapists and caregivers in rehabilitation centers .


class: first -> second, base class: no relation, new model class: no relation
first:
The main contributions of this paper are two-fold and materialize into i ) a <m> user research study </m> that covers aspects such as perception , cognition , mental and psychosocial changes that occur with age and ii ) an example-based description of the process of creating personas .
second:
The determination of cluster competitive advantages is carried out by the method of <m> focus group conduct </m> among experts , and their quantitative assessment is carried out using the questionnaire method , which gives the assessment of implementation and importance .


class: first -> second, base class: no relation, new model class: no relation
first:
The main contributions of this paper are two-fold and materialize into i ) a <m> user research study </m> that covers aspects such as perception , cognition , mental and psychosocial changes that occur with age and ii ) an example-based description of the process of creating personas .
second:
The discussion includes a review of the library literature on <m> focus group use </m> , practical aspects of focus group methodology , and the benefits of employing professionals where librarian expertise is low .


class: first -> second, base class: no relation, new model class: no relation
first:
The main contributions of this paper are two-fold and materialize into i ) a <m> user research study </m> that covers aspects such as perception , cognition , mental and psychosocial changes that occur with age and ii ) an example-based description of the process of creating personas .
second:
The paper finally presents a <m> user experiment </m> , carried out with 30 participants according to a between-subject protocol , to evaluate the usability of ImAtHome and compare it with the official app for home automation recently released by Apple .


class: first -> second, base class: no relation, new model class: no relation
first:
The main contributions of this paper are two-fold and materialize into i ) a <m> user research study </m> that covers aspects such as perception , cognition , mental and psychosocial changes that occur with age and ii ) an example-based description of the process of creating personas .
second:
This tutorial is for researchers and engineers , working in the field of Virtual Reality ( VR ) and Augmented Reality ( AR ) , who wish to conduct <m> user-based experiments </m> with a specific aim of promoting both traditional quantitative human-subject experiments and qualitative methods for assessing usability .


class: first -> second, base class: no relation, new model class: no relation
first:
The main contributions of this paper are two-fold and materialize into i ) a <m> user research study </m> that covers aspects such as perception , cognition , mental and psychosocial changes that occur with age and ii ) an example-based description of the process of creating personas .
second:
Keyword Outdoor Mixed-Reality , Augmented-Reality annotation , <m> User experiment 1 </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The main contributions of this paper are two-fold and materialize into i ) a <m> user research study </m> that covers aspects such as perception , cognition , mental and psychosocial changes that occur with age and ii ) an example-based description of the process of creating personas .
second:
We evaluate four interactive segmentation algorithms using these strategies , and compare the results with our previous <m> user experiment-based evaluation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In the core of our paper , we survey colormap generation techniques , including the latest advances in the field by grouping these techniques into four classes : procedural methods , <m> user-study based methods </m> , rule-based methods , and data-driven methods ; we also include a section on methods that are beyond pure data comprehension purposes .
second:
A standard <m> survey </m> was applied in five experiments .


class: first -> second, base class: no relation, new model class: no relation
first:
In the core of our paper , we survey colormap generation techniques , including the latest advances in the field by grouping these techniques into four classes : procedural methods , <m> user-study based methods </m> , rule-based methods , and data-driven methods ; we also include a section on methods that are beyond pure data comprehension purposes .
second:
In the <m> survey and exploration phase </m> , a number of studies have been proposed to carry out mineral potential mapping by integrating geochemical and geophysical explorations , geological maps , fault lines , and topographic information .


class: first -> second, base class: no relation, new model class: no relation
first:
In the core of our paper , we survey colormap generation techniques , including the latest advances in the field by grouping these techniques into four classes : procedural methods , <m> user-study based methods </m> , rule-based methods , and data-driven methods ; we also include a section on methods that are beyond pure data comprehension purposes .
second:
The other two research studies in this trilogy used ( 1 ) <m> focus group approach </m> and ( 2 ) survey research to identify such impacting factors .


class: first -> second, base class: no relation, new model class: no relation
first:
In the core of our paper , we survey colormap generation techniques , including the latest advances in the field by grouping these techniques into four classes : procedural methods , <m> user-study based methods </m> , rule-based methods , and data-driven methods ; we also include a section on methods that are beyond pure data comprehension purposes .
second:
In this paper , a user study was designed to be conducted to validate behavioral archetypes of these groups through interviews and <m> focus group sessions </m> with different physicians , therapists and caregivers in rehabilitation centers .


class: first -> second, base class: no relation, new model class: no relation
first:
In the core of our paper , we survey colormap generation techniques , including the latest advances in the field by grouping these techniques into four classes : procedural methods , <m> user-study based methods </m> , rule-based methods , and data-driven methods ; we also include a section on methods that are beyond pure data comprehension purposes .
second:
The determination of cluster competitive advantages is carried out by the method of <m> focus group conduct </m> among experts , and their quantitative assessment is carried out using the questionnaire method , which gives the assessment of implementation and importance .


class: first -> second, base class: no relation, new model class: no relation
first:
In the core of our paper , we survey colormap generation techniques , including the latest advances in the field by grouping these techniques into four classes : procedural methods , <m> user-study based methods </m> , rule-based methods , and data-driven methods ; we also include a section on methods that are beyond pure data comprehension purposes .
second:
The discussion includes a review of the library literature on <m> focus group use </m> , practical aspects of focus group methodology , and the benefits of employing professionals where librarian expertise is low .


class: first -> second, base class: no relation, new model class: no relation
first:
In the core of our paper , we survey colormap generation techniques , including the latest advances in the field by grouping these techniques into four classes : procedural methods , <m> user-study based methods </m> , rule-based methods , and data-driven methods ; we also include a section on methods that are beyond pure data comprehension purposes .
second:
The paper finally presents a <m> user experiment </m> , carried out with 30 participants according to a between-subject protocol , to evaluate the usability of ImAtHome and compare it with the official app for home automation recently released by Apple .


class: first -> second, base class: no relation, new model class: no relation
first:
In the core of our paper , we survey colormap generation techniques , including the latest advances in the field by grouping these techniques into four classes : procedural methods , <m> user-study based methods </m> , rule-based methods , and data-driven methods ; we also include a section on methods that are beyond pure data comprehension purposes .
second:
This tutorial is for researchers and engineers , working in the field of Virtual Reality ( VR ) and Augmented Reality ( AR ) , who wish to conduct <m> user-based experiments </m> with a specific aim of promoting both traditional quantitative human-subject experiments and qualitative methods for assessing usability .


class: first -> second, base class: no relation, new model class: no relation
first:
In the core of our paper , we survey colormap generation techniques , including the latest advances in the field by grouping these techniques into four classes : procedural methods , <m> user-study based methods </m> , rule-based methods , and data-driven methods ; we also include a section on methods that are beyond pure data comprehension purposes .
second:
Keyword Outdoor Mixed-Reality , Augmented-Reality annotation , <m> User experiment 1 </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In the core of our paper , we survey colormap generation techniques , including the latest advances in the field by grouping these techniques into four classes : procedural methods , <m> user-study based methods </m> , rule-based methods , and data-driven methods ; we also include a section on methods that are beyond pure data comprehension purposes .
second:
We evaluate four interactive segmentation algorithms using these strategies , and compare the results with our previous <m> user experiment-based evaluation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The laboratory setting allows evaluators to create not only the environment , but also the scenario in which a <m> user study of system </m> is conducted .
second:
A standard <m> survey </m> was applied in five experiments .


class: first -> second, base class: no relation, new model class: no relation
first:
The laboratory setting allows evaluators to create not only the environment , but also the scenario in which a <m> user study of system </m> is conducted .
second:
In the <m> survey and exploration phase </m> , a number of studies have been proposed to carry out mineral potential mapping by integrating geochemical and geophysical explorations , geological maps , fault lines , and topographic information .


class: first -> second, base class: no relation, new model class: no relation
first:
The laboratory setting allows evaluators to create not only the environment , but also the scenario in which a <m> user study of system </m> is conducted .
second:
The other two research studies in this trilogy used ( 1 ) <m> focus group approach </m> and ( 2 ) survey research to identify such impacting factors .


class: first -> second, base class: no relation, new model class: no relation
first:
The laboratory setting allows evaluators to create not only the environment , but also the scenario in which a <m> user study of system </m> is conducted .
second:
In this paper , a user study was designed to be conducted to validate behavioral archetypes of these groups through interviews and <m> focus group sessions </m> with different physicians , therapists and caregivers in rehabilitation centers .


class: first -> second, base class: no relation, new model class: no relation
first:
The laboratory setting allows evaluators to create not only the environment , but also the scenario in which a <m> user study of system </m> is conducted .
second:
The determination of cluster competitive advantages is carried out by the method of <m> focus group conduct </m> among experts , and their quantitative assessment is carried out using the questionnaire method , which gives the assessment of implementation and importance .


class: first -> second, base class: no relation, new model class: no relation
first:
The laboratory setting allows evaluators to create not only the environment , but also the scenario in which a <m> user study of system </m> is conducted .
second:
The discussion includes a review of the library literature on <m> focus group use </m> , practical aspects of focus group methodology , and the benefits of employing professionals where librarian expertise is low .


class: first -> second, base class: no relation, new model class: no relation
first:
The laboratory setting allows evaluators to create not only the environment , but also the scenario in which a <m> user study of system </m> is conducted .
second:
The paper finally presents a <m> user experiment </m> , carried out with 30 participants according to a between-subject protocol , to evaluate the usability of ImAtHome and compare it with the official app for home automation recently released by Apple .


class: first -> second, base class: no relation, new model class: no relation
first:
The laboratory setting allows evaluators to create not only the environment , but also the scenario in which a <m> user study of system </m> is conducted .
second:
This tutorial is for researchers and engineers , working in the field of Virtual Reality ( VR ) and Augmented Reality ( AR ) , who wish to conduct <m> user-based experiments </m> with a specific aim of promoting both traditional quantitative human-subject experiments and qualitative methods for assessing usability .


class: first -> second, base class: no relation, new model class: no relation
first:
The laboratory setting allows evaluators to create not only the environment , but also the scenario in which a <m> user study of system </m> is conducted .
second:
Keyword Outdoor Mixed-Reality , Augmented-Reality annotation , <m> User experiment 1 </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The laboratory setting allows evaluators to create not only the environment , but also the scenario in which a <m> user study of system </m> is conducted .
second:
We evaluate four interactive segmentation algorithms using these strategies , and compare the results with our previous <m> user experiment-based evaluation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we present the results of a <m> user study of a technique </m> used to pair two devices ( such as two cell phones ) which have good quality output interfaces in the form of a display , speaker , and/or vibration .
second:
A standard <m> survey </m> was applied in five experiments .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we present the results of a <m> user study of a technique </m> used to pair two devices ( such as two cell phones ) which have good quality output interfaces in the form of a display , speaker , and/or vibration .
second:
In the <m> survey and exploration phase </m> , a number of studies have been proposed to carry out mineral potential mapping by integrating geochemical and geophysical explorations , geological maps , fault lines , and topographic information .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we present the results of a <m> user study of a technique </m> used to pair two devices ( such as two cell phones ) which have good quality output interfaces in the form of a display , speaker , and/or vibration .
second:
The other two research studies in this trilogy used ( 1 ) <m> focus group approach </m> and ( 2 ) survey research to identify such impacting factors .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we present the results of a <m> user study of a technique </m> used to pair two devices ( such as two cell phones ) which have good quality output interfaces in the form of a display , speaker , and/or vibration .
second:
In this paper , a user study was designed to be conducted to validate behavioral archetypes of these groups through interviews and <m> focus group sessions </m> with different physicians , therapists and caregivers in rehabilitation centers .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we present the results of a <m> user study of a technique </m> used to pair two devices ( such as two cell phones ) which have good quality output interfaces in the form of a display , speaker , and/or vibration .
second:
The determination of cluster competitive advantages is carried out by the method of <m> focus group conduct </m> among experts , and their quantitative assessment is carried out using the questionnaire method , which gives the assessment of implementation and importance .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we present the results of a <m> user study of a technique </m> used to pair two devices ( such as two cell phones ) which have good quality output interfaces in the form of a display , speaker , and/or vibration .
second:
The discussion includes a review of the library literature on <m> focus group use </m> , practical aspects of focus group methodology , and the benefits of employing professionals where librarian expertise is low .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we present the results of a <m> user study of a technique </m> used to pair two devices ( such as two cell phones ) which have good quality output interfaces in the form of a display , speaker , and/or vibration .
second:
The paper finally presents a <m> user experiment </m> , carried out with 30 participants according to a between-subject protocol , to evaluate the usability of ImAtHome and compare it with the official app for home automation recently released by Apple .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we present the results of a <m> user study of a technique </m> used to pair two devices ( such as two cell phones ) which have good quality output interfaces in the form of a display , speaker , and/or vibration .
second:
This tutorial is for researchers and engineers , working in the field of Virtual Reality ( VR ) and Augmented Reality ( AR ) , who wish to conduct <m> user-based experiments </m> with a specific aim of promoting both traditional quantitative human-subject experiments and qualitative methods for assessing usability .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we present the results of a <m> user study of a technique </m> used to pair two devices ( such as two cell phones ) which have good quality output interfaces in the form of a display , speaker , and/or vibration .
second:
Keyword Outdoor Mixed-Reality , Augmented-Reality annotation , <m> User experiment 1 </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we present the results of a <m> user study of a technique </m> used to pair two devices ( such as two cell phones ) which have good quality output interfaces in the form of a display , speaker , and/or vibration .
second:
We evaluate four interactive segmentation algorithms using these strategies , and compare the results with our previous <m> user experiment-based evaluation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
System evaluation and <m> user study research </m> thus can play a significant role in infectious disease information research .
second:
A standard <m> survey </m> was applied in five experiments .


class: first -> second, base class: no relation, new model class: no relation
first:
System evaluation and <m> user study research </m> thus can play a significant role in infectious disease information research .
second:
In the <m> survey and exploration phase </m> , a number of studies have been proposed to carry out mineral potential mapping by integrating geochemical and geophysical explorations , geological maps , fault lines , and topographic information .


class: first -> second, base class: no relation, new model class: no relation
first:
System evaluation and <m> user study research </m> thus can play a significant role in infectious disease information research .
second:
The other two research studies in this trilogy used ( 1 ) <m> focus group approach </m> and ( 2 ) survey research to identify such impacting factors .


class: first -> second, base class: no relation, new model class: no relation
first:
System evaluation and <m> user study research </m> thus can play a significant role in infectious disease information research .
second:
In this paper , a user study was designed to be conducted to validate behavioral archetypes of these groups through interviews and <m> focus group sessions </m> with different physicians , therapists and caregivers in rehabilitation centers .


class: first -> second, base class: no relation, new model class: no relation
first:
System evaluation and <m> user study research </m> thus can play a significant role in infectious disease information research .
second:
The determination of cluster competitive advantages is carried out by the method of <m> focus group conduct </m> among experts , and their quantitative assessment is carried out using the questionnaire method , which gives the assessment of implementation and importance .


class: first -> second, base class: no relation, new model class: no relation
first:
System evaluation and <m> user study research </m> thus can play a significant role in infectious disease information research .
second:
The discussion includes a review of the library literature on <m> focus group use </m> , practical aspects of focus group methodology , and the benefits of employing professionals where librarian expertise is low .


class: first -> second, base class: no relation, new model class: no relation
first:
System evaluation and <m> user study research </m> thus can play a significant role in infectious disease information research .
second:
The paper finally presents a <m> user experiment </m> , carried out with 30 participants according to a between-subject protocol , to evaluate the usability of ImAtHome and compare it with the official app for home automation recently released by Apple .


class: first -> second, base class: no relation, new model class: no relation
first:
System evaluation and <m> user study research </m> thus can play a significant role in infectious disease information research .
second:
This tutorial is for researchers and engineers , working in the field of Virtual Reality ( VR ) and Augmented Reality ( AR ) , who wish to conduct <m> user-based experiments </m> with a specific aim of promoting both traditional quantitative human-subject experiments and qualitative methods for assessing usability .


class: first -> second, base class: no relation, new model class: no relation
first:
System evaluation and <m> user study research </m> thus can play a significant role in infectious disease information research .
second:
Keyword Outdoor Mixed-Reality , Augmented-Reality annotation , <m> User experiment 1 </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
System evaluation and <m> user study research </m> thus can play a significant role in infectious disease information research .
second:
We evaluate four interactive segmentation algorithms using these strategies , and compare the results with our previous <m> user experiment-based evaluation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
To evaluate the green navigation service efficiency , we conducted a <m> user subject study </m> consisting of 22 users driving different vehicles over the course of several months in Urbana-Champaign , IL .
second:
A standard <m> survey </m> was applied in five experiments .


class: first -> second, base class: no relation, new model class: no relation
first:
To evaluate the green navigation service efficiency , we conducted a <m> user subject study </m> consisting of 22 users driving different vehicles over the course of several months in Urbana-Champaign , IL .
second:
In the <m> survey and exploration phase </m> , a number of studies have been proposed to carry out mineral potential mapping by integrating geochemical and geophysical explorations , geological maps , fault lines , and topographic information .


class: first -> second, base class: no relation, new model class: no relation
first:
To evaluate the green navigation service efficiency , we conducted a <m> user subject study </m> consisting of 22 users driving different vehicles over the course of several months in Urbana-Champaign , IL .
second:
The other two research studies in this trilogy used ( 1 ) <m> focus group approach </m> and ( 2 ) survey research to identify such impacting factors .


class: first -> second, base class: no relation, new model class: no relation
first:
To evaluate the green navigation service efficiency , we conducted a <m> user subject study </m> consisting of 22 users driving different vehicles over the course of several months in Urbana-Champaign , IL .
second:
In this paper , a user study was designed to be conducted to validate behavioral archetypes of these groups through interviews and <m> focus group sessions </m> with different physicians , therapists and caregivers in rehabilitation centers .


class: first -> second, base class: no relation, new model class: no relation
first:
To evaluate the green navigation service efficiency , we conducted a <m> user subject study </m> consisting of 22 users driving different vehicles over the course of several months in Urbana-Champaign , IL .
second:
The determination of cluster competitive advantages is carried out by the method of <m> focus group conduct </m> among experts , and their quantitative assessment is carried out using the questionnaire method , which gives the assessment of implementation and importance .


class: first -> second, base class: no relation, new model class: no relation
first:
To evaluate the green navigation service efficiency , we conducted a <m> user subject study </m> consisting of 22 users driving different vehicles over the course of several months in Urbana-Champaign , IL .
second:
The discussion includes a review of the library literature on <m> focus group use </m> , practical aspects of focus group methodology , and the benefits of employing professionals where librarian expertise is low .


class: first -> second, base class: no relation, new model class: no relation
first:
To evaluate the green navigation service efficiency , we conducted a <m> user subject study </m> consisting of 22 users driving different vehicles over the course of several months in Urbana-Champaign , IL .
second:
The paper finally presents a <m> user experiment </m> , carried out with 30 participants according to a between-subject protocol , to evaluate the usability of ImAtHome and compare it with the official app for home automation recently released by Apple .


class: first -> second, base class: no relation, new model class: no relation
first:
To evaluate the green navigation service efficiency , we conducted a <m> user subject study </m> consisting of 22 users driving different vehicles over the course of several months in Urbana-Champaign , IL .
second:
This tutorial is for researchers and engineers , working in the field of Virtual Reality ( VR ) and Augmented Reality ( AR ) , who wish to conduct <m> user-based experiments </m> with a specific aim of promoting both traditional quantitative human-subject experiments and qualitative methods for assessing usability .


class: first -> second, base class: no relation, new model class: no relation
first:
To evaluate the green navigation service efficiency , we conducted a <m> user subject study </m> consisting of 22 users driving different vehicles over the course of several months in Urbana-Champaign , IL .
second:
Keyword Outdoor Mixed-Reality , Augmented-Reality annotation , <m> User experiment 1 </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
To evaluate the green navigation service efficiency , we conducted a <m> user subject study </m> consisting of 22 users driving different vehicles over the course of several months in Urbana-Champaign , IL .
second:
We evaluate four interactive segmentation algorithms using these strategies , and compare the results with our previous <m> user experiment-based evaluation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
For realism , we conduct a <m> user study </m> using pairwise comparison .
second:
A standard <m> survey </m> was applied in five experiments .


class: first -> second, base class: no relation, new model class: no relation
first:
For realism , we conduct a <m> user study </m> using pairwise comparison .
second:
In the <m> survey and exploration phase </m> , a number of studies have been proposed to carry out mineral potential mapping by integrating geochemical and geophysical explorations , geological maps , fault lines , and topographic information .


class: first -> second, base class: no relation, new model class: no relation
first:
For realism , we conduct a <m> user study </m> using pairwise comparison .
second:
The other two research studies in this trilogy used ( 1 ) <m> focus group approach </m> and ( 2 ) survey research to identify such impacting factors .


class: first -> second, base class: no relation, new model class: no relation
first:
For realism , we conduct a <m> user study </m> using pairwise comparison .
second:
In this paper , a user study was designed to be conducted to validate behavioral archetypes of these groups through interviews and <m> focus group sessions </m> with different physicians , therapists and caregivers in rehabilitation centers .


class: first -> second, base class: no relation, new model class: no relation
first:
For realism , we conduct a <m> user study </m> using pairwise comparison .
second:
The determination of cluster competitive advantages is carried out by the method of <m> focus group conduct </m> among experts , and their quantitative assessment is carried out using the questionnaire method , which gives the assessment of implementation and importance .


class: first -> second, base class: no relation, new model class: no relation
first:
For realism , we conduct a <m> user study </m> using pairwise comparison .
second:
The discussion includes a review of the library literature on <m> focus group use </m> , practical aspects of focus group methodology , and the benefits of employing professionals where librarian expertise is low .


class: first -> second, base class: no relation, new model class: no relation
first:
For realism , we conduct a <m> user study </m> using pairwise comparison .
second:
The paper finally presents a <m> user experiment </m> , carried out with 30 participants according to a between-subject protocol , to evaluate the usability of ImAtHome and compare it with the official app for home automation recently released by Apple .


class: first -> second, base class: no relation, new model class: no relation
first:
For realism , we conduct a <m> user study </m> using pairwise comparison .
second:
This tutorial is for researchers and engineers , working in the field of Virtual Reality ( VR ) and Augmented Reality ( AR ) , who wish to conduct <m> user-based experiments </m> with a specific aim of promoting both traditional quantitative human-subject experiments and qualitative methods for assessing usability .


class: first -> second, base class: no relation, new model class: no relation
first:
For realism , we conduct a <m> user study </m> using pairwise comparison .
second:
Keyword Outdoor Mixed-Reality , Augmented-Reality annotation , <m> User experiment 1 </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
For realism , we conduct a <m> user study </m> using pairwise comparison .
second:
We evaluate four interactive segmentation algorithms using these strategies , and compare the results with our previous <m> user experiment-based evaluation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
A standard <m> survey </m> was applied in five experiments .
second:
Finally , it is shown how <m> survey studies of word order </m> can be relevant to forming initial hypotheses about certain properties of grammar , and how the study of language typology might proceed within the generative framework .


class: first -> second, base class: no relation, new model class: no relation
first:
A standard <m> survey </m> was applied in five experiments .
second:
Case studies include a survey of an Iron Age fort , a <m> rapid survey of exposed segments </m> of an intertidal wreck , both commissioned for heritage management purposes and a community survey of a 17th century gravestone undertaken by children under the age of 16 .


class: first -> second, base class: no relation, new model class: no relation
first:
A standard <m> survey </m> was applied in five experiments .
second:
The thirteen possible interviewees were sorted into different types of users according to a ) their opinions of the automated scorer ( taken from <m> post-assignment survey </m> ) , b ) actual essay scores assigned by the instructor , and c ) average number of drafts they completed per response section for both cases .


class: first -> second, base class: no relation, new model class: no relation
first:
A standard <m> survey </m> was applied in five experiments .
second:
The <m> baseline behavioural survey </m> identified an increasing general awareness of open access , but a lower awareness of institutional and subject repositories .


class: first -> second, base class: no relation, new model class: no relation
first:
In the <m> survey and exploration phase </m> , a number of studies have been proposed to carry out mineral potential mapping by integrating geochemical and geophysical explorations , geological maps , fault lines , and topographic information .
second:
Nowadays , attempts have been made to automatically infer transportation modes from positional data , such as the data collected by using GPS devices so that the cost in time and budget of conventional <m> travel diary survey </m> could be significantly reduced .


class: first -> second, base class: no relation, new model class: no relation
first:
In the <m> survey and exploration phase </m> , a number of studies have been proposed to carry out mineral potential mapping by integrating geochemical and geophysical explorations , geological maps , fault lines , and topographic information .
second:
Among these practices are GPS - based travel surveys , where participants carry a GPS device for a certain duration of time and following this up by a <m> prompt recall survey </m> to report trip information , such as the transportation modes they used in every trip ( e.g. cycle , walk , bus and so forth ) ( Stopher , 2008 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
In the <m> survey and exploration phase </m> , a number of studies have been proposed to carry out mineral potential mapping by integrating geochemical and geophysical explorations , geological maps , fault lines , and topographic information .
second:
<m> Travel survey definitions </m> In order to de - construct a GPS track , some definitions have been standardised to be used for the description of different fragments of the trip .


class: first -> second, base class: no relation, new model class: no relation
first:
Design/methodology/approach – After reviewing the existing literature , particularly focusing on QA frameworks , procedures , and methodology , a process‐oriented model structured around three sequential non‐linear phases is presented : before : planning and analysis ; during : design , <m> prototype </m> and production ; and after : post‐production and delivery .
second:
The design of an <m> integrated DVCO prototype </m> is discussed and results are reported showing good performance in terms of phase noise , tuning range and power consumption .


class: first -> second, base class: no relation, new model class: no relation
first:
Design/methodology/approach – After reviewing the existing literature , particularly focusing on QA frameworks , procedures , and methodology , a process‐oriented model structured around three sequential non‐linear phases is presented : before : planning and analysis ; during : design , <m> prototype </m> and production ; and after : post‐production and delivery .
second:
This has been implemented in a <m> prototype sweep-line library </m> for Coloured Petri nets .


class: first -> second, base class: no relation, new model class: no relation
first:
Design/methodology/approach – After reviewing the existing literature , particularly focusing on QA frameworks , procedures , and methodology , a process‐oriented model structured around three sequential non‐linear phases is presented : before : planning and analysis ; during : design , <m> prototype </m> and production ; and after : post‐production and delivery .
second:
A feasibility study undertaken in simulation showed that with the use of a conservative power management scheme , the DC S-P hydraulic hybrid excavator with 50 % reduced engine power , offered efficiency improvements over the <m> prototype DC excavator </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Then , it describes robust sparse PCA which can be efficiently used in <m> feature learning </m> .
second:
The experiments show promising results over other <m> feature based learning approaches </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Then , it describes robust sparse PCA which can be efficiently used in <m> feature learning </m> .
second:
The big data trend has inspired <m> feature-driven learning tasks </m> , which can not be handled by conventional machine learning models .


class: first -> second, base class: second -> first, new model class: same cluster
first:
Then , it describes robust sparse PCA which can be efficiently used in <m> feature learning </m> .
second:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .


class: first -> second, base class: second -> first, new model class: same cluster
first:
Then , it describes robust sparse PCA which can be efficiently used in <m> feature learning </m> .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: first -> second, base class: second -> first, new model class: same cluster
first:
Then , it describes robust sparse PCA which can be efficiently used in <m> feature learning </m> .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Specifically , it employs an adversarial learning guided multi-label attention module to enhance the <m> feature learning part </m> which can learn discriminative feature representations and keep the cross-modal invariability .
second:
The big data trend has inspired <m> feature-driven learning tasks </m> , which can not be handled by conventional machine learning models .


class: first -> second, base class: second -> first, new model class: same cluster
first:
Specifically , it employs an adversarial learning guided multi-label attention module to enhance the <m> feature learning part </m> which can learn discriminative feature representations and keep the cross-modal invariability .
second:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .


class: first -> second, base class: second -> first, new model class: same cluster
first:
Specifically , it employs an adversarial learning guided multi-label attention module to enhance the <m> feature learning part </m> which can learn discriminative feature representations and keep the cross-modal invariability .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: first -> second, base class: second -> first, new model class: same cluster
first:
Specifically , it employs an adversarial learning guided multi-label attention module to enhance the <m> feature learning part </m> which can learn discriminative feature representations and keep the cross-modal invariability .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: second -> first, new model class: same cluster
first:
<m> Feature learning methods </m> based on the Skip - gram architecture have been originally developed in the context of natural language .
second:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .


class: first -> second, base class: second -> first, new model class: same cluster
first:
<m> Feature learning methods </m> based on the Skip - gram architecture have been originally developed in the context of natural language .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: first -> second, base class: second -> first, new model class: same cluster
first:
<m> Feature learning methods </m> based on the Skip - gram architecture have been originally developed in the context of natural language .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: second -> first, new model class: same cluster
first:
We find that this simple <m> feature learning algorithm </m> is surprisingly successful when applied to visual object recognition .
second:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .


class: first -> second, base class: second -> first, new model class: same cluster
first:
We find that this simple <m> feature learning algorithm </m> is surprisingly successful when applied to visual object recognition .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: first -> second, base class: second -> first, new model class: same cluster
first:
We find that this simple <m> feature learning algorithm </m> is surprisingly successful when applied to visual object recognition .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: second -> first, new model class: same cluster
first:
We formulate such a dual path architecture as follows : where and denote the extracted information at - th step from individual path , is a <m> feature learning function </m> as .
second:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .


class: first -> second, base class: second -> first, new model class: same cluster
first:
We formulate such a dual path architecture as follows : where and denote the extracted information at - th step from individual path , is a <m> feature learning function </m> as .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: first -> second, base class: second -> first, new model class: same cluster
first:
We formulate such a dual path architecture as follows : where and denote the extracted information at - th step from individual path , is a <m> feature learning function </m> as .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: second -> first, new model class: same cluster
first:
In this paper , we address the <m> feature learning problem </m> in DL by presenting a new algorithm , deeply - supervised nets ( DSN ) , which enforces direct and early supervision for both the hidden layers and the output layer .
second:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .


class: first -> second, base class: second -> first, new model class: same cluster
first:
In this paper , we address the <m> feature learning problem </m> in DL by presenting a new algorithm , deeply - supervised nets ( DSN ) , which enforces direct and early supervision for both the hidden layers and the output layer .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: first -> second, base class: second -> first, new model class: same cluster
first:
In this paper , we address the <m> feature learning problem </m> in DL by presenting a new algorithm , deeply - supervised nets ( DSN ) , which enforces direct and early supervision for both the hidden layers and the output layer .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: second -> first, new model class: same cluster
first:
The process of training a model 's parameters such that a few gradient steps , or even a single gradient step , can produce good results on a new task can be viewed from a <m> feature learning standpoint </m> as building an internal representation that is broadly suitable for many tasks .
second:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .


class: first -> second, base class: second -> first, new model class: same cluster
first:
The process of training a model 's parameters such that a few gradient steps , or even a single gradient step , can produce good results on a new task can be viewed from a <m> feature learning standpoint </m> as building an internal representation that is broadly suitable for many tasks .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: first -> second, base class: second -> first, new model class: same cluster
first:
The process of training a model 's parameters such that a few gradient steps , or even a single gradient step , can produce good results on a new task can be viewed from a <m> feature learning standpoint </m> as building an internal representation that is broadly suitable for many tasks .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: second -> first, new model class: same cluster
first:
In recent years , deep neural networks ( DNNs ) have become successful in computer vision , speech recognition , and natural language processing with their great power of <m> feature representation learning </m> .
second:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .


class: first -> second, base class: second -> first, new model class: same cluster
first:
In recent years , deep neural networks ( DNNs ) have become successful in computer vision , speech recognition , and natural language processing with their great power of <m> feature representation learning </m> .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: first -> second, base class: second -> first, new model class: same cluster
first:
In recent years , deep neural networks ( DNNs ) have become successful in computer vision , speech recognition , and natural language processing with their great power of <m> feature representation learning </m> .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: second -> first, new model class: same cluster
first:
With the increasing sample size of ReID dataset , the <m> learning of features </m> from multi - class person identification tasks , denoted as ID - discriminative Embedding ( IDE ) , has shown great potentials on current large - scale person ReID datasets , such as MARS and PRW , where the IDE features are taken from the last hidden layer of Deep Convolutional Neural Networks ( DCNN ) .
second:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .


class: first -> second, base class: second -> first, new model class: same cluster
first:
With the increasing sample size of ReID dataset , the <m> learning of features </m> from multi - class person identification tasks , denoted as ID - discriminative Embedding ( IDE ) , has shown great potentials on current large - scale person ReID datasets , such as MARS and PRW , where the IDE features are taken from the last hidden layer of Deep Convolutional Neural Networks ( DCNN ) .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: first -> second, base class: second -> first, new model class: same cluster
first:
With the increasing sample size of ReID dataset , the <m> learning of features </m> from multi - class person identification tasks , denoted as ID - discriminative Embedding ( IDE ) , has shown great potentials on current large - scale person ReID datasets , such as MARS and PRW , where the IDE features are taken from the last hidden layer of Deep Convolutional Neural Networks ( DCNN ) .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: second -> first, new model class: same cluster
first:
Thus , it is significant to explicitly involve the semantic boundary to guide the <m> learning of the features </m> .
second:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .


class: first -> second, base class: second -> first, new model class: same cluster
first:
Thus , it is significant to explicitly involve the semantic boundary to guide the <m> learning of the features </m> .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: first -> second, base class: second -> first, new model class: same cluster
first:
Thus , it is significant to explicitly involve the semantic boundary to guide the <m> learning of the features </m> .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .


class: first -> second, base class: same cluster, new model class: same cluster
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: no relation, new model class: no relation
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .


class: first -> second, base class: no relation, new model class: no relation
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .


class: first -> second, base class: no relation, new model class: no relation
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .


class: first -> second, base class: no relation, new model class: no relation
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .


class: first -> second, base class: no relation, new model class: no relation
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
We review how data-analysis concepts and techniques developed by computational neuroscientists can be useful for analyzing representations in DNNs , and in turn , how recently developed techniques for <m> analysis of DNNs </m> can be useful for understanding representations in biological neural networks .


class: first -> second, base class: no relation, new model class: no relation
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
Accordingly , techniques that enable efficient <m> processing of DNNs </m> to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI system .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: first -> second, base class: no relation, new model class: no relation
first:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .
second:
They demonstrate large improvements in reducing the number of parameters of the output softmax layer , but only modest improvements for the <m> hidden fully connected layers </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: first -> second, base class: no relation, new model class: no relation
first:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .
second:
The resulting feature vectors are then scored using a non - linear function , namely a multi - layer perceptron with <m> one hidden layer </m> ( MLP ) :


class: first -> second, base class: no relation, new model class: no relation
first:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .
second:
Very recently , Ma et al. ( ICLR 2018 ) proposed to use local intrinsic dimensionality ( LID ) in <m> layer-wise hidden representations of DNNs </m> to study adversarial subspaces .


class: first -> second, base class: no relation, new model class: no relation
first:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .
second:
Our work is also related to the recent efforts in visualizing the <m> internal representations of DNNs </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .
second:
They demonstrate large improvements in reducing the number of parameters of the output softmax layer , but only modest improvements for the <m> hidden fully connected layers </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: first -> second, base class: no relation, new model class: no relation
first:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .
second:
The resulting feature vectors are then scored using a non - linear function , namely a multi - layer perceptron with <m> one hidden layer </m> ( MLP ) :


class: first -> second, base class: no relation, new model class: no relation
first:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .
second:
Very recently , Ma et al. ( ICLR 2018 ) proposed to use local intrinsic dimensionality ( LID ) in <m> layer-wise hidden representations of DNNs </m> to study adversarial subspaces .


class: first -> second, base class: no relation, new model class: no relation
first:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .
second:
Our work is also related to the recent efforts in visualizing the <m> internal representations of DNNs </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .
second:
They demonstrate large improvements in reducing the number of parameters of the output softmax layer , but only modest improvements for the <m> hidden fully connected layers </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .
second:
The resulting feature vectors are then scored using a non - linear function , namely a multi - layer perceptron with <m> one hidden layer </m> ( MLP ) :


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .
second:
Very recently , Ma et al. ( ICLR 2018 ) proposed to use local intrinsic dimensionality ( LID ) in <m> layer-wise hidden representations of DNNs </m> to study adversarial subspaces .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .
second:
Our work is also related to the recent efforts in visualizing the <m> internal representations of DNNs </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The added advantage of <m> hidden layer </m> for lip reading is that it takes into account the nonlinear variation of lip features while speaking .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The added advantage of <m> hidden layer </m> for lip reading is that it takes into account the nonlinear variation of lip features while speaking .
second:
The resulting feature vectors are then scored using a non - linear function , namely a multi - layer perceptron with <m> one hidden layer </m> ( MLP ) :


class: first -> second, base class: no relation, new model class: no relation
first:
The added advantage of <m> hidden layer </m> for lip reading is that it takes into account the nonlinear variation of lip features while speaking .
second:
Very recently , Ma et al. ( ICLR 2018 ) proposed to use local intrinsic dimensionality ( LID ) in <m> layer-wise hidden representations of DNNs </m> to study adversarial subspaces .


class: first -> second, base class: no relation, new model class: no relation
first:
The added advantage of <m> hidden layer </m> for lip reading is that it takes into account the nonlinear variation of lip features while speaking .
second:
Our work is also related to the recent efforts in visualizing the <m> internal representations of DNNs </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The experiment showed that the best <m> hidden layer architecture </m> ( HLA ) is 5 - 10 - 11 - 12 - 13 - 1 with learning function ( LF ) of trainlm , activation function ( AF ) of logsig and purelin , and learning rate ( LR ) of 0.5 .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The experiment showed that the best <m> hidden layer architecture </m> ( HLA ) is 5 - 10 - 11 - 12 - 13 - 1 with learning function ( LF ) of trainlm , activation function ( AF ) of logsig and purelin , and learning rate ( LR ) of 0.5 .
second:
The resulting feature vectors are then scored using a non - linear function , namely a multi - layer perceptron with <m> one hidden layer </m> ( MLP ) :


class: first -> second, base class: no relation, new model class: no relation
first:
The experiment showed that the best <m> hidden layer architecture </m> ( HLA ) is 5 - 10 - 11 - 12 - 13 - 1 with learning function ( LF ) of trainlm , activation function ( AF ) of logsig and purelin , and learning rate ( LR ) of 0.5 .
second:
Very recently , Ma et al. ( ICLR 2018 ) proposed to use local intrinsic dimensionality ( LID ) in <m> layer-wise hidden representations of DNNs </m> to study adversarial subspaces .


class: first -> second, base class: no relation, new model class: no relation
first:
The experiment showed that the best <m> hidden layer architecture </m> ( HLA ) is 5 - 10 - 11 - 12 - 13 - 1 with learning function ( LF ) of trainlm , activation function ( AF ) of logsig and purelin , and learning rate ( LR ) of 0.5 .
second:
Our work is also related to the recent efforts in visualizing the <m> internal representations of DNNs </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Further <m> Hidden layer 1 </m> helps to extract data to estimate the forecasting equation and also it helps for evaluating estimated Model using different measures .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Further <m> Hidden layer 1 </m> helps to extract data to estimate the forecasting equation and also it helps for evaluating estimated Model using different measures .
second:
The resulting feature vectors are then scored using a non - linear function , namely a multi - layer perceptron with <m> one hidden layer </m> ( MLP ) :


class: first -> second, base class: no relation, new model class: no relation
first:
Further <m> Hidden layer 1 </m> helps to extract data to estimate the forecasting equation and also it helps for evaluating estimated Model using different measures .
second:
Very recently , Ma et al. ( ICLR 2018 ) proposed to use local intrinsic dimensionality ( LID ) in <m> layer-wise hidden representations of DNNs </m> to study adversarial subspaces .


class: first -> second, base class: no relation, new model class: no relation
first:
Further <m> Hidden layer 1 </m> helps to extract data to estimate the forecasting equation and also it helps for evaluating estimated Model using different measures .
second:
Our work is also related to the recent efforts in visualizing the <m> internal representations of DNNs </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
where and are the question and the answer encoded vectors and denotes the output of the <m> hidden layer </m> of the MLP .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: first -> second, base class: same cluster, new model class: same cluster
first:
where and are the question and the answer encoded vectors and denotes the output of the <m> hidden layer </m> of the MLP .
second:
The resulting feature vectors are then scored using a non - linear function , namely a multi - layer perceptron with <m> one hidden layer </m> ( MLP ) :


class: first -> second, base class: no relation, new model class: no relation
first:
where and are the question and the answer encoded vectors and denotes the output of the <m> hidden layer </m> of the MLP .
second:
Very recently , Ma et al. ( ICLR 2018 ) proposed to use local intrinsic dimensionality ( LID ) in <m> layer-wise hidden representations of DNNs </m> to study adversarial subspaces .


class: first -> second, base class: no relation, new model class: no relation
first:
where and are the question and the answer encoded vectors and denotes the output of the <m> hidden layer </m> of the MLP .
second:
Our work is also related to the recent efforts in visualizing the <m> internal representations of DNNs </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Similar to our LoNGAE model , the GCN model can learn <m> hidden layer representations </m> that encode both local graph structure and features of nodes .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Similar to our LoNGAE model , the GCN model can learn <m> hidden layer representations </m> that encode both local graph structure and features of nodes .
second:
The resulting feature vectors are then scored using a non - linear function , namely a multi - layer perceptron with <m> one hidden layer </m> ( MLP ) :


class: first -> second, base class: no relation, new model class: no relation
first:
Similar to our LoNGAE model , the GCN model can learn <m> hidden layer representations </m> that encode both local graph structure and features of nodes .
second:
Very recently , Ma et al. ( ICLR 2018 ) proposed to use local intrinsic dimensionality ( LID ) in <m> layer-wise hidden representations of DNNs </m> to study adversarial subspaces .


class: first -> second, base class: no relation, new model class: no relation
first:
Similar to our LoNGAE model , the GCN model can learn <m> hidden layer representations </m> that encode both local graph structure and features of nodes .
second:
Our work is also related to the recent efforts in visualizing the <m> internal representations of DNNs </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .
second:
They demonstrate large improvements in reducing the number of parameters of the output softmax layer , but only modest improvements for the <m> hidden fully connected layers </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: first -> second, base class: no relation, new model class: no relation
first:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .
second:
The resulting feature vectors are then scored using a non - linear function , namely a multi - layer perceptron with <m> one hidden layer </m> ( MLP ) :


class: first -> second, base class: no relation, new model class: no relation
first:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .
second:
Our work is also related to the recent efforts in visualizing the <m> internal representations of DNNs </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
PNN : For the purpose of capturing high - order feature interactions , PNN imposes a product layer between the embedding layer and the <m> first hidden layer </m> .
second:
They demonstrate large improvements in reducing the number of parameters of the output softmax layer , but only modest improvements for the <m> hidden fully connected layers </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
PNN : For the purpose of capturing high - order feature interactions , PNN imposes a product layer between the embedding layer and the <m> first hidden layer </m> .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: first -> second, base class: same cluster, new model class: no relation
first:
PNN : For the purpose of capturing high - order feature interactions , PNN imposes a product layer between the embedding layer and the <m> first hidden layer </m> .
second:
The resulting feature vectors are then scored using a non - linear function , namely a multi - layer perceptron with <m> one hidden layer </m> ( MLP ) :


class: first -> second, base class: no relation, new model class: no relation
first:
PNN : For the purpose of capturing high - order feature interactions , PNN imposes a product layer between the embedding layer and the <m> first hidden layer </m> .
second:
Our work is also related to the recent efforts in visualizing the <m> internal representations of DNNs </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Their method simultaneously minimizes classification error while improving the directness and transparency of the <m> hidden layer learning process </m> .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Their method simultaneously minimizes classification error while improving the directness and transparency of the <m> hidden layer learning process </m> .
second:
The resulting feature vectors are then scored using a non - linear function , namely a multi - layer perceptron with <m> one hidden layer </m> ( MLP ) :


class: first -> second, base class: no relation, new model class: no relation
first:
Their method simultaneously minimizes classification error while improving the directness and transparency of the <m> hidden layer learning process </m> .
second:
Very recently , Ma et al. ( ICLR 2018 ) proposed to use local intrinsic dimensionality ( LID ) in <m> layer-wise hidden representations of DNNs </m> to study adversarial subspaces .


class: first -> second, base class: no relation, new model class: no relation
first:
Their method simultaneously minimizes classification error while improving the directness and transparency of the <m> hidden layer learning process </m> .
second:
Our work is also related to the recent efforts in visualizing the <m> internal representations of DNNs </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
This was found sufficient , with the following <m> hidden layers learning </m> to combine the multi - scale features .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: first -> second, base class: same cluster, new model class: same cluster
first:
This was found sufficient , with the following <m> hidden layers learning </m> to combine the multi - scale features .
second:
The resulting feature vectors are then scored using a non - linear function , namely a multi - layer perceptron with <m> one hidden layer </m> ( MLP ) :


class: first -> second, base class: no relation, new model class: no relation
first:
This was found sufficient , with the following <m> hidden layers learning </m> to combine the multi - scale features .
second:
Very recently , Ma et al. ( ICLR 2018 ) proposed to use local intrinsic dimensionality ( LID ) in <m> layer-wise hidden representations of DNNs </m> to study adversarial subspaces .


class: first -> second, base class: no relation, new model class: no relation
first:
This was found sufficient , with the following <m> hidden layers learning </m> to combine the multi - scale features .
second:
Our work is also related to the recent efforts in visualizing the <m> internal representations of DNNs </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We review how data-analysis concepts and techniques developed by computational neuroscientists can be useful for analyzing representations in DNNs , and in turn , how recently developed techniques for <m> analysis of DNNs </m> can be useful for understanding representations in biological neural networks .
second:
The <m> firstlevel DNNs </m> are bottleneck feature extractors and the secondlevel DNNs serve as not only acoustic models but also feature combination modules .


class: first -> second, base class: no relation, new model class: no relation
first:
We review how data-analysis concepts and techniques developed by computational neuroscientists can be useful for analyzing representations in DNNs , and in turn , how recently developed techniques for <m> analysis of DNNs </m> can be useful for understanding representations in biological neural networks .
second:
Our experimental results show that DAN/K-DAN outperform the present <m> S-DNNs </m> and also the BP-trained DNNs , including multiplayer perceptron , deep belief network , etc . , without data augmentation applied .


class: first -> second, base class: no relation, new model class: no relation
first:
Accordingly , techniques that enable efficient <m> processing of DNNs </m> to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI system .
second:
The <m> firstlevel DNNs </m> are bottleneck feature extractors and the secondlevel DNNs serve as not only acoustic models but also feature combination modules .


class: first -> second, base class: no relation, new model class: no relation
first:
Accordingly , techniques that enable efficient <m> processing of DNNs </m> to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI system .
second:
Our experimental results show that DAN/K-DAN outperform the present <m> S-DNNs </m> and also the BP-trained DNNs , including multiplayer perceptron , deep belief network , etc . , without data augmentation applied .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .
second:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .
second:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: first -> second, base class: same cluster, new model class: same cluster
first:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: first -> second, base class: same cluster, new model class: same cluster
first:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .
second:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: first -> second, base class: same cluster, new model class: same cluster
first:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .
second:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: first -> second, base class: same cluster, new model class: same cluster
first:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: first -> second, base class: same cluster, new model class: same cluster
first:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .
second:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: first -> second, base class: same cluster, new model class: same cluster
first:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: first -> second, base class: same cluster, new model class: same cluster
first:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: first -> second, base class: same cluster, new model class: same cluster
first:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .
second:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: first -> second, base class: same cluster, new model class: same cluster
first:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: first -> second, base class: no relation, new model class: same cluster
first:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: first -> second, base class: no relation, new model class: same cluster
first:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .
second:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .


class: first -> second, base class: no relation, new model class: same cluster
first:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: first -> second, base class: no relation, new model class: same cluster
first:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: first -> second, base class: same cluster, new model class: same cluster
first:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: first -> second, base class: same cluster, new model class: same cluster
first:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .
second:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: first -> second, base class: same cluster, new model class: same cluster
first:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: first -> second, base class: same cluster, new model class: same cluster
first:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: first -> second, base class: same cluster, new model class: same cluster
first:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .
second:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: first -> second, base class: same cluster, new model class: same cluster
first:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .
second:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .
second:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: first -> second, base class: same cluster, new model class: same cluster
first:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .
second:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: first -> second, base class: same cluster, new model class: same cluster
first:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .
second:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: first -> second, base class: same cluster, new model class: same cluster
first:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: first -> second, base class: same cluster, new model class: same cluster
first:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: first -> second, base class: same cluster, new model class: same cluster
first:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: first -> second, base class: same cluster, new model class: same cluster
first:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: first -> second, base class: same cluster, new model class: same cluster
first:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: first -> second, base class: same cluster, new model class: same cluster
first:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: first -> second, base class: second -> first, new model class: second -> first
first:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: first -> second, base class: second -> first, new model class: same cluster
first:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: first -> second, base class: same cluster, new model class: same cluster
first:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: first -> second, base class: same cluster, new model class: same cluster
first:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .
second:
Tuple-versioning of relations is the adopted method of <m> temporal data representation </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .
second:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .
second:
We claim that these refinement iterations should have a more direct and shared <m> temporal representation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .
second:
Proposed networks which perform <m> temporal sequence generation </m> are often in the form of a modification to an auto-associative memory by using heteroassociative or time-varying synaptic strengths , requiring some pre-chosen temporal functions .


class: first -> second, base class: no relation, new model class: no relation
first:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .
second:
In this work , we designed tasks around <m> temporal event sequence </m> and timing analysis and conducted a controlled experiment on Amazon Mechanical Turk to examine four sentinel event alignment approaches : no sentinel event alignment ( NoAlign ) , single-event alignment ( SingleAlign ) , dual-event alignment with left justification ( DualLeft ) , and dual-event alignment with stretch justification ( DualStretch ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .
second:
Tuple-versioning of relations is the adopted method of <m> temporal data representation </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .
second:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .
second:
We claim that these refinement iterations should have a more direct and shared <m> temporal representation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .
second:
Proposed networks which perform <m> temporal sequence generation </m> are often in the form of a modification to an auto-associative memory by using heteroassociative or time-varying synaptic strengths , requiring some pre-chosen temporal functions .


class: first -> second, base class: no relation, new model class: no relation
first:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .
second:
In this work , we designed tasks around <m> temporal event sequence </m> and timing analysis and conducted a controlled experiment on Amazon Mechanical Turk to examine four sentinel event alignment approaches : no sentinel event alignment ( NoAlign ) , single-event alignment ( SingleAlign ) , dual-event alignment with left justification ( DualLeft ) , and dual-event alignment with stretch justification ( DualStretch ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .
second:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .
second:
We claim that these refinement iterations should have a more direct and shared <m> temporal representation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .
second:
Proposed networks which perform <m> temporal sequence generation </m> are often in the form of a modification to an auto-associative memory by using heteroassociative or time-varying synaptic strengths , requiring some pre-chosen temporal functions .


class: first -> second, base class: no relation, new model class: no relation
first:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .
second:
In this work , we designed tasks around <m> temporal event sequence </m> and timing analysis and conducted a controlled experiment on Amazon Mechanical Turk to examine four sentinel event alignment approaches : no sentinel event alignment ( NoAlign ) , single-event alignment ( SingleAlign ) , dual-event alignment with left justification ( DualLeft ) , and dual-event alignment with stretch justification ( DualStretch ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .
second:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .
second:
We claim that these refinement iterations should have a more direct and shared <m> temporal representation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .
second:
Proposed networks which perform <m> temporal sequence generation </m> are often in the form of a modification to an auto-associative memory by using heteroassociative or time-varying synaptic strengths , requiring some pre-chosen temporal functions .


class: first -> second, base class: no relation, new model class: no relation
first:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .
second:
In this work , we designed tasks around <m> temporal event sequence </m> and timing analysis and conducted a controlled experiment on Amazon Mechanical Turk to examine four sentinel event alignment approaches : no sentinel event alignment ( NoAlign ) , single-event alignment ( SingleAlign ) , dual-event alignment with left justification ( DualLeft ) , and dual-event alignment with stretch justification ( DualStretch ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .
second:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .
second:
We claim that these refinement iterations should have a more direct and shared <m> temporal representation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .
second:
Proposed networks which perform <m> temporal sequence generation </m> are often in the form of a modification to an auto-associative memory by using heteroassociative or time-varying synaptic strengths , requiring some pre-chosen temporal functions .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .
second:
In this work , we designed tasks around <m> temporal event sequence </m> and timing analysis and conducted a controlled experiment on Amazon Mechanical Turk to examine four sentinel event alignment approaches : no sentinel event alignment ( NoAlign ) , single-event alignment ( SingleAlign ) , dual-event alignment with left justification ( DualLeft ) , and dual-event alignment with stretch justification ( DualStretch ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .
second:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .
second:
We claim that these refinement iterations should have a more direct and shared <m> temporal representation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .
second:
Proposed networks which perform <m> temporal sequence generation </m> are often in the form of a modification to an auto-associative memory by using heteroassociative or time-varying synaptic strengths , requiring some pre-chosen temporal functions .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .
second:
In this work , we designed tasks around <m> temporal event sequence </m> and timing analysis and conducted a controlled experiment on Amazon Mechanical Turk to examine four sentinel event alignment approaches : no sentinel event alignment ( NoAlign ) , single-event alignment ( SingleAlign ) , dual-event alignment with left justification ( DualLeft ) , and dual-event alignment with stretch justification ( DualStretch ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .
second:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .
second:
We claim that these refinement iterations should have a more direct and shared <m> temporal representation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .
second:
Proposed networks which perform <m> temporal sequence generation </m> are often in the form of a modification to an auto-associative memory by using heteroassociative or time-varying synaptic strengths , requiring some pre-chosen temporal functions .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .
second:
In this work , we designed tasks around <m> temporal event sequence </m> and timing analysis and conducted a controlled experiment on Amazon Mechanical Turk to examine four sentinel event alignment approaches : no sentinel event alignment ( NoAlign ) , single-event alignment ( SingleAlign ) , dual-event alignment with left justification ( DualLeft ) , and dual-event alignment with stretch justification ( DualStretch ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .
second:
We claim that these refinement iterations should have a more direct and shared <m> temporal representation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .
second:
Proposed networks which perform <m> temporal sequence generation </m> are often in the form of a modification to an auto-associative memory by using heteroassociative or time-varying synaptic strengths , requiring some pre-chosen temporal functions .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .
second:
In this work , we designed tasks around <m> temporal event sequence </m> and timing analysis and conducted a controlled experiment on Amazon Mechanical Turk to examine four sentinel event alignment approaches : no sentinel event alignment ( NoAlign ) , single-event alignment ( SingleAlign ) , dual-event alignment with left justification ( DualLeft ) , and dual-event alignment with stretch justification ( DualStretch ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Under DBNs , Bayesian inference provides an optimal framework for <m> sequential data analysis </m> and produces solutions potentially better than other non Bayesian-based methods .
second:
Unlike <m> sequential models </m> , recursive neural networks compose word phrases over syntactic tree structure and have shown improved performance in sentiment analysis .


class: first -> second, base class: no relation, new model class: no relation
first:
Under DBNs , Bayesian inference provides an optimal framework for <m> sequential data analysis </m> and produces solutions potentially better than other non Bayesian-based methods .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Under DBNs , Bayesian inference provides an optimal framework for <m> sequential data analysis </m> and produces solutions potentially better than other non Bayesian-based methods .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: first -> second, base class: no relation, new model class: no relation
first:
Under DBNs , Bayesian inference provides an optimal framework for <m> sequential data analysis </m> and produces solutions potentially better than other non Bayesian-based methods .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: first -> second, base class: no relation, new model class: no relation
first:
Under DBNs , Bayesian inference provides an optimal framework for <m> sequential data analysis </m> and produces solutions potentially better than other non Bayesian-based methods .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: first -> second, base class: no relation, new model class: no relation
first:
Under DBNs , Bayesian inference provides an optimal framework for <m> sequential data analysis </m> and produces solutions potentially better than other non Bayesian-based methods .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: first -> second, base class: no relation, new model class: no relation
first:
Therefore , the introduced algorithm is especially suitable for the applications requiring <m> sequential data processing </m> at large scales/high rates .
second:
Unlike <m> sequential models </m> , recursive neural networks compose word phrases over syntactic tree structure and have shown improved performance in sentiment analysis .


class: first -> second, base class: no relation, new model class: no relation
first:
Therefore , the introduced algorithm is especially suitable for the applications requiring <m> sequential data processing </m> at large scales/high rates .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Therefore , the introduced algorithm is especially suitable for the applications requiring <m> sequential data processing </m> at large scales/high rates .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: first -> second, base class: no relation, new model class: no relation
first:
Therefore , the introduced algorithm is especially suitable for the applications requiring <m> sequential data processing </m> at large scales/high rates .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: first -> second, base class: no relation, new model class: no relation
first:
Therefore , the introduced algorithm is especially suitable for the applications requiring <m> sequential data processing </m> at large scales/high rates .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: first -> second, base class: no relation, new model class: no relation
first:
Therefore , the introduced algorithm is especially suitable for the applications requiring <m> sequential data processing </m> at large scales/high rates .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: first -> second, base class: no relation, new model class: no relation
first:
However , complex enhancement algorithms also require the <m> sequential processing of data </m> and this can not be easily achieved in real-time on a GPU .
second:
Unlike <m> sequential models </m> , recursive neural networks compose word phrases over syntactic tree structure and have shown improved performance in sentiment analysis .


class: first -> second, base class: no relation, new model class: no relation
first:
However , complex enhancement algorithms also require the <m> sequential processing of data </m> and this can not be easily achieved in real-time on a GPU .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
However , complex enhancement algorithms also require the <m> sequential processing of data </m> and this can not be easily achieved in real-time on a GPU .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: first -> second, base class: no relation, new model class: no relation
first:
However , complex enhancement algorithms also require the <m> sequential processing of data </m> and this can not be easily achieved in real-time on a GPU .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: first -> second, base class: no relation, new model class: no relation
first:
However , complex enhancement algorithms also require the <m> sequential processing of data </m> and this can not be easily achieved in real-time on a GPU .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: first -> second, base class: no relation, new model class: no relation
first:
However , complex enhancement algorithms also require the <m> sequential processing of data </m> and this can not be easily achieved in real-time on a GPU .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: first -> second, base class: no relation, new model class: no relation
first:
Furthermore , we develop a solution for generating differentially private sequential data , which will bring us one step closer to publicly available medical datasets via <m> sequential data </m> .
second:
Unlike <m> sequential models </m> , recursive neural networks compose word phrases over syntactic tree structure and have shown improved performance in sentiment analysis .


class: first -> second, base class: no relation, new model class: no relation
first:
Furthermore , we develop a solution for generating differentially private sequential data , which will bring us one step closer to publicly available medical datasets via <m> sequential data </m> .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Furthermore , we develop a solution for generating differentially private sequential data , which will bring us one step closer to publicly available medical datasets via <m> sequential data </m> .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: first -> second, base class: no relation, new model class: no relation
first:
Furthermore , we develop a solution for generating differentially private sequential data , which will bring us one step closer to publicly available medical datasets via <m> sequential data </m> .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: first -> second, base class: no relation, new model class: no relation
first:
Furthermore , we develop a solution for generating differentially private sequential data , which will bring us one step closer to publicly available medical datasets via <m> sequential data </m> .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: first -> second, base class: no relation, new model class: no relation
first:
Furthermore , we develop a solution for generating differentially private sequential data , which will bring us one step closer to publicly available medical datasets via <m> sequential data </m> .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: first -> second, base class: no relation, new model class: no relation
first:
These <m> sequential data collection procedures </m> are known by different names , such as sequential experimental design , active learning , or adaptive sensing/sampling .
second:
Unlike <m> sequential models </m> , recursive neural networks compose word phrases over syntactic tree structure and have shown improved performance in sentiment analysis .


class: first -> second, base class: no relation, new model class: no relation
first:
These <m> sequential data collection procedures </m> are known by different names , such as sequential experimental design , active learning , or adaptive sensing/sampling .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
These <m> sequential data collection procedures </m> are known by different names , such as sequential experimental design , active learning , or adaptive sensing/sampling .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: first -> second, base class: no relation, new model class: no relation
first:
These <m> sequential data collection procedures </m> are known by different names , such as sequential experimental design , active learning , or adaptive sensing/sampling .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: first -> second, base class: no relation, new model class: no relation
first:
These <m> sequential data collection procedures </m> are known by different names , such as sequential experimental design , active learning , or adaptive sensing/sampling .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: first -> second, base class: no relation, new model class: no relation
first:
These <m> sequential data collection procedures </m> are known by different names , such as sequential experimental design , active learning , or adaptive sensing/sampling .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper gives a review of the most important methods for blood velocity vector flow imaging ( VFI ) for conventional <m> sequential data acquisition </m> .
second:
Unlike <m> sequential models </m> , recursive neural networks compose word phrases over syntactic tree structure and have shown improved performance in sentiment analysis .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper gives a review of the most important methods for blood velocity vector flow imaging ( VFI ) for conventional <m> sequential data acquisition </m> .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper gives a review of the most important methods for blood velocity vector flow imaging ( VFI ) for conventional <m> sequential data acquisition </m> .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper gives a review of the most important methods for blood velocity vector flow imaging ( VFI ) for conventional <m> sequential data acquisition </m> .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper gives a review of the most important methods for blood velocity vector flow imaging ( VFI ) for conventional <m> sequential data acquisition </m> .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper gives a review of the most important methods for blood velocity vector flow imaging ( VFI ) for conventional <m> sequential data acquisition </m> .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: first -> second, base class: no relation, new model class: no relation
first:
T-patterns detection analysis provides a <m> sequential analysis of data </m> , illustrating the communicative style and flow of each coach .
second:
Unlike <m> sequential models </m> , recursive neural networks compose word phrases over syntactic tree structure and have shown improved performance in sentiment analysis .


class: first -> second, base class: no relation, new model class: no relation
first:
T-patterns detection analysis provides a <m> sequential analysis of data </m> , illustrating the communicative style and flow of each coach .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
T-patterns detection analysis provides a <m> sequential analysis of data </m> , illustrating the communicative style and flow of each coach .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: first -> second, base class: no relation, new model class: no relation
first:
T-patterns detection analysis provides a <m> sequential analysis of data </m> , illustrating the communicative style and flow of each coach .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: first -> second, base class: no relation, new model class: no relation
first:
T-patterns detection analysis provides a <m> sequential analysis of data </m> , illustrating the communicative style and flow of each coach .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: first -> second, base class: no relation, new model class: no relation
first:
T-patterns detection analysis provides a <m> sequential analysis of data </m> , illustrating the communicative style and flow of each coach .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: first -> second, base class: no relation, new model class: no relation
first:
Recurrent Neural Networks ( RNNs ) capture long dependencies and context , and 2 hence are the key component of typical <m> sequential data based tasks </m> .
second:
Unlike <m> sequential models </m> , recursive neural networks compose word phrases over syntactic tree structure and have shown improved performance in sentiment analysis .


class: first -> second, base class: no relation, new model class: no relation
first:
Recurrent Neural Networks ( RNNs ) capture long dependencies and context , and 2 hence are the key component of typical <m> sequential data based tasks </m> .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Recurrent Neural Networks ( RNNs ) capture long dependencies and context , and 2 hence are the key component of typical <m> sequential data based tasks </m> .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: first -> second, base class: no relation, new model class: no relation
first:
Recurrent Neural Networks ( RNNs ) capture long dependencies and context , and 2 hence are the key component of typical <m> sequential data based tasks </m> .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: first -> second, base class: no relation, new model class: no relation
first:
Recurrent Neural Networks ( RNNs ) capture long dependencies and context , and 2 hence are the key component of typical <m> sequential data based tasks </m> .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: first -> second, base class: no relation, new model class: no relation
first:
Recurrent Neural Networks ( RNNs ) capture long dependencies and context , and 2 hence are the key component of typical <m> sequential data based tasks </m> .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: first -> second, base class: no relation, new model class: no relation
first:
We present related techniques including data normalization , dimension reduction , classification , and spatial information integration and the way to accommodate these techniques to the context of <m> sequential data collecting and processing </m> .
second:
Unlike <m> sequential models </m> , recursive neural networks compose word phrases over syntactic tree structure and have shown improved performance in sentiment analysis .


class: first -> second, base class: no relation, new model class: no relation
first:
We present related techniques including data normalization , dimension reduction , classification , and spatial information integration and the way to accommodate these techniques to the context of <m> sequential data collecting and processing </m> .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We present related techniques including data normalization , dimension reduction , classification , and spatial information integration and the way to accommodate these techniques to the context of <m> sequential data collecting and processing </m> .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: first -> second, base class: no relation, new model class: no relation
first:
We present related techniques including data normalization , dimension reduction , classification , and spatial information integration and the way to accommodate these techniques to the context of <m> sequential data collecting and processing </m> .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: first -> second, base class: no relation, new model class: no relation
first:
We present related techniques including data normalization , dimension reduction , classification , and spatial information integration and the way to accommodate these techniques to the context of <m> sequential data collecting and processing </m> .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: first -> second, base class: no relation, new model class: no relation
first:
We present related techniques including data normalization , dimension reduction , classification , and spatial information integration and the way to accommodate these techniques to the context of <m> sequential data collecting and processing </m> .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : Recurrent neural networks ( RNN ) Different from Artificial neural network ( ANN ) , RNN , a neural network with recurrent unit , has a better performance in solving many challenging problems involving <m> sequential data analysis </m> .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : Recurrent neural networks ( RNN ) Different from Artificial neural network ( ANN ) , RNN , a neural network with recurrent unit , has a better performance in solving many challenging problems involving <m> sequential data analysis </m> .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : Recurrent neural networks ( RNN ) Different from Artificial neural network ( ANN ) , RNN , a neural network with recurrent unit , has a better performance in solving many challenging problems involving <m> sequential data analysis </m> .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : Recurrent neural networks ( RNN ) Different from Artificial neural network ( ANN ) , RNN , a neural network with recurrent unit , has a better performance in solving many challenging problems involving <m> sequential data analysis </m> .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: first -> second, base class: no relation, new model class: no relation
first:
subsection : Recurrent neural networks ( RNN ) Different from Artificial neural network ( ANN ) , RNN , a neural network with recurrent unit , has a better performance in solving many challenging problems involving <m> sequential data analysis </m> .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: first -> second, base class: same cluster, new model class: same cluster
first:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .
second:
The results demonstrate that the algorithm can also learn <m> temporal sequence detection </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .
second:
Proposed networks which perform <m> temporal sequence generation </m> are often in the form of a modification to an auto-associative memory by using heteroassociative or time-varying synaptic strengths , requiring some pre-chosen temporal functions .


class: first -> second, base class: same cluster, new model class: same cluster
first:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .
second:
This user modeling approach comprises the following new novel aspects ( 1 ) Modeling microblog users behavior evolution by considering the different event phases ( 2 ) Characterizing users activity over time through a <m> temporal sequence representation </m> ( 3 ) Time-series-based selection of the most discriminative features characterizing users at each event phase .


class: first -> second, base class: same cluster, new model class: same cluster
first:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .
second:
The results demonstrate that the algorithm can also learn <m> temporal sequence detection </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .
second:
Proposed networks which perform <m> temporal sequence generation </m> are often in the form of a modification to an auto-associative memory by using heteroassociative or time-varying synaptic strengths , requiring some pre-chosen temporal functions .


class: first -> second, base class: same cluster, new model class: same cluster
first:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .
second:
This user modeling approach comprises the following new novel aspects ( 1 ) Modeling microblog users behavior evolution by considering the different event phases ( 2 ) Characterizing users activity over time through a <m> temporal sequence representation </m> ( 3 ) Time-series-based selection of the most discriminative features characterizing users at each event phase .


class: first -> second, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
The <m> categorical parameterization </m> , using the projected KL loss , has also been used in recent work to improve the critic of a policy gradient algorithm , D4PG , achieving significantly improved robustness and state - of - the - art performance across a variety of continuous control tasks .


class: first -> second, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .


class: first -> second, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .


class: first -> second, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .


class: first -> second, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
The <m> categorical parameterization </m> , using the projected KL loss , has also been used in recent work to improve the critic of a policy gradient algorithm , D4PG , achieving significantly improved robustness and state - of - the - art performance across a variety of continuous control tasks .


class: first -> second, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
subsection : PixelGAN Autoencoders with <m> Categorical Priors </m> In this section , we present an architecture of the PixelGAN autoencoder that can separate the discrete information ( e.g. , class label ) from the continuous information ( e.g. , style information ) in the images .


class: first -> second, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .


class: first -> second, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .


class: first -> second, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .
second:
The <m> categorical parameterization </m> , using the projected KL loss , has also been used in recent work to improve the critic of a policy gradient algorithm , D4PG , achieving significantly improved robustness and state - of - the - art performance across a variety of continuous control tasks .


class: first -> second, base class: no relation, new model class: no relation
first:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .
second:
subsection : PixelGAN Autoencoders with <m> Categorical Priors </m> In this section , we present an architecture of the PixelGAN autoencoder that can separate the discrete information ( e.g. , class label ) from the continuous information ( e.g. , style information ) in the images .


class: first -> second, base class: no relation, new model class: no relation
first:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .
second:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .


class: first -> second, base class: no relation, new model class: no relation
first:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .
second:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .
second:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .


class: first -> second, base class: no relation, new model class: no relation
first:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .
second:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .
second:
The <m> categorical parameterization </m> , using the projected KL loss , has also been used in recent work to improve the critic of a policy gradient algorithm , D4PG , achieving significantly improved robustness and state - of - the - art performance across a variety of continuous control tasks .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .
second:
subsection : PixelGAN Autoencoders with <m> Categorical Priors </m> In this section , we present an architecture of the PixelGAN autoencoder that can separate the discrete information ( e.g. , class label ) from the continuous information ( e.g. , style information ) in the images .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .
second:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .
second:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .
second:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .
second:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .


class: first -> second, base class: no relation, new model class: no relation
first:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .
second:
The <m> categorical parameterization </m> , using the projected KL loss , has also been used in recent work to improve the critic of a policy gradient algorithm , D4PG , achieving significantly improved robustness and state - of - the - art performance across a variety of continuous control tasks .


class: first -> second, base class: no relation, new model class: no relation
first:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .
second:
subsection : PixelGAN Autoencoders with <m> Categorical Priors </m> In this section , we present an architecture of the PixelGAN autoencoder that can separate the discrete information ( e.g. , class label ) from the continuous information ( e.g. , style information ) in the images .


class: first -> second, base class: no relation, new model class: no relation
first:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .
second:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .


class: first -> second, base class: no relation, new model class: no relation
first:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .
second:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .
second:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .


class: first -> second, base class: no relation, new model class: no relation
first:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .
second:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .


class: first -> second, base class: no relation, new model class: no relation
first:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .
second:
The <m> categorical parameterization </m> , using the projected KL loss , has also been used in recent work to improve the critic of a policy gradient algorithm , D4PG , achieving significantly improved robustness and state - of - the - art performance across a variety of continuous control tasks .


class: first -> second, base class: no relation, new model class: no relation
first:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .
second:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .


class: first -> second, base class: no relation, new model class: no relation
first:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .
second:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .
second:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .


class: first -> second, base class: no relation, new model class: no relation
first:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .
second:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .
second:
Basic methods of the research : <m> categorical analysis </m> , determining of statistic parameters of word-forming models .


class: first -> second, base class: no relation, new model class: same cluster
first:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .
second:
In this paper , two centroid-based classifiers are proposed for <m> categorical data classification </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .
second:
Analysis of quantitative ( Bias , Relative RMSE ) and <m> categorical statistics </m> ( POD , FAR ) for the whole period show a more accurate spatial distribution of mean daily rainfall estimations in the lowlands than in the Andean regions .


class: first -> second, base class: same cluster, new model class: same cluster
first:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .
second:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .


class: first -> second, base class: same cluster, new model class: no relation
first:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .
second:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .
second:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .
second:
section : Inference Across all of the presented experiments , we use <m> categorical sampling </m> during decoding with a tempered PixelRecursiveSuperResolution .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .
second:
This is due to the generator network in GAN is designed to be able to adjust the output continuously , which does not work on <m> discrete data generation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: first -> second, base class: no relation, new model class: same cluster
first:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .
second:
In this paper , two centroid-based classifiers are proposed for <m> categorical data classification </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .
second:
Analysis of quantitative ( Bias , Relative RMSE ) and <m> categorical statistics </m> ( POD , FAR ) for the whole period show a more accurate spatial distribution of mean daily rainfall estimations in the lowlands than in the Andean regions .


class: first -> second, base class: same cluster, new model class: same cluster
first:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .
second:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .


class: first -> second, base class: same cluster, new model class: no relation
first:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .
second:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .


class: first -> second, base class: no relation, new model class: no relation
first:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .
second:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .


class: first -> second, base class: no relation, new model class: no relation
first:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .
second:
section : Inference Across all of the presented experiments , we use <m> categorical sampling </m> during decoding with a tempered PixelRecursiveSuperResolution .


class: first -> second, base class: no relation, new model class: no relation
first:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: first -> second, base class: no relation, new model class: no relation
first:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: first -> second, base class: no relation, new model class: no relation
first:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: first -> second, base class: no relation, new model class: no relation
first:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .
second:
This is due to the generator network in GAN is designed to be able to adjust the output continuously , which does not work on <m> discrete data generation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: first -> second, base class: no relation, new model class: no relation
first:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .
second:
The <m> categorical parameterization </m> , using the projected KL loss , has also been used in recent work to improve the critic of a policy gradient algorithm , D4PG , achieving significantly improved robustness and state - of - the - art performance across a variety of continuous control tasks .


class: first -> second, base class: no relation, new model class: no relation
first:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .
second:
subsection : PixelGAN Autoencoders with <m> Categorical Priors </m> In this section , we present an architecture of the PixelGAN autoencoder that can separate the discrete information ( e.g. , class label ) from the continuous information ( e.g. , style information ) in the images .


class: first -> second, base class: no relation, new model class: no relation
first:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .
second:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .


class: first -> second, base class: no relation, new model class: no relation
first:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .
second:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .
second:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .


class: first -> second, base class: no relation, new model class: no relation
first:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .
second:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .


class: first -> second, base class: same cluster, new model class: no relation
first:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .
second:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .
second:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .
second:
section : Inference Across all of the presented experiments , we use <m> categorical sampling </m> during decoding with a tempered PixelRecursiveSuperResolution .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .
second:
This is due to the generator network in GAN is designed to be able to adjust the output continuously , which does not work on <m> discrete data generation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: first -> second, base class: no relation, new model class: no relation
first:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .
second:
The <m> categorical parameterization </m> , using the projected KL loss , has also been used in recent work to improve the critic of a policy gradient algorithm , D4PG , achieving significantly improved robustness and state - of - the - art performance across a variety of continuous control tasks .


class: first -> second, base class: no relation, new model class: no relation
first:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .
second:
subsection : PixelGAN Autoencoders with <m> Categorical Priors </m> In this section , we present an architecture of the PixelGAN autoencoder that can separate the discrete information ( e.g. , class label ) from the continuous information ( e.g. , style information ) in the images .


class: first -> second, base class: no relation, new model class: no relation
first:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .
second:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .


class: first -> second, base class: no relation, new model class: no relation
first:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .
second:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .
second:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .


class: first -> second, base class: no relation, new model class: no relation
first:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .
second:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .
second:
The <m> categorical parameterization </m> , using the projected KL loss , has also been used in recent work to improve the critic of a policy gradient algorithm , D4PG , achieving significantly improved robustness and state - of - the - art performance across a variety of continuous control tasks .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .
second:
subsection : PixelGAN Autoencoders with <m> Categorical Priors </m> In this section , we present an architecture of the PixelGAN autoencoder that can separate the discrete information ( e.g. , class label ) from the continuous information ( e.g. , style information ) in the images .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .
second:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .
second:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .
second:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .


class: first -> second, base class: no relation, new model class: no relation
first:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .
second:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: first -> second, base class: no relation, new model class: no relation
first:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: first -> second, base class: no relation, new model class: no relation
first:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: first -> second, base class: no relation, new model class: no relation
first:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .
second:
This is due to the generator network in GAN is designed to be able to adjust the output continuously , which does not work on <m> discrete data generation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: first -> second, base class: no relation, new model class: no relation
first:
These motifs ( <m> numerical descriptors </m> ) are automatically targeted from the human genome without any clear definition .
second:
Aiming at this problem , a novel <m> numerical feature extraction approach </m> EPLS is proposed .


class: first -> second, base class: no relation, new model class: no relation
first:
These motifs ( <m> numerical descriptors </m> ) are automatically targeted from the human genome without any clear definition .
second:
For a <m> continuous feature extraction </m> , we use a template matching of the nose bridge in combination with selected features derived from the optical flow .


class: first -> second, base class: no relation, new model class: no relation
first:
Conventional analyses address discriminative tasks of behaviors , e.g. , classification and clustering typically using the subsequences extracted from the trajectory of an object as a <m> numerical feature representation </m> .
second:
For a <m> continuous feature extraction </m> , we use a template matching of the nose bridge in combination with selected features derived from the optical flow .


class: first -> second, base class: no relation, new model class: no relation
first:
Experimental results show that the proposed method presents an efficient and effective solution for <m> numerical feature analysis </m> .
second:
For a <m> continuous feature extraction </m> , we use a template matching of the nose bridge in combination with selected features derived from the optical flow .


class: first -> second, base class: no relation, new model class: no relation
first:
Here , we present iFeature , a versatile Python‐based toolkit for generating various <m> numerical feature representation schemes </m> for both protein and peptide sequences .
second:
For a <m> continuous feature extraction </m> , we use a template matching of the nose bridge in combination with selected features derived from the optical flow .


class: first -> second, base class: no relation, new model class: no relation
first:
Experimental comparisons also show the new method can significantly outperform standard <m> numerical feature-type methods </m> in terms of agreement with the clinical diagnosis gold standard .
second:
For a <m> continuous feature extraction </m> , we use a template matching of the nose bridge in combination with selected features derived from the optical flow .


class: first -> second, base class: no relation, new model class: no relation
first:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .
second:
For a <m> continuous feature extraction </m> , we use a template matching of the nose bridge in combination with selected features derived from the optical flow .


class: first -> second, base class: no relation, new model class: no relation
first:
MPEG-7 visual descriptors are <m> numerical representations of features </m> - such as : texture , shape and color - extracted from an image .
second:
For a <m> continuous feature extraction </m> , we use a template matching of the nose bridge in combination with selected features derived from the optical flow .


class: first -> second, base class: no relation, new model class: no relation
first:
A text parser to match keywords against vocabularies and <m> numerical value descriptors </m> is introduced .
second:
For a <m> continuous feature extraction </m> , we use a template matching of the nose bridge in combination with selected features derived from the optical flow .


class: first -> second, base class: no relation, new model class: no relation
first:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .
second:
For a <m> continuous feature extraction </m> , we use a template matching of the nose bridge in combination with selected features derived from the optical flow .


class: first -> second, base class: no relation, new model class: no relation
first:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .
second:
For a <m> continuous feature extraction </m> , we use a template matching of the nose bridge in combination with selected features derived from the optical flow .


class: first -> second, base class: no relation, new model class: no relation
first:
While previous studies have considered categorical emotions , expressive speech during human interaction conveys subtle behaviors that are better characterized with <m> continuous descriptors </m> ( e.g. , attributes such as arousal , valence , dominance ) .
second:
For a <m> continuous feature extraction </m> , we use a template matching of the nose bridge in combination with selected features derived from the optical flow .


class: first -> second, base class: no relation, new model class: no relation
first:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .
second:
For a <m> continuous feature extraction </m> , we use a template matching of the nose bridge in combination with selected features derived from the optical flow .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .
second:
This is due to the generator network in GAN is designed to be able to adjust the output continuously , which does not work on <m> discrete data generation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: first -> second, base class: same cluster, new model class: no relation
first:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .
second:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .


class: first -> second, base class: same cluster, new model class: same cluster
first:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .
second:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .


class: first -> second, base class: same cluster, new model class: no relation
first:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .
second:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .


class: first -> second, base class: same cluster, new model class: same cluster
first:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .
second:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: first -> second, base class: no relation, new model class: no relation
first:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .
second:
This is due to the generator network in GAN is designed to be able to adjust the output continuously , which does not work on <m> discrete data generation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: first -> second, base class: same cluster, new model class: same cluster
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .


class: first -> second, base class: same cluster, new model class: same cluster
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .


class: first -> second, base class: no relation, new model class: same cluster
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: first -> second, base class: no relation, new model class: no relation
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
subsubsection : Comparison of Metric Learning Algorithms We evaluated the proposed XQDA algorithm and several metric learning algorithms , including Euclidean distance , Mahalanobis distance trained with genuine pairs , LMNN v2.5 , <m> ITML </m> , KISSME , and RLDA , with the same LOMO feature .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .


class: first -> second, base class: no relation, new model class: same cluster
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: first -> second, base class: no relation, new model class: no relation
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
subsubsection : Comparison of Metric Learning Algorithms We evaluated the proposed XQDA algorithm and several metric learning algorithms , including Euclidean distance , Mahalanobis distance trained with genuine pairs , LMNN v2.5 , <m> ITML </m> , KISSME , and RLDA , with the same LOMO feature .


class: first -> second, base class: same cluster, new model class: same cluster
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .


class: first -> second, base class: same cluster, new model class: same cluster
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .


class: first -> second, base class: no relation, new model class: same cluster
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: first -> second, base class: no relation, new model class: no relation
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
subsubsection : Comparison of Metric Learning Algorithms We evaluated the proposed XQDA algorithm and several metric learning algorithms , including Euclidean distance , Mahalanobis distance trained with genuine pairs , LMNN v2.5 , <m> ITML </m> , KISSME , and RLDA , with the same LOMO feature .


class: first -> second, base class: second -> first, new model class: same cluster
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .


class: first -> second, base class: second -> first, new model class: same cluster
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .


class: first -> second, base class: second -> first, new model class: same cluster
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .


class: first -> second, base class: no relation, new model class: no relation
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
subsection : Architecture - I ( Arc - I ) Architecture - I ( Arc - I ) , as illustrated in Figure [ reference ] , takes a conventional approach : It first finds the representation of each sentence , and then compares the representation for the two sentences with a multi - layer perceptron ( <m> MLP </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
subsubsection : Comparison of Metric Learning Algorithms We evaluated the proposed XQDA algorithm and several metric learning algorithms , including Euclidean distance , Mahalanobis distance trained with genuine pairs , LMNN v2.5 , <m> ITML </m> , KISSME , and RLDA , with the same LOMO feature .


class: first -> second, base class: no relation, new model class: no relation
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
Adding <m> MLP in </m> does not seem to help , yielding slightly worse result than without MLP .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .


class: first -> second, base class: no relation, new model class: same cluster
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: first -> second, base class: no relation, new model class: no relation
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
subsubsection : Comparison of Metric Learning Algorithms We evaluated the proposed XQDA algorithm and several metric learning algorithms , including Euclidean distance , Mahalanobis distance trained with genuine pairs , LMNN v2.5 , <m> ITML </m> , KISSME , and RLDA , with the same LOMO feature .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .


class: first -> second, base class: no relation, new model class: same cluster
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: first -> second, base class: no relation, new model class: no relation
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
subsubsection : Comparison of Metric Learning Algorithms We evaluated the proposed XQDA algorithm and several metric learning algorithms , including Euclidean distance , Mahalanobis distance trained with genuine pairs , LMNN v2.5 , <m> ITML </m> , KISSME , and RLDA , with the same LOMO feature .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .


class: first -> second, base class: no relation, new model class: same cluster
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: first -> second, base class: no relation, new model class: no relation
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
subsubsection : Comparison of Metric Learning Algorithms We evaluated the proposed XQDA algorithm and several metric learning algorithms , including Euclidean distance , Mahalanobis distance trained with genuine pairs , LMNN v2.5 , <m> ITML </m> , KISSME , and RLDA , with the same LOMO feature .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .


class: first -> second, base class: no relation, new model class: same cluster
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: first -> second, base class: no relation, new model class: no relation
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
subsubsection : Comparison of Metric Learning Algorithms We evaluated the proposed XQDA algorithm and several metric learning algorithms , including Euclidean distance , Mahalanobis distance trained with genuine pairs , LMNN v2.5 , <m> ITML </m> , KISSME , and RLDA , with the same LOMO feature .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .


class: first -> second, base class: no relation, new model class: same cluster
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: first -> second, base class: no relation, new model class: no relation
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
subsubsection : Comparison of Metric Learning Algorithms We evaluated the proposed XQDA algorithm and several metric learning algorithms , including Euclidean distance , Mahalanobis distance trained with genuine pairs , LMNN v2.5 , <m> ITML </m> , KISSME , and RLDA , with the same LOMO feature .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .


class: first -> second, base class: no relation, new model class: same cluster
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: first -> second, base class: no relation, new model class: no relation
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
subsubsection : Comparison of Metric Learning Algorithms We evaluated the proposed XQDA algorithm and several metric learning algorithms , including Euclidean distance , Mahalanobis distance trained with genuine pairs , LMNN v2.5 , <m> ITML </m> , KISSME , and RLDA , with the same LOMO feature .


class: first -> second, base class: same cluster, new model class: same cluster
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .


class: first -> second, base class: no relation, new model class: same cluster
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: first -> second, base class: no relation, new model class: no relation
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
subsubsection : Comparison of Metric Learning Algorithms We evaluated the proposed XQDA algorithm and several metric learning algorithms , including Euclidean distance , Mahalanobis distance trained with genuine pairs , LMNN v2.5 , <m> ITML </m> , KISSME , and RLDA , with the same LOMO feature .


class: first -> second, base class: no relation, new model class: same cluster
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: first -> second, base class: no relation, new model class: no relation
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
subsubsection : Comparison of Metric Learning Algorithms We evaluated the proposed XQDA algorithm and several metric learning algorithms , including Euclidean distance , Mahalanobis distance trained with genuine pairs , LMNN v2.5 , <m> ITML </m> , KISSME , and RLDA , with the same LOMO feature .


class: first -> second, base class: no relation, new model class: same cluster
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: first -> second, base class: no relation, new model class: no relation
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
subsubsection : Comparison of Metric Learning Algorithms We evaluated the proposed XQDA algorithm and several metric learning algorithms , including Euclidean distance , Mahalanobis distance trained with genuine pairs , LMNN v2.5 , <m> ITML </m> , KISSME , and RLDA , with the same LOMO feature .


class: first -> second, base class: no relation, new model class: same cluster
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: first -> second, base class: no relation, new model class: no relation
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
subsubsection : Comparison of Metric Learning Algorithms We evaluated the proposed XQDA algorithm and several metric learning algorithms , including Euclidean distance , Mahalanobis distance trained with genuine pairs , LMNN v2.5 , <m> ITML </m> , KISSME , and RLDA , with the same LOMO feature .


class: first -> second, base class: no relation, new model class: same cluster
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: first -> second, base class: no relation, new model class: no relation
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
subsubsection : Comparison of Metric Learning Algorithms We evaluated the proposed XQDA algorithm and several metric learning algorithms , including Euclidean distance , Mahalanobis distance trained with genuine pairs , LMNN v2.5 , <m> ITML </m> , KISSME , and RLDA , with the same LOMO feature .


class: first -> second, base class: no relation, new model class: same cluster
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: first -> second, base class: no relation, new model class: no relation
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
subsubsection : Comparison of Metric Learning Algorithms We evaluated the proposed XQDA algorithm and several metric learning algorithms , including Euclidean distance , Mahalanobis distance trained with genuine pairs , LMNN v2.5 , <m> ITML </m> , KISSME , and RLDA , with the same LOMO feature .


class: first -> second, base class: no relation, new model class: same cluster
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: first -> second, base class: no relation, new model class: no relation
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
subsubsection : Comparison of Metric Learning Algorithms We evaluated the proposed XQDA algorithm and several metric learning algorithms , including Euclidean distance , Mahalanobis distance trained with genuine pairs , LMNN v2.5 , <m> ITML </m> , KISSME , and RLDA , with the same LOMO feature .


class: first -> second, base class: same cluster, new model class: same cluster
first:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: first -> second, base class: no relation, new model class: same cluster
first:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .
second:
An integration architecture tactic to guard <m> AI-first components </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: first -> second, base class: no relation, new model class: no relation
first:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: first -> second, base class: no relation, new model class: same cluster
first:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .
second:
An integration architecture tactic to guard <m> AI-first components </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: first -> second, base class: no relation, new model class: no relation
first:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: first -> second, base class: no relation, new model class: same cluster
first:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .
second:
An integration architecture tactic to guard <m> AI-first components </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: first -> second, base class: no relation, new model class: no relation
first:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: first -> second, base class: no relation, new model class: same cluster
first:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .
second:
An integration architecture tactic to guard <m> AI-first components </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: first -> second, base class: no relation, new model class: no relation
first:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: first -> second, base class: no relation, new model class: same cluster
first:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .
second:
An integration architecture tactic to guard <m> AI-first components </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: first -> second, base class: no relation, new model class: no relation
first:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: first -> second, base class: no relation, new model class: same cluster
first:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .
second:
An integration architecture tactic to guard <m> AI-first components </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: first -> second, base class: no relation, new model class: no relation
first:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: first -> second, base class: no relation, new model class: same cluster
first:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .
second:
An integration architecture tactic to guard <m> AI-first components </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: first -> second, base class: same cluster, new model class: same cluster
first:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: first -> second, base class: no relation, new model class: no relation
first:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: first -> second, base class: no relation, new model class: same cluster
first:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .
second:
An integration architecture tactic to guard <m> AI-first components </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: first -> second, base class: same cluster, new model class: same cluster
first:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: first -> second, base class: no relation, new model class: no relation
first:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: first -> second, base class: no relation, new model class: same cluster
first:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .
second:
An integration architecture tactic to guard <m> AI-first components </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: first -> second, base class: no relation, new model class: no relation
first:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: first -> second, base class: no relation, new model class: same cluster
first:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .
second:
An integration architecture tactic to guard <m> AI-first components </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: first -> second, base class: no relation, new model class: same cluster
first:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: first -> second, base class: no relation, new model class: same cluster
first:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: first -> second, base class: no relation, new model class: same cluster
first:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: first -> second, base class: no relation, new model class: same cluster
first:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: first -> second, base class: no relation, new model class: same cluster
first:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: first -> second, base class: no relation, new model class: same cluster
first:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: first -> second, base class: no relation, new model class: no relation
first:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?
second:
subsection : Architecture - I ( Arc - I ) Architecture - I ( Arc - I ) , as illustrated in Figure [ reference ] , takes a conventional approach : It first finds the representation of each sentence , and then compares the representation for the two sentences with a multi - layer perceptron ( <m> MLP </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?
second:
Adding <m> MLP in </m> does not seem to help , yielding slightly worse result than without MLP .


class: first -> second, base class: no relation, new model class: no relation
first:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .
second:
subsection : Architecture - I ( Arc - I ) Architecture - I ( Arc - I ) , as illustrated in Figure [ reference ] , takes a conventional approach : It first finds the representation of each sentence , and then compares the representation for the two sentences with a multi - layer perceptron ( <m> MLP </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .
second:
Adding <m> MLP in </m> does not seem to help , yielding slightly worse result than without MLP .


class: first -> second, base class: no relation, new model class: no relation
first:
Most researchers in <m> ML </m> use average overall accuracy .
second:
Adding <m> MLP in </m> does not seem to help , yielding slightly worse result than without MLP .


class: first -> second, base class: no relation, new model class: same cluster
first:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: first -> second, base class: no relation, new model class: no relation
first:
It first depicts the performance of the proposed models on different length of sentences , and then conducts a sensitivity analysis of <m> 2D filter </m> and max pooling size .
second:
The next step of the mean - field iteration is taking a weighted sum of the M filter outputs from the previous step , for each class label l. When each class label is considered individually , this can be viewed as usual convolution with a <m> 1 × 1 filter </m> with M input channels , and one output channel .


class: first -> second, base class: no relation, new model class: no relation
first:
It first depicts the performance of the proposed models on different length of sentences , and then conducts a sensitivity analysis of <m> 2D filter </m> and max pooling size .
second:
For <m> graphlet kernel </m> ( GK ) , we chose graphlets size and for deep graph kernels ( DGK ) , we report the best classification accuracy obtained among : deep graphlet kernel , deep shortest path kernel and deep Weisfeiler - Lehman kernel .


class: first -> second, base class: no relation, new model class: no relation
first:
It first depicts the performance of the proposed models on different length of sentences , and then conducts a sensitivity analysis of <m> 2D filter </m> and max pooling size .
second:
Different from image deblurring , the <m> blur kernel setting </m> of SISR is usually simple .


class: first -> second, base class: no relation, new model class: no relation
first:
It first depicts the performance of the proposed models on different length of sentences , and then conducts a sensitivity analysis of <m> 2D filter </m> and max pooling size .
second:
For example , defines a <m> kernel with dilation </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
It first depicts the performance of the proposed models on different length of sentences , and then conducts a sensitivity analysis of <m> 2D filter </m> and max pooling size .
second:
We apply a <m> 5×5 kernel </m> with 512 outputs , followed by sibling fully connected layers to predict a 14×14 mask ( 14 2 outputs ) and object score ( 1 output ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Before the DU - Net , a <m> Conv ( ) filter </m> with stride 2 and a max pooling would produce 128 features with resolution 64 64 .
second:
Unlike activations of intermediate layers of a CNN , <m> linear filter weights </m> have a well defined semantics that can be visualized and analyzed using well developed tools of linear signal processing .


class: first -> second, base class: no relation, new model class: no relation
first:
Before the DU - Net , a <m> Conv ( ) filter </m> with stride 2 and a max pooling would produce 128 features with resolution 64 64 .
second:
In addition , not using an explicit <m> interpolation filter </m> means that the network implicitly learns the processing necessary for SR .


class: first -> second, base class: no relation, new model class: no relation
first:
Before the DU - Net , a <m> Conv ( ) filter </m> with stride 2 and a max pooling would produce 128 features with resolution 64 64 .
second:
Such kernels are built with a parameterized “ neural response ” function , which consists in computing the maximal response of a <m> base kernel </m> over a local neighborhood .


class: first -> second, base class: no relation, new model class: no relation
first:
Before the DU - Net , a <m> Conv ( ) filter </m> with stride 2 and a max pooling would produce 128 features with resolution 64 64 .
second:
Assume an <m> isotropic stationary kernel </m> ( ) is used in MMD .


class: first -> second, base class: no relation, new model class: no relation
first:
Before the DU - Net , a <m> Conv ( ) filter </m> with stride 2 and a max pooling would produce 128 features with resolution 64 64 .
second:
a <m> kernel size </m> c × f 1 ×


class: first -> second, base class: no relation, new model class: no relation
first:
Before the DU - Net , a <m> Conv ( ) filter </m> with stride 2 and a max pooling would produce 128 features with resolution 64 64 .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: first -> second, base class: no relation, new model class: no relation
first:
Before the DU - Net , a <m> Conv ( ) filter </m> with stride 2 and a max pooling would produce 128 features with resolution 64 64 .
second:
Learning pooling functions is analogous to <m> receptive field learning </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Before the DU - Net , a <m> Conv ( ) filter </m> with stride 2 and a max pooling would produce 128 features with resolution 64 64 .
second:
[ t ! ] <m> ReceptiveField </m> : Create Receptive Field { algorithmic} [ 1 ] vertex v , graph labeling ℓ , receptive field size k Gnorm We have the following result concerning the complexity of the optimal normalization problem .


class: first -> second, base class: no relation, new model class: no relation
first:
Before the DU - Net , a <m> Conv ( ) filter </m> with stride 2 and a max pooling would produce 128 features with resolution 64 64 .
second:
The convolutions performed in each stage use <m> volumetric kernels </m> having size 5 × 5 × 5 voxels .


class: first -> second, base class: no relation, new model class: no relation
first:
Before the DU - Net , a <m> Conv ( ) filter </m> with stride 2 and a max pooling would produce 128 features with resolution 64 64 .
second:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .


class: first -> second, base class: no relation, new model class: no relation
first:
Before the DU - Net , a <m> Conv ( ) filter </m> with stride 2 and a max pooling would produce 128 features with resolution 64 64 .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: first -> second, base class: no relation, new model class: no relation
first:
Before the DU - Net , a <m> Conv ( ) filter </m> with stride 2 and a max pooling would produce 128 features with resolution 64 64 .
second:
The convolutions performed in each stage use <m> volumetric kernels </m> having size voxels .


class: first -> second, base class: no relation, new model class: no relation
first:
Before the DU - Net , a <m> Conv ( ) filter </m> with stride 2 and a max pooling would produce 128 features with resolution 64 64 .
second:
This is performed through convolution with <m> voxels wide kernels </m> applied with stride ( Figure [ reference ] ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Moreover , we give some model analysis on the <m> filter size configuration </m> .
second:
Unlike activations of intermediate layers of a CNN , <m> linear filter weights </m> have a well defined semantics that can be visualized and analyzed using well developed tools of linear signal processing .


class: first -> second, base class: no relation, new model class: no relation
first:
Moreover , we give some model analysis on the <m> filter size configuration </m> .
second:
In addition , not using an explicit <m> interpolation filter </m> means that the network implicitly learns the processing necessary for SR .


class: first -> second, base class: no relation, new model class: no relation
first:
Moreover , we give some model analysis on the <m> filter size configuration </m> .
second:
Such kernels are built with a parameterized “ neural response ” function , which consists in computing the maximal response of a <m> base kernel </m> over a local neighborhood .


class: first -> second, base class: no relation, new model class: no relation
first:
Moreover , we give some model analysis on the <m> filter size configuration </m> .
second:
Assume an <m> isotropic stationary kernel </m> ( ) is used in MMD .


class: first -> second, base class: no relation, new model class: no relation
first:
Moreover , we give some model analysis on the <m> filter size configuration </m> .
second:
a <m> kernel size </m> c × f 1 ×


class: first -> second, base class: no relation, new model class: no relation
first:
Moreover , we give some model analysis on the <m> filter size configuration </m> .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: first -> second, base class: no relation, new model class: no relation
first:
Moreover , we give some model analysis on the <m> filter size configuration </m> .
second:
Learning pooling functions is analogous to <m> receptive field learning </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Moreover , we give some model analysis on the <m> filter size configuration </m> .
second:
[ t ! ] <m> ReceptiveField </m> : Create Receptive Field { algorithmic} [ 1 ] vertex v , graph labeling ℓ , receptive field size k Gnorm We have the following result concerning the complexity of the optimal normalization problem .


class: first -> second, base class: no relation, new model class: no relation
first:
Moreover , we give some model analysis on the <m> filter size configuration </m> .
second:
The convolutions performed in each stage use <m> volumetric kernels </m> having size 5 × 5 × 5 voxels .


class: first -> second, base class: no relation, new model class: no relation
first:
Moreover , we give some model analysis on the <m> filter size configuration </m> .
second:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .


class: first -> second, base class: no relation, new model class: no relation
first:
Moreover , we give some model analysis on the <m> filter size configuration </m> .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: first -> second, base class: no relation, new model class: no relation
first:
Moreover , we give some model analysis on the <m> filter size configuration </m> .
second:
The convolutions performed in each stage use <m> volumetric kernels </m> having size voxels .


class: first -> second, base class: no relation, new model class: no relation
first:
Moreover , we give some model analysis on the <m> filter size configuration </m> .
second:
This is performed through convolution with <m> voxels wide kernels </m> applied with stride ( Figure [ reference ] ) .


class: first -> second, base class: no relation, new model class: no relation
first:
This was achieved by iterating over the differences using a <m> maximum filter </m> with decreasing filter size until 20 frames have been found .
second:
Unlike activations of intermediate layers of a CNN , <m> linear filter weights </m> have a well defined semantics that can be visualized and analyzed using well developed tools of linear signal processing .


class: first -> second, base class: second -> first, new model class: no relation
first:
This was achieved by iterating over the differences using a <m> maximum filter </m> with decreasing filter size until 20 frames have been found .
second:
In addition , not using an explicit <m> interpolation filter </m> means that the network implicitly learns the processing necessary for SR .


class: first -> second, base class: no relation, new model class: no relation
first:
This was achieved by iterating over the differences using a <m> maximum filter </m> with decreasing filter size until 20 frames have been found .
second:
Such kernels are built with a parameterized “ neural response ” function , which consists in computing the maximal response of a <m> base kernel </m> over a local neighborhood .


class: first -> second, base class: no relation, new model class: no relation
first:
This was achieved by iterating over the differences using a <m> maximum filter </m> with decreasing filter size until 20 frames have been found .
second:
Assume an <m> isotropic stationary kernel </m> ( ) is used in MMD .


class: first -> second, base class: no relation, new model class: no relation
first:
This was achieved by iterating over the differences using a <m> maximum filter </m> with decreasing filter size until 20 frames have been found .
second:
a <m> kernel size </m> c × f 1 ×


class: first -> second, base class: no relation, new model class: no relation
first:
This was achieved by iterating over the differences using a <m> maximum filter </m> with decreasing filter size until 20 frames have been found .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: first -> second, base class: no relation, new model class: no relation
first:
This was achieved by iterating over the differences using a <m> maximum filter </m> with decreasing filter size until 20 frames have been found .
second:
Learning pooling functions is analogous to <m> receptive field learning </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
This was achieved by iterating over the differences using a <m> maximum filter </m> with decreasing filter size until 20 frames have been found .
second:
[ t ! ] <m> ReceptiveField </m> : Create Receptive Field { algorithmic} [ 1 ] vertex v , graph labeling ℓ , receptive field size k Gnorm We have the following result concerning the complexity of the optimal normalization problem .


class: first -> second, base class: no relation, new model class: no relation
first:
This was achieved by iterating over the differences using a <m> maximum filter </m> with decreasing filter size until 20 frames have been found .
second:
The convolutions performed in each stage use <m> volumetric kernels </m> having size 5 × 5 × 5 voxels .


class: first -> second, base class: no relation, new model class: no relation
first:
This was achieved by iterating over the differences using a <m> maximum filter </m> with decreasing filter size until 20 frames have been found .
second:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .


class: first -> second, base class: no relation, new model class: no relation
first:
This was achieved by iterating over the differences using a <m> maximum filter </m> with decreasing filter size until 20 frames have been found .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: first -> second, base class: no relation, new model class: no relation
first:
This was achieved by iterating over the differences using a <m> maximum filter </m> with decreasing filter size until 20 frames have been found .
second:
The convolutions performed in each stage use <m> volumetric kernels </m> having size voxels .


class: first -> second, base class: no relation, new model class: no relation
first:
This was achieved by iterating over the differences using a <m> maximum filter </m> with decreasing filter size until 20 frames have been found .
second:
This is performed through convolution with <m> voxels wide kernels </m> applied with stride ( Figure [ reference ] ) .


class: first -> second, base class: no relation, new model class: no relation
first:
. , ŷ ll , n ) is a diagonal matrix of spectral multipliers representing a <m> learnable filter </m> in the spectral domain , and ξ is a nonlinearity ( e.g. ReLU ) applied on the vertex - wise function values .
second:
Such kernels are built with a parameterized “ neural response ” function , which consists in computing the maximal response of a <m> base kernel </m> over a local neighborhood .


class: first -> second, base class: no relation, new model class: no relation
first:
. , ŷ ll , n ) is a diagonal matrix of spectral multipliers representing a <m> learnable filter </m> in the spectral domain , and ξ is a nonlinearity ( e.g. ReLU ) applied on the vertex - wise function values .
second:
Assume an <m> isotropic stationary kernel </m> ( ) is used in MMD .


class: first -> second, base class: no relation, new model class: no relation
first:
. , ŷ ll , n ) is a diagonal matrix of spectral multipliers representing a <m> learnable filter </m> in the spectral domain , and ξ is a nonlinearity ( e.g. ReLU ) applied on the vertex - wise function values .
second:
a <m> kernel size </m> c × f 1 ×


class: first -> second, base class: no relation, new model class: no relation
first:
. , ŷ ll , n ) is a diagonal matrix of spectral multipliers representing a <m> learnable filter </m> in the spectral domain , and ξ is a nonlinearity ( e.g. ReLU ) applied on the vertex - wise function values .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: first -> second, base class: no relation, new model class: no relation
first:
. , ŷ ll , n ) is a diagonal matrix of spectral multipliers representing a <m> learnable filter </m> in the spectral domain , and ξ is a nonlinearity ( e.g. ReLU ) applied on the vertex - wise function values .
second:
Learning pooling functions is analogous to <m> receptive field learning </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
. , ŷ ll , n ) is a diagonal matrix of spectral multipliers representing a <m> learnable filter </m> in the spectral domain , and ξ is a nonlinearity ( e.g. ReLU ) applied on the vertex - wise function values .
second:
[ t ! ] <m> ReceptiveField </m> : Create Receptive Field { algorithmic} [ 1 ] vertex v , graph labeling ℓ , receptive field size k Gnorm We have the following result concerning the complexity of the optimal normalization problem .


class: first -> second, base class: no relation, new model class: no relation
first:
. , ŷ ll , n ) is a diagonal matrix of spectral multipliers representing a <m> learnable filter </m> in the spectral domain , and ξ is a nonlinearity ( e.g. ReLU ) applied on the vertex - wise function values .
second:
The convolutions performed in each stage use <m> volumetric kernels </m> having size 5 × 5 × 5 voxels .


class: first -> second, base class: no relation, new model class: no relation
first:
. , ŷ ll , n ) is a diagonal matrix of spectral multipliers representing a <m> learnable filter </m> in the spectral domain , and ξ is a nonlinearity ( e.g. ReLU ) applied on the vertex - wise function values .
second:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .


class: first -> second, base class: no relation, new model class: no relation
first:
. , ŷ ll , n ) is a diagonal matrix of spectral multipliers representing a <m> learnable filter </m> in the spectral domain , and ξ is a nonlinearity ( e.g. ReLU ) applied on the vertex - wise function values .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: first -> second, base class: no relation, new model class: no relation
first:
. , ŷ ll , n ) is a diagonal matrix of spectral multipliers representing a <m> learnable filter </m> in the spectral domain , and ξ is a nonlinearity ( e.g. ReLU ) applied on the vertex - wise function values .
second:
The convolutions performed in each stage use <m> volumetric kernels </m> having size voxels .


class: first -> second, base class: no relation, new model class: no relation
first:
. , ŷ ll , n ) is a diagonal matrix of spectral multipliers representing a <m> learnable filter </m> in the spectral domain , and ξ is a nonlinearity ( e.g. ReLU ) applied on the vertex - wise function values .
second:
This is performed through convolution with <m> voxels wide kernels </m> applied with stride ( Figure [ reference ] ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Iris location is a <m> kernel procession </m> in an iris recognition system .
second:
Using tools from representation theory , we explain the limited expressive power of the Kendall kernel by characterizing its degenerate spectrum , and in sharp contrast , we prove that <m> Mallows ' kernel </m> is universal and characteristic .


class: first -> second, base class: no relation, new model class: no relation
first:
Iris location is a <m> kernel procession </m> in an iris recognition system .
second:
However , the approach used fixed convolutional features , and can not jointly learn the <m> kernel classifier </m> and convolutional filters .


class: first -> second, base class: no relation, new model class: no relation
first:
Iris location is a <m> kernel procession </m> in an iris recognition system .
second:
It has been demonstrated that many distance metric learning or discriminative subspace based methods for person re - i d benefit from <m> kernelisation </m> because of the non - linearity in person ’s appearance .


class: first -> second, base class: no relation, new model class: no relation
first:
Iris location is a <m> kernel procession </m> in an iris recognition system .
second:
Subsequently , we improved the RTLSR model into a four-parameter version ( RTLSRV4p ) with a new <m> volumetric scattering kernel </m> derived from the assumption of vertical leaf inclination .


class: first -> second, base class: no relation, new model class: no relation
first:
Iris location is a <m> kernel procession </m> in an iris recognition system .
second:
Graphical abstractDisplay Omitted HighlightsDiscretization and spectrum-free computation of the <m> volumetric heat kernel </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Iris location is a <m> kernel procession </m> in an iris recognition system .
second:
They can be used for <m> highest order Volterra kernel realization </m> as well .


class: first -> second, base class: no relation, new model class: no relation
first:
Iris location is a <m> kernel procession </m> in an iris recognition system .
second:
In this paper , we present an algorithm that allows for different memory lengths of the <m> linear and quadratic Volterra kernel </m> while preserving the advantages of fast convolution techniques in the frequency domain for a second-order Volterra filter .


class: first -> second, base class: no relation, new model class: no relation
first:
Iris location is a <m> kernel procession </m> in an iris recognition system .
second:
Subsequently , a general model that computes the ratio of two non-linear functionals , each comprising <m> linear ( first order Volterra kernel </m> ) and quadratic ( second-order Volterra kernel ) filters , is proposed .


class: first -> second, base class: no relation, new model class: no relation
first:
Iris location is a <m> kernel procession </m> in an iris recognition system .
second:
It provides a set of diagnostic features formed on base of <m> multidimensional Volterra kernels </m> : discrete values of Volterra kernels , heuristic features , moments and wavelet transform coefficients .


class: first -> second, base class: no relation, new model class: no relation
first:
Iris location is a <m> kernel procession </m> in an iris recognition system .
second:
The learned kernel features include : Spatio_dim kernel feature , Grad_nor kernel feature , <m> Volum_shape kernel feature </m> , and KPCA kernel feature .


class: first -> second, base class: same cluster, new model class: no relation
first:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .
second:
However , the approach used fixed convolutional features , and can not jointly learn the <m> kernel classifier </m> and convolutional filters .


class: first -> second, base class: same cluster, new model class: no relation
first:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .
second:
It has been demonstrated that many distance metric learning or discriminative subspace based methods for person re - i d benefit from <m> kernelisation </m> because of the non - linearity in person ’s appearance .


class: first -> second, base class: no relation, new model class: no relation
first:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .
second:
Subsequently , we improved the RTLSR model into a four-parameter version ( RTLSRV4p ) with a new <m> volumetric scattering kernel </m> derived from the assumption of vertical leaf inclination .


class: first -> second, base class: no relation, new model class: no relation
first:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .
second:
Graphical abstractDisplay Omitted HighlightsDiscretization and spectrum-free computation of the <m> volumetric heat kernel </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .
second:
The learned kernel features include : Spatio_dim kernel feature , Grad_nor kernel feature , <m> Volum_shape kernel feature </m> , and KPCA kernel feature .


class: first -> second, base class: same cluster, new model class: no relation
first:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .
second:
It has been demonstrated that many distance metric learning or discriminative subspace based methods for person re - i d benefit from <m> kernelisation </m> because of the non - linearity in person ’s appearance .


class: first -> second, base class: no relation, new model class: no relation
first:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .
second:
Graphical abstractDisplay Omitted HighlightsDiscretization and spectrum-free computation of the <m> volumetric heat kernel </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .
second:
They can be used for <m> highest order Volterra kernel realization </m> as well .


class: first -> second, base class: no relation, new model class: no relation
first:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .
second:
In this paper , we present an algorithm that allows for different memory lengths of the <m> linear and quadratic Volterra kernel </m> while preserving the advantages of fast convolution techniques in the frequency domain for a second-order Volterra filter .


class: first -> second, base class: no relation, new model class: no relation
first:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .
second:
Subsequently , a general model that computes the ratio of two non-linear functionals , each comprising <m> linear ( first order Volterra kernel </m> ) and quadratic ( second-order Volterra kernel ) filters , is proposed .


class: first -> second, base class: no relation, new model class: no relation
first:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .
second:
It provides a set of diagnostic features formed on base of <m> multidimensional Volterra kernels </m> : discrete values of Volterra kernels , heuristic features , moments and wavelet transform coefficients .


class: first -> second, base class: no relation, new model class: no relation
first:
Such kernels are built with a parameterized “ neural response ” function , which consists in computing the maximal response of a <m> base kernel </m> over a local neighborhood .
second:
For <m> graphlet kernel </m> ( GK ) , we chose graphlets size and for deep graph kernels ( DGK ) , we report the best classification accuracy obtained among : deep graphlet kernel , deep shortest path kernel and deep Weisfeiler - Lehman kernel .


class: first -> second, base class: no relation, new model class: no relation
first:
Such kernels are built with a parameterized “ neural response ” function , which consists in computing the maximal response of a <m> base kernel </m> over a local neighborhood .
second:
Different from image deblurring , the <m> blur kernel setting </m> of SISR is usually simple .


class: first -> second, base class: no relation, new model class: no relation
first:
Such kernels are built with a parameterized “ neural response ” function , which consists in computing the maximal response of a <m> base kernel </m> over a local neighborhood .
second:
For example , defines a <m> kernel with dilation </m> .


class: first -> second, base class: same cluster, new model class: no relation
first:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .
second:
It has been demonstrated that many distance metric learning or discriminative subspace based methods for person re - i d benefit from <m> kernelisation </m> because of the non - linearity in person ’s appearance .


class: first -> second, base class: no relation, new model class: no relation
first:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .
second:
Graphical abstractDisplay Omitted HighlightsDiscretization and spectrum-free computation of the <m> volumetric heat kernel </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .
second:
They can be used for <m> highest order Volterra kernel realization </m> as well .


class: first -> second, base class: no relation, new model class: no relation
first:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .
second:
In this paper , we present an algorithm that allows for different memory lengths of the <m> linear and quadratic Volterra kernel </m> while preserving the advantages of fast convolution techniques in the frequency domain for a second-order Volterra filter .


class: first -> second, base class: no relation, new model class: no relation
first:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .
second:
Subsequently , a general model that computes the ratio of two non-linear functionals , each comprising <m> linear ( first order Volterra kernel </m> ) and quadratic ( second-order Volterra kernel ) filters , is proposed .


class: first -> second, base class: no relation, new model class: no relation
first:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .
second:
It provides a set of diagnostic features formed on base of <m> multidimensional Volterra kernels </m> : discrete values of Volterra kernels , heuristic features , moments and wavelet transform coefficients .


class: first -> second, base class: same cluster, new model class: no relation
first:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .
second:
It has been demonstrated that many distance metric learning or discriminative subspace based methods for person re - i d benefit from <m> kernelisation </m> because of the non - linearity in person ’s appearance .


class: first -> second, base class: no relation, new model class: no relation
first:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .
second:
Graphical abstractDisplay Omitted HighlightsDiscretization and spectrum-free computation of the <m> volumetric heat kernel </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .
second:
They can be used for <m> highest order Volterra kernel realization </m> as well .


class: first -> second, base class: no relation, new model class: no relation
first:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .
second:
In this paper , we present an algorithm that allows for different memory lengths of the <m> linear and quadratic Volterra kernel </m> while preserving the advantages of fast convolution techniques in the frequency domain for a second-order Volterra filter .


class: first -> second, base class: no relation, new model class: no relation
first:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .
second:
Subsequently , a general model that computes the ratio of two non-linear functionals , each comprising <m> linear ( first order Volterra kernel </m> ) and quadratic ( second-order Volterra kernel ) filters , is proposed .


class: first -> second, base class: no relation, new model class: no relation
first:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .
second:
It provides a set of diagnostic features formed on base of <m> multidimensional Volterra kernels </m> : discrete values of Volterra kernels , heuristic features , moments and wavelet transform coefficients .


class: first -> second, base class: no relation, new model class: no relation
first:
Moreover , since it does not need <m> word segmentation </m> , it can be applied to Eastern languages where they do not put clear spacing between words .
second:
By integrating this feature into the <m> multigram segmentation </m> , we considerably enhance segmentation performance .


class: first -> second, base class: no relation, new model class: no relation
first:
Moreover , since it does not need <m> word segmentation </m> , it can be applied to Eastern languages where they do not put clear spacing between words .
second:
The model is based on ( Goldwater et al. , 2006 ) <m> unigram word segmentation model </m> and assumes a simple prior distribution over morph length .


class: first -> second, base class: no relation, new model class: no relation
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .


class: first -> second, base class: no relation, new model class: no relation
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .


class: first -> second, base class: no relation, new model class: no relation
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .


class: first -> second, base class: no relation, new model class: no relation
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .


class: first -> second, base class: no relation, new model class: no relation
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .


class: first -> second, base class: no relation, new model class: no relation
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .


class: first -> second, base class: no relation, new model class: no relation
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .


class: first -> second, base class: no relation, new model class: no relation
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .


class: first -> second, base class: no relation, new model class: no relation
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .


class: first -> second, base class: no relation, new model class: no relation
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: first -> second, base class: no relation, new model class: no relation
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: first -> second, base class: no relation, new model class: no relation
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: first -> second, base class: no relation, new model class: no relation
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .


class: first -> second, base class: no relation, new model class: no relation
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .


class: first -> second, base class: no relation, new model class: no relation
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .


class: first -> second, base class: no relation, new model class: no relation
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .


class: first -> second, base class: no relation, new model class: no relation
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .


class: first -> second, base class: no relation, new model class: no relation
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .


class: first -> second, base class: no relation, new model class: no relation
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .


class: first -> second, base class: no relation, new model class: no relation
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .


class: first -> second, base class: no relation, new model class: no relation
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .


class: first -> second, base class: no relation, new model class: no relation
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: first -> second, base class: no relation, new model class: no relation
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: first -> second, base class: no relation, new model class: no relation
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: first -> second, base class: no relation, new model class: no relation
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .


class: first -> second, base class: no relation, new model class: no relation
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .


class: first -> second, base class: no relation, new model class: no relation
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .


class: first -> second, base class: no relation, new model class: no relation
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .


class: first -> second, base class: no relation, new model class: no relation
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .


class: first -> second, base class: no relation, new model class: no relation
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .


class: first -> second, base class: no relation, new model class: no relation
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .


class: first -> second, base class: no relation, new model class: no relation
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .


class: first -> second, base class: no relation, new model class: no relation
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .


class: first -> second, base class: no relation, new model class: no relation
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: first -> second, base class: no relation, new model class: no relation
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: first -> second, base class: no relation, new model class: no relation
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: first -> second, base class: no relation, new model class: no relation
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .


class: first -> second, base class: no relation, new model class: no relation
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .


class: first -> second, base class: no relation, new model class: no relation
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .


class: first -> second, base class: no relation, new model class: no relation
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .


class: first -> second, base class: no relation, new model class: no relation
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .


class: first -> second, base class: no relation, new model class: no relation
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .


class: first -> second, base class: no relation, new model class: no relation
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .


class: first -> second, base class: no relation, new model class: no relation
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .


class: first -> second, base class: no relation, new model class: no relation
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .


class: first -> second, base class: no relation, new model class: no relation
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: first -> second, base class: no relation, new model class: no relation
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: first -> second, base class: no relation, new model class: no relation
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: first -> second, base class: no relation, new model class: no relation
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .


class: first -> second, base class: no relation, new model class: no relation
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .


class: first -> second, base class: no relation, new model class: no relation
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .


class: first -> second, base class: no relation, new model class: no relation
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .


class: first -> second, base class: no relation, new model class: no relation
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .


class: first -> second, base class: no relation, new model class: no relation
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .


class: first -> second, base class: no relation, new model class: no relation
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .


class: first -> second, base class: no relation, new model class: no relation
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .


class: first -> second, base class: no relation, new model class: no relation
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .


class: first -> second, base class: no relation, new model class: no relation
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: first -> second, base class: no relation, new model class: no relation
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: first -> second, base class: no relation, new model class: no relation
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: first -> second, base class: no relation, new model class: no relation
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
Inspired by three characteristics , we develop three steps to <m> word and subword segmentation approach </m> : in the word or sub-words segmentation step , a peaks detection function is adopted to model the maximum and minimum peaks .
second:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .


class: first -> second, base class: no relation, new model class: no relation
first:
Inspired by three characteristics , we develop three steps to <m> word and subword segmentation approach </m> : in the word or sub-words segmentation step , a peaks detection function is adopted to model the maximum and minimum peaks .
second:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .


class: first -> second, base class: no relation, new model class: no relation
first:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .
second:
The improved Chinese word segmentation is made of a bigram segmentation and a segmentation correction , new words recognition and disambiguation through the <m> bigram segmentation </m> , check the accuracy of segmentation results using the segmentation correction from the perspective of syntax .


class: first -> second, base class: no relation, new model class: no relation
first:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .
second:
By integrating this feature into the <m> multigram segmentation </m> , we considerably enhance segmentation performance .


class: first -> second, base class: no relation, new model class: no relation
first:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .
second:
The model is based on ( Goldwater et al. , 2006 ) <m> unigram word segmentation model </m> and assumes a simple prior distribution over morph length .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Among the different techniques , the <m> word/sub-word segmentation </m> is simple and produces very good results .
second:
Therefore , in this paper , we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit <m> word segmentation </m> .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Among the different techniques , the <m> word/sub-word segmentation </m> is simple and produces very good results .
second:
This <m> word segmentation procedure </m> can be as simple as tokenization followed by some punctuation normalization , but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Among the different techniques , the <m> word/sub-word segmentation </m> is simple and produces very good results .
second:
paragraph : Why not Word - Level Translation ? The most pressing issue with word - level processing is that we do not have a perfect <m> word segmentation algorithm </m> for any one language .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Among the different techniques , the <m> word/sub-word segmentation </m> is simple and produces very good results .
second:
We discuss the suitability of different <m> word segmentation techniques </m> , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: no relation, new model class: same cluster
first:
Among the different techniques , the <m> word/sub-word segmentation </m> is simple and produces very good results .
second:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .


class: first -> second, base class: no relation, new model class: no relation
first:
Therefore , in this paper , we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit <m> word segmentation </m> .
second:
By integrating this feature into the <m> multigram segmentation </m> , we considerably enhance segmentation performance .


class: first -> second, base class: no relation, new model class: no relation
first:
Therefore , in this paper , we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit <m> word segmentation </m> .
second:
The model is based on ( Goldwater et al. , 2006 ) <m> unigram word segmentation model </m> and assumes a simple prior distribution over morph length .


class: first -> second, base class: no relation, new model class: no relation
first:
This <m> word segmentation procedure </m> can be as simple as tokenization followed by some punctuation normalization , but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance .
second:
By integrating this feature into the <m> multigram segmentation </m> , we considerably enhance segmentation performance .


class: first -> second, base class: no relation, new model class: no relation
first:
This <m> word segmentation procedure </m> can be as simple as tokenization followed by some punctuation normalization , but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance .
second:
The model is based on ( Goldwater et al. , 2006 ) <m> unigram word segmentation model </m> and assumes a simple prior distribution over morph length .


class: first -> second, base class: no relation, new model class: no relation
first:
paragraph : Why not Word - Level Translation ? The most pressing issue with word - level processing is that we do not have a perfect <m> word segmentation algorithm </m> for any one language .
second:
By integrating this feature into the <m> multigram segmentation </m> , we considerably enhance segmentation performance .


class: first -> second, base class: no relation, new model class: no relation
first:
paragraph : Why not Word - Level Translation ? The most pressing issue with word - level processing is that we do not have a perfect <m> word segmentation algorithm </m> for any one language .
second:
The model is based on ( Goldwater et al. , 2006 ) <m> unigram word segmentation model </m> and assumes a simple prior distribution over morph length .


class: first -> second, base class: no relation, new model class: no relation
first:
We discuss the suitability of different <m> word segmentation techniques </m> , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .
second:
By integrating this feature into the <m> multigram segmentation </m> , we considerably enhance segmentation performance .


class: first -> second, base class: no relation, new model class: no relation
first:
We discuss the suitability of different <m> word segmentation techniques </m> , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .
second:
The model is based on ( Goldwater et al. , 2006 ) <m> unigram word segmentation model </m> and assumes a simple prior distribution over morph length .


class: first -> second, base class: no relation, new model class: no relation
first:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .
second:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .


class: first -> second, base class: no relation, new model class: no relation
first:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .
second:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .


class: first -> second, base class: no relation, new model class: no relation
first:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .
second:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .


class: first -> second, base class: no relation, new model class: no relation
first:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .
second:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .


class: first -> second, base class: no relation, new model class: no relation
first:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .
second:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .


class: first -> second, base class: no relation, new model class: no relation
first:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .
second:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .


class: first -> second, base class: no relation, new model class: no relation
first:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .
second:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .


class: first -> second, base class: no relation, new model class: no relation
first:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .
second:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .


class: first -> second, base class: no relation, new model class: no relation
first:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .
second:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .


class: first -> second, base class: no relation, new model class: no relation
first:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: first -> second, base class: no relation, new model class: no relation
first:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: first -> second, base class: no relation, new model class: no relation
first:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: first -> second, base class: no relation, new model class: no relation
first:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: first -> second, base class: no relation, new model class: no relation
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
For Switchboard , our <m> phone-based BPE system </m> achieves 6.8%/14.4 % word error rate ( WER ) on the Switchboard/CallHome portion of the test set while the ensemble system achieves 6.3%/13.3 % WER .


class: first -> second, base class: no relation, new model class: no relation
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
We test both <m> word - based and BPE vocabularies </m> ( § 4 ) .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
We used the <m> byte pair encoding compression algorithm </m> to build a new vocabulary for the inputs of the classifier .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
We discuss the suitability of different word segmentation techniques , including simple character n-gram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
In this paper , a try has been made to develop few algorithms for random access text compression based on the <m> byte pair encoding scheme </m> ( Gage , 1997 ) .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
Five algorithms are developed based on this <m> byte pair encoding scheme </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
A three-stage training based on three levels of architectural granularity namely , character encoder , <m> byte pair encoding ( BPE ) based encoder </m> , and attention decoder , is proposed .


class: first -> second, base class: no relation, new model class: same cluster
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
For Switchboard , our <m> phone-based BPE system </m> achieves 6.8%/14.4 % word error rate ( WER ) on the Switchboard/CallHome portion of the test set while the ensemble system achieves 6.3%/13.3 % WER .


class: first -> second, base class: no relation, new model class: no relation
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
We test both <m> word - based and BPE vocabularies </m> ( § 4 ) .


class: first -> second, base class: no relation, new model class: same cluster
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
We used the <m> byte pair encoding compression algorithm </m> to build a new vocabulary for the inputs of the classifier .


class: first -> second, base class: no relation, new model class: same cluster
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
We discuss the suitability of different word segmentation techniques , including simple character n-gram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: no relation, new model class: same cluster
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
In this paper , a try has been made to develop few algorithms for random access text compression based on the <m> byte pair encoding scheme </m> ( Gage , 1997 ) .


class: first -> second, base class: no relation, new model class: same cluster
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
Five algorithms are developed based on this <m> byte pair encoding scheme </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
A three-stage training based on three levels of architectural granularity namely , character encoder , <m> byte pair encoding ( BPE ) based encoder </m> , and attention decoder , is proposed .


class: first -> second, base class: no relation, new model class: same cluster
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: no relation, new model class: second -> first
first:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .
second:
We test both <m> word - based and BPE vocabularies </m> ( § 4 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .
second:
We used the <m> byte pair encoding compression algorithm </m> to build a new vocabulary for the inputs of the classifier .


class: first -> second, base class: no relation, new model class: no relation
first:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .
second:
We discuss the suitability of different word segmentation techniques , including simple character n-gram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .
second:
In this paper , a try has been made to develop few algorithms for random access text compression based on the <m> byte pair encoding scheme </m> ( Gage , 1997 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .
second:
Five algorithms are developed based on this <m> byte pair encoding scheme </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .
second:
A three-stage training based on three levels of architectural granularity namely , character encoder , <m> byte pair encoding ( BPE ) based encoder </m> , and attention decoder , is proposed .


class: first -> second, base class: no relation, new model class: no relation
first:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .
second:
We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .
second:
We test both <m> word - based and BPE vocabularies </m> ( § 4 ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .
second:
We used the <m> byte pair encoding compression algorithm </m> to build a new vocabulary for the inputs of the classifier .


class: first -> second, base class: same cluster, new model class: same cluster
first:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .
second:
We discuss the suitability of different word segmentation techniques , including simple character n-gram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: same cluster, new model class: same cluster
first:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .
second:
In this paper , a try has been made to develop few algorithms for random access text compression based on the <m> byte pair encoding scheme </m> ( Gage , 1997 ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .
second:
Five algorithms are developed based on this <m> byte pair encoding scheme </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .
second:
A three-stage training based on three levels of architectural granularity namely , character encoder , <m> byte pair encoding ( BPE ) based encoder </m> , and attention decoder , is proposed .


class: first -> second, base class: same cluster, new model class: same cluster
first:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .
second:
We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: no relation, new model class: second -> first
first:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .
second:
We test both <m> word - based and BPE vocabularies </m> ( § 4 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .
second:
We used the <m> byte pair encoding compression algorithm </m> to build a new vocabulary for the inputs of the classifier .


class: first -> second, base class: no relation, new model class: no relation
first:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .
second:
We discuss the suitability of different word segmentation techniques , including simple character n-gram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: no relation, new model class: no relation
first:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .
second:
In this paper , a try has been made to develop few algorithms for random access text compression based on the <m> byte pair encoding scheme </m> ( Gage , 1997 ) .


class: first -> second, base class: no relation, new model class: no relation
first:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .
second:
Five algorithms are developed based on this <m> byte pair encoding scheme </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .
second:
A three-stage training based on three levels of architectural granularity namely , character encoder , <m> byte pair encoding ( BPE ) based encoder </m> , and attention decoder , is proposed .


class: first -> second, base class: no relation, new model class: no relation
first:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .
second:
We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: no relation, new model class: same cluster
first:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .
second:
We used the <m> byte pair encoding compression algorithm </m> to build a new vocabulary for the inputs of the classifier .


class: first -> second, base class: no relation, new model class: same cluster
first:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .
second:
We discuss the suitability of different word segmentation techniques , including simple character n-gram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: no relation, new model class: same cluster
first:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .
second:
In this paper , a try has been made to develop few algorithms for random access text compression based on the <m> byte pair encoding scheme </m> ( Gage , 1997 ) .


class: first -> second, base class: no relation, new model class: same cluster
first:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .
second:
Five algorithms are developed based on this <m> byte pair encoding scheme </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .
second:
A three-stage training based on three levels of architectural granularity namely , character encoder , <m> byte pair encoding ( BPE ) based encoder </m> , and attention decoder , is proposed .


class: first -> second, base class: no relation, new model class: same cluster
first:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .
second:
We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: no relation, new model class: same cluster
first:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .
second:
We used the <m> byte pair encoding compression algorithm </m> to build a new vocabulary for the inputs of the classifier .


class: first -> second, base class: no relation, new model class: same cluster
first:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .
second:
We discuss the suitability of different word segmentation techniques , including simple character n-gram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: no relation, new model class: same cluster
first:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .
second:
In this paper , a try has been made to develop few algorithms for random access text compression based on the <m> byte pair encoding scheme </m> ( Gage , 1997 ) .


class: first -> second, base class: no relation, new model class: same cluster
first:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .
second:
Five algorithms are developed based on this <m> byte pair encoding scheme </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .
second:
A three-stage training based on three levels of architectural granularity namely , character encoder , <m> byte pair encoding ( BPE ) based encoder </m> , and attention decoder , is proposed .


class: first -> second, base class: no relation, new model class: same cluster
first:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .
second:
We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: no relation, new model class: same cluster
first:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .
second:
We used the <m> byte pair encoding compression algorithm </m> to build a new vocabulary for the inputs of the classifier .


class: first -> second, base class: no relation, new model class: same cluster
first:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .
second:
We discuss the suitability of different word segmentation techniques , including simple character n-gram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: no relation, new model class: same cluster
first:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .
second:
In this paper , a try has been made to develop few algorithms for random access text compression based on the <m> byte pair encoding scheme </m> ( Gage , 1997 ) .


class: first -> second, base class: no relation, new model class: same cluster
first:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .
second:
Five algorithms are developed based on this <m> byte pair encoding scheme </m> .


class: first -> second, base class: no relation, new model class: same cluster
first:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .
second:
A three-stage training based on three levels of architectural granularity namely , character encoder , <m> byte pair encoding ( BPE ) based encoder </m> , and attention decoder , is proposed .


class: first -> second, base class: no relation, new model class: same cluster
first:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .
second:
We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .
second:
We used the <m> byte pair encoding compression algorithm </m> to build a new vocabulary for the inputs of the classifier .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .
second:
We discuss the suitability of different word segmentation techniques , including simple character n-gram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .
second:
In this paper , a try has been made to develop few algorithms for random access text compression based on the <m> byte pair encoding scheme </m> ( Gage , 1997 ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .
second:
Five algorithms are developed based on this <m> byte pair encoding scheme </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .
second:
A three-stage training based on three levels of architectural granularity namely , character encoder , <m> byte pair encoding ( BPE ) based encoder </m> , and attention decoder , is proposed .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .
second:
We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .
second:
We used the <m> byte pair encoding compression algorithm </m> to build a new vocabulary for the inputs of the classifier .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .
second:
We discuss the suitability of different word segmentation techniques , including simple character n-gram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .
second:
In this paper , a try has been made to develop few algorithms for random access text compression based on the <m> byte pair encoding scheme </m> ( Gage , 1997 ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .
second:
Five algorithms are developed based on this <m> byte pair encoding scheme </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .
second:
A three-stage training based on three levels of architectural granularity namely , character encoder , <m> byte pair encoding ( BPE ) based encoder </m> , and attention decoder , is proposed .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .
second:
We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .
second:
We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .
second:
We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: same cluster, new model class: same cluster
first:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .
second:
We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The results show that combining unigrams and PoS-filtered skipgrams leads to a significant improvement in classification scores over the <m> unigram baseline </m> .
second:
We report results in Table [ reference ] , along with ablation studies for removing the cross - bigram features ( leaving only the cross - <m> unigram feature </m> ) and for removing all lexicalized features .


class: first -> second, base class: same cluster, new model class: same cluster
first:
The results show that combining unigrams and PoS-filtered skipgrams leads to a significant improvement in classification scores over the <m> unigram baseline </m> .
second:
Only the <m> unigram representation </m> is truly open - vocabulary .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Traditional approaches used <m> unigram based models </m> for text classification .
second:
We report results in Table [ reference ] , along with ablation studies for removing the cross - bigram features ( leaving only the cross - <m> unigram feature </m> ) and for removing all lexicalized features .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Traditional approaches used <m> unigram based models </m> for text classification .
second:
Only the <m> unigram representation </m> is truly open - vocabulary .


class: first -> second, base class: same cluster, new model class: same cluster
first:
A <m> unigram analysis </m> of our annotated data shows that student learning correlates both with the tutor 's dialogue acts and with the student 's dialogue acts .
second:
We report results in Table [ reference ] , along with ablation studies for removing the cross - bigram features ( leaving only the cross - <m> unigram feature </m> ) and for removing all lexicalized features .


class: first -> second, base class: same cluster, new model class: same cluster
first:
A <m> unigram analysis </m> of our annotated data shows that student learning correlates both with the tutor 's dialogue acts and with the student 's dialogue acts .
second:
Only the <m> unigram representation </m> is truly open - vocabulary .


class: first -> second, base class: same cluster, new model class: same cluster
first:
By augmenting conventional techniques of topic modeling with <m> unigram analysis </m> and community detection , we establish an automated method that generates a comprehensive and meaningful summary of forum conversations over time that also sheds light on patterns of user behavior .
second:
We report results in Table [ reference ] , along with ablation studies for removing the cross - bigram features ( leaving only the cross - <m> unigram feature </m> ) and for removing all lexicalized features .


class: first -> second, base class: same cluster, new model class: same cluster
first:
By augmenting conventional techniques of topic modeling with <m> unigram analysis </m> and community detection , we establish an automated method that generates a comprehensive and meaningful summary of forum conversations over time that also sheds light on patterns of user behavior .
second:
Only the <m> unigram representation </m> is truly open - vocabulary .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Unbiased tweets were extracted from Twitter related to this specific campaign , and on comparing with manual tagging we were able to achieve 84.47 % accuracy using <m> unigram machine learning approach </m> .
second:
We report results in Table [ reference ] , along with ablation studies for removing the cross - bigram features ( leaving only the cross - <m> unigram feature </m> ) and for removing all lexicalized features .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Unbiased tweets were extracted from Twitter related to this specific campaign , and on comparing with manual tagging we were able to achieve 84.47 % accuracy using <m> unigram machine learning approach </m> .
second:
Only the <m> unigram representation </m> is truly open - vocabulary .


class: first -> second, base class: second -> first, new model class: second -> first
first:
One may also use a <m> unigram distribution </m> which has been shown to work better on some tasks pereyra2017regularize .
second:
We report results in Table [ reference ] , along with ablation studies for removing the cross - bigram features ( leaving only the cross - <m> unigram feature </m> ) and for removing all lexicalized features .


class: first -> second, base class: second -> first, new model class: second -> first
first:
One may also use a <m> unigram distribution </m> which has been shown to work better on some tasks pereyra2017regularize .
second:
Only the <m> unigram representation </m> is truly open - vocabulary .


class: first -> second, base class: no relation, new model class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
By analyzing the unstructured characteristics of online ads and search engine user behavior data , proposed a kind of <m> personalized ads recommendation method </m> based on User-Interest-Behavior model , which can extract the user ’s interest preferences by the topic model and generate the recommended list of ads based on the nearest neighbor and user behavior .


class: first -> second, base class: no relation, new model class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
With the popularity of social networks such as Facebook and twitter , social recommendations have become possible , which rely on individual 's social connections in order to make <m> personalized recommendations of ads </m> , content , products , and people .


class: first -> second, base class: no relation, new model class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
In the first paper , Adany , Kraus and Ordonez investigate the area of <m> personalized TV ads </m> to improve the effectiveness of ads distribution to targeted viewers .


class: first -> second, base class: no relation, new model class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
By analyzing the unstructured characteristics of online ads and search engine user behavior data , proposed a kind of <m> personalized ads recommendation method </m> based on User-Interest-Behavior model , which can extract the user ’s interest preferences by the topic model and generate the recommended list of ads based on the nearest neighbor and user behavior .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
With the popularity of social networks such as Facebook and twitter , social recommendations have become possible , which rely on individual 's social connections in order to make <m> personalized recommendations of ads </m> , content , products , and people .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
In the first paper , Adany , Kraus and Ordonez investigate the area of <m> personalized TV ads </m> to improve the effectiveness of ads distribution to targeted viewers .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: first -> second, base class: no relation, new model class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
By analyzing the unstructured characteristics of online ads and search engine user behavior data , proposed a kind of <m> personalized ads recommendation method </m> based on User-Interest-Behavior model , which can extract the user ’s interest preferences by the topic model and generate the recommended list of ads based on the nearest neighbor and user behavior .


class: first -> second, base class: no relation, new model class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
With the popularity of social networks such as Facebook and twitter , social recommendations have become possible , which rely on individual 's social connections in order to make <m> personalized recommendations of ads </m> , content , products , and people .


class: first -> second, base class: no relation, new model class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
In the first paper , Adany , Kraus and Ordonez investigate the area of <m> personalized TV ads </m> to improve the effectiveness of ads distribution to targeted viewers .


class: first -> second, base class: no relation, new model class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: first -> second, base class: no relation, new model class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
By analyzing the unstructured characteristics of online ads and search engine user behavior data , proposed a kind of <m> personalized ads recommendation method </m> based on User-Interest-Behavior model , which can extract the user ’s interest preferences by the topic model and generate the recommended list of ads based on the nearest neighbor and user behavior .


class: first -> second, base class: no relation, new model class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
With the popularity of social networks such as Facebook and twitter , social recommendations have become possible , which rely on individual 's social connections in order to make <m> personalized recommendations of ads </m> , content , products , and people .


class: first -> second, base class: no relation, new model class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
In the first paper , Adany , Kraus and Ordonez investigate the area of <m> personalized TV ads </m> to improve the effectiveness of ads distribution to targeted viewers .


class: first -> second, base class: no relation, new model class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: first -> second, base class: no relation, new model class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
By analyzing the unstructured characteristics of online ads and search engine user behavior data , proposed a kind of <m> personalized ads recommendation method </m> based on User-Interest-Behavior model , which can extract the user ’s interest preferences by the topic model and generate the recommended list of ads based on the nearest neighbor and user behavior .


class: first -> second, base class: no relation, new model class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
With the popularity of social networks such as Facebook and twitter , social recommendations have become possible , which rely on individual 's social connections in order to make <m> personalized recommendations of ads </m> , content , products , and people .


class: first -> second, base class: no relation, new model class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
In the first paper , Adany , Kraus and Ordonez investigate the area of <m> personalized TV ads </m> to improve the effectiveness of ads distribution to targeted viewers .


class: first -> second, base class: no relation, new model class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: first -> second, base class: no relation, new model class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
By analyzing the unstructured characteristics of online ads and search engine user behavior data , proposed a kind of <m> personalized ads recommendation method </m> based on User-Interest-Behavior model , which can extract the user ’s interest preferences by the topic model and generate the recommended list of ads based on the nearest neighbor and user behavior .


class: first -> second, base class: no relation, new model class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
With the popularity of social networks such as Facebook and twitter , social recommendations have become possible , which rely on individual 's social connections in order to make <m> personalized recommendations of ads </m> , content , products , and people .


class: first -> second, base class: no relation, new model class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
In the first paper , Adany , Kraus and Ordonez investigate the area of <m> personalized TV ads </m> to improve the effectiveness of ads distribution to targeted viewers .


class: first -> second, base class: no relation, new model class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: first -> second, base class: no relation, new model class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
By analyzing the unstructured characteristics of online ads and search engine user behavior data , proposed a kind of <m> personalized ads recommendation method </m> based on User-Interest-Behavior model , which can extract the user ’s interest preferences by the topic model and generate the recommended list of ads based on the nearest neighbor and user behavior .


class: first -> second, base class: no relation, new model class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
With the popularity of social networks such as Facebook and twitter , social recommendations have become possible , which rely on individual 's social connections in order to make <m> personalized recommendations of ads </m> , content , products , and people .


class: first -> second, base class: no relation, new model class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
In the first paper , Adany , Kraus and Ordonez investigate the area of <m> personalized TV ads </m> to improve the effectiveness of ads distribution to targeted viewers .


class: first -> second, base class: no relation, new model class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: first -> second, base class: no relation, new model class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
By analyzing the unstructured characteristics of online ads and search engine user behavior data , proposed a kind of <m> personalized ads recommendation method </m> based on User-Interest-Behavior model , which can extract the user ’s interest preferences by the topic model and generate the recommended list of ads based on the nearest neighbor and user behavior .


class: first -> second, base class: no relation, new model class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
With the popularity of social networks such as Facebook and twitter , social recommendations have become possible , which rely on individual 's social connections in order to make <m> personalized recommendations of ads </m> , content , products , and people .


class: first -> second, base class: no relation, new model class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
In the first paper , Adany , Kraus and Ordonez investigate the area of <m> personalized TV ads </m> to improve the effectiveness of ads distribution to targeted viewers .


class: first -> second, base class: no relation, new model class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: first -> second, base class: no relation, new model class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
By analyzing the unstructured characteristics of online ads and search engine user behavior data , proposed a kind of <m> personalized ads recommendation method </m> based on User-Interest-Behavior model , which can extract the user ’s interest preferences by the topic model and generate the recommended list of ads based on the nearest neighbor and user behavior .


class: first -> second, base class: no relation, new model class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
With the popularity of social networks such as Facebook and twitter , social recommendations have become possible , which rely on individual 's social connections in order to make <m> personalized recommendations of ads </m> , content , products , and people .


class: first -> second, base class: no relation, new model class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
In the first paper , Adany , Kraus and Ordonez investigate the area of <m> personalized TV ads </m> to improve the effectiveness of ads distribution to targeted viewers .


class: first -> second, base class: no relation, new model class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: first -> second, base class: no relation, new model class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
By analyzing the unstructured characteristics of online ads and search engine user behavior data , proposed a kind of <m> personalized ads recommendation method </m> based on User-Interest-Behavior model , which can extract the user ’s interest preferences by the topic model and generate the recommended list of ads based on the nearest neighbor and user behavior .


class: first -> second, base class: no relation, new model class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
With the popularity of social networks such as Facebook and twitter , social recommendations have become possible , which rely on individual 's social connections in order to make <m> personalized recommendations of ads </m> , content , products , and people .


class: first -> second, base class: no relation, new model class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
In the first paper , Adany , Kraus and Ordonez investigate the area of <m> personalized TV ads </m> to improve the effectiveness of ads distribution to targeted viewers .


class: first -> second, base class: no relation, new model class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: first -> second, base class: no relation, new model class: same cluster
first:
In the first part , the scope and features of <m> digital advertising </m> as well as the tools deployed are presented and defined .
second:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In the first part , the scope and features of <m> digital advertising </m> as well as the tools deployed are presented and defined .
second:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .


class: first -> second, base class: no relation, new model class: no relation
first:
In the first part , the scope and features of <m> digital advertising </m> as well as the tools deployed are presented and defined .
second:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .


class: first -> second, base class: no relation, new model class: no relation
first:
In the first part , the scope and features of <m> digital advertising </m> as well as the tools deployed are presented and defined .
second:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In the first part , the scope and features of <m> digital advertising </m> as well as the tools deployed are presented and defined .
second:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
In the first part , the scope and features of <m> digital advertising </m> as well as the tools deployed are presented and defined .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: first -> second, base class: no relation, new model class: no relation
first:
In the first part , the scope and features of <m> digital advertising </m> as well as the tools deployed are presented and defined .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: first -> second, base class: no relation, new model class: same cluster
first:
While predictive modeling has been thoroughly researched in recent years in the <m> digital advertising domain </m> , the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification .
second:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
While predictive modeling has been thoroughly researched in recent years in the <m> digital advertising domain </m> , the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification .
second:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .


class: first -> second, base class: no relation, new model class: no relation
first:
While predictive modeling has been thoroughly researched in recent years in the <m> digital advertising domain </m> , the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification .
second:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .


class: first -> second, base class: no relation, new model class: no relation
first:
While predictive modeling has been thoroughly researched in recent years in the <m> digital advertising domain </m> , the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification .
second:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
While predictive modeling has been thoroughly researched in recent years in the <m> digital advertising domain </m> , the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification .
second:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
While predictive modeling has been thoroughly researched in recent years in the <m> digital advertising domain </m> , the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: first -> second, base class: no relation, new model class: no relation
first:
While predictive modeling has been thoroughly researched in recent years in the <m> digital advertising domain </m> , the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: first -> second, base class: no relation, new model class: same cluster
first:
This paper reports a multi-phase study to address the following questions in the <m> digital advertising context </m> : ( 1 ) to what extent did consumers use ads in the past ? ( 2 ) What are consumers ’ perceived values toward ads ?
second:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper reports a multi-phase study to address the following questions in the <m> digital advertising context </m> : ( 1 ) to what extent did consumers use ads in the past ? ( 2 ) What are consumers ’ perceived values toward ads ?
second:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper reports a multi-phase study to address the following questions in the <m> digital advertising context </m> : ( 1 ) to what extent did consumers use ads in the past ? ( 2 ) What are consumers ’ perceived values toward ads ?
second:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper reports a multi-phase study to address the following questions in the <m> digital advertising context </m> : ( 1 ) to what extent did consumers use ads in the past ? ( 2 ) What are consumers ’ perceived values toward ads ?
second:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper reports a multi-phase study to address the following questions in the <m> digital advertising context </m> : ( 1 ) to what extent did consumers use ads in the past ? ( 2 ) What are consumers ’ perceived values toward ads ?
second:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper reports a multi-phase study to address the following questions in the <m> digital advertising context </m> : ( 1 ) to what extent did consumers use ads in the past ? ( 2 ) What are consumers ’ perceived values toward ads ?
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper reports a multi-phase study to address the following questions in the <m> digital advertising context </m> : ( 1 ) to what extent did consumers use ads in the past ? ( 2 ) What are consumers ’ perceived values toward ads ?
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: first -> second, base class: no relation, new model class: same cluster
first:
What is the simplicity and complexity of the digital advertising design structure?The purpose of the research is to focus on the discovery of the rug and the complexity of the structure of the design of digital advertising , while the importance of research by contributing to enrich the knowledge and application of the specialists and specialists to study the simplicity and complexity in the structure of <m> digital advertising design </m> and the introduction of specialized designers working on the subject of simplicity and complexity in the design of digital advertising And the terminology in it , and the researcher has developed spatial boundaries represented by a number of Iraqi digital ad designs , and time limits ( limited in the period of 2017 ) As for the substantive boundaries represented in ( study of simplicity and complexity in the structure of design Digital advertising.while the second section chapter which consist the theoretical frame which interest in three researcher , the first research turning to ( Simplicity in the digital advertising design)while the the second research interest in ( complexity in the digital advertising design)and the third research turning into about the(structure digital advertising design ) .
second:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
What is the simplicity and complexity of the digital advertising design structure?The purpose of the research is to focus on the discovery of the rug and the complexity of the structure of the design of digital advertising , while the importance of research by contributing to enrich the knowledge and application of the specialists and specialists to study the simplicity and complexity in the structure of <m> digital advertising design </m> and the introduction of specialized designers working on the subject of simplicity and complexity in the design of digital advertising And the terminology in it , and the researcher has developed spatial boundaries represented by a number of Iraqi digital ad designs , and time limits ( limited in the period of 2017 ) As for the substantive boundaries represented in ( study of simplicity and complexity in the structure of design Digital advertising.while the second section chapter which consist the theoretical frame which interest in three researcher , the first research turning to ( Simplicity in the digital advertising design)while the the second research interest in ( complexity in the digital advertising design)and the third research turning into about the(structure digital advertising design ) .
second:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .


class: first -> second, base class: no relation, new model class: no relation
first:
What is the simplicity and complexity of the digital advertising design structure?The purpose of the research is to focus on the discovery of the rug and the complexity of the structure of the design of digital advertising , while the importance of research by contributing to enrich the knowledge and application of the specialists and specialists to study the simplicity and complexity in the structure of <m> digital advertising design </m> and the introduction of specialized designers working on the subject of simplicity and complexity in the design of digital advertising And the terminology in it , and the researcher has developed spatial boundaries represented by a number of Iraqi digital ad designs , and time limits ( limited in the period of 2017 ) As for the substantive boundaries represented in ( study of simplicity and complexity in the structure of design Digital advertising.while the second section chapter which consist the theoretical frame which interest in three researcher , the first research turning to ( Simplicity in the digital advertising design)while the the second research interest in ( complexity in the digital advertising design)and the third research turning into about the(structure digital advertising design ) .
second:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .


class: first -> second, base class: no relation, new model class: no relation
first:
What is the simplicity and complexity of the digital advertising design structure?The purpose of the research is to focus on the discovery of the rug and the complexity of the structure of the design of digital advertising , while the importance of research by contributing to enrich the knowledge and application of the specialists and specialists to study the simplicity and complexity in the structure of <m> digital advertising design </m> and the introduction of specialized designers working on the subject of simplicity and complexity in the design of digital advertising And the terminology in it , and the researcher has developed spatial boundaries represented by a number of Iraqi digital ad designs , and time limits ( limited in the period of 2017 ) As for the substantive boundaries represented in ( study of simplicity and complexity in the structure of design Digital advertising.while the second section chapter which consist the theoretical frame which interest in three researcher , the first research turning to ( Simplicity in the digital advertising design)while the the second research interest in ( complexity in the digital advertising design)and the third research turning into about the(structure digital advertising design ) .
second:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
What is the simplicity and complexity of the digital advertising design structure?The purpose of the research is to focus on the discovery of the rug and the complexity of the structure of the design of digital advertising , while the importance of research by contributing to enrich the knowledge and application of the specialists and specialists to study the simplicity and complexity in the structure of <m> digital advertising design </m> and the introduction of specialized designers working on the subject of simplicity and complexity in the design of digital advertising And the terminology in it , and the researcher has developed spatial boundaries represented by a number of Iraqi digital ad designs , and time limits ( limited in the period of 2017 ) As for the substantive boundaries represented in ( study of simplicity and complexity in the structure of design Digital advertising.while the second section chapter which consist the theoretical frame which interest in three researcher , the first research turning to ( Simplicity in the digital advertising design)while the the second research interest in ( complexity in the digital advertising design)and the third research turning into about the(structure digital advertising design ) .
second:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
What is the simplicity and complexity of the digital advertising design structure?The purpose of the research is to focus on the discovery of the rug and the complexity of the structure of the design of digital advertising , while the importance of research by contributing to enrich the knowledge and application of the specialists and specialists to study the simplicity and complexity in the structure of <m> digital advertising design </m> and the introduction of specialized designers working on the subject of simplicity and complexity in the design of digital advertising And the terminology in it , and the researcher has developed spatial boundaries represented by a number of Iraqi digital ad designs , and time limits ( limited in the period of 2017 ) As for the substantive boundaries represented in ( study of simplicity and complexity in the structure of design Digital advertising.while the second section chapter which consist the theoretical frame which interest in three researcher , the first research turning to ( Simplicity in the digital advertising design)while the the second research interest in ( complexity in the digital advertising design)and the third research turning into about the(structure digital advertising design ) .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: first -> second, base class: no relation, new model class: no relation
first:
What is the simplicity and complexity of the digital advertising design structure?The purpose of the research is to focus on the discovery of the rug and the complexity of the structure of the design of digital advertising , while the importance of research by contributing to enrich the knowledge and application of the specialists and specialists to study the simplicity and complexity in the structure of <m> digital advertising design </m> and the introduction of specialized designers working on the subject of simplicity and complexity in the design of digital advertising And the terminology in it , and the researcher has developed spatial boundaries represented by a number of Iraqi digital ad designs , and time limits ( limited in the period of 2017 ) As for the substantive boundaries represented in ( study of simplicity and complexity in the structure of design Digital advertising.while the second section chapter which consist the theoretical frame which interest in three researcher , the first research turning to ( Simplicity in the digital advertising design)while the the second research interest in ( complexity in the digital advertising design)and the third research turning into about the(structure digital advertising design ) .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: first -> second, base class: no relation, new model class: no relation
first:
We discuss in detail the effects of roles on each NE and on semantic categories of noun and verb features in terms of accuracy , precision/recall and F-score measures for the <m> text classification task </m> .
second:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .


class: first -> second, base class: no relation, new model class: no relation
first:
In this paper , we address the <m> text classification problem </m> that a period of time created test data is different from the training data , and present a method for text classification based on temporal adaptation .
second:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .


class: first -> second, base class: no relation, new model class: no relation
first:
Given a small subset of relations between the individuals , the problem of learning social network is translated into a <m> text classification problem </m> .
second:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .


class: first -> second, base class: no relation, new model class: no relation
first:
We present a geo-enriched classifier joining established methods for <m> text-based classification </m> with location-based topic prediction .
second:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .


class: first -> second, base class: no relation, new model class: no relation
first:
Using a matrix of closeness and a set of additional fusion rules , the proposed method improves the classification performance by only subjecting likely misclassified samples to a text-based classifier followed by additional fusion of both image-based classification and <m> text-based classification </m> results .
second:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .


class: first -> second, base class: no relation, new model class: no relation
first:
The model of <m> text classifying </m> using non-labeled training sample with more extensive application is realized .
second:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .


class: first -> second, base class: no relation, new model class: no relation
first:
For example , in <m> text data classification </m> , groups of documents of different types are categorized by different subsets of terms .
second:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .


class: first -> second, base class: no relation, new model class: no relation
first:
This study contributes to the field of network troubleshooting , and the field of <m> text data classification </m> .
second:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .


class: first -> second, base class: no relation, new model class: no relation
first:
Text information extraction comprises of <m> text image classification </m> , text detection , localization , segmentation , enhancement and recognition .
second:
These traditional methods adopt a bottom - up strategy and often needs several steps to detect texts ( e.g. , character detection , text line construction and <m> text line classification </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
KeywoRDS Confidence Computation , Document Image , Fuzzy Matching , Handwritten Documents , Performance Analysis , Printed Documents , SVM , <m> Text Image Classification </m> , Word Association
second:
These traditional methods adopt a bottom - up strategy and often needs several steps to detect texts ( e.g. , character detection , text line construction and <m> text line classification </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Abstract The genetic selection of keyword sets , the text frequencies of which are considered as attributes in <m> text classification analysis </m> , has been analysed .
second:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .


class: first -> second, base class: no relation, new model class: no relation
first:
In <m> Text Classification of Methods </m> , there is a sentence “ Where P(B|A ) is the probability of B appearance when A is known ?
second:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .


class: first -> second, base class: no relation, new model class: no relation
first:
<m> TEXT CLASSIFICATION Text classification </m> is a commonly used technique as a basis for applications in document processing and Visualization , web mining , surveillance technology , patent Analysis , etc .
second:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .


class: first -> second, base class: no relation, new model class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .


class: first -> second, base class: no relation, new model class: no relation
first:
However , how well do these models perform on practical <m> text classification problems </m> , with real world data ?
second:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .


class: first -> second, base class: no relation, new model class: no relation
first:
It is difficult to fit a single model for <m> text classification </m> across domains , due to unknown words , specialized context , colloquial language , and other differences between domains .
second:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .


class: first -> second, base class: no relation, new model class: no relation
first:
section : Conclusion In this work we demonstrate that unsupervised pretraining and finetuning provides a flexible framework that is effective for difficult <m> text classification tasks </m> .
second:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .


class: first -> second, base class: no relation, new model class: no relation
first:
This paper studies CNN on <m> text categorization </m> to exploit the 1D structure ( namely , word order ) of text data for accurate prediction .
second:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .


class: first -> second, base class: no relation, new model class: no relation
first:
Learning distributed sentence representation through neural network models requires little external domain knowledge and can reach satisfactory results in related tasks like sentiment classification , <m> text categorization </m> .
second:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .


class: first -> second, base class: no relation, new model class: no relation
first:
Meanwhile , linear classifiers are often considered as strong baselines for <m> text classification problems </m> .
second:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
In order to test the applicability of the obtained ontologies , a <m> text categorization </m> experiment has been proposed and the obtained results indicate that the approach can be applied with satisfactory results and warrants further research .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
This paper describes a study on <m> text categorization </m> using a character n­grams approach for the morphological normalization .


class: first -> second, base class: second -> first, new model class: no relation
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
Authorship attribution may be considered as a <m> text categorization problem </m> .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
In contrast to previous <m> text categorization research </m> in e-rulemaking [ 5 , 6 ] , and in an attempt to more closely duplicate the comment analysis process in federal agencies , we employ a set of rule-specific categories , each of which corresponds to a significant issue raised in the comments .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
Therefore , <m> text categorization research </m> has become more important .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
This method can improve the accuracy of <m> text categorizing </m> efficiently , which is proved by experiments .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
Making authoritative corpus and standard of <m> text categorizing </m> are suggested for evaluation and application of text categorizing model .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
In direct proportion to the heavy increase of online information data , the attention to <m> text categorization ( classification </m> ) has also increased .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
While the former problem may be addressed using one of the well known techniques of <m> text categorization ( classification </m> ) , the latter seems to require some distinct approaches due to the fact that the set of cases is unknown in advance , as well as due to the assumed limited number of training documents , if a case should be interpreted as a classic category .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
The use of word embedding models and deep learning algorithms are currently the most common and popular trends to enhance the overall performance of a <m> text classification/categorization system </m> .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
For <m> text classification/categorization </m> , we investigate a steepest descent induction algorithm combined with a two-level preference relation on user ranking .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
There are two approaches for this <m> text-based categorization </m> : manual and automatic .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
<m> Text based categorization </m> is made use of for document classification with pattern recognition and machine learning .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
The results and the statistical evaluation of this procedure showed that the proposed method may be characterized as highly accurate for <m> text categorization purposes </m> .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
In this particular paper we describe the design and implementation of a distributed tree-based algorithm for <m> text categorization purposes </m> .


class: first -> second, base class: second -> first, new model class: second -> first
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
To date , our focus has been on <m> text information categorization </m> using both existing text classification techniques ( e.g. Weighted Feature Vector ( WFV ) classification ) and machine learning algorithms based on inductive logic programming ( ILP ) and natural language processing techniques .


class: first -> second, base class: second -> first, new model class: second -> first
first:
The RPN is used for proposing text regions and the Fast R - CNN model [ reference ] is modified to do <m> text region classification </m> , refinement and inclined box prediction .
second:
These traditional methods adopt a bottom - up strategy and often needs several steps to detect texts ( e.g. , character detection , text line construction and <m> text line classification </m> ) .


class: first -> second, base class: no relation, new model class: no relation
first:
Authorship attribution may be considered as a <m> text categorization problem </m> .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation, new model class: no relation
first:
Authorship attribution may be considered as a <m> text categorization problem </m> .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation, new model class: no relation
first:
Authorship attribution may be considered as a <m> text categorization problem </m> .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation, new model class: no relation
first:
Authorship attribution may be considered as a <m> text categorization problem </m> .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Authorship attribution may be considered as a <m> text categorization problem </m> .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation, new model class: no relation
first:
Authorship attribution may be considered as a <m> text categorization problem </m> .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Authorship attribution may be considered as a <m> text categorization problem </m> .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
Authorship attribution may be considered as a <m> text categorization problem </m> .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation, new model class: no relation
first:
Authorship attribution may be considered as a <m> text categorization problem </m> .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .
second:
We examine supervised learning for <m> multi-class , multi-label text classification </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .
second:
We explore supervised learning for <m> multi-class , multi-label text classification </m> , focusing on real-world settings , where the distribution of labels changes dynamically over time .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .
second:
As the input information is in the form of free textual entries , techniques used in <m> multi-class , multi-label text categorization </m> are well suited .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .
second:
In this paper , an artificial neural network with a quasi-Newton updating procedure is presented for <m> multi-label multi-class text classification </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .
second:
We examine supervised learning for <m> multi-class , multi-label text classification </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .
second:
We explore supervised learning for <m> multi-class , multi-label text classification </m> , focusing on real-world settings , where the distribution of labels changes dynamically over time .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .
second:
As the input information is in the form of free textual entries , techniques used in <m> multi-class , multi-label text categorization </m> are well suited .


class: first -> second, base class: same cluster, new model class: same cluster
first:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .
second:
In this paper , an artificial neural network with a quasi-Newton updating procedure is presented for <m> multi-label multi-class text classification </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .
second:
We examine supervised learning for <m> multi-class , multi-label text classification </m> .


class: first -> second, base class: same cluster, new model class: same cluster
first:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .
second:
We explore supervised learning for <m> multi-class , multi-label text classification </m> , focusing on real-world settings , where the distribution of labels changes dynamically over time .


class: first -> second, base class: same cluster, new model class: same cluster
first:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .
second:
As the input information is in the form of free textual entries , techniques used in <m> multi-class , multi-label text categorization </m> are well suited .


class: first -> second, base class: same cluster, new model class: same cluster
first:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .
second:
In this paper , an artificial neural network with a quasi-Newton updating procedure is presented for <m> multi-label multi-class text classification </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We examine supervised learning for <m> multi-class , multi-label text classification </m> .
second:
We design a fast Class-Feature-Centroid ( CFC ) classifier for <m> multi-class , single-label text categorization </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We examine supervised learning for <m> multi-class , multi-label text classification </m> .
second:
A number of language-independent text pre-processing techniques , to support <m> multi-class single-label text classification </m> , are described and compared .


class: first -> second, base class: no relation, new model class: no relation
first:
We explore supervised learning for <m> multi-class , multi-label text classification </m> , focusing on real-world settings , where the distribution of labels changes dynamically over time .
second:
We design a fast Class-Feature-Centroid ( CFC ) classifier for <m> multi-class , single-label text categorization </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
We explore supervised learning for <m> multi-class , multi-label text classification </m> , focusing on real-world settings , where the distribution of labels changes dynamically over time .
second:
A number of language-independent text pre-processing techniques , to support <m> multi-class single-label text classification </m> , are described and compared .


class: first -> second, base class: same cluster, new model class: same cluster
first:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .
second:
As the input information is in the form of free textual entries , techniques used in <m> multi-class , multi-label text categorization </m> are well suited .


class: first -> second, base class: same cluster, new model class: same cluster
first:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .
second:
In this paper , an artificial neural network with a quasi-Newton updating procedure is presented for <m> multi-label multi-class text classification </m> .


class: first -> second, base class: no relation, new model class: no relation
first:
As the input information is in the form of free textual entries , techniques used in <m> multi-class , multi-label text categorization </m> are well suited .
second:
A number of language-independent text pre-processing techniques , to support <m> multi-class single-label text classification </m> , are described and compared .


