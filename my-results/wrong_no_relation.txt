class: no relation, base class: first -> second, new model class: same cluster
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
On a physiological level , this corresponds to adjusting the rate coding model to the force generating capabilities of the simulated muscle , while from a control theoretic point of view , this step is equivalent to an exact <m> linearizing transformation </m> of the controlled neuromuscular system .


class: no relation, base class: first -> second, new model class: same cluster
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
A <m> linearization transformation approach </m> is also presented to facilitate the solution process .


class: no relation, base class: second -> first, new model class: same cluster
first:
On a physiological level , this corresponds to adjusting the rate coding model to the force generating capabilities of the simulated muscle , while from a control theoretic point of view , this step is equivalent to an exact <m> linearizing transformation </m> of the controlled neuromuscular system .
second:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .


class: no relation, base class: second -> first, new model class: same cluster
first:
On a physiological level , this corresponds to adjusting the rate coding model to the force generating capabilities of the simulated muscle , while from a control theoretic point of view , this step is equivalent to an exact <m> linearizing transformation </m> of the controlled neuromuscular system .
second:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .


class: no relation, base class: second -> first, new model class: same cluster
first:
On a physiological level , this corresponds to adjusting the rate coding model to the force generating capabilities of the simulated muscle , while from a control theoretic point of view , this step is equivalent to an exact <m> linearizing transformation </m> of the controlled neuromuscular system .
second:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .


class: no relation, base class: second -> first, new model class: same cluster
first:
On a physiological level , this corresponds to adjusting the rate coding model to the force generating capabilities of the simulated muscle , while from a control theoretic point of view , this step is equivalent to an exact <m> linearizing transformation </m> of the controlled neuromuscular system .
second:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .


class: no relation, base class: second -> first, new model class: same cluster
first:
On a physiological level , this corresponds to adjusting the rate coding model to the force generating capabilities of the simulated muscle , while from a control theoretic point of view , this step is equivalent to an exact <m> linearizing transformation </m> of the controlled neuromuscular system .
second:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .


class: no relation, base class: second -> first, new model class: same cluster
first:
On a physiological level , this corresponds to adjusting the rate coding model to the force generating capabilities of the simulated muscle , while from a control theoretic point of view , this step is equivalent to an exact <m> linearizing transformation </m> of the controlled neuromuscular system .
second:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .


class: no relation, base class: second -> first, new model class: same cluster
first:
On a physiological level , this corresponds to adjusting the rate coding model to the force generating capabilities of the simulated muscle , while from a control theoretic point of view , this step is equivalent to an exact <m> linearizing transformation </m> of the controlled neuromuscular system .
second:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .


class: no relation, base class: first -> second, new model class: same cluster
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
A <m> linearization transformation approach </m> is also presented to facilitate the solution process .


class: no relation, base class: first -> second, new model class: same cluster
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
A <m> linearization transformation approach </m> is also presented to facilitate the solution process .


class: no relation, base class: first -> second, new model class: same cluster
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
A <m> linearization transformation approach </m> is also presented to facilitate the solution process .


class: no relation, base class: first -> second, new model class: same cluster
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
A <m> linearization transformation approach </m> is also presented to facilitate the solution process .


class: no relation, base class: second -> first, new model class: same cluster
first:
The model is described by a continuous time three-dimensional autonomous system with hyperbolic sine nonlinearity , and may be viewed as a <m> linear transformation of model </m> MO15 previously introduced in [ Sprott , 2010 ] .
second:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .


class: no relation, base class: first -> second, new model class: same cluster
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
A <m> linearization transformation approach </m> is also presented to facilitate the solution process .


class: no relation, base class: second -> first, new model class: same cluster
first:
A <m> linearization transformation approach </m> is also presented to facilitate the solution process .
second:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .


class: no relation, base class: second -> first, new model class: same cluster
first:
A <m> linearization transformation approach </m> is also presented to facilitate the solution process .
second:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .


class: no relation, base class: same cluster, new model class: same cluster
first:
Two assumptions are made : 1 ) In a new pair of images the deformation is approximately the same size and has only been spatially relocated in the image , and that by a simple <m> affine transformation </m> one can identify the optimal configuration on this new pair of images ; and 2 ) The deformation is of similar size and shape on the original pair of images .
second:
While <m> affine image transformation </m> in conventional image processing is a relatively simple task , learning these transformations is an important part of having neural networks ( NNs ) function as generalized image processors .


class: no relation, base class: same cluster, new model class: same cluster
first:
Two assumptions are made : 1 ) In a new pair of images the deformation is approximately the same size and has only been spatially relocated in the image , and that by a simple <m> affine transformation </m> one can identify the optimal configuration on this new pair of images ; and 2 ) The deformation is of similar size and shape on the original pair of images .
second:
This visualisation , combined with an <m> affine transformation process </m> provided a simple , cost-effective way to accurately co-register photographs and MR images of subcutaneous hematomas located on the thigh .


class: no relation, base class: same cluster, new model class: same cluster
first:
Two assumptions are made : 1 ) In a new pair of images the deformation is approximately the same size and has only been spatially relocated in the image , and that by a simple <m> affine transformation </m> one can identify the optimal configuration on this new pair of images ; and 2 ) The deformation is of similar size and shape on the original pair of images .
second:
We have also experimented with more sophisticated data augmentation techniques , including random histogram matching , <m> affine image transforms </m> , and random image filtering , which did not demonstrate any additional improvements .


class: no relation, base class: same cluster, new model class: same cluster
first:
Sum of Square Difference ( SSD ) and Bilinear interpolation models were used to establish the similarity measure between the images to be registered , resampling of the pixel-values and computation of non-integer coordinates respectively while Random Sampling Consensus ( RANSAC ) algorithm was used to exclude the outliers and to compute the transformation matrix using <m> affine transformation function </m> .
second:
While <m> affine image transformation </m> in conventional image processing is a relatively simple task , learning these transformations is an important part of having neural networks ( NNs ) function as generalized image processors .


class: no relation, base class: same cluster, new model class: same cluster
first:
Sum of Square Difference ( SSD ) and Bilinear interpolation models were used to establish the similarity measure between the images to be registered , resampling of the pixel-values and computation of non-integer coordinates respectively while Random Sampling Consensus ( RANSAC ) algorithm was used to exclude the outliers and to compute the transformation matrix using <m> affine transformation function </m> .
second:
This visualisation , combined with an <m> affine transformation process </m> provided a simple , cost-effective way to accurately co-register photographs and MR images of subcutaneous hematomas located on the thigh .


class: no relation, base class: same cluster, new model class: same cluster
first:
Sum of Square Difference ( SSD ) and Bilinear interpolation models were used to establish the similarity measure between the images to be registered , resampling of the pixel-values and computation of non-integer coordinates respectively while Random Sampling Consensus ( RANSAC ) algorithm was used to exclude the outliers and to compute the transformation matrix using <m> affine transformation function </m> .
second:
We have also experimented with more sophisticated data augmentation techniques , including random histogram matching , <m> affine image transforms </m> , and random image filtering , which did not demonstrate any additional improvements .


class: no relation, base class: same cluster, new model class: same cluster
first:
After that , the pattern elements are extracted with the <m> affine transformation theory </m> and bilinear interpolation algorithm .
second:
While <m> affine image transformation </m> in conventional image processing is a relatively simple task , learning these transformations is an important part of having neural networks ( NNs ) function as generalized image processors .


class: no relation, base class: same cluster, new model class: same cluster
first:
After that , the pattern elements are extracted with the <m> affine transformation theory </m> and bilinear interpolation algorithm .
second:
This visualisation , combined with an <m> affine transformation process </m> provided a simple , cost-effective way to accurately co-register photographs and MR images of subcutaneous hematomas located on the thigh .


class: no relation, base class: same cluster, new model class: same cluster
first:
After that , the pattern elements are extracted with the <m> affine transformation theory </m> and bilinear interpolation algorithm .
second:
We have also experimented with more sophisticated data augmentation techniques , including random histogram matching , <m> affine image transforms </m> , and random image filtering , which did not demonstrate any additional improvements .


class: no relation, base class: same cluster, new model class: same cluster
first:
While <m> affine image transformation </m> in conventional image processing is a relatively simple task , learning these transformations is an important part of having neural networks ( NNs ) function as generalized image processors .
second:
The content includes : dividing an image frame including a target object , determining a target object region , using a feature information classifier to scan the determined target object region , determining central point position information of a region that feature information to be positioned occupies in the target object region , performing <m> affine transformation operation </m> on the determined central point position information , and obtaining initial position information corresponding to the region that the feature information to be positioned occupies in the target object region , thereby improving precision of initial positioning ; performing iteration processing on the obtained initial position information , obtaining position information corresponding to the region that the feature information to be positioned occupies in the target object region , integrating the obtained position information corresponding to the region that the feature information to be positioned occupies in the target object region , and obtaining the feature information of the target object , thereby improving precision of positioning the feature information of the target object .


class: no relation, base class: same cluster, new model class: same cluster
first:
While <m> affine image transformation </m> in conventional image processing is a relatively simple task , learning these transformations is an important part of having neural networks ( NNs ) function as generalized image processors .
second:
Specific topics include : Loading images from JAR files Describing shapes and the affine transformation in Java 2D Designing a class library of shapes Designing a component class for interactive drawing Designing a component class for displaying images Printing in Java 2D Implementing pan and zoom Implementing interactive rotate , shear , and flip Manipulating images at the pixel level using the Java 2D API Working with tiles in JAI to render large images Image manipulation in JAI <m> usingaffine transformation </m> , projections , and warping Image analysis in JAI , including edge detection , statistics , and region of interest computation ( ROI ) Remote imaging using RMI and JAI Internet imaging , including the use of Java 2D and JAI with servlets and JSP , the design of Web-based imaging applications , and the Internet Imaging Protocol ( IIP ) Using the new Image I/O framework to read and write images " Putting It All Together " sections help you put vital concepts and techniques into practice with interactive examples using actual applications .


class: no relation, base class: same cluster, new model class: same cluster
first:
While <m> affine image transformation </m> in conventional image processing is a relatively simple task , learning these transformations is an important part of having neural networks ( NNs ) function as generalized image processors .
second:
Since the textual and visual vectors have different dimensionality and belong to different spaces , we first map them to a mutual space using an <m> affine transformation </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
While <m> affine image transformation </m> in conventional image processing is a relatively simple task , learning these transformations is an important part of having neural networks ( NNs ) function as generalized image processors .
second:
Furthermore , the learned <m> affine transform </m> applied to these normalized activations allows the BN transform to represent the identity transformation and preserves the network capacity .


class: no relation, base class: same cluster, new model class: same cluster
first:
While <m> affine image transformation </m> in conventional image processing is a relatively simple task , learning these transformations is an important part of having neural networks ( NNs ) function as generalized image processors .
second:
The algorithm estimates the 12 calibration parameters , which are elements of an <m> affine transformation mapping </m> measured , and thus , erroneous magnetometer measurements into corrected field measurements .


class: no relation, base class: same cluster, new model class: same cluster
first:
The content includes : dividing an image frame including a target object , determining a target object region , using a feature information classifier to scan the determined target object region , determining central point position information of a region that feature information to be positioned occupies in the target object region , performing <m> affine transformation operation </m> on the determined central point position information , and obtaining initial position information corresponding to the region that the feature information to be positioned occupies in the target object region , thereby improving precision of initial positioning ; performing iteration processing on the obtained initial position information , obtaining position information corresponding to the region that the feature information to be positioned occupies in the target object region , integrating the obtained position information corresponding to the region that the feature information to be positioned occupies in the target object region , and obtaining the feature information of the target object , thereby improving precision of positioning the feature information of the target object .
second:
This visualisation , combined with an <m> affine transformation process </m> provided a simple , cost-effective way to accurately co-register photographs and MR images of subcutaneous hematomas located on the thigh .


class: no relation, base class: same cluster, new model class: same cluster
first:
The content includes : dividing an image frame including a target object , determining a target object region , using a feature information classifier to scan the determined target object region , determining central point position information of a region that feature information to be positioned occupies in the target object region , performing <m> affine transformation operation </m> on the determined central point position information , and obtaining initial position information corresponding to the region that the feature information to be positioned occupies in the target object region , thereby improving precision of initial positioning ; performing iteration processing on the obtained initial position information , obtaining position information corresponding to the region that the feature information to be positioned occupies in the target object region , integrating the obtained position information corresponding to the region that the feature information to be positioned occupies in the target object region , and obtaining the feature information of the target object , thereby improving precision of positioning the feature information of the target object .
second:
We have also experimented with more sophisticated data augmentation techniques , including random histogram matching , <m> affine image transforms </m> , and random image filtering , which did not demonstrate any additional improvements .


class: no relation, base class: same cluster, new model class: same cluster
first:
This visualisation , combined with an <m> affine transformation process </m> provided a simple , cost-effective way to accurately co-register photographs and MR images of subcutaneous hematomas located on the thigh .
second:
Specific topics include : Loading images from JAR files Describing shapes and the affine transformation in Java 2D Designing a class library of shapes Designing a component class for interactive drawing Designing a component class for displaying images Printing in Java 2D Implementing pan and zoom Implementing interactive rotate , shear , and flip Manipulating images at the pixel level using the Java 2D API Working with tiles in JAI to render large images Image manipulation in JAI <m> usingaffine transformation </m> , projections , and warping Image analysis in JAI , including edge detection , statistics , and region of interest computation ( ROI ) Remote imaging using RMI and JAI Internet imaging , including the use of Java 2D and JAI with servlets and JSP , the design of Web-based imaging applications , and the Internet Imaging Protocol ( IIP ) Using the new Image I/O framework to read and write images " Putting It All Together " sections help you put vital concepts and techniques into practice with interactive examples using actual applications .


class: no relation, base class: same cluster, new model class: same cluster
first:
This visualisation , combined with an <m> affine transformation process </m> provided a simple , cost-effective way to accurately co-register photographs and MR images of subcutaneous hematomas located on the thigh .
second:
Since the textual and visual vectors have different dimensionality and belong to different spaces , we first map them to a mutual space using an <m> affine transformation </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
This visualisation , combined with an <m> affine transformation process </m> provided a simple , cost-effective way to accurately co-register photographs and MR images of subcutaneous hematomas located on the thigh .
second:
Furthermore , the learned <m> affine transform </m> applied to these normalized activations allows the BN transform to represent the identity transformation and preserves the network capacity .


class: no relation, base class: same cluster, new model class: same cluster
first:
This visualisation , combined with an <m> affine transformation process </m> provided a simple , cost-effective way to accurately co-register photographs and MR images of subcutaneous hematomas located on the thigh .
second:
The algorithm estimates the 12 calibration parameters , which are elements of an <m> affine transformation mapping </m> measured , and thus , erroneous magnetometer measurements into corrected field measurements .


class: no relation, base class: same cluster, new model class: same cluster
first:
Specific topics include : Loading images from JAR files Describing shapes and the affine transformation in Java 2D Designing a class library of shapes Designing a component class for interactive drawing Designing a component class for displaying images Printing in Java 2D Implementing pan and zoom Implementing interactive rotate , shear , and flip Manipulating images at the pixel level using the Java 2D API Working with tiles in JAI to render large images Image manipulation in JAI <m> usingaffine transformation </m> , projections , and warping Image analysis in JAI , including edge detection , statistics , and region of interest computation ( ROI ) Remote imaging using RMI and JAI Internet imaging , including the use of Java 2D and JAI with servlets and JSP , the design of Web-based imaging applications , and the Internet Imaging Protocol ( IIP ) Using the new Image I/O framework to read and write images " Putting It All Together " sections help you put vital concepts and techniques into practice with interactive examples using actual applications .
second:
We have also experimented with more sophisticated data augmentation techniques , including random histogram matching , <m> affine image transforms </m> , and random image filtering , which did not demonstrate any additional improvements .


class: no relation, base class: same cluster, new model class: same cluster
first:
Since the textual and visual vectors have different dimensionality and belong to different spaces , we first map them to a mutual space using an <m> affine transformation </m> .
second:
We have also experimented with more sophisticated data augmentation techniques , including random histogram matching , <m> affine image transforms </m> , and random image filtering , which did not demonstrate any additional improvements .


class: no relation, base class: same cluster, new model class: same cluster
first:
Furthermore , the learned <m> affine transform </m> applied to these normalized activations allows the BN transform to represent the identity transformation and preserves the network capacity .
second:
We have also experimented with more sophisticated data augmentation techniques , including random histogram matching , <m> affine image transforms </m> , and random image filtering , which did not demonstrate any additional improvements .


class: no relation, base class: same cluster, new model class: same cluster
first:
We have also experimented with more sophisticated data augmentation techniques , including random histogram matching , <m> affine image transforms </m> , and random image filtering , which did not demonstrate any additional improvements .
second:
The algorithm estimates the 12 calibration parameters , which are elements of an <m> affine transformation mapping </m> measured , and thus , erroneous magnetometer measurements into corrected field measurements .


class: no relation, base class: second -> first, new model class: second -> first
first:
To obtain accurate classification results , this paper proposes the Ensemble-Margin Based Random Forests ( EMRFs ) method , which combines RFs and a new <m> subsampling iterative technique </m> making use of computed ensemble margin values .
second:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
To obtain accurate classification results , this paper proposes the Ensemble-Margin Based Random Forests ( EMRFs ) method , which combines RFs and a new <m> subsampling iterative technique </m> making use of computed ensemble margin values .
second:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
To obtain accurate classification results , this paper proposes the Ensemble-Margin Based Random Forests ( EMRFs ) method , which combines RFs and a new <m> subsampling iterative technique </m> making use of computed ensemble margin values .
second:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .


class: no relation, base class: same cluster, new model class: second -> first
first:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .
second:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .


class: no relation, base class: same cluster, new model class: second -> first
first:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .
second:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .


class: no relation, base class: same cluster, new model class: second -> first
first:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .
second:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .


class: no relation, base class: second -> first, new model class: second -> first
first:
The spatial frequency decomposition is accomplished by an efficient encoding algorithm incorporating a hierarchical cascading Gaussian pyramid algorithm which is an alternating sequence of image output passing through Nyquist low pass spatial filter and <m> subsampling local operators </m> for image encoding .
second:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
The spatial frequency decomposition is accomplished by an efficient encoding algorithm incorporating a hierarchical cascading Gaussian pyramid algorithm which is an alternating sequence of image output passing through Nyquist low pass spatial filter and <m> subsampling local operators </m> for image encoding .
second:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
The spatial frequency decomposition is accomplished by an efficient encoding algorithm incorporating a hierarchical cascading Gaussian pyramid algorithm which is an alternating sequence of image output passing through Nyquist low pass spatial filter and <m> subsampling local operators </m> for image encoding .
second:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .


class: no relation, base class: second -> first, new model class: second -> first
first:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .
second:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .
second:
After segmentation , the HR cell image is downsampled by the <m> bicubic downsampling algorithm </m> to obtain a 20 × 20 low resolution ( LR ) cell image .


class: no relation, base class: first -> second, new model class: first -> second
first:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .
second:
This paper explores the problem <m> oftask-oriented downsampling </m> , which aims to downsample a pointcloud and maintain the performance of subsequent applications asmuch as possible .


class: no relation, base class: second -> first, new model class: second -> first
first:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: no relation, base class: second -> first, new model class: second -> first
first:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .
second:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .
second:
After segmentation , the HR cell image is downsampled by the <m> bicubic downsampling algorithm </m> to obtain a 20 × 20 low resolution ( LR ) cell image .


class: no relation, base class: first -> second, new model class: first -> second
first:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .
second:
This paper explores the problem <m> oftask-oriented downsampling </m> , which aims to downsample a pointcloud and maintain the performance of subsequent applications asmuch as possible .


class: no relation, base class: second -> first, new model class: second -> first
first:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: no relation, base class: second -> first, new model class: second -> first
first:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .
second:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .
second:
After segmentation , the HR cell image is downsampled by the <m> bicubic downsampling algorithm </m> to obtain a 20 × 20 low resolution ( LR ) cell image .


class: no relation, base class: first -> second, new model class: first -> second
first:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .
second:
This paper explores the problem <m> oftask-oriented downsampling </m> , which aims to downsample a pointcloud and maintain the performance of subsequent applications asmuch as possible .


class: no relation, base class: second -> first, new model class: second -> first
first:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: no relation, base class: first -> second, new model class: first -> second
first:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .
second:
The overall best performance was obtained with a normalized correlation metric coupled with <m> HFUS downsampling </m> and a one-plus-one evolutionary optimizer , yielding a mean registration error of 0.05 mm .


class: no relation, base class: first -> second, new model class: first -> second
first:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: no relation, base class: first -> second, new model class: first -> second
first:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: no relation, base class: first -> second, new model class: first -> second
first:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: no relation, base class: first -> second, new model class: first -> second
first:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: no relation, base class: first -> second, new model class: first -> second
first:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: no relation, base class: first -> second, new model class: first -> second
first:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: no relation, base class: second -> first, new model class: second -> first
first:
After segmentation , the HR cell image is downsampled by the <m> bicubic downsampling algorithm </m> to obtain a 20 × 20 low resolution ( LR ) cell image .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: no relation, base class: second -> first, new model class: second -> first
first:
After segmentation , the HR cell image is downsampled by the <m> bicubic downsampling algorithm </m> to obtain a 20 × 20 low resolution ( LR ) cell image .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
After segmentation , the HR cell image is downsampled by the <m> bicubic downsampling algorithm </m> to obtain a 20 × 20 low resolution ( LR ) cell image .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
After segmentation , the HR cell image is downsampled by the <m> bicubic downsampling algorithm </m> to obtain a 20 × 20 low resolution ( LR ) cell image .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: no relation, base class: second -> first, new model class: second -> first
first:
After segmentation , the HR cell image is downsampled by the <m> bicubic downsampling algorithm </m> to obtain a 20 × 20 low resolution ( LR ) cell image .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: no relation, base class: second -> first, new model class: second -> first
first:
After segmentation , the HR cell image is downsampled by the <m> bicubic downsampling algorithm </m> to obtain a 20 × 20 low resolution ( LR ) cell image .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: no relation, base class: second -> first, new model class: second -> first
first:
After segmentation , the HR cell image is downsampled by the <m> bicubic downsampling algorithm </m> to obtain a 20 × 20 low resolution ( LR ) cell image .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: no relation, base class: second -> first, new model class: second -> first
first:
After segmentation , the HR cell image is downsampled by the <m> bicubic downsampling algorithm </m> to obtain a 20 × 20 low resolution ( LR ) cell image .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: no relation, base class: second -> first, new model class: second -> first
first:
After segmentation , the HR cell image is downsampled by the <m> bicubic downsampling algorithm </m> to obtain a 20 × 20 low resolution ( LR ) cell image .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: no relation, base class: second -> first, new model class: second -> first
first:
After segmentation , the HR cell image is downsampled by the <m> bicubic downsampling algorithm </m> to obtain a 20 × 20 low resolution ( LR ) cell image .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: no relation, base class: first -> second, new model class: first -> second
first:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .
second:
This paper explores the problem <m> oftask-oriented downsampling </m> , which aims to downsample a pointcloud and maintain the performance of subsequent applications asmuch as possible .


class: no relation, base class: second -> first, new model class: second -> first
first:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: no relation, base class: second -> first, new model class: second -> first
first:
This paper explores the problem <m> oftask-oriented downsampling </m> , which aims to downsample a pointcloud and maintain the performance of subsequent applications asmuch as possible .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
This paper explores the problem <m> oftask-oriented downsampling </m> , which aims to downsample a pointcloud and maintain the performance of subsequent applications asmuch as possible .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: no relation, base class: second -> first, new model class: second -> first
first:
This paper explores the problem <m> oftask-oriented downsampling </m> , which aims to downsample a pointcloud and maintain the performance of subsequent applications asmuch as possible .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: no relation, base class: second -> first, new model class: second -> first
first:
This paper explores the problem <m> oftask-oriented downsampling </m> , which aims to downsample a pointcloud and maintain the performance of subsequent applications asmuch as possible .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: no relation, base class: second -> first, new model class: second -> first
first:
This paper explores the problem <m> oftask-oriented downsampling </m> , which aims to downsample a pointcloud and maintain the performance of subsequent applications asmuch as possible .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: no relation, base class: second -> first, new model class: second -> first
first:
This paper explores the problem <m> oftask-oriented downsampling </m> , which aims to downsample a pointcloud and maintain the performance of subsequent applications asmuch as possible .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: no relation, base class: second -> first, new model class: second -> first
first:
We further evaluate three potential defense methods to mitigate such attacks , including adversarial training , <m> audio downsampling </m> , and moving average filtering , which leads to promising directions for further research .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
We further evaluate three potential defense methods to mitigate such attacks , including adversarial training , <m> audio downsampling </m> , and moving average filtering , which leads to promising directions for further research .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: no relation, base class: second -> first, new model class: second -> first
first:
We further evaluate three potential defense methods to mitigate such attacks , including adversarial training , <m> audio downsampling </m> , and moving average filtering , which leads to promising directions for further research .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: no relation, base class: second -> first, new model class: second -> first
first:
We further evaluate three potential defense methods to mitigate such attacks , including adversarial training , <m> audio downsampling </m> , and moving average filtering , which leads to promising directions for further research .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: no relation, base class: same cluster, new model class: second -> first
first:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: no relation, base class: same cluster, new model class: second -> first
first:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: no relation, base class: same cluster, new model class: second -> first
first:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: no relation, base class: same cluster, new model class: second -> first
first:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: no relation, base class: same cluster, new model class: same cluster
first:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: no relation, base class: second -> first, new model class: second -> first
first:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: no relation, base class: second -> first, new model class: second -> first
first:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: no relation, base class: same cluster, new model class: same cluster
first:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: no relation, base class: second -> first, new model class: second -> first
first:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: no relation, base class: same cluster, new model class: same cluster
first:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: no relation, base class: second -> first, new model class: second -> first
first:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: no relation, base class: first -> second, new model class: first -> second
first:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: no relation, base class: first -> second, new model class: first -> second
first:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: no relation, base class: first -> second, new model class: first -> second
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .


class: no relation, base class: second -> first, new model class: second -> first
first:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .
second:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .


class: no relation, base class: second -> first, new model class: second -> first
first:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .
second:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .


class: no relation, base class: first -> second, new model class: first -> second
first:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .
second:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .


class: no relation, base class: first -> second, new model class: first -> second
first:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .
second:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .
second:
Therefore , website ’s ranking could be figured out via the rates of computers with the <m> Alexa tool bar </m> and Internet traffic data collected .


class: no relation, base class: same cluster, new model class: same cluster
first:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .
second:
Design/methodology/approach – The study applies web analytical tools , such as <m> Alexa.com </m> , in the collection of data about Canadian libraries ' visibility performance in the ranking of search engine results .


class: no relation, base class: same cluster, new model class: same cluster
first:
Therefore , website ’s ranking could be figured out via the rates of computers with the <m> Alexa tool bar </m> and Internet traffic data collected .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: no relation, base class: first -> second, new model class: same cluster
first:
Therefore , website ’s ranking could be figured out via the rates of computers with the <m> Alexa tool bar </m> and Internet traffic data collected .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: no relation, base class: same cluster, new model class: same cluster
first:
Therefore , website ’s ranking could be figured out via the rates of computers with the <m> Alexa tool bar </m> and Internet traffic data collected .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: no relation, base class: same cluster, new model class: same cluster
first:
Therefore , website ’s ranking could be figured out via the rates of computers with the <m> Alexa tool bar </m> and Internet traffic data collected .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: no relation, base class: same cluster, new model class: same cluster
first:
Therefore , website ’s ranking could be figured out via the rates of computers with the <m> Alexa tool bar </m> and Internet traffic data collected .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: no relation, base class: same cluster, new model class: same cluster
first:
Design/methodology/approach – The study applies web analytical tools , such as <m> Alexa.com </m> , in the collection of data about Canadian libraries ' visibility performance in the ranking of search engine results .
second:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .


class: no relation, base class: first -> second, new model class: same cluster
first:
Design/methodology/approach – The study applies web analytical tools , such as <m> Alexa.com </m> , in the collection of data about Canadian libraries ' visibility performance in the ranking of search engine results .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: no relation, base class: same cluster, new model class: same cluster
first:
Design/methodology/approach – The study applies web analytical tools , such as <m> Alexa.com </m> , in the collection of data about Canadian libraries ' visibility performance in the ranking of search engine results .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: no relation, base class: same cluster, new model class: same cluster
first:
Design/methodology/approach – The study applies web analytical tools , such as <m> Alexa.com </m> , in the collection of data about Canadian libraries ' visibility performance in the ranking of search engine results .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: no relation, base class: same cluster, new model class: same cluster
first:
Design/methodology/approach – The study applies web analytical tools , such as <m> Alexa.com </m> , in the collection of data about Canadian libraries ' visibility performance in the ranking of search engine results .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .
second:
This paper describes an <m> AI-based legal assistant </m> that would support case assessment based on the extraction of arguments from court decisions .


class: no relation, base class: first -> second, new model class: first -> second
first:
Through a numerical experiment using benchmark datasets from the UCI machine-learning repository , we confirm that ( i ) the suggested <m> node partition method </m> is efficient compared to a random partition method , and ( ii ) the classification performance of HMC-LAD is superior to existing multi-class LAD algorithms and other supervised learning approaches .
second:
In this article , Based on the idea of coverage compensation a <m> distributed node partition algorithm </m> for random deployments is presented to generate a minimum cover set by using the optimal node distributions created by the centralized algorithms such as GA .


class: no relation, base class: same cluster, new model class: same cluster
first:
Through a numerical experiment using benchmark datasets from the UCI machine-learning repository , we confirm that ( i ) the suggested <m> node partition method </m> is efficient compared to a random partition method , and ( ii ) the classification performance of HMC-LAD is superior to existing multi-class LAD algorithms and other supervised learning approaches .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: no relation, base class: same cluster, new model class: same cluster
first:
Through a numerical experiment using benchmark datasets from the UCI machine-learning repository , we confirm that ( i ) the suggested <m> node partition method </m> is efficient compared to a random partition method , and ( ii ) the classification performance of HMC-LAD is superior to existing multi-class LAD algorithms and other supervised learning approaches .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: no relation, base class: same cluster, new model class: same cluster
first:
Through a numerical experiment using benchmark datasets from the UCI machine-learning repository , we confirm that ( i ) the suggested <m> node partition method </m> is efficient compared to a random partition method , and ( ii ) the classification performance of HMC-LAD is superior to existing multi-class LAD algorithms and other supervised learning approaches .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: no relation, base class: same cluster, new model class: same cluster
first:
Through a numerical experiment using benchmark datasets from the UCI machine-learning repository , we confirm that ( i ) the suggested <m> node partition method </m> is efficient compared to a random partition method , and ( ii ) the classification performance of HMC-LAD is superior to existing multi-class LAD algorithms and other supervised learning approaches .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: no relation, base class: same cluster, new model class: same cluster
first:
Through a numerical experiment using benchmark datasets from the UCI machine-learning repository , we confirm that ( i ) the suggested <m> node partition method </m> is efficient compared to a random partition method , and ( ii ) the classification performance of HMC-LAD is superior to existing multi-class LAD algorithms and other supervised learning approaches .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
In this article , Based on the idea of coverage compensation a <m> distributed node partition algorithm </m> for random deployments is presented to generate a minimum cover set by using the optimal node distributions created by the centralized algorithms such as GA .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: no relation, base class: second -> first, new model class: second -> first
first:
In this article , Based on the idea of coverage compensation a <m> distributed node partition algorithm </m> for random deployments is presented to generate a minimum cover set by using the optimal node distributions created by the centralized algorithms such as GA .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: no relation, base class: second -> first, new model class: second -> first
first:
In this article , Based on the idea of coverage compensation a <m> distributed node partition algorithm </m> for random deployments is presented to generate a minimum cover set by using the optimal node distributions created by the centralized algorithms such as GA .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: no relation, base class: second -> first, new model class: second -> first
first:
In this article , Based on the idea of coverage compensation a <m> distributed node partition algorithm </m> for random deployments is presented to generate a minimum cover set by using the optimal node distributions created by the centralized algorithms such as GA .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: no relation, base class: second -> first, new model class: second -> first
first:
In this article , Based on the idea of coverage compensation a <m> distributed node partition algorithm </m> for random deployments is presented to generate a minimum cover set by using the optimal node distributions created by the centralized algorithms such as GA .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
Moreover , the scheme incorporates several source coding techniques , such as Wyner-Ziv coding , <m> binning </m> and superposition coding .
second:
<m> Sub-pixel binning </m> : Chandra coordinates contain positional accuracy finer than one ACIS pixel ( 0.492 arcsec ) through dither and aspect correction .


class: no relation, base class: first -> second, new model class: first -> second
first:
Moreover , the scheme incorporates several source coding techniques , such as Wyner-Ziv coding , <m> binning </m> and superposition coding .
second:
Although conceptually simple , the calculation of radial distribution functions via <m> distance binning </m> requires the evaluation of O ( N 2 ) particle-pair distances where N is the number of particles under consideration .


class: no relation, base class: second -> first, new model class: second -> first
first:
<m> Quantile binning </m> is popular in several fields .
second:
We describe a new <m> binning technic </m> for informed data hiding problem .


class: no relation, base class: first -> second, new model class: first -> second
first:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .
second:
We establish a new achievable rate region for the ZC , using <m> Marton 's binning technique </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
To that end , this paper proposes a <m> word embeddings based approach </m> to enhance the accuracy of PMM .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: no relation, base class: same cluster, new model class: second -> first
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: same cluster, new model class: second -> first
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: same cluster, new model class: second -> first
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: same cluster, new model class: second -> first
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: same cluster, new model class: second -> first
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: same cluster, new model class: second -> first
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: same cluster, new model class: second -> first
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: same cluster, new model class: second -> first
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: same cluster, new model class: second -> first
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: same cluster, new model class: second -> first
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: same cluster, new model class: second -> first
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: same cluster, new model class: second -> first
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: same cluster, new model class: second -> first
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: same cluster, new model class: second -> first
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: same cluster, new model class: second -> first
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: same cluster, new model class: second -> first
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: same cluster, new model class: second -> first
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: same cluster, new model class: second -> first
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: same cluster, new model class: second -> first
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: same cluster, new model class: second -> first
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: same cluster, new model class: second -> first
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: same cluster, new model class: second -> first
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: same cluster, new model class: second -> first
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: same cluster, new model class: second -> first
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: same cluster, new model class: second -> first
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: same cluster, new model class: second -> first
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: same cluster, new model class: second -> first
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: same cluster, new model class: second -> first
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: same cluster, new model class: second -> first
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: same cluster, new model class: second -> first
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: same cluster, new model class: second -> first
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: same cluster, new model class: second -> first
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: same cluster, new model class: second -> first
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: same cluster, new model class: second -> first
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: same cluster, new model class: second -> first
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: same cluster, new model class: second -> first
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: same cluster, new model class: second -> first
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: same cluster, new model class: second -> first
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: same cluster, new model class: second -> first
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: same cluster, new model class: second -> first
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: second -> first, new model class: second -> first
first:
In our experiments , each paragraph vector is a combination of two vectors : one learned by the standard paragraph vector with distributed memory ( PV - DM ) and one learned by the paragraph vector with <m> distributed bag of words </m> ( PV - DBOW ) .
second:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .


class: no relation, base class: same cluster, new model class: same cluster
first:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .
second:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .
second:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .


class: no relation, base class: same cluster, new model class: same cluster
first:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .
second:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .


class: no relation, base class: same cluster, new model class: same cluster
first:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: same cluster, new model class: same cluster
first:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .
second:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .


class: no relation, base class: same cluster, new model class: same cluster
first:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: same cluster, new model class: same cluster
first:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: no relation, base class: same cluster, new model class: same cluster
first:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: no relation, base class: same cluster, new model class: same cluster
first:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: no relation, base class: same cluster, new model class: same cluster
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: same cluster, new model class: same cluster
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: first -> second, new model class: first -> second
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .
second:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .


class: no relation, base class: same cluster, new model class: same cluster
first:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: same cluster, new model class: same cluster
first:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .
second:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .


class: no relation, base class: same cluster, new model class: same cluster
first:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: same cluster, new model class: same cluster
first:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: no relation, base class: same cluster, new model class: same cluster
first:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: no relation, base class: first -> second, new model class: first -> second
first:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: no relation, base class: same cluster, new model class: same cluster
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: same cluster, new model class: same cluster
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: first -> second, new model class: first -> second
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: same cluster, new model class: same cluster
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: first -> second, new model class: first -> second
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .
second:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .


class: no relation, base class: same cluster, new model class: same cluster
first:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: same cluster, new model class: same cluster
first:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: no relation, base class: same cluster, new model class: same cluster
first:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: no relation, base class: first -> second, new model class: first -> second
first:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: no relation, base class: same cluster, new model class: same cluster
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: first -> second, new model class: first -> second
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: no relation, base class: same cluster, new model class: same cluster
first:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: no relation, base class: first -> second, new model class: first -> second
first:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: no relation, base class: first -> second, new model class: first -> second
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: first -> second, new model class: first -> second
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: second -> first, new model class: second -> first
first:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: no relation, base class: first -> second, new model class: first -> second
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .


class: no relation, base class: first -> second, new model class: first -> second
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .


class: no relation, base class: first -> second, new model class: first -> second
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .


class: no relation, base class: first -> second, new model class: first -> second
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .


class: no relation, base class: first -> second, new model class: first -> second
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .


class: no relation, base class: first -> second, new model class: first -> second
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .


class: no relation, base class: first -> second, new model class: first -> second
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .


class: no relation, base class: first -> second, new model class: first -> second
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
The research results have showed that learners can basically complete shallow level in multiple dimensions and expand some <m> deep level learning </m> , but the degree of deep learning needs to be improved .


class: no relation, base class: first -> second, new model class: first -> second
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: no relation, base class: same cluster, new model class: same cluster
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
The research results have showed that learners can basically complete shallow level in multiple dimensions and expand some <m> deep level learning </m> , but the degree of deep learning needs to be improved .


class: no relation, base class: first -> second, new model class: first -> second
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: no relation, base class: same cluster, new model class: same cluster
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
The research results have showed that learners can basically complete shallow level in multiple dimensions and expand some <m> deep level learning </m> , but the degree of deep learning needs to be improved .


class: no relation, base class: first -> second, new model class: first -> second
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: no relation, base class: same cluster, new model class: same cluster
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
The research results have showed that learners can basically complete shallow level in multiple dimensions and expand some <m> deep level learning </m> , but the degree of deep learning needs to be improved .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: no relation, base class: same cluster, new model class: same cluster
first:
The research results have showed that learners can basically complete shallow level in multiple dimensions and expand some <m> deep level learning </m> , but the degree of deep learning needs to be improved .
second:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
The research results have showed that learners can basically complete shallow level in multiple dimensions and expand some <m> deep level learning </m> , but the degree of deep learning needs to be improved .
second:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .


class: no relation, base class: same cluster, new model class: same cluster
first:
The research results have showed that learners can basically complete shallow level in multiple dimensions and expand some <m> deep level learning </m> , but the degree of deep learning needs to be improved .
second:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .


class: no relation, base class: same cluster, new model class: same cluster
first:
The research results have showed that learners can basically complete shallow level in multiple dimensions and expand some <m> deep level learning </m> , but the degree of deep learning needs to be improved .
second:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .


class: no relation, base class: same cluster, new model class: same cluster
first:
The research results have showed that learners can basically complete shallow level in multiple dimensions and expand some <m> deep level learning </m> , but the degree of deep learning needs to be improved .
second:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .


class: no relation, base class: same cluster, new model class: same cluster
first:
The research results have showed that learners can basically complete shallow level in multiple dimensions and expand some <m> deep level learning </m> , but the degree of deep learning needs to be improved .
second:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .


class: no relation, base class: same cluster, new model class: same cluster
first:
The research results have showed that learners can basically complete shallow level in multiple dimensions and expand some <m> deep level learning </m> , but the degree of deep learning needs to be improved .
second:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: no relation, base class: same cluster, new model class: same cluster
first:
The research results have showed that learners can basically complete shallow level in multiple dimensions and expand some <m> deep level learning </m> , but the degree of deep learning needs to be improved .
second:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .


class: no relation, base class: same cluster, new model class: same cluster
first:
The research results have showed that learners can basically complete shallow level in multiple dimensions and expand some <m> deep level learning </m> , but the degree of deep learning needs to be improved .
second:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .


class: no relation, base class: same cluster, new model class: same cluster
first:
The research results have showed that learners can basically complete shallow level in multiple dimensions and expand some <m> deep level learning </m> , but the degree of deep learning needs to be improved .
second:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
The research results have showed that learners can basically complete shallow level in multiple dimensions and expand some <m> deep level learning </m> , but the degree of deep learning needs to be improved .
second:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .


class: no relation, base class: same cluster, new model class: same cluster
first:
The research results have showed that learners can basically complete shallow level in multiple dimensions and expand some <m> deep level learning </m> , but the degree of deep learning needs to be improved .
second:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .


class: no relation, base class: first -> second, new model class: first -> second
first:
The research results have showed that learners can basically complete shallow level in multiple dimensions and expand some <m> deep level learning </m> , but the degree of deep learning needs to be improved .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: no relation, base class: first -> second, new model class: first -> second
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: no relation, base class: first -> second, new model class: first -> second
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: no relation, base class: first -> second, new model class: first -> second
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: no relation, base class: first -> second, new model class: first -> second
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: no relation, base class: first -> second, new model class: first -> second
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: no relation, base class: first -> second, new model class: first -> second
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: no relation, base class: first -> second, new model class: first -> second
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: no relation, base class: first -> second, new model class: first -> second
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: no relation, base class: first -> second, new model class: first -> second
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: no relation, base class: first -> second, new model class: first -> second
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: no relation, base class: first -> second, new model class: first -> second
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: no relation, base class: first -> second, new model class: first -> second
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: no relation, base class: second -> first, new model class: second -> first
first:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .
second:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .


class: no relation, base class: second -> first, new model class: second -> first
first:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .
second:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .


class: no relation, base class: second -> first, new model class: second -> first
first:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .
second:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .


class: no relation, base class: second -> first, new model class: second -> first
first:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .
second:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .


class: no relation, base class: second -> first, new model class: second -> first
first:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .
second:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .
second:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .


class: no relation, base class: second -> first, new model class: second -> first
first:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .
second:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .


class: no relation, base class: second -> first, new model class: second -> first
first:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .
second:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: no relation, base class: second -> first, new model class: second -> first
first:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: no relation, base class: second -> first, new model class: second -> first
first:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: no relation, base class: second -> first, new model class: second -> first
first:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: no relation, base class: second -> first, new model class: second -> first
first:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: no relation, base class: second -> first, new model class: second -> first
first:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: no relation, base class: second -> first, new model class: second -> first
first:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .
second:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .


class: no relation, base class: second -> first, new model class: second -> first
first:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .
second:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .


class: no relation, base class: second -> first, new model class: second -> first
first:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .
second:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .


class: no relation, base class: second -> first, new model class: second -> first
first:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .
second:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .


class: no relation, base class: second -> first, new model class: second -> first
first:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .
second:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .
second:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .


class: no relation, base class: second -> first, new model class: second -> first
first:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .
second:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .


class: no relation, base class: second -> first, new model class: second -> first
first:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .
second:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: no relation, base class: second -> first, new model class: second -> first
first:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: no relation, base class: second -> first, new model class: second -> first
first:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: no relation, base class: second -> first, new model class: second -> first
first:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: no relation, base class: second -> first, new model class: second -> first
first:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: no relation, base class: second -> first, new model class: second -> first
first:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: no relation, base class: second -> first, new model class: second -> first
first:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .
second:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .


class: no relation, base class: second -> first, new model class: second -> first
first:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .
second:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .


class: no relation, base class: second -> first, new model class: second -> first
first:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .
second:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .


class: no relation, base class: second -> first, new model class: second -> first
first:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .
second:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .


class: no relation, base class: second -> first, new model class: second -> first
first:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .
second:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .
second:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .


class: no relation, base class: second -> first, new model class: second -> first
first:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .
second:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .


class: no relation, base class: second -> first, new model class: second -> first
first:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .
second:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: no relation, base class: second -> first, new model class: second -> first
first:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: no relation, base class: second -> first, new model class: second -> first
first:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: no relation, base class: second -> first, new model class: second -> first
first:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: no relation, base class: second -> first, new model class: second -> first
first:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: no relation, base class: second -> first, new model class: second -> first
first:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: no relation, base class: first -> second, new model class: first -> second
first:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: no relation, base class: second -> first, new model class: first -> second
first:
After that , <m> Conv - LSTM layers </m> with 256 channels are inserted into the single - shot TLL network .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
After that , <m> Conv - LSTM layers </m> with 256 channels are inserted into the single - shot TLL network .
second:
By comparing with other two types deep learning approaches ( DCNN and LSTM ) , the presented <m> deep C-LSTM </m> obtains the best performance for classifying these five classes .


class: no relation, base class: same cluster, new model class: same cluster
first:
The English Arts and Crafts architect and theorist W. Lethaby wrote in 1891 : ' ... so is building but the vehicle of <m> architecture </m> which is the thought behind form , embodied and realized for the purpose of its manifestation and transmission ' . Or we call that architecture is the built environment with aesthetically and spiritually stimulating space and form .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: no relation, base class: second -> first, new model class: second -> first
first:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: no relation, base class: second -> first, new model class: second -> first
first:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: no relation, base class: second -> first, new model class: second -> first
first:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: no relation, base class: second -> first, new model class: second -> first
first:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: no relation, base class: second -> first, new model class: second -> first
first:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: no relation, base class: second -> first, new model class: second -> first
first:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: no relation, base class: second -> first, new model class: second -> first
first:
We apply domainadversarial learning , as we consider a <m> descriptor predictor </m> trained with a Siamese - like loss instead of the label predictor trained with a classification loss .
second:
In both approaches , the actual <m> classifier / predictor </m> is learned in a separate step using the feature representation learned by autoencoder ( s ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
However , SVM has a weakness for parameter selection or suitable <m> feature </m> .
second:
The <m> event-driven feature </m> improves the system performance in terms of resources utilization and power consumption compared to the counter classical ones .


class: no relation, base class: first -> second, new model class: first -> second
first:
However , SVM has a weakness for parameter selection or suitable <m> feature </m> .
second:
For video face recognition , most of these methods either use <m> pairwise frame feature similarity computation </m> [ reference ][ reference ] or naive ( average / max ) frame feature pooling [ reference ][ reference ][ reference ] .


class: no relation, base class: second -> first, new model class: second -> first
first:
subsection : Multi - scale feature maps The main goal of constructing <m> multi - scale feature maps </m> is to add more context without increasing the computational cost .
second:
This would amount to an additional <m> feature level </m> GAN loss ( see Figure [ reference ] orange portion ) : Taken together , these loss functions form our complete objective : This ultimately corresponds to solving for a target model according to the optimization problem We have introduced a method for unsupervised adaptation which generalizes adversarial objectives to be viewed as operating at the pixel or feature level .


class: no relation, base class: same cluster, new model class: same cluster
first:
The empirical results show that both the confirmation data of the <m> independent variable system </m> and the confirmation data of the dependent variable have the satisfying reliability .
second:
While we ensure the causal variables of the previous study are considered for inclusion in the models developed , this paper focuses on the ability to accurately predict conflict over individual <m> independent variable analysis </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
The empirical results show that both the confirmation data of the <m> independent variable system </m> and the confirmation data of the dependent variable have the satisfying reliability .
second:
We pursue two separate analyses : an <m> analysis ofindependent variables </m> , in which we tie each study to a particular guiding learning theory in an attempt to determine which guiding theory has had the most predictive success ; and an analysis of dependent variables , which enables us to determine which measurement techniques have been most sensitive to the learning benefits of AV technology .


class: no relation, base class: same cluster, new model class: same cluster
first:
Objectives : This article aims to produce a <m> covariate adjustment method </m> that allows for automatic variable selection , so that practitioners need not commit to any specific set of covariates prior to seeing the data .
second:
One of the most widely used approaches to estimating ATE is <m> covariate adjustment </m> , also known as back - door adjustment or the G - computation formula .


class: no relation, base class: same cluster, new model class: same cluster
first:
Objectives : This article aims to produce a <m> covariate adjustment method </m> that allows for automatic variable selection , so that practitioners need not commit to any specific set of covariates prior to seeing the data .
second:
Therefore , <m> covariate adjustment methods </m> are the most natural candidates for estimating ITE as well as ATE , using the estimates of .


class: no relation, base class: first -> second, new model class: first -> second
first:
Up to date in the area of feature extraction , image feature representation method based on the spatial gradient is still lacking in efficiency especially for the <m> covariate case </m> like carrying bag and wearing a coat .
second:
This article examines four techniques : comparative RDD , <m> covariate matching RDD </m> , treatment effect derivatives , and statistical tests for local selection bias .


class: no relation, base class: first -> second, new model class: first -> second
first:
Up to date in the area of feature extraction , image feature representation method based on the spatial gradient is still lacking in efficiency especially for the <m> covariate case </m> like carrying bag and wearing a coat .
second:
Due to some special features of gene expression data , several new methods have been proposed , including the weighted voting scheme of Golub et al. , the <m> compound covariate method </m> of Hedenfalk et al. ( originally proposed by Tukey ) , and the shrunken centroids method of Tibshirani et al. These methods look different and are more or less ad hoc .


class: no relation, base class: first -> second, new model class: first -> second
first:
I should like to express my gratitude to Keith Rapley ( now Innovation Manager ) at British Airways for arranging sponsorship of this work , to Mark Raskino and Neil Morrison for early stimulation of ideas concerning neural networks ; and to Dr Maurice Barr , Louis Busuttil , Paul Summerbell , Dr Himadri Chatterjee and Rupert Blackley for making possible the forecasting and <m> attribute </m> work .
second:
Compared with conventional perceptual loss , ours is more effective in computation , tailored to our specific <m> attribute transfer task </m> , and can serve as a kind of hidden - layer supervision or regularization to ease the training of the DIAT model .


class: no relation, base class: same cluster, new model class: same cluster
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
These qualities make it suitable for a broad class of real world applications such as <m> network classification </m> , and anomaly detection .


class: no relation, base class: same cluster, new model class: same cluster
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
These qualities make it suitable for a broad class of real world applications such as <m> network classification </m> , and anomaly detection .


class: no relation, base class: same cluster, new model class: same cluster
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
These qualities make it suitable for a broad class of real world applications such as <m> network classification </m> , and anomaly detection .


class: no relation, base class: same cluster, new model class: same cluster
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
These qualities make it suitable for a broad class of real world applications such as <m> network classification </m> , and anomaly detection .


class: no relation, base class: same cluster, new model class: same cluster
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
These qualities make it suitable for a broad class of real world applications such as <m> network classification </m> , and anomaly detection .


class: no relation, base class: same cluster, new model class: same cluster
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
These qualities make it suitable for a broad class of real world applications such as <m> network classification </m> , and anomaly detection .


class: no relation, base class: same cluster, new model class: same cluster
first:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .
second:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .
second:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .
second:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .
second:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: no relation, base class: same cluster, new model class: same cluster
first:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: no relation, base class: same cluster, new model class: same cluster
first:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: no relation, base class: first -> second, new model class: first -> second
first:
In our work , activation function of <m> neuron </m> is implemented with simple CMOS inverter to save area overhead .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: no relation, base class: same cluster, new model class: same cluster
first:
In our work , activation function of <m> neuron </m> is implemented with simple CMOS inverter to save area overhead .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: no relation, base class: second -> first, new model class: second -> first
first:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: no relation, base class: second -> first, new model class: second -> first
first:
Recent years have seen substantial effort being put in the development of algorithms for the <m> systematic evaluation and optimization of neuron models </m> with respect to electrophysiological data .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: no relation, base class: same cluster, new model class: same cluster
first:
Our evaluation results show that the use of Bi-NN indeed produces promising <m> algorithm classification </m> results both within one language and across languages , and the encoding of dependencies from code into the underlying neural networks helps improve algorithm classification accuracy further .
second:
Acoustic feature and its statistical moments passed through the different types of the <m> algorithm-based classifier </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
The other two research studies in this trilogy used ( 1 ) <m> focus group approach </m> and ( 2 ) survey research to identify such impacting factors .
second:
This research has the potential to improve our understanding of challenges and solutions in M&A activity , we attend to this potential through an explorative research approach .. In this paper , we adopted the <m> focus group gathering technique </m> ( new method ) , specifically a ‘ less-structured ’ focus group process as well as the KJ method for analysis .


class: no relation, base class: same cluster, new model class: same cluster
first:
In this paper , a user study was designed to be conducted to validate behavioral archetypes of these groups through interviews and <m> focus group sessions </m> with different physicians , therapists and caregivers in rehabilitation centers .
second:
This research has the potential to improve our understanding of challenges and solutions in M&A activity , we attend to this potential through an explorative research approach .. In this paper , we adopted the <m> focus group gathering technique </m> ( new method ) , specifically a ‘ less-structured ’ focus group process as well as the KJ method for analysis .


class: no relation, base class: same cluster, new model class: same cluster
first:
The determination of cluster competitive advantages is carried out by the method of <m> focus group conduct </m> among experts , and their quantitative assessment is carried out using the questionnaire method , which gives the assessment of implementation and importance .
second:
This research has the potential to improve our understanding of challenges and solutions in M&A activity , we attend to this potential through an explorative research approach .. In this paper , we adopted the <m> focus group gathering technique </m> ( new method ) , specifically a ‘ less-structured ’ focus group process as well as the KJ method for analysis .


class: no relation, base class: same cluster, new model class: same cluster
first:
This research has the potential to improve our understanding of challenges and solutions in M&A activity , we attend to this potential through an explorative research approach .. In this paper , we adopted the <m> focus group gathering technique </m> ( new method ) , specifically a ‘ less-structured ’ focus group process as well as the KJ method for analysis .
second:
The discussion includes a review of the library literature on <m> focus group use </m> , practical aspects of focus group methodology , and the benefits of employing professionals where librarian expertise is low .


class: no relation, base class: same cluster, new model class: first -> second
first:
Then , it describes robust sparse PCA which can be efficiently used in <m> feature learning </m> .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: same cluster, new model class: first -> second
first:
Then , it describes robust sparse PCA which can be efficiently used in <m> feature learning </m> .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: second -> first, new model class: same cluster
first:
The experiments show promising results over other <m> feature based learning approaches </m> .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: no relation, base class: same cluster, new model class: first -> second
first:
The experiments show promising results over other <m> feature based learning approaches </m> .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: same cluster, new model class: first -> second
first:
The experiments show promising results over other <m> feature based learning approaches </m> .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: same cluster, new model class: first -> second
first:
Specifically , it employs an adversarial learning guided multi-label attention module to enhance the <m> feature learning part </m> which can learn discriminative feature representations and keep the cross-modal invariability .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: same cluster, new model class: first -> second
first:
Specifically , it employs an adversarial learning guided multi-label attention module to enhance the <m> feature learning part </m> which can learn discriminative feature representations and keep the cross-modal invariability .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: second -> first, new model class: same cluster
first:
The big data trend has inspired <m> feature-driven learning tasks </m> , which can not be handled by conventional machine learning models .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: no relation, base class: same cluster, new model class: first -> second
first:
The big data trend has inspired <m> feature-driven learning tasks </m> , which can not be handled by conventional machine learning models .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: same cluster, new model class: first -> second
first:
The big data trend has inspired <m> feature-driven learning tasks </m> , which can not be handled by conventional machine learning models .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: same cluster, new model class: first -> second
first:
<m> Feature learning methods </m> based on the Skip - gram architecture have been originally developed in the context of natural language .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: same cluster, new model class: first -> second
first:
<m> Feature learning methods </m> based on the Skip - gram architecture have been originally developed in the context of natural language .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: same cluster, new model class: first -> second
first:
We find that this simple <m> feature learning algorithm </m> is surprisingly successful when applied to visual object recognition .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: same cluster, new model class: first -> second
first:
We find that this simple <m> feature learning algorithm </m> is surprisingly successful when applied to visual object recognition .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: same cluster, new model class: first -> second
first:
We formulate such a dual path architecture as follows : where and denote the extracted information at - th step from individual path , is a <m> feature learning function </m> as .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: same cluster, new model class: first -> second
first:
We formulate such a dual path architecture as follows : where and denote the extracted information at - th step from individual path , is a <m> feature learning function </m> as .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: same cluster, new model class: first -> second
first:
In this paper , we address the <m> feature learning problem </m> in DL by presenting a new algorithm , deeply - supervised nets ( DSN ) , which enforces direct and early supervision for both the hidden layers and the output layer .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: same cluster, new model class: first -> second
first:
In this paper , we address the <m> feature learning problem </m> in DL by presenting a new algorithm , deeply - supervised nets ( DSN ) , which enforces direct and early supervision for both the hidden layers and the output layer .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: same cluster, new model class: first -> second
first:
The process of training a model 's parameters such that a few gradient steps , or even a single gradient step , can produce good results on a new task can be viewed from a <m> feature learning standpoint </m> as building an internal representation that is broadly suitable for many tasks .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: same cluster, new model class: first -> second
first:
The process of training a model 's parameters such that a few gradient steps , or even a single gradient step , can produce good results on a new task can be viewed from a <m> feature learning standpoint </m> as building an internal representation that is broadly suitable for many tasks .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: same cluster, new model class: first -> second
first:
In recent years , deep neural networks ( DNNs ) have become successful in computer vision , speech recognition , and natural language processing with their great power of <m> feature representation learning </m> .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: same cluster, new model class: first -> second
first:
In recent years , deep neural networks ( DNNs ) have become successful in computer vision , speech recognition , and natural language processing with their great power of <m> feature representation learning </m> .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: same cluster, new model class: first -> second
first:
With the increasing sample size of ReID dataset , the <m> learning of features </m> from multi - class person identification tasks , denoted as ID - discriminative Embedding ( IDE ) , has shown great potentials on current large - scale person ReID datasets , such as MARS and PRW , where the IDE features are taken from the last hidden layer of Deep Convolutional Neural Networks ( DCNN ) .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: same cluster, new model class: first -> second
first:
With the increasing sample size of ReID dataset , the <m> learning of features </m> from multi - class person identification tasks , denoted as ID - discriminative Embedding ( IDE ) , has shown great potentials on current large - scale person ReID datasets , such as MARS and PRW , where the IDE features are taken from the last hidden layer of Deep Convolutional Neural Networks ( DCNN ) .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: same cluster, new model class: first -> second
first:
Thus , it is significant to explicitly involve the semantic boundary to guide the <m> learning of the features </m> .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: same cluster, new model class: first -> second
first:
Thus , it is significant to explicitly involve the semantic boundary to guide the <m> learning of the features </m> .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: first -> second, new model class: first -> second
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: first -> second, new model class: first -> second
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: first -> second, new model class: first -> second
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: first -> second, new model class: first -> second
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .


class: no relation, base class: first -> second, new model class: first -> second
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .


class: no relation, base class: first -> second, new model class: first -> second
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: first -> second, new model class: first -> second
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: first -> second, new model class: first -> second
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: first -> second, new model class: first -> second
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: no relation, base class: first -> second, new model class: first -> second
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .


class: no relation, base class: first -> second, new model class: first -> second
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .


class: no relation, base class: first -> second, new model class: first -> second
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .


class: no relation, base class: first -> second, new model class: first -> second
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: no relation, base class: first -> second, new model class: first -> second
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: first -> second, new model class: first -> second
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: first -> second, new model class: first -> second
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: first -> second, new model class: first -> second
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .


class: no relation, base class: first -> second, new model class: first -> second
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .


class: no relation, base class: first -> second, new model class: first -> second
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: first -> second, new model class: first -> second
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: first -> second, new model class: first -> second
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: first -> second, new model class: first -> second
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: no relation, base class: first -> second, new model class: first -> second
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .


class: no relation, base class: first -> second, new model class: first -> second
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .


class: no relation, base class: first -> second, new model class: first -> second
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .


class: no relation, base class: first -> second, new model class: first -> second
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: no relation, base class: first -> second, new model class: first -> second
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: first -> second, new model class: first -> second
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: first -> second, new model class: first -> second
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: first -> second, new model class: first -> second
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .


class: no relation, base class: first -> second, new model class: first -> second
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .


class: no relation, base class: first -> second, new model class: first -> second
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: first -> second, new model class: first -> second
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: first -> second, new model class: first -> second
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: first -> second, new model class: first -> second
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: no relation, base class: first -> second, new model class: first -> second
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .


class: no relation, base class: first -> second, new model class: first -> second
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .


class: no relation, base class: first -> second, new model class: first -> second
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .


class: no relation, base class: first -> second, new model class: first -> second
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: first -> second, new model class: first -> second
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: first -> second, new model class: first -> second
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: first -> second, new model class: first -> second
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .


class: no relation, base class: first -> second, new model class: first -> second
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .


class: no relation, base class: first -> second, new model class: first -> second
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: first -> second, new model class: first -> second
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: first -> second, new model class: first -> second
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: first -> second, new model class: first -> second
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: no relation, base class: first -> second, new model class: first -> second
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .


class: no relation, base class: first -> second, new model class: first -> second
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .


class: no relation, base class: first -> second, new model class: first -> second
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .


class: no relation, base class: first -> second, new model class: first -> second
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: no relation, base class: first -> second, new model class: first -> second
first:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: first -> second, new model class: first -> second
first:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: first -> second, new model class: first -> second
first:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: first -> second, new model class: first -> second
first:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .
second:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .


class: no relation, base class: first -> second, new model class: first -> second
first:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .
second:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .


class: no relation, base class: first -> second, new model class: first -> second
first:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: first -> second, new model class: first -> second
first:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: first -> second, new model class: first -> second
first:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: first -> second, new model class: first -> second
first:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: no relation, base class: first -> second, new model class: first -> second
first:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .
second:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .


class: no relation, base class: first -> second, new model class: first -> second
first:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .
second:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .


class: no relation, base class: first -> second, new model class: first -> second
first:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .
second:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .


class: no relation, base class: first -> second, new model class: first -> second
first:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: no relation, base class: first -> second, new model class: first -> second
first:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: first -> second, new model class: first -> second
first:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: first -> second, new model class: first -> second
first:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: first -> second, new model class: first -> second
first:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .
second:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .


class: no relation, base class: first -> second, new model class: first -> second
first:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .
second:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .


class: no relation, base class: first -> second, new model class: first -> second
first:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: first -> second, new model class: first -> second
first:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: first -> second, new model class: first -> second
first:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: first -> second, new model class: first -> second
first:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: no relation, base class: first -> second, new model class: first -> second
first:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .
second:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .


class: no relation, base class: first -> second, new model class: first -> second
first:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .
second:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .


class: no relation, base class: first -> second, new model class: first -> second
first:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .
second:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .


class: no relation, base class: first -> second, new model class: first -> second
first:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: no relation, base class: first -> second, new model class: first -> second
first:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: first -> second, new model class: first -> second
first:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: first -> second, new model class: first -> second
first:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: first -> second, new model class: first -> second
first:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .
second:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .


class: no relation, base class: first -> second, new model class: first -> second
first:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .
second:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .


class: no relation, base class: first -> second, new model class: first -> second
first:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: first -> second, new model class: first -> second
first:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: first -> second, new model class: first -> second
first:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: first -> second, new model class: first -> second
first:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: no relation, base class: first -> second, new model class: first -> second
first:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .
second:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .


class: no relation, base class: first -> second, new model class: first -> second
first:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .
second:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .


class: no relation, base class: first -> second, new model class: first -> second
first:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .
second:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .


class: no relation, base class: first -> second, new model class: first -> second
first:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: no relation, base class: first -> second, new model class: first -> second
first:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: first -> second, new model class: first -> second
first:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: first -> second, new model class: first -> second
first:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: first -> second, new model class: first -> second
first:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .
second:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .


class: no relation, base class: first -> second, new model class: first -> second
first:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .
second:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .


class: no relation, base class: first -> second, new model class: first -> second
first:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: first -> second, new model class: first -> second
first:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: first -> second, new model class: first -> second
first:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: first -> second, new model class: first -> second
first:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: no relation, base class: first -> second, new model class: first -> second
first:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .
second:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .


class: no relation, base class: first -> second, new model class: first -> second
first:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .
second:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .


class: no relation, base class: first -> second, new model class: first -> second
first:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .
second:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .


class: no relation, base class: first -> second, new model class: first -> second
first:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: no relation, base class: first -> second, new model class: first -> second
first:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: first -> second, new model class: first -> second
first:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: first -> second, new model class: first -> second
first:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: first -> second, new model class: first -> second
first:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .
second:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .


class: no relation, base class: first -> second, new model class: first -> second
first:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .
second:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .


class: no relation, base class: first -> second, new model class: first -> second
first:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: first -> second, new model class: first -> second
first:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: first -> second, new model class: first -> second
first:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: first -> second, new model class: first -> second
first:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: no relation, base class: first -> second, new model class: first -> second
first:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .
second:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .


class: no relation, base class: first -> second, new model class: first -> second
first:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .
second:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .


class: no relation, base class: first -> second, new model class: first -> second
first:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .
second:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .


class: no relation, base class: first -> second, new model class: first -> second
first:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: no relation, base class: first -> second, new model class: first -> second
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .


class: no relation, base class: first -> second, new model class: first -> second
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .


class: no relation, base class: first -> second, new model class: first -> second
first:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: first -> second, new model class: first -> second
first:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: first -> second, new model class: first -> second
first:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: first -> second, new model class: first -> second
first:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .
second:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .


class: no relation, base class: first -> second, new model class: first -> second
first:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .
second:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .


class: no relation, base class: first -> second, new model class: first -> second
first:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: first -> second, new model class: first -> second
first:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: first -> second, new model class: first -> second
first:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: first -> second, new model class: first -> second
first:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: no relation, base class: first -> second, new model class: first -> second
first:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .
second:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .


class: no relation, base class: first -> second, new model class: first -> second
first:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .
second:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .


class: no relation, base class: first -> second, new model class: first -> second
first:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .
second:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .


class: no relation, base class: first -> second, new model class: first -> second
first:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: no relation, base class: first -> second, new model class: first -> second
first:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: first -> second, new model class: first -> second
first:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: first -> second, new model class: first -> second
first:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: first -> second, new model class: first -> second
first:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .
second:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .


class: no relation, base class: first -> second, new model class: first -> second
first:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .
second:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .


class: no relation, base class: first -> second, new model class: first -> second
first:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: first -> second, new model class: first -> second
first:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: first -> second, new model class: first -> second
first:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: first -> second, new model class: first -> second
first:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: no relation, base class: first -> second, new model class: first -> second
first:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .
second:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .


class: no relation, base class: first -> second, new model class: first -> second
first:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .
second:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .


class: no relation, base class: first -> second, new model class: first -> second
first:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .
second:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .


class: no relation, base class: first -> second, new model class: first -> second
first:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: no relation, base class: first -> second, new model class: first -> second
first:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: first -> second, new model class: first -> second
first:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: first -> second, new model class: first -> second
first:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: first -> second, new model class: first -> second
first:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .
second:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .


class: no relation, base class: first -> second, new model class: first -> second
first:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .
second:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .


class: no relation, base class: first -> second, new model class: first -> second
first:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: first -> second, new model class: first -> second
first:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: first -> second, new model class: first -> second
first:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: first -> second, new model class: first -> second
first:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: no relation, base class: first -> second, new model class: first -> second
first:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .
second:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .


class: no relation, base class: first -> second, new model class: first -> second
first:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .
second:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .


class: no relation, base class: first -> second, new model class: first -> second
first:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .
second:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .


class: no relation, base class: first -> second, new model class: first -> second
first:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: no relation, base class: first -> second, new model class: first -> second
first:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: first -> second, new model class: first -> second
first:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: first -> second, new model class: first -> second
first:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: first -> second, new model class: first -> second
first:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .
second:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .


class: no relation, base class: first -> second, new model class: first -> second
first:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .
second:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .


class: no relation, base class: first -> second, new model class: first -> second
first:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: first -> second, new model class: first -> second
first:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: no relation, base class: first -> second, new model class: first -> second
first:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: first -> second, new model class: first -> second
first:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: no relation, base class: first -> second, new model class: first -> second
first:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .
second:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .


class: no relation, base class: first -> second, new model class: first -> second
first:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .
second:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .


class: no relation, base class: first -> second, new model class: first -> second
first:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .
second:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .


class: no relation, base class: first -> second, new model class: first -> second
first:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
It includes the designs of <m> intermediate layer </m> and mapping mechanism , and in detail describes the virtual station data structure which can indicate the data change of wireless terminal station , and the mapping relationship between this data structure and wireless stations .
second:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .


class: no relation, base class: same cluster, new model class: same cluster
first:
It includes the designs of <m> intermediate layer </m> and mapping mechanism , and in detail describes the virtual station data structure which can indicate the data change of wireless terminal station , and the mapping relationship between this data structure and wireless stations .
second:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .


class: no relation, base class: same cluster, new model class: same cluster
first:
It includes the designs of <m> intermediate layer </m> and mapping mechanism , and in detail describes the virtual station data structure which can indicate the data change of wireless terminal station , and the mapping relationship between this data structure and wireless stations .
second:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .


class: no relation, base class: second -> first, new model class: second -> first
first:
It includes the designs of <m> intermediate layer </m> and mapping mechanism , and in detail describes the virtual station data structure which can indicate the data change of wireless terminal station , and the mapping relationship between this data structure and wireless stations .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: second -> first, new model class: second -> first
first:
It includes the designs of <m> intermediate layer </m> and mapping mechanism , and in detail describes the virtual station data structure which can indicate the data change of wireless terminal station , and the mapping relationship between this data structure and wireless stations .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: second -> first, new model class: second -> first
first:
It includes the designs of <m> intermediate layer </m> and mapping mechanism , and in detail describes the virtual station data structure which can indicate the data change of wireless terminal station , and the mapping relationship between this data structure and wireless stations .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: second -> first, new model class: second -> first
first:
It includes the designs of <m> intermediate layer </m> and mapping mechanism , and in detail describes the virtual station data structure which can indicate the data change of wireless terminal station , and the mapping relationship between this data structure and wireless stations .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: second -> first, new model class: second -> first
first:
It includes the designs of <m> intermediate layer </m> and mapping mechanism , and in detail describes the virtual station data structure which can indicate the data change of wireless terminal station , and the mapping relationship between this data structure and wireless stations .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: second -> first, new model class: second -> first
first:
It includes the designs of <m> intermediate layer </m> and mapping mechanism , and in detail describes the virtual station data structure which can indicate the data change of wireless terminal station , and the mapping relationship between this data structure and wireless stations .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: second -> first, new model class: second -> first
first:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: second -> first, new model class: second -> first
first:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: second -> first, new model class: second -> first
first:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: second -> first, new model class: second -> first
first:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: second -> first, new model class: second -> first
first:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: second -> first, new model class: second -> first
first:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: second -> first, new model class: second -> first
first:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: second -> first, new model class: second -> first
first:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: second -> first, new model class: second -> first
first:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: second -> first, new model class: second -> first
first:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: second -> first, new model class: second -> first
first:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: second -> first, new model class: second -> first
first:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: second -> first, new model class: second -> first
first:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: second -> first, new model class: second -> first
first:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: second -> first, new model class: second -> first
first:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: second -> first, new model class: second -> first
first:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: second -> first, new model class: second -> first
first:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: second -> first, new model class: second -> first
first:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: same cluster, new model class: same cluster
first:
The added advantage of <m> hidden layer </m> for lip reading is that it takes into account the nonlinear variation of lip features while speaking .
second:
<m> Hidden Data Layer </m> consists of three essential elements , i.e. a Head Resident which doubles the functions of a public SDN controller and is a superior unit concerning public one , a Hidden Layer Protocol realising communication between individual network elements .


class: no relation, base class: same cluster, new model class: same cluster
first:
The experiment showed that the best <m> hidden layer architecture </m> ( HLA ) is 5 - 10 - 11 - 12 - 13 - 1 with learning function ( LF ) of trainlm , activation function ( AF ) of logsig and purelin , and learning rate ( LR ) of 0.5 .
second:
<m> Hidden Data Layer </m> consists of three essential elements , i.e. a Head Resident which doubles the functions of a public SDN controller and is a superior unit concerning public one , a Hidden Layer Protocol realising communication between individual network elements .


class: no relation, base class: same cluster, new model class: same cluster
first:
<m> Hidden Data Layer </m> consists of three essential elements , i.e. a Head Resident which doubles the functions of a public SDN controller and is a superior unit concerning public one , a Hidden Layer Protocol realising communication between individual network elements .
second:
Further <m> Hidden layer 1 </m> helps to extract data to estimate the forecasting equation and also it helps for evaluating estimated Model using different measures .


class: no relation, base class: same cluster, new model class: same cluster
first:
<m> Hidden Data Layer </m> consists of three essential elements , i.e. a Head Resident which doubles the functions of a public SDN controller and is a superior unit concerning public one , a Hidden Layer Protocol realising communication between individual network elements .
second:
where and are the question and the answer encoded vectors and denotes the output of the <m> hidden layer </m> of the MLP .


class: no relation, base class: same cluster, new model class: same cluster
first:
<m> Hidden Data Layer </m> consists of three essential elements , i.e. a Head Resident which doubles the functions of a public SDN controller and is a superior unit concerning public one , a Hidden Layer Protocol realising communication between individual network elements .
second:
Similar to our LoNGAE model , the GCN model can learn <m> hidden layer representations </m> that encode both local graph structure and features of nodes .


class: no relation, base class: same cluster, new model class: same cluster
first:
<m> Hidden Data Layer </m> consists of three essential elements , i.e. a Head Resident which doubles the functions of a public SDN controller and is a superior unit concerning public one , a Hidden Layer Protocol realising communication between individual network elements .
second:
Their method simultaneously minimizes classification error while improving the directness and transparency of the <m> hidden layer learning process </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
<m> Hidden Data Layer </m> consists of three essential elements , i.e. a Head Resident which doubles the functions of a public SDN controller and is a superior unit concerning public one , a Hidden Layer Protocol realising communication between individual network elements .
second:
This was found sufficient , with the following <m> hidden layers learning </m> to combine the multi - scale features .


class: no relation, base class: same cluster, new model class: same cluster
first:
<m> Hidden Data Layer </m> consists of three essential elements , i.e. a Head Resident which doubles the functions of a public SDN controller and is a superior unit concerning public one , a Hidden Layer Protocol realising communication between individual network elements .
second:
The resulting feature vectors are then scored using a non - linear function , namely a multi - layer perceptron with <m> one hidden layer </m> ( MLP ) :


class: no relation, base class: second -> first, new model class: second -> first
first:
We review how data-analysis concepts and techniques developed by computational neuroscientists can be useful for analyzing representations in DNNs , and in turn , how recently developed techniques for <m> analysis of DNNs </m> can be useful for understanding representations in biological neural networks .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: second -> first, new model class: second -> first
first:
We review how data-analysis concepts and techniques developed by computational neuroscientists can be useful for analyzing representations in DNNs , and in turn , how recently developed techniques for <m> analysis of DNNs </m> can be useful for understanding representations in biological neural networks .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: second -> first, new model class: second -> first
first:
We review how data-analysis concepts and techniques developed by computational neuroscientists can be useful for analyzing representations in DNNs , and in turn , how recently developed techniques for <m> analysis of DNNs </m> can be useful for understanding representations in biological neural networks .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: second -> first, new model class: second -> first
first:
Accordingly , techniques that enable efficient <m> processing of DNNs </m> to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI system .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: second -> first, new model class: second -> first
first:
Accordingly , techniques that enable efficient <m> processing of DNNs </m> to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI system .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: second -> first, new model class: second -> first
first:
Accordingly , techniques that enable efficient <m> processing of DNNs </m> to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI system .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: same cluster, new model class: same cluster
first:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: same cluster, new model class: same cluster
first:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: same cluster, new model class: same cluster
first:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: same cluster, new model class: same cluster
first:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: same cluster, new model class: same cluster
first:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: same cluster, new model class: same cluster
first:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: same cluster, new model class: same cluster
first:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: same cluster, new model class: same cluster
first:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: same cluster, new model class: same cluster
first:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: same cluster, new model class: same cluster
first:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .
second:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .


class: no relation, base class: same cluster, new model class: same cluster
first:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .
second:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .


class: no relation, base class: same cluster, new model class: same cluster
first:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .
second:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .


class: no relation, base class: same cluster, new model class: same cluster
first:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .
second:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .


class: no relation, base class: same cluster, new model class: same cluster
first:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .
second:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .


class: no relation, base class: same cluster, new model class: same cluster
first:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .
second:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .


class: no relation, base class: same cluster, new model class: same cluster
first:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .
second:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .


class: no relation, base class: same cluster, new model class: same cluster
first:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .
second:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .


class: no relation, base class: same cluster, new model class: same cluster
first:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .
second:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .


class: no relation, base class: same cluster, new model class: same cluster
first:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .
second:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .


class: no relation, base class: same cluster, new model class: same cluster
first:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .
second:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .


class: no relation, base class: same cluster, new model class: same cluster
first:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: no relation, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: no relation, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: no relation, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: no relation, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: no relation, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: no relation, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: no relation, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: no relation, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: no relation, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: no relation, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: no relation, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: no relation, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: no relation, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: no relation, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: no relation, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: no relation, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: no relation, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: no relation, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: no relation, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .


class: no relation, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .
second:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .


class: no relation, base class: same cluster, new model class: same cluster
first:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .
second:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .


class: no relation, base class: same cluster, new model class: same cluster
first:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: same cluster, new model class: same cluster
first:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: no relation, base class: same cluster, new model class: same cluster
first:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: no relation, base class: same cluster, new model class: same cluster
first:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: no relation, base class: same cluster, new model class: same cluster
first:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .
second:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .


class: no relation, base class: first -> second, new model class: first -> second
first:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: no relation, base class: first -> second, new model class: same cluster
first:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: no relation, base class: same cluster, new model class: same cluster
first:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: no relation, base class: same cluster, new model class: same cluster
first:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: no relation, base class: same cluster, new model class: same cluster
first:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: no relation, base class: same cluster, new model class: same cluster
first:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
Tuple-versioning of relations is the adopted method of <m> temporal data representation </m> .
second:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .
second:
We claim that these refinement iterations should have a more direct and shared <m> temporal representation </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
The results demonstrate that the algorithm can also learn <m> temporal sequence detection </m> .
second:
In this work , we designed tasks around <m> temporal event sequence </m> and timing analysis and conducted a controlled experiment on Amazon Mechanical Turk to examine four sentinel event alignment approaches : no sentinel event alignment ( NoAlign ) , single-event alignment ( SingleAlign ) , dual-event alignment with left justification ( DualLeft ) , and dual-event alignment with stretch justification ( DualStretch ) .


class: no relation, base class: same cluster, new model class: same cluster
first:
The results demonstrate that the algorithm can also learn <m> temporal sequence detection </m> .
second:
This user modeling approach comprises the following new novel aspects ( 1 ) Modeling microblog users behavior evolution by considering the different event phases ( 2 ) Characterizing users activity over time through a <m> temporal sequence representation </m> ( 3 ) Time-series-based selection of the most discriminative features characterizing users at each event phase .


class: no relation, base class: second -> first, new model class: second -> first
first:
In this work , we designed tasks around <m> temporal event sequence </m> and timing analysis and conducted a controlled experiment on Amazon Mechanical Turk to examine four sentinel event alignment approaches : no sentinel event alignment ( NoAlign ) , single-event alignment ( SingleAlign ) , dual-event alignment with left justification ( DualLeft ) , and dual-event alignment with stretch justification ( DualStretch ) .
second:
This user modeling approach comprises the following new novel aspects ( 1 ) Modeling microblog users behavior evolution by considering the different event phases ( 2 ) Characterizing users activity over time through a <m> temporal sequence representation </m> ( 3 ) Time-series-based selection of the most discriminative features characterizing users at each event phase .


class: no relation, base class: first -> second, new model class: first -> second
first:
We also present a <m> numerical approach </m> to the solution in the general case .
second:
Furthermore , the optimal organization of <m> numerical data processing </m> in enterprises is analyzed on the example of predicting demand in the firm .


class: no relation, base class: first -> second, new model class: first -> second
first:
We also present a <m> numerical approach </m> to the solution in the general case .
second:
It is designed to support distributed computing and data sharing on the Internet through the use of distributed objects and a very general <m> numerical data model </m> .


class: no relation, base class: same cluster, new model class: first -> second
first:
We also present a <m> numerical approach </m> to the solution in the general case .
second:
The rapidly increasing power and availability of small computers have made sophisticated <m> numerical analysis of data </m> a routine feature of modem chemistry .


class: no relation, base class: first -> second, new model class: same cluster
first:
We also present a <m> numerical approach </m> to the solution in the general case .
second:
The fourth and last module , entitled Backtesting and <m> Numerical Results </m> , evaluates the accuracy of the trend predictive numerical models over a " significant " test set via two generic backtesting plans .


class: no relation, base class: first -> second, new model class: first -> second
first:
We also present a <m> numerical approach </m> to the solution in the general case .
second:
Formation features of the <m> numerical data source model </m> were examined .


class: no relation, base class: same cluster, new model class: same cluster
first:
We also present a <m> numerical approach </m> to the solution in the general case .
second:
On the downside , todayâs computing infrastructures are very inefficient when it comes to <m> numerical calculation </m> on non - uniform sparse data structures .


class: no relation, base class: same cluster, new model class: same cluster
first:
We also present a <m> numerical approach </m> to the solution in the general case .
second:
FST thus runs in <m> numerical problems </m> and common solutions include reducing by PCA or adding a regularisation term to .


class: no relation, base class: same cluster, new model class: same cluster
first:
We also present a <m> numerical approach </m> to the solution in the general case .
second:
Because sea surface propagation causes multipath effect , it 's necessary to study the multipath ray model and the <m> numerical characteristics </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
Both analytical derivation and <m> numerical analysis </m> have been conducted on Tucson network in Arizona , as well as on the Capital Area Metropolitan Planning Organization ( CAMPO ) network in Austin , TX .
second:
Furthermore , the optimal organization of <m> numerical data processing </m> in enterprises is analyzed on the example of predicting demand in the firm .


class: no relation, base class: first -> second, new model class: first -> second
first:
Both analytical derivation and <m> numerical analysis </m> have been conducted on Tucson network in Arizona , as well as on the Capital Area Metropolitan Planning Organization ( CAMPO ) network in Austin , TX .
second:
It is designed to support distributed computing and data sharing on the Internet through the use of distributed objects and a very general <m> numerical data model </m> .


class: no relation, base class: same cluster, new model class: first -> second
first:
Both analytical derivation and <m> numerical analysis </m> have been conducted on Tucson network in Arizona , as well as on the Capital Area Metropolitan Planning Organization ( CAMPO ) network in Austin , TX .
second:
The rapidly increasing power and availability of small computers have made sophisticated <m> numerical analysis of data </m> a routine feature of modem chemistry .


class: no relation, base class: first -> second, new model class: same cluster
first:
Both analytical derivation and <m> numerical analysis </m> have been conducted on Tucson network in Arizona , as well as on the Capital Area Metropolitan Planning Organization ( CAMPO ) network in Austin , TX .
second:
The fourth and last module , entitled Backtesting and <m> Numerical Results </m> , evaluates the accuracy of the trend predictive numerical models over a " significant " test set via two generic backtesting plans .


class: no relation, base class: first -> second, new model class: first -> second
first:
Both analytical derivation and <m> numerical analysis </m> have been conducted on Tucson network in Arizona , as well as on the Capital Area Metropolitan Planning Organization ( CAMPO ) network in Austin , TX .
second:
Formation features of the <m> numerical data source model </m> were examined .


class: no relation, base class: same cluster, new model class: same cluster
first:
Both analytical derivation and <m> numerical analysis </m> have been conducted on Tucson network in Arizona , as well as on the Capital Area Metropolitan Planning Organization ( CAMPO ) network in Austin , TX .
second:
On the downside , todayâs computing infrastructures are very inefficient when it comes to <m> numerical calculation </m> on non - uniform sparse data structures .


class: no relation, base class: same cluster, new model class: same cluster
first:
Both analytical derivation and <m> numerical analysis </m> have been conducted on Tucson network in Arizona , as well as on the Capital Area Metropolitan Planning Organization ( CAMPO ) network in Austin , TX .
second:
FST thus runs in <m> numerical problems </m> and common solutions include reducing by PCA or adding a regularisation term to .


class: no relation, base class: same cluster, new model class: same cluster
first:
Both analytical derivation and <m> numerical analysis </m> have been conducted on Tucson network in Arizona , as well as on the Capital Area Metropolitan Planning Organization ( CAMPO ) network in Austin , TX .
second:
Because sea surface propagation causes multipath effect , it 's necessary to study the multipath ray model and the <m> numerical characteristics </m> .


class: no relation, base class: second -> first, new model class: same cluster
first:
Furthermore , the optimal organization of <m> numerical data processing </m> in enterprises is analyzed on the example of predicting demand in the firm .
second:
Finally , a <m> numerical data analysis </m> is performed in order to illustrate all the methods of inference discussed here .


class: no relation, base class: second -> first, new model class: second -> first
first:
Furthermore , the optimal organization of <m> numerical data processing </m> in enterprises is analyzed on the example of predicting demand in the firm .
second:
However , performing singular value decomposition of real valued matrices using <m> numerical approaches </m> often encounter roundoff errors .


class: no relation, base class: second -> first, new model class: second -> first
first:
Furthermore , the optimal organization of <m> numerical data processing </m> in enterprises is analyzed on the example of predicting demand in the firm .
second:
FST thus runs in <m> numerical problems </m> and common solutions include reducing by PCA or adding a regularisation term to .


class: no relation, base class: first -> second, new model class: same cluster
first:
Finally , a <m> numerical data analysis </m> is performed in order to illustrate all the methods of inference discussed here .
second:
It is designed to support distributed computing and data sharing on the Internet through the use of distributed objects and a very general <m> numerical data model </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
Finally , a <m> numerical data analysis </m> is performed in order to illustrate all the methods of inference discussed here .
second:
The rapidly increasing power and availability of small computers have made sophisticated <m> numerical analysis of data </m> a routine feature of modem chemistry .


class: no relation, base class: first -> second, new model class: second -> first
first:
Finally , a <m> numerical data analysis </m> is performed in order to illustrate all the methods of inference discussed here .
second:
The fourth and last module , entitled Backtesting and <m> Numerical Results </m> , evaluates the accuracy of the trend predictive numerical models over a " significant " test set via two generic backtesting plans .


class: no relation, base class: first -> second, new model class: same cluster
first:
Finally , a <m> numerical data analysis </m> is performed in order to illustrate all the methods of inference discussed here .
second:
Formation features of the <m> numerical data source model </m> were examined .


class: no relation, base class: same cluster, new model class: second -> first
first:
Finally , a <m> numerical data analysis </m> is performed in order to illustrate all the methods of inference discussed here .
second:
On the downside , todayâs computing infrastructures are very inefficient when it comes to <m> numerical calculation </m> on non - uniform sparse data structures .


class: no relation, base class: same cluster, new model class: second -> first
first:
Finally , a <m> numerical data analysis </m> is performed in order to illustrate all the methods of inference discussed here .
second:
FST thus runs in <m> numerical problems </m> and common solutions include reducing by PCA or adding a regularisation term to .


class: no relation, base class: same cluster, new model class: second -> first
first:
Finally , a <m> numerical data analysis </m> is performed in order to illustrate all the methods of inference discussed here .
second:
Because sea surface propagation causes multipath effect , it 's necessary to study the multipath ray model and the <m> numerical characteristics </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
It is designed to support distributed computing and data sharing on the Internet through the use of distributed objects and a very general <m> numerical data model </m> .
second:
However , performing singular value decomposition of real valued matrices using <m> numerical approaches </m> often encounter roundoff errors .


class: no relation, base class: second -> first, new model class: second -> first
first:
It is designed to support distributed computing and data sharing on the Internet through the use of distributed objects and a very general <m> numerical data model </m> .
second:
FST thus runs in <m> numerical problems </m> and common solutions include reducing by PCA or adding a regularisation term to .


class: no relation, base class: same cluster, new model class: second -> first
first:
The rapidly increasing power and availability of small computers have made sophisticated <m> numerical analysis of data </m> a routine feature of modem chemistry .
second:
However , performing singular value decomposition of real valued matrices using <m> numerical approaches </m> often encounter roundoff errors .


class: no relation, base class: same cluster, new model class: second -> first
first:
The rapidly increasing power and availability of small computers have made sophisticated <m> numerical analysis of data </m> a routine feature of modem chemistry .
second:
FST thus runs in <m> numerical problems </m> and common solutions include reducing by PCA or adding a regularisation term to .


class: no relation, base class: second -> first, new model class: same cluster
first:
The fourth and last module , entitled Backtesting and <m> Numerical Results </m> , evaluates the accuracy of the trend predictive numerical models over a " significant " test set via two generic backtesting plans .
second:
However , performing singular value decomposition of real valued matrices using <m> numerical approaches </m> often encounter roundoff errors .


class: no relation, base class: second -> first, new model class: same cluster
first:
The fourth and last module , entitled Backtesting and <m> Numerical Results </m> , evaluates the accuracy of the trend predictive numerical models over a " significant " test set via two generic backtesting plans .
second:
FST thus runs in <m> numerical problems </m> and common solutions include reducing by PCA or adding a regularisation term to .


class: no relation, base class: second -> first, new model class: second -> first
first:
Formation features of the <m> numerical data source model </m> were examined .
second:
However , performing singular value decomposition of real valued matrices using <m> numerical approaches </m> often encounter roundoff errors .


class: no relation, base class: second -> first, new model class: second -> first
first:
Formation features of the <m> numerical data source model </m> were examined .
second:
FST thus runs in <m> numerical problems </m> and common solutions include reducing by PCA or adding a regularisation term to .


class: no relation, base class: same cluster, new model class: same cluster
first:
However , performing singular value decomposition of real valued matrices using <m> numerical approaches </m> often encounter roundoff errors .
second:
On the downside , todayâs computing infrastructures are very inefficient when it comes to <m> numerical calculation </m> on non - uniform sparse data structures .


class: no relation, base class: same cluster, new model class: same cluster
first:
However , performing singular value decomposition of real valued matrices using <m> numerical approaches </m> often encounter roundoff errors .
second:
FST thus runs in <m> numerical problems </m> and common solutions include reducing by PCA or adding a regularisation term to .


class: no relation, base class: same cluster, new model class: same cluster
first:
However , performing singular value decomposition of real valued matrices using <m> numerical approaches </m> often encounter roundoff errors .
second:
Because sea surface propagation causes multipath effect , it 's necessary to study the multipath ray model and the <m> numerical characteristics </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
On the downside , todayâs computing infrastructures are very inefficient when it comes to <m> numerical calculation </m> on non - uniform sparse data structures .
second:
FST thus runs in <m> numerical problems </m> and common solutions include reducing by PCA or adding a regularisation term to .


class: no relation, base class: same cluster, new model class: same cluster
first:
FST thus runs in <m> numerical problems </m> and common solutions include reducing by PCA or adding a regularisation term to .
second:
Because sea surface propagation causes multipath effect , it 's necessary to study the multipath ray model and the <m> numerical characteristics </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
Furthermore , existing models ( in particular <m> categorical ones </m> ) are often too “ coarse-grained ” insofar as they impose various equalities on proofs which are , at the least , debatable .
second:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .


class: no relation, base class: same cluster, new model class: same cluster
first:
Furthermore , existing models ( in particular <m> categorical ones </m> ) are often too “ coarse-grained ” insofar as they impose various equalities on proofs which are , at the least , debatable .
second:
Basic methods of the research : <m> categorical analysis </m> , determining of statistic parameters of word-forming models .


class: no relation, base class: same cluster, new model class: same cluster
first:
Furthermore , existing models ( in particular <m> categorical ones </m> ) are often too “ coarse-grained ” insofar as they impose various equalities on proofs which are , at the least , debatable .
second:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
Furthermore , existing models ( in particular <m> categorical ones </m> ) are often too “ coarse-grained ” insofar as they impose various equalities on proofs which are , at the least , debatable .
second:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .


class: no relation, base class: same cluster, new model class: same cluster
first:
Furthermore , existing models ( in particular <m> categorical ones </m> ) are often too “ coarse-grained ” insofar as they impose various equalities on proofs which are , at the least , debatable .
second:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .


class: no relation, base class: same cluster, new model class: same cluster
first:
Furthermore , existing models ( in particular <m> categorical ones </m> ) are often too “ coarse-grained ” insofar as they impose various equalities on proofs which are , at the least , debatable .
second:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .


class: no relation, base class: same cluster, new model class: same cluster
first:
Basic methods of the research : <m> categorical analysis </m> , determining of statistic parameters of word-forming models .
second:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .


class: no relation, base class: same cluster, new model class: same cluster
first:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .
second:
<m> Continuous feature set </m> and content monitoring of real-time monitoring traffic are designed .


class: no relation, base class: same cluster, new model class: same cluster
first:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .
second:
<m> Continuous feature set </m> and content monitoring of real-time monitoring traffic are designed .


class: no relation, base class: same cluster, new model class: same cluster
first:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .
second:
<m> Continuous feature set </m> and content monitoring of real-time monitoring traffic are designed .


class: no relation, base class: first -> second, new model class: same cluster
first:
<m> Continuous feature set </m> and content monitoring of real-time monitoring traffic are designed .
second:
The main goal of this dissertation work is to provide syntactically and semantically related words based on <m> continuous feature vector representation </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
<m> Continuous feature set </m> and content monitoring of real-time monitoring traffic are designed .
second:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: no relation, base class: first -> second, new model class: first -> second
first:
When uniform noise is added ( with values in the interval [ 0 , 1 ] ) , then the log - likelihoods of <m> continuous </m> and discrete models are directly comparable .
second:
When uniform noise is added ( with values in the interval [ 0 , 1 ] ) , then the log - likelihoods of continuous and <m> discrete models </m> are directly comparable .


class: no relation, base class: same cluster, new model class: same cluster
first:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .
second:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .


class: no relation, base class: same cluster, new model class: same cluster
first:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .
second:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .


class: no relation, base class: same cluster, new model class: same cluster
first:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: no relation, base class: same cluster, new model class: same cluster
first:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .
second:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: no relation, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: no relation, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: no relation, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: no relation, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: no relation, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: no relation, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: no relation, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: no relation, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: no relation, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: no relation, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: no relation, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: same cluster, new model class: same cluster
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .


class: no relation, base class: same cluster, new model class: same cluster
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: no relation, base class: first -> second, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: no relation, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: no relation, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: no relation, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: same cluster, new model class: same cluster
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: no relation, base class: first -> second, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: no relation, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: no relation, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: no relation, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: no relation, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: no relation, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: no relation, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: no relation, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: no relation, base class: first -> second, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: no relation, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: no relation, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: no relation, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: no relation, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: no relation, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: no relation, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: no relation, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: no relation, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: no relation, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: no relation, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: no relation, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .


class: no relation, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .


class: no relation, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: first -> second, new model class: second -> first
first:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: no relation, base class: first -> second, new model class: second -> first
first:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: no relation, base class: first -> second, new model class: second -> first
first:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: no relation, base class: first -> second, new model class: second -> first
first:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: no relation, base class: first -> second, new model class: second -> first
first:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: no relation, base class: first -> second, new model class: second -> first
first:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: no relation, base class: first -> second, new model class: second -> first
first:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: no relation, base class: first -> second, new model class: second -> first
first:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: no relation, base class: first -> second, new model class: second -> first
first:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: no relation, base class: first -> second, new model class: second -> first
first:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: no relation, base class: first -> second, new model class: second -> first
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: no relation, base class: first -> second, new model class: second -> first
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: no relation, base class: first -> second, new model class: second -> first
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: no relation, base class: second -> first, new model class: second -> first
first:
Based on the simple and well specified autonomous driving and navigation environment called ' Duckietown ' , <m> AI-DO </m> includes a series of tasks of increasing complexity -- from simple lane-following to fleet management .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: second -> first, new model class: same cluster
first:
An integration architecture tactic to guard <m> AI-first components </m> .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: first -> second, new model class: second -> first
first:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: no relation, base class: first -> second, new model class: second -> first
first:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: no relation, base class: first -> second, new model class: second -> first
first:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: no relation, base class: first -> second, new model class: second -> first
first:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: no relation, base class: first -> second, new model class: second -> first
first:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: no relation, base class: first -> second, new model class: second -> first
first:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .
second:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .


class: no relation, base class: first -> second, new model class: first -> second
first:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .
second:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .


class: no relation, base class: first -> second, new model class: first -> second
first:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .
second:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .
second:
We choose 7 popular conferences including AAAI , CIKM , <m> ICML </m> , KDD , NIPS , SIGIR , and WWW as the classification categories .


class: no relation, base class: first -> second, new model class: first -> second
first:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: no relation, base class: second -> first, new model class: first -> second
first:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: no relation, base class: second -> first, new model class: first -> second
first:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: no relation, base class: second -> first, new model class: first -> second
first:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: no relation, base class: second -> first, new model class: first -> second
first:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: no relation, base class: second -> first, new model class: first -> second
first:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: no relation, base class: second -> first, new model class: first -> second
first:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: no relation, base class: second -> first, new model class: first -> second
first:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: no relation, base class: second -> first, new model class: first -> second
first:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: no relation, base class: second -> first, new model class: first -> second
first:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: no relation, base class: first -> second, new model class: same cluster
first:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .
second:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .


class: no relation, base class: same cluster, new model class: first -> second
first:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second, new model class: first -> second
first:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .
second:
subsubsection : Comparison of Metric Learning Algorithms We evaluated the proposed XQDA algorithm and several metric learning algorithms , including Euclidean distance , Mahalanobis distance trained with genuine pairs , LMNN v2.5 , <m> ITML </m> , KISSME , and RLDA , with the same LOMO feature .


class: no relation, base class: first -> second, new model class: same cluster
first:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: no relation, base class: second -> first, new model class: first -> second
first:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: same cluster, new model class: same cluster
first:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: no relation, base class: second -> first, new model class: second -> first
first:
To reduce the complexity of the joint grid search in ML algorithm , a Modified <m> ML ( MML ) algorithm </m> with multiple one-dimensional searches is also proposed .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: no relation, base class: second -> first, new model class: second -> first
first:
To reduce the complexity of the joint grid search in ML algorithm , a Modified <m> ML ( MML ) algorithm </m> with multiple one-dimensional searches is also proposed .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: no relation, base class: same cluster, new model class: second -> first
first:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: no relation, base class: first -> second, new model class: second -> first
first:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
In other words , can one design a <m> filter </m> which , when given a query to any item I in the dataset , returns a sound item J that , although not necessarily in the dataset , differs from I as infrequently as possible .
second:
In the proposed work segmentation using K-means clustering of feature vectors and denoising techniques such as Gaussian noise method , <m> Weiner filter methods </m> are used to improve the resolution of the images .


class: no relation, base class: second -> first, new model class: second -> first
first:
In the feature extraction , iris feature information is extracted by the method of <m> 2D Log-Gabo filter </m> which can generate multi-channel filter though multi-scale and multiple directions , the method also can reduce the length of the iris code and the time of feature matching , improve the feature matching accuracy , can effectively improve the accuracy of iris recognition .
second:
It first depicts the performance of the proposed models on different length of sentences , and then conducts a sensitivity analysis of <m> 2D filter </m> and max pooling size .


class: no relation, base class: second -> first, new model class: second -> first
first:
By solving the proposed conditions , a desired <m> l2 - l∞ filter </m> can be constructed for any admissible random mismatch .
second:
The proposal step also serves as a <m> filter </m> whose goal is to preserve the object boxes with high recall rate , while removing the easy negatives .


class: no relation, base class: first -> second, new model class: first -> second
first:
Behind the API it consists of a <m> kernel </m> implementing a reference monitor which controls access to security-relevant objects and attributes based on a configurable security policy .
second:
The <m> reproducing kernel particle method </m> is an efficient mesh free technique for the numerical solution of partial differential equations .


class: no relation, base class: first -> second, new model class: first -> second
first:
Behind the API it consists of a <m> kernel </m> implementing a reference monitor which controls access to security-relevant objects and attributes based on a configurable security policy .
second:
The candidate models belong to the Neyman-Scott Process ( NSP ) and results show that the NSP with <m> variance gamma kernel </m> is the most accurate for matching the microcells distribution for different scenarios .


class: no relation, base class: same cluster, new model class: first -> second
first:
Behind the API it consists of a <m> kernel </m> implementing a reference monitor which controls access to security-relevant objects and attributes based on a configurable security policy .
second:
However , the approach used fixed convolutional features , and can not jointly learn the <m> kernel classifier </m> and convolutional filters .


class: no relation, base class: same cluster, new model class: same cluster
first:
Behind the API it consists of a <m> kernel </m> implementing a reference monitor which controls access to security-relevant objects and attributes based on a configurable security policy .
second:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .


class: no relation, base class: same cluster, new model class: same cluster
first:
Behind the API it consists of a <m> kernel </m> implementing a reference monitor which controls access to security-relevant objects and attributes based on a configurable security policy .
second:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .


class: no relation, base class: same cluster, new model class: same cluster
first:
Behind the API it consists of a <m> kernel </m> implementing a reference monitor which controls access to security-relevant objects and attributes based on a configurable security policy .
second:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .


class: no relation, base class: second -> first, new model class: second -> first
first:
The <m> reproducing kernel particle method </m> is an efficient mesh free technique for the numerical solution of partial differential equations .
second:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .


class: no relation, base class: second -> first, new model class: second -> first
first:
The <m> reproducing kernel particle method </m> is an efficient mesh free technique for the numerical solution of partial differential equations .
second:
This thesis addresses TCP/IP 's application to wireless and secure area to discuss , it analyzes several problems about a few of critical parts within TCP/IP in details , simulaneously delves into slow start algorithm , initial window size , increasing policy of ini- tial window , RTO estimator , delay acknowledgement , deficiency of <m> kernel implementation </m> and security leakage and gives some constructive solutions in order to further research to pave a basis .


class: no relation, base class: second -> first, new model class: second -> first
first:
The <m> reproducing kernel particle method </m> is an efficient mesh free technique for the numerical solution of partial differential equations .
second:
The <m> kernel-level implementation framework </m> of VMDFS is illustrated based on Linux 2.4.22 .


class: no relation, base class: second -> first, new model class: second -> first
first:
The <m> reproducing kernel particle method </m> is an efficient mesh free technique for the numerical solution of partial differential equations .
second:
From the implementation perspective , we show that using application level measurements is highly CPU intensive , while a <m> kernel based implementation </m> has comparably a very low CPU usage .


class: no relation, base class: second -> first, new model class: second -> first
first:
The <m> reproducing kernel particle method </m> is an efficient mesh free technique for the numerical solution of partial differential equations .
second:
This paper describes a <m> kernel-mode implementation </m> and compares two different design alternatives .


class: no relation, base class: second -> first, new model class: second -> first
first:
The <m> reproducing kernel particle method </m> is an efficient mesh free technique for the numerical solution of partial differential equations .
second:
The principle and <m> kernel implementing technology </m> of Sybase web solution are introduced in detail .


class: no relation, base class: second -> first, new model class: second -> first
first:
The <m> reproducing kernel particle method </m> is an efficient mesh free technique for the numerical solution of partial differential equations .
second:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .


class: no relation, base class: second -> first, new model class: second -> first
first:
The <m> reproducing kernel particle method </m> is an efficient mesh free technique for the numerical solution of partial differential equations .
second:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .


class: no relation, base class: second -> first, new model class: second -> first
first:
The candidate models belong to the Neyman-Scott Process ( NSP ) and results show that the NSP with <m> variance gamma kernel </m> is the most accurate for matching the microcells distribution for different scenarios .
second:
This thesis addresses TCP/IP 's application to wireless and secure area to discuss , it analyzes several problems about a few of critical parts within TCP/IP in details , simulaneously delves into slow start algorithm , initial window size , increasing policy of ini- tial window , RTO estimator , delay acknowledgement , deficiency of <m> kernel implementation </m> and security leakage and gives some constructive solutions in order to further research to pave a basis .


class: no relation, base class: second -> first, new model class: second -> first
first:
The candidate models belong to the Neyman-Scott Process ( NSP ) and results show that the NSP with <m> variance gamma kernel </m> is the most accurate for matching the microcells distribution for different scenarios .
second:
The <m> kernel-level implementation framework </m> of VMDFS is illustrated based on Linux 2.4.22 .


class: no relation, base class: second -> first, new model class: second -> first
first:
The candidate models belong to the Neyman-Scott Process ( NSP ) and results show that the NSP with <m> variance gamma kernel </m> is the most accurate for matching the microcells distribution for different scenarios .
second:
From the implementation perspective , we show that using application level measurements is highly CPU intensive , while a <m> kernel based implementation </m> has comparably a very low CPU usage .


class: no relation, base class: second -> first, new model class: second -> first
first:
The candidate models belong to the Neyman-Scott Process ( NSP ) and results show that the NSP with <m> variance gamma kernel </m> is the most accurate for matching the microcells distribution for different scenarios .
second:
This paper describes a <m> kernel-mode implementation </m> and compares two different design alternatives .


class: no relation, base class: second -> first, new model class: second -> first
first:
The candidate models belong to the Neyman-Scott Process ( NSP ) and results show that the NSP with <m> variance gamma kernel </m> is the most accurate for matching the microcells distribution for different scenarios .
second:
The principle and <m> kernel implementing technology </m> of Sybase web solution are introduced in detail .


class: no relation, base class: second -> first, new model class: second -> first
first:
<m> Linear-vertex kernel </m> .
second:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .


class: no relation, base class: second -> first, new model class: second -> first
first:
<m> Linear-vertex kernel </m> .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: no relation, base class: same cluster, new model class: first -> second
first:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .
second:
For example , defines a <m> kernel with dilation </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .
second:
Assume an <m> isotropic stationary kernel </m> ( ) is used in MMD .


class: no relation, base class: same cluster, new model class: same cluster
first:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: no relation, base class: same cluster, new model class: second -> first
first:
However , the approach used fixed convolutional features , and can not jointly learn the <m> kernel classifier </m> and convolutional filters .
second:
This thesis addresses TCP/IP 's application to wireless and secure area to discuss , it analyzes several problems about a few of critical parts within TCP/IP in details , simulaneously delves into slow start algorithm , initial window size , increasing policy of ini- tial window , RTO estimator , delay acknowledgement , deficiency of <m> kernel implementation </m> and security leakage and gives some constructive solutions in order to further research to pave a basis .


class: no relation, base class: same cluster, new model class: second -> first
first:
However , the approach used fixed convolutional features , and can not jointly learn the <m> kernel classifier </m> and convolutional filters .
second:
The <m> kernel-level implementation framework </m> of VMDFS is illustrated based on Linux 2.4.22 .


class: no relation, base class: same cluster, new model class: second -> first
first:
However , the approach used fixed convolutional features , and can not jointly learn the <m> kernel classifier </m> and convolutional filters .
second:
From the implementation perspective , we show that using application level measurements is highly CPU intensive , while a <m> kernel based implementation </m> has comparably a very low CPU usage .


class: no relation, base class: same cluster, new model class: second -> first
first:
However , the approach used fixed convolutional features , and can not jointly learn the <m> kernel classifier </m> and convolutional filters .
second:
This paper describes a <m> kernel-mode implementation </m> and compares two different design alternatives .


class: no relation, base class: same cluster, new model class: second -> first
first:
However , the approach used fixed convolutional features , and can not jointly learn the <m> kernel classifier </m> and convolutional filters .
second:
The principle and <m> kernel implementing technology </m> of Sybase web solution are introduced in detail .


class: no relation, base class: same cluster, new model class: same cluster
first:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .
second:
This thesis addresses TCP/IP 's application to wireless and secure area to discuss , it analyzes several problems about a few of critical parts within TCP/IP in details , simulaneously delves into slow start algorithm , initial window size , increasing policy of ini- tial window , RTO estimator , delay acknowledgement , deficiency of <m> kernel implementation </m> and security leakage and gives some constructive solutions in order to further research to pave a basis .


class: no relation, base class: same cluster, new model class: same cluster
first:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .
second:
The <m> kernel-level implementation framework </m> of VMDFS is illustrated based on Linux 2.4.22 .


class: no relation, base class: same cluster, new model class: same cluster
first:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .
second:
From the implementation perspective , we show that using application level measurements is highly CPU intensive , while a <m> kernel based implementation </m> has comparably a very low CPU usage .


class: no relation, base class: same cluster, new model class: same cluster
first:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .
second:
This paper describes a <m> kernel-mode implementation </m> and compares two different design alternatives .


class: no relation, base class: same cluster, new model class: same cluster
first:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .
second:
The principle and <m> kernel implementing technology </m> of Sybase web solution are introduced in detail .


class: no relation, base class: second -> first, new model class: second -> first
first:
Conventional methods include asymmetric metric learning , subspace interpolation , <m> geodesic flow kernel </m> , subspace alignment , covariance matrix alignment , .
second:
It has been demonstrated that many distance metric learning or discriminative subspace based methods for person re - i d benefit from <m> kernelisation </m> because of the non - linearity in person ’s appearance .


class: no relation, base class: second -> first, new model class: second -> first
first:
Assume an <m> isotropic stationary kernel </m> ( ) is used in MMD .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: no relation, base class: same cluster, new model class: same cluster
first:
This thesis addresses TCP/IP 's application to wireless and secure area to discuss , it analyzes several problems about a few of critical parts within TCP/IP in details , simulaneously delves into slow start algorithm , initial window size , increasing policy of ini- tial window , RTO estimator , delay acknowledgement , deficiency of <m> kernel implementation </m> and security leakage and gives some constructive solutions in order to further research to pave a basis .
second:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .


class: no relation, base class: same cluster, new model class: same cluster
first:
This thesis addresses TCP/IP 's application to wireless and secure area to discuss , it analyzes several problems about a few of critical parts within TCP/IP in details , simulaneously delves into slow start algorithm , initial window size , increasing policy of ini- tial window , RTO estimator , delay acknowledgement , deficiency of <m> kernel implementation </m> and security leakage and gives some constructive solutions in order to further research to pave a basis .
second:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .


class: no relation, base class: same cluster, new model class: same cluster
first:
The <m> kernel-level implementation framework </m> of VMDFS is illustrated based on Linux 2.4.22 .
second:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .


class: no relation, base class: same cluster, new model class: same cluster
first:
The <m> kernel-level implementation framework </m> of VMDFS is illustrated based on Linux 2.4.22 .
second:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .


class: no relation, base class: same cluster, new model class: same cluster
first:
From the implementation perspective , we show that using application level measurements is highly CPU intensive , while a <m> kernel based implementation </m> has comparably a very low CPU usage .
second:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .


class: no relation, base class: same cluster, new model class: same cluster
first:
From the implementation perspective , we show that using application level measurements is highly CPU intensive , while a <m> kernel based implementation </m> has comparably a very low CPU usage .
second:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .


class: no relation, base class: same cluster, new model class: same cluster
first:
This paper describes a <m> kernel-mode implementation </m> and compares two different design alternatives .
second:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .


class: no relation, base class: same cluster, new model class: same cluster
first:
This paper describes a <m> kernel-mode implementation </m> and compares two different design alternatives .
second:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .


class: no relation, base class: same cluster, new model class: same cluster
first:
The principle and <m> kernel implementing technology </m> of Sybase web solution are introduced in detail .
second:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .


class: no relation, base class: same cluster, new model class: same cluster
first:
The principle and <m> kernel implementing technology </m> of Sybase web solution are introduced in detail .
second:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .


class: no relation, base class: first -> second, new model class: first -> second
first:
Moreover , since it does not need <m> word segmentation </m> , it can be applied to Eastern languages where they do not put clear spacing between words .
second:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
Moreover , since it does not need <m> word segmentation </m> , it can be applied to Eastern languages where they do not put clear spacing between words .
second:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .


class: no relation, base class: first -> second, new model class: first -> second
first:
Moreover , since it does not need <m> word segmentation </m> , it can be applied to Eastern languages where they do not put clear spacing between words .
second:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .


class: no relation, base class: first -> second, new model class: first -> second
first:
Moreover , since it does not need <m> word segmentation </m> , it can be applied to Eastern languages where they do not put clear spacing between words .
second:
An acoustically-derived lexicon is used to construct word models based upon <m> subword segment models </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
Moreover , since it does not need <m> word segmentation </m> , it can be applied to Eastern languages where they do not put clear spacing between words .
second:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .


class: no relation, base class: first -> second, new model class: first -> second
first:
Moreover , since it does not need <m> word segmentation </m> , it can be applied to Eastern languages where they do not put clear spacing between words .
second:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .


class: no relation, base class: first -> second, new model class: first -> second
first:
Moreover , since it does not need <m> word segmentation </m> , it can be applied to Eastern languages where they do not put clear spacing between words .
second:
The parallelism of the convolutional , pooling , and highway layers allows training speed comparable to <m> subword - level models </m> without hard - coded text segmentation .


class: no relation, base class: first -> second, new model class: first -> second
first:
Moreover , since it does not need <m> word segmentation </m> , it can be applied to Eastern languages where they do not put clear spacing between words .
second:
The beam widths are and respectively for the <m> subword - level </m> and character - level decoders .


class: no relation, base class: first -> second, new model class: first -> second
first:
Moreover , since it does not need <m> word segmentation </m> , it can be applied to Eastern languages where they do not put clear spacing between words .
second:
Neural Machine Translation of Rare Words with <m> Subword Units </m> section :


class: no relation, base class: first -> second, new model class: first -> second
first:
Moreover , since it does not need <m> word segmentation </m> , it can be applied to Eastern languages where they do not put clear spacing between words .
second:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .


class: no relation, base class: same cluster, new model class: same cluster
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
An acoustically-derived lexicon is used to construct word models based upon <m> subword segment models </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .


class: no relation, base class: same cluster, new model class: same cluster
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
The parallelism of the convolutional , pooling , and highway layers allows training speed comparable to <m> subword - level models </m> without hard - coded text segmentation .


class: no relation, base class: second -> first, new model class: second -> first
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
Therefore , in this paper , we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit <m> word segmentation </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
This <m> word segmentation procedure </m> can be as simple as tokenization followed by some punctuation normalization , but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance .


class: no relation, base class: second -> first, new model class: second -> first
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
paragraph : Why not Word - Level Translation ? The most pressing issue with word - level processing is that we do not have a perfect <m> word segmentation algorithm </m> for any one language .


class: no relation, base class: same cluster, new model class: same cluster
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
The beam widths are and respectively for the <m> subword - level </m> and character - level decoders .


class: no relation, base class: same cluster, new model class: same cluster
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
Neural Machine Translation of Rare Words with <m> Subword Units </m> section :


class: no relation, base class: second -> first, new model class: second -> first
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
We discuss the suitability of different <m> word segmentation techniques </m> , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: no relation, base class: same cluster, new model class: same cluster
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
An acoustically-derived lexicon is used to construct word models based upon <m> subword segment models </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .


class: no relation, base class: same cluster, new model class: same cluster
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
The parallelism of the convolutional , pooling , and highway layers allows training speed comparable to <m> subword - level models </m> without hard - coded text segmentation .


class: no relation, base class: second -> first, new model class: second -> first
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
Therefore , in this paper , we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit <m> word segmentation </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
This <m> word segmentation procedure </m> can be as simple as tokenization followed by some punctuation normalization , but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance .


class: no relation, base class: second -> first, new model class: second -> first
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
paragraph : Why not Word - Level Translation ? The most pressing issue with word - level processing is that we do not have a perfect <m> word segmentation algorithm </m> for any one language .


class: no relation, base class: same cluster, new model class: same cluster
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
The beam widths are and respectively for the <m> subword - level </m> and character - level decoders .


class: no relation, base class: same cluster, new model class: same cluster
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
Neural Machine Translation of Rare Words with <m> Subword Units </m> section :


class: no relation, base class: second -> first, new model class: second -> first
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
We discuss the suitability of different <m> word segmentation techniques </m> , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: no relation, base class: same cluster, new model class: same cluster
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
An acoustically-derived lexicon is used to construct word models based upon <m> subword segment models </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .


class: no relation, base class: same cluster, new model class: same cluster
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
The parallelism of the convolutional , pooling , and highway layers allows training speed comparable to <m> subword - level models </m> without hard - coded text segmentation .


class: no relation, base class: second -> first, new model class: second -> first
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
Therefore , in this paper , we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit <m> word segmentation </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
This <m> word segmentation procedure </m> can be as simple as tokenization followed by some punctuation normalization , but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance .


class: no relation, base class: second -> first, new model class: second -> first
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
paragraph : Why not Word - Level Translation ? The most pressing issue with word - level processing is that we do not have a perfect <m> word segmentation algorithm </m> for any one language .


class: no relation, base class: same cluster, new model class: same cluster
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
The beam widths are and respectively for the <m> subword - level </m> and character - level decoders .


class: no relation, base class: same cluster, new model class: same cluster
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
Neural Machine Translation of Rare Words with <m> Subword Units </m> section :


class: no relation, base class: second -> first, new model class: second -> first
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
We discuss the suitability of different <m> word segmentation techniques </m> , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: no relation, base class: same cluster, new model class: same cluster
first:
An acoustically-derived lexicon is used to construct word models based upon <m> subword segment models </m> .
second:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .


class: no relation, base class: same cluster, new model class: same cluster
first:
An acoustically-derived lexicon is used to construct word models based upon <m> subword segment models </m> .
second:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .


class: no relation, base class: same cluster, new model class: same cluster
first:
An acoustically-derived lexicon is used to construct word models based upon <m> subword segment models </m> .
second:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .


class: no relation, base class: same cluster, new model class: same cluster
first:
An acoustically-derived lexicon is used to construct word models based upon <m> subword segment models </m> .
second:
The parallelism of the convolutional , pooling , and highway layers allows training speed comparable to <m> subword - level models </m> without hard - coded text segmentation .


class: no relation, base class: second -> first, new model class: second -> first
first:
An acoustically-derived lexicon is used to construct word models based upon <m> subword segment models </m> .
second:
Therefore , in this paper , we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit <m> word segmentation </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
An acoustically-derived lexicon is used to construct word models based upon <m> subword segment models </m> .
second:
This <m> word segmentation procedure </m> can be as simple as tokenization followed by some punctuation normalization , but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance .


class: no relation, base class: second -> first, new model class: second -> first
first:
An acoustically-derived lexicon is used to construct word models based upon <m> subword segment models </m> .
second:
paragraph : Why not Word - Level Translation ? The most pressing issue with word - level processing is that we do not have a perfect <m> word segmentation algorithm </m> for any one language .


class: no relation, base class: same cluster, new model class: same cluster
first:
An acoustically-derived lexicon is used to construct word models based upon <m> subword segment models </m> .
second:
The beam widths are and respectively for the <m> subword - level </m> and character - level decoders .


class: no relation, base class: same cluster, new model class: same cluster
first:
An acoustically-derived lexicon is used to construct word models based upon <m> subword segment models </m> .
second:
Neural Machine Translation of Rare Words with <m> Subword Units </m> section :


class: no relation, base class: second -> first, new model class: second -> first
first:
An acoustically-derived lexicon is used to construct word models based upon <m> subword segment models </m> .
second:
We discuss the suitability of different <m> word segmentation techniques </m> , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: no relation, base class: same cluster, new model class: same cluster
first:
An acoustically-derived lexicon is used to construct word models based upon <m> subword segment models </m> .
second:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .


class: no relation, base class: same cluster, new model class: same cluster
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .


class: no relation, base class: same cluster, new model class: same cluster
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
The parallelism of the convolutional , pooling , and highway layers allows training speed comparable to <m> subword - level models </m> without hard - coded text segmentation .


class: no relation, base class: second -> first, new model class: second -> first
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
Therefore , in this paper , we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit <m> word segmentation </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
This <m> word segmentation procedure </m> can be as simple as tokenization followed by some punctuation normalization , but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance .


class: no relation, base class: second -> first, new model class: second -> first
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
paragraph : Why not Word - Level Translation ? The most pressing issue with word - level processing is that we do not have a perfect <m> word segmentation algorithm </m> for any one language .


class: no relation, base class: same cluster, new model class: same cluster
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
The beam widths are and respectively for the <m> subword - level </m> and character - level decoders .


class: no relation, base class: same cluster, new model class: same cluster
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
Neural Machine Translation of Rare Words with <m> Subword Units </m> section :


class: no relation, base class: second -> first, new model class: second -> first
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
We discuss the suitability of different <m> word segmentation techniques </m> , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: no relation, base class: same cluster, new model class: same cluster
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .


class: no relation, base class: same cluster, new model class: same cluster
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
The parallelism of the convolutional , pooling , and highway layers allows training speed comparable to <m> subword - level models </m> without hard - coded text segmentation .


class: no relation, base class: second -> first, new model class: second -> first
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
Therefore , in this paper , we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit <m> word segmentation </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
This <m> word segmentation procedure </m> can be as simple as tokenization followed by some punctuation normalization , but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance .


class: no relation, base class: second -> first, new model class: second -> first
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
paragraph : Why not Word - Level Translation ? The most pressing issue with word - level processing is that we do not have a perfect <m> word segmentation algorithm </m> for any one language .


class: no relation, base class: same cluster, new model class: same cluster
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
The beam widths are and respectively for the <m> subword - level </m> and character - level decoders .


class: no relation, base class: same cluster, new model class: same cluster
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
Neural Machine Translation of Rare Words with <m> Subword Units </m> section :


class: no relation, base class: second -> first, new model class: second -> first
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
We discuss the suitability of different <m> word segmentation techniques </m> , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: no relation, base class: same cluster, new model class: same cluster
first:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .
second:
The parallelism of the convolutional , pooling , and highway layers allows training speed comparable to <m> subword - level models </m> without hard - coded text segmentation .


class: no relation, base class: same cluster, new model class: same cluster
first:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .
second:
The beam widths are and respectively for the <m> subword - level </m> and character - level decoders .


class: no relation, base class: same cluster, new model class: same cluster
first:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .
second:
Neural Machine Translation of Rare Words with <m> Subword Units </m> section :


class: no relation, base class: same cluster, new model class: same cluster
first:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .
second:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .


class: no relation, base class: second -> first, new model class: second -> first
first:
The parallelism of the convolutional , pooling , and highway layers allows training speed comparable to <m> subword - level models </m> without hard - coded text segmentation .
second:
Therefore , in this paper , we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit <m> word segmentation </m> .


class: no relation, base class: second -> first, new model class: second -> first
first:
The parallelism of the convolutional , pooling , and highway layers allows training speed comparable to <m> subword - level models </m> without hard - coded text segmentation .
second:
This <m> word segmentation procedure </m> can be as simple as tokenization followed by some punctuation normalization , but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance .


class: no relation, base class: second -> first, new model class: second -> first
first:
The parallelism of the convolutional , pooling , and highway layers allows training speed comparable to <m> subword - level models </m> without hard - coded text segmentation .
second:
paragraph : Why not Word - Level Translation ? The most pressing issue with word - level processing is that we do not have a perfect <m> word segmentation algorithm </m> for any one language .


class: no relation, base class: second -> first, new model class: second -> first
first:
The parallelism of the convolutional , pooling , and highway layers allows training speed comparable to <m> subword - level models </m> without hard - coded text segmentation .
second:
We discuss the suitability of different <m> word segmentation techniques </m> , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: no relation, base class: same cluster, new model class: same cluster
first:
The parallelism of the convolutional , pooling , and highway layers allows training speed comparable to <m> subword - level models </m> without hard - coded text segmentation .
second:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .


class: no relation, base class: first -> second, new model class: first -> second
first:
Therefore , in this paper , we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit <m> word segmentation </m> .
second:
The beam widths are and respectively for the <m> subword - level </m> and character - level decoders .


class: no relation, base class: first -> second, new model class: first -> second
first:
Therefore , in this paper , we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit <m> word segmentation </m> .
second:
Neural Machine Translation of Rare Words with <m> Subword Units </m> section :


class: no relation, base class: first -> second, new model class: first -> second
first:
Therefore , in this paper , we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit <m> word segmentation </m> .
second:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .


class: no relation, base class: first -> second, new model class: first -> second
first:
This <m> word segmentation procedure </m> can be as simple as tokenization followed by some punctuation normalization , but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance .
second:
The beam widths are and respectively for the <m> subword - level </m> and character - level decoders .


class: no relation, base class: first -> second, new model class: first -> second
first:
This <m> word segmentation procedure </m> can be as simple as tokenization followed by some punctuation normalization , but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance .
second:
Neural Machine Translation of Rare Words with <m> Subword Units </m> section :


class: no relation, base class: first -> second, new model class: first -> second
first:
This <m> word segmentation procedure </m> can be as simple as tokenization followed by some punctuation normalization , but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance .
second:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .


class: no relation, base class: first -> second, new model class: first -> second
first:
paragraph : Why not Word - Level Translation ? The most pressing issue with word - level processing is that we do not have a perfect <m> word segmentation algorithm </m> for any one language .
second:
The beam widths are and respectively for the <m> subword - level </m> and character - level decoders .


class: no relation, base class: first -> second, new model class: first -> second
first:
paragraph : Why not Word - Level Translation ? The most pressing issue with word - level processing is that we do not have a perfect <m> word segmentation algorithm </m> for any one language .
second:
Neural Machine Translation of Rare Words with <m> Subword Units </m> section :


class: no relation, base class: first -> second, new model class: first -> second
first:
paragraph : Why not Word - Level Translation ? The most pressing issue with word - level processing is that we do not have a perfect <m> word segmentation algorithm </m> for any one language .
second:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .


class: no relation, base class: second -> first, new model class: second -> first
first:
The beam widths are and respectively for the <m> subword - level </m> and character - level decoders .
second:
We discuss the suitability of different <m> word segmentation techniques </m> , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: no relation, base class: same cluster, new model class: same cluster
first:
The beam widths are and respectively for the <m> subword - level </m> and character - level decoders .
second:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .


class: no relation, base class: second -> first, new model class: second -> first
first:
Neural Machine Translation of Rare Words with <m> Subword Units </m> section :
second:
We discuss the suitability of different <m> word segmentation techniques </m> , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: no relation, base class: same cluster, new model class: same cluster
first:
Neural Machine Translation of Rare Words with <m> Subword Units </m> section :
second:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .


class: no relation, base class: first -> second, new model class: first -> second
first:
We discuss the suitability of different <m> word segmentation techniques </m> , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .
second:
Recall our introductory example Abwasserbehandlungsanlange , for which a <m> subword segmentation </m> avoids the information bottleneck of a fixed - length representation .


class: no relation, base class: same cluster, new model class: same cluster
first:
An extended comparison between CA and Back Propagation Error ( <m> BPE </m> ) are presented and the potential of information induction and reduction from the training set is revised for both methods .
second:
The resultant signal-to-noise ratio ( SNR ) often varies across the reconstructed image when using the <m> BPE technique </m> , and the image SNR depends on the reconstruction method .


class: no relation, base class: second -> first, new model class: second -> first
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
In order to compare orthogonal encoding , five <m> byte encoding </m> , Codon encoding(two ) and Profile encoding to find out their virtues and shortcomings , we carefully studied them with artificial neural network .


class: no relation, base class: second -> first, new model class: second -> first
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
In this paper , we review internal and external representations of the UCS , and propose a <m> byte encoding </m> for the UCS .


class: no relation, base class: second -> first, new model class: second -> first
first:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .
second:
In order to compare orthogonal encoding , five <m> byte encoding </m> , Codon encoding(two ) and Profile encoding to find out their virtues and shortcomings , we carefully studied them with artificial neural network .


class: no relation, base class: second -> first, new model class: second -> first
first:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .
second:
In this paper , we review internal and external representations of the UCS , and propose a <m> byte encoding </m> for the UCS .


class: no relation, base class: second -> first, new model class: second -> first
first:
For English→German , we observe the best BLEU score of 25.3 with C2 - 50k , but the best CHRF3 score of 54.1 with <m> BPE - J90k </m> .
second:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .


class: no relation, base class: second -> first, new model class: second -> first
first:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .
second:
In order to compare orthogonal encoding , five <m> byte encoding </m> , Codon encoding(two ) and Profile encoding to find out their virtues and shortcomings , we carefully studied them with artificial neural network .


class: no relation, base class: second -> first, new model class: second -> first
first:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .
second:
In this paper , we review internal and external representations of the UCS , and propose a <m> byte encoding </m> for the UCS .


class: no relation, base class: second -> first, new model class: second -> first
first:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .
second:
In order to compare orthogonal encoding , five <m> byte encoding </m> , Codon encoding(two ) and Profile encoding to find out their virtues and shortcomings , we carefully studied them with artificial neural network .


class: no relation, base class: second -> first, new model class: second -> first
first:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .
second:
In this paper , we review internal and external representations of the UCS , and propose a <m> byte encoding </m> for the UCS .


class: no relation, base class: second -> first, new model class: second -> first
first:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .
second:
In order to compare orthogonal encoding , five <m> byte encoding </m> , Codon encoding(two ) and Profile encoding to find out their virtues and shortcomings , we carefully studied them with artificial neural network .


class: no relation, base class: second -> first, new model class: second -> first
first:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .
second:
In this paper , we review internal and external representations of the UCS , and propose a <m> byte encoding </m> for the UCS .


class: no relation, base class: same cluster, new model class: same cluster
first:
In order to compare orthogonal encoding , five <m> byte encoding </m> , Codon encoding(two ) and Profile encoding to find out their virtues and shortcomings , we carefully studied them with artificial neural network .
second:
In this paper , we review internal and external representations of the UCS , and propose a <m> byte encoding </m> for the UCS .


class: no relation, base class: first -> second, new model class: first -> second
first:
In order to compare orthogonal encoding , five <m> byte encoding </m> , Codon encoding(two ) and Profile encoding to find out their virtues and shortcomings , we carefully studied them with artificial neural network .
second:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .


class: no relation, base class: first -> second, new model class: first -> second
first:
In order to compare orthogonal encoding , five <m> byte encoding </m> , Codon encoding(two ) and Profile encoding to find out their virtues and shortcomings , we carefully studied them with artificial neural network .
second:
We used the <m> byte pair encoding compression algorithm </m> to build a new vocabulary for the inputs of the classifier .


class: no relation, base class: first -> second, new model class: first -> second
first:
In order to compare orthogonal encoding , five <m> byte encoding </m> , Codon encoding(two ) and Profile encoding to find out their virtues and shortcomings , we carefully studied them with artificial neural network .
second:
We discuss the suitability of different word segmentation techniques , including simple character n-gram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU , respectively .


class: no relation, base class: first -> second, new model class: first -> second
first:
In order to compare orthogonal encoding , five <m> byte encoding </m> , Codon encoding(two ) and Profile encoding to find out their virtues and shortcomings , we carefully studied them with artificial neural network .
second:
In this paper , a try has been made to develop few algorithms for random access text compression based on the <m> byte pair encoding scheme </m> ( Gage , 1997 ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
In order to compare orthogonal encoding , five <m> byte encoding </m> , Codon encoding(two ) and Profile encoding to find out their virtues and shortcomings , we carefully studied them with artificial neural network .
second:
Five algorithms are developed based on this <m> byte pair encoding scheme </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
In order to compare orthogonal encoding , five <m> byte encoding </m> , Codon encoding(two ) and Profile encoding to find out their virtues and shortcomings , we carefully studied them with artificial neural network .
second:
A three-stage training based on three levels of architectural granularity namely , character encoder , <m> byte pair encoding ( BPE ) based encoder </m> , and attention decoder , is proposed .


class: no relation, base class: first -> second, new model class: first -> second
first:
In order to compare orthogonal encoding , five <m> byte encoding </m> , Codon encoding(two ) and Profile encoding to find out their virtues and shortcomings , we carefully studied them with artificial neural network .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: no relation, base class: first -> second, new model class: first -> second
first:
In order to compare orthogonal encoding , five <m> byte encoding </m> , Codon encoding(two ) and Profile encoding to find out their virtues and shortcomings , we carefully studied them with artificial neural network .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: no relation, base class: first -> second, new model class: first -> second
first:
In order to compare orthogonal encoding , five <m> byte encoding </m> , Codon encoding(two ) and Profile encoding to find out their virtues and shortcomings , we carefully studied them with artificial neural network .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: no relation, base class: first -> second, new model class: first -> second
first:
In order to compare orthogonal encoding , five <m> byte encoding </m> , Codon encoding(two ) and Profile encoding to find out their virtues and shortcomings , we carefully studied them with artificial neural network .
second:
We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: no relation, base class: first -> second, new model class: first -> second
first:
In order to compare orthogonal encoding , five <m> byte encoding </m> , Codon encoding(two ) and Profile encoding to find out their virtues and shortcomings , we carefully studied them with artificial neural network .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this paper , we review internal and external representations of the UCS , and propose a <m> byte encoding </m> for the UCS .
second:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this paper , we review internal and external representations of the UCS , and propose a <m> byte encoding </m> for the UCS .
second:
We used the <m> byte pair encoding compression algorithm </m> to build a new vocabulary for the inputs of the classifier .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this paper , we review internal and external representations of the UCS , and propose a <m> byte encoding </m> for the UCS .
second:
We discuss the suitability of different word segmentation techniques , including simple character n-gram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU , respectively .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this paper , we review internal and external representations of the UCS , and propose a <m> byte encoding </m> for the UCS .
second:
In this paper , a try has been made to develop few algorithms for random access text compression based on the <m> byte pair encoding scheme </m> ( Gage , 1997 ) .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this paper , we review internal and external representations of the UCS , and propose a <m> byte encoding </m> for the UCS .
second:
Five algorithms are developed based on this <m> byte pair encoding scheme </m> .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this paper , we review internal and external representations of the UCS , and propose a <m> byte encoding </m> for the UCS .
second:
A three-stage training based on three levels of architectural granularity namely , character encoder , <m> byte pair encoding ( BPE ) based encoder </m> , and attention decoder , is proposed .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this paper , we review internal and external representations of the UCS , and propose a <m> byte encoding </m> for the UCS .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this paper , we review internal and external representations of the UCS , and propose a <m> byte encoding </m> for the UCS .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this paper , we review internal and external representations of the UCS , and propose a <m> byte encoding </m> for the UCS .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this paper , we review internal and external representations of the UCS , and propose a <m> byte encoding </m> for the UCS .
second:
We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: no relation, base class: first -> second, new model class: first -> second
first:
In this paper , we review internal and external representations of the UCS , and propose a <m> byte encoding </m> for the UCS .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
The results show that combining unigrams and PoS-filtered skipgrams leads to a significant improvement in classification scores over the <m> unigram baseline </m> .
second:
To train the <m> unigram classifier </m> , a set of SIFT keypoints are obtained from a small set of ground truth images where the license plates are labeled .


class: no relation, base class: same cluster, new model class: first -> second
first:
The results show that combining unigrams and PoS-filtered skipgrams leads to a significant improvement in classification scores over the <m> unigram baseline </m> .
second:
The model is based on ( Goldwater et al. , 2006 ) <m> unigram word segmentation model </m> and assumes a simple prior distribution over morph length .


class: no relation, base class: same cluster, new model class: same cluster
first:
Traditional approaches used <m> unigram based models </m> for text classification .
second:
To train the <m> unigram classifier </m> , a set of SIFT keypoints are obtained from a small set of ground truth images where the license plates are labeled .


class: no relation, base class: same cluster, new model class: first -> second
first:
Traditional approaches used <m> unigram based models </m> for text classification .
second:
The model is based on ( Goldwater et al. , 2006 ) <m> unigram word segmentation model </m> and assumes a simple prior distribution over morph length .


class: no relation, base class: same cluster, new model class: same cluster
first:
A <m> unigram analysis </m> of our annotated data shows that student learning correlates both with the tutor 's dialogue acts and with the student 's dialogue acts .
second:
To train the <m> unigram classifier </m> , a set of SIFT keypoints are obtained from a small set of ground truth images where the license plates are labeled .


class: no relation, base class: same cluster, new model class: first -> second
first:
A <m> unigram analysis </m> of our annotated data shows that student learning correlates both with the tutor 's dialogue acts and with the student 's dialogue acts .
second:
The model is based on ( Goldwater et al. , 2006 ) <m> unigram word segmentation model </m> and assumes a simple prior distribution over morph length .


class: no relation, base class: same cluster, new model class: same cluster
first:
By augmenting conventional techniques of topic modeling with <m> unigram analysis </m> and community detection , we establish an automated method that generates a comprehensive and meaningful summary of forum conversations over time that also sheds light on patterns of user behavior .
second:
To train the <m> unigram classifier </m> , a set of SIFT keypoints are obtained from a small set of ground truth images where the license plates are labeled .


class: no relation, base class: same cluster, new model class: first -> second
first:
By augmenting conventional techniques of topic modeling with <m> unigram analysis </m> and community detection , we establish an automated method that generates a comprehensive and meaningful summary of forum conversations over time that also sheds light on patterns of user behavior .
second:
The model is based on ( Goldwater et al. , 2006 ) <m> unigram word segmentation model </m> and assumes a simple prior distribution over morph length .


class: no relation, base class: same cluster, new model class: same cluster
first:
To train the <m> unigram classifier </m> , a set of SIFT keypoints are obtained from a small set of ground truth images where the license plates are labeled .
second:
Unbiased tweets were extracted from Twitter related to this specific campaign , and on comparing with manual tagging we were able to achieve 84.47 % accuracy using <m> unigram machine learning approach </m> .


class: no relation, base class: same cluster, new model class: first -> second
first:
To train the <m> unigram classifier </m> , a set of SIFT keypoints are obtained from a small set of ground truth images where the license plates are labeled .
second:
We evaluate the effect of reordering using three simple methods : unigram count ( UC ) , <m> unigram ratio </m> ( UR ) , and first four characters match ( FFCM ) .


class: no relation, base class: same cluster, new model class: first -> second
first:
To train the <m> unigram classifier </m> , a set of SIFT keypoints are obtained from a small set of ground truth images where the license plates are labeled .
second:
The model is based on ( Goldwater et al. , 2006 ) <m> unigram word segmentation model </m> and assumes a simple prior distribution over morph length .


class: no relation, base class: first -> second, new model class: first -> second
first:
To train the <m> unigram classifier </m> , a set of SIFT keypoints are obtained from a small set of ground truth images where the license plates are labeled .
second:
One may also use a <m> unigram distribution </m> which has been shown to work better on some tasks pereyra2017regularize .


class: no relation, base class: same cluster, new model class: same cluster
first:
To train the <m> unigram classifier </m> , a set of SIFT keypoints are obtained from a small set of ground truth images where the license plates are labeled .
second:
We report results in Table [ reference ] , along with ablation studies for removing the cross - bigram features ( leaving only the cross - <m> unigram feature </m> ) and for removing all lexicalized features .


class: no relation, base class: same cluster, new model class: same cluster
first:
To train the <m> unigram classifier </m> , a set of SIFT keypoints are obtained from a small set of ground truth images where the license plates are labeled .
second:
Only the <m> unigram representation </m> is truly open - vocabulary .


class: no relation, base class: same cluster, new model class: first -> second
first:
Unbiased tweets were extracted from Twitter related to this specific campaign , and on comparing with manual tagging we were able to achieve 84.47 % accuracy using <m> unigram machine learning approach </m> .
second:
The model is based on ( Goldwater et al. , 2006 ) <m> unigram word segmentation model </m> and assumes a simple prior distribution over morph length .


class: no relation, base class: same cluster, new model class: second -> first
first:
We evaluate the effect of reordering using three simple methods : unigram count ( UC ) , <m> unigram ratio </m> ( UR ) , and first four characters match ( FFCM ) .
second:
We report results in Table [ reference ] , along with ablation studies for removing the cross - bigram features ( leaving only the cross - <m> unigram feature </m> ) and for removing all lexicalized features .


class: no relation, base class: same cluster, new model class: second -> first
first:
We evaluate the effect of reordering using three simple methods : unigram count ( UC ) , <m> unigram ratio </m> ( UR ) , and first four characters match ( FFCM ) .
second:
Only the <m> unigram representation </m> is truly open - vocabulary .


class: no relation, base class: same cluster, new model class: second -> first
first:
The model is based on ( Goldwater et al. , 2006 ) <m> unigram word segmentation model </m> and assumes a simple prior distribution over morph length .
second:
We report results in Table [ reference ] , along with ablation studies for removing the cross - bigram features ( leaving only the cross - <m> unigram feature </m> ) and for removing all lexicalized features .


class: no relation, base class: same cluster, new model class: second -> first
first:
The model is based on ( Goldwater et al. , 2006 ) <m> unigram word segmentation model </m> and assumes a simple prior distribution over morph length .
second:
Only the <m> unigram representation </m> is truly open - vocabulary .


class: no relation, base class: second -> first, new model class: second -> first
first:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .
second:
These traditional methods adopt a bottom - up strategy and often needs several steps to detect texts ( e.g. , character detection , text line construction and <m> text line classification </m> ) .


class: no relation, base class: same cluster, new model class: same cluster
first:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .
second:
Although uniform scaling is the norm in the <m> multi - label classification literature </m> , single - task performance is significantly better .


class: no relation, base class: same cluster, new model class: same cluster
first:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .
second:
The KKT condition for this Lagrangian yields the desired result as section : B Additional Results on <m> Multi - label Classification </m> In this section , we present the experimental results we did not include in the main text .


class: no relation, base class: same cluster, new model class: same cluster
first:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .
second:
section : <m> Multi - Label Classification </m> To facilitate the comparison between our method and the relevant baselines , we use the exact same datasets and experimental procedure as in [ reference ][ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .
second:
We demonstrate the efficacy of node2vec over existing state - of - the - art techniques on <m> multi - label classification </m> and link prediction in several real - world networks from diverse domains .


class: no relation, base class: second -> first, new model class: same cluster
first:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .
second:
In the image analysis part , we first use supervised learning to predict a set of attributes , based on words commonly found in image captions We solve this as a <m> multi - label classification problem </m> and train a corresponding deep CNN by minimizing an element - wise logistic loss function .


class: no relation, base class: same cluster, new model class: same cluster
first:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .
second:
Although uniform scaling is the norm in the <m> multi - label classification literature </m> , single - task performance is significantly better .


class: no relation, base class: same cluster, new model class: same cluster
first:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .
second:
The KKT condition for this Lagrangian yields the desired result as section : B Additional Results on <m> Multi - label Classification </m> In this section , we present the experimental results we did not include in the main text .


class: no relation, base class: same cluster, new model class: same cluster
first:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .
second:
section : <m> Multi - Label Classification </m> To facilitate the comparison between our method and the relevant baselines , we use the exact same datasets and experimental procedure as in [ reference ][ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .
second:
We demonstrate the efficacy of node2vec over existing state - of - the - art techniques on <m> multi - label classification </m> and link prediction in several real - world networks from diverse domains .


class: no relation, base class: second -> first, new model class: same cluster
first:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .
second:
In the image analysis part , we first use supervised learning to predict a set of attributes , based on words commonly found in image captions We solve this as a <m> multi - label classification problem </m> and train a corresponding deep CNN by minimizing an element - wise logistic loss function .


class: no relation, base class: same cluster, new model class: same cluster
first:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .
second:
Although uniform scaling is the norm in the <m> multi - label classification literature </m> , single - task performance is significantly better .


class: no relation, base class: same cluster, new model class: same cluster
first:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .
second:
The KKT condition for this Lagrangian yields the desired result as section : B Additional Results on <m> Multi - label Classification </m> In this section , we present the experimental results we did not include in the main text .


class: no relation, base class: same cluster, new model class: same cluster
first:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .
second:
section : <m> Multi - Label Classification </m> To facilitate the comparison between our method and the relevant baselines , we use the exact same datasets and experimental procedure as in [ reference ][ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .
second:
We demonstrate the efficacy of node2vec over existing state - of - the - art techniques on <m> multi - label classification </m> and link prediction in several real - world networks from diverse domains .


class: no relation, base class: second -> first, new model class: same cluster
first:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .
second:
In the image analysis part , we first use supervised learning to predict a set of attributes , based on words commonly found in image captions We solve this as a <m> multi - label classification problem </m> and train a corresponding deep CNN by minimizing an element - wise logistic loss function .


class: no relation, base class: same cluster, new model class: same cluster
first:
We examine supervised learning for <m> multi-class , multi-label text classification </m> .
second:
Although uniform scaling is the norm in the <m> multi - label classification literature </m> , single - task performance is significantly better .


class: no relation, base class: same cluster, new model class: same cluster
first:
We examine supervised learning for <m> multi-class , multi-label text classification </m> .
second:
The KKT condition for this Lagrangian yields the desired result as section : B Additional Results on <m> Multi - label Classification </m> In this section , we present the experimental results we did not include in the main text .


class: no relation, base class: same cluster, new model class: same cluster
first:
We examine supervised learning for <m> multi-class , multi-label text classification </m> .
second:
section : <m> Multi - Label Classification </m> To facilitate the comparison between our method and the relevant baselines , we use the exact same datasets and experimental procedure as in [ reference ][ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
We examine supervised learning for <m> multi-class , multi-label text classification </m> .
second:
We demonstrate the efficacy of node2vec over existing state - of - the - art techniques on <m> multi - label classification </m> and link prediction in several real - world networks from diverse domains .


class: no relation, base class: second -> first, new model class: same cluster
first:
We examine supervised learning for <m> multi-class , multi-label text classification </m> .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
We examine supervised learning for <m> multi-class , multi-label text classification </m> .
second:
In the image analysis part , we first use supervised learning to predict a set of attributes , based on words commonly found in image captions We solve this as a <m> multi - label classification problem </m> and train a corresponding deep CNN by minimizing an element - wise logistic loss function .


class: no relation, base class: same cluster, new model class: same cluster
first:
We explore supervised learning for <m> multi-class , multi-label text classification </m> , focusing on real-world settings , where the distribution of labels changes dynamically over time .
second:
Although uniform scaling is the norm in the <m> multi - label classification literature </m> , single - task performance is significantly better .


class: no relation, base class: same cluster, new model class: same cluster
first:
We explore supervised learning for <m> multi-class , multi-label text classification </m> , focusing on real-world settings , where the distribution of labels changes dynamically over time .
second:
The KKT condition for this Lagrangian yields the desired result as section : B Additional Results on <m> Multi - label Classification </m> In this section , we present the experimental results we did not include in the main text .


class: no relation, base class: same cluster, new model class: same cluster
first:
We explore supervised learning for <m> multi-class , multi-label text classification </m> , focusing on real-world settings , where the distribution of labels changes dynamically over time .
second:
section : <m> Multi - Label Classification </m> To facilitate the comparison between our method and the relevant baselines , we use the exact same datasets and experimental procedure as in [ reference ][ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
We explore supervised learning for <m> multi-class , multi-label text classification </m> , focusing on real-world settings , where the distribution of labels changes dynamically over time .
second:
We demonstrate the efficacy of node2vec over existing state - of - the - art techniques on <m> multi - label classification </m> and link prediction in several real - world networks from diverse domains .


class: no relation, base class: second -> first, new model class: same cluster
first:
We explore supervised learning for <m> multi-class , multi-label text classification </m> , focusing on real-world settings , where the distribution of labels changes dynamically over time .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
We explore supervised learning for <m> multi-class , multi-label text classification </m> , focusing on real-world settings , where the distribution of labels changes dynamically over time .
second:
In the image analysis part , we first use supervised learning to predict a set of attributes , based on words commonly found in image captions We solve this as a <m> multi - label classification problem </m> and train a corresponding deep CNN by minimizing an element - wise logistic loss function .


class: no relation, base class: same cluster, new model class: same cluster
first:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .
second:
Although uniform scaling is the norm in the <m> multi - label classification literature </m> , single - task performance is significantly better .


class: no relation, base class: same cluster, new model class: same cluster
first:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .
second:
The KKT condition for this Lagrangian yields the desired result as section : B Additional Results on <m> Multi - label Classification </m> In this section , we present the experimental results we did not include in the main text .


class: no relation, base class: same cluster, new model class: same cluster
first:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .
second:
section : <m> Multi - Label Classification </m> To facilitate the comparison between our method and the relevant baselines , we use the exact same datasets and experimental procedure as in [ reference ][ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .
second:
We demonstrate the efficacy of node2vec over existing state - of - the - art techniques on <m> multi - label classification </m> and link prediction in several real - world networks from diverse domains .


class: no relation, base class: second -> first, new model class: same cluster
first:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .
second:
In the image analysis part , we first use supervised learning to predict a set of attributes , based on words commonly found in image captions We solve this as a <m> multi - label classification problem </m> and train a corresponding deep CNN by minimizing an element - wise logistic loss function .


class: no relation, base class: same cluster, new model class: same cluster
first:
As the input information is in the form of free textual entries , techniques used in <m> multi-class , multi-label text categorization </m> are well suited .
second:
Although uniform scaling is the norm in the <m> multi - label classification literature </m> , single - task performance is significantly better .


class: no relation, base class: same cluster, new model class: same cluster
first:
As the input information is in the form of free textual entries , techniques used in <m> multi-class , multi-label text categorization </m> are well suited .
second:
The KKT condition for this Lagrangian yields the desired result as section : B Additional Results on <m> Multi - label Classification </m> In this section , we present the experimental results we did not include in the main text .


class: no relation, base class: same cluster, new model class: same cluster
first:
As the input information is in the form of free textual entries , techniques used in <m> multi-class , multi-label text categorization </m> are well suited .
second:
section : <m> Multi - Label Classification </m> To facilitate the comparison between our method and the relevant baselines , we use the exact same datasets and experimental procedure as in [ reference ][ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
As the input information is in the form of free textual entries , techniques used in <m> multi-class , multi-label text categorization </m> are well suited .
second:
We demonstrate the efficacy of node2vec over existing state - of - the - art techniques on <m> multi - label classification </m> and link prediction in several real - world networks from diverse domains .


class: no relation, base class: second -> first, new model class: same cluster
first:
As the input information is in the form of free textual entries , techniques used in <m> multi-class , multi-label text categorization </m> are well suited .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
As the input information is in the form of free textual entries , techniques used in <m> multi-class , multi-label text categorization </m> are well suited .
second:
In the image analysis part , we first use supervised learning to predict a set of attributes , based on words commonly found in image captions We solve this as a <m> multi - label classification problem </m> and train a corresponding deep CNN by minimizing an element - wise logistic loss function .


class: no relation, base class: same cluster, new model class: same cluster
first:
In this paper , an artificial neural network with a quasi-Newton updating procedure is presented for <m> multi-label multi-class text classification </m> .
second:
Although uniform scaling is the norm in the <m> multi - label classification literature </m> , single - task performance is significantly better .


class: no relation, base class: same cluster, new model class: same cluster
first:
In this paper , an artificial neural network with a quasi-Newton updating procedure is presented for <m> multi-label multi-class text classification </m> .
second:
The KKT condition for this Lagrangian yields the desired result as section : B Additional Results on <m> Multi - label Classification </m> In this section , we present the experimental results we did not include in the main text .


class: no relation, base class: same cluster, new model class: same cluster
first:
In this paper , an artificial neural network with a quasi-Newton updating procedure is presented for <m> multi-label multi-class text classification </m> .
second:
section : <m> Multi - Label Classification </m> To facilitate the comparison between our method and the relevant baselines , we use the exact same datasets and experimental procedure as in [ reference ][ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
In this paper , an artificial neural network with a quasi-Newton updating procedure is presented for <m> multi-label multi-class text classification </m> .
second:
We demonstrate the efficacy of node2vec over existing state - of - the - art techniques on <m> multi - label classification </m> and link prediction in several real - world networks from diverse domains .


class: no relation, base class: second -> first, new model class: same cluster
first:
In this paper , an artificial neural network with a quasi-Newton updating procedure is presented for <m> multi-label multi-class text classification </m> .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
In this paper , an artificial neural network with a quasi-Newton updating procedure is presented for <m> multi-label multi-class text classification </m> .
second:
In the image analysis part , we first use supervised learning to predict a set of attributes , based on words commonly found in image captions We solve this as a <m> multi - label classification problem </m> and train a corresponding deep CNN by minimizing an element - wise logistic loss function .


class: no relation, base class: same cluster, new model class: same cluster
first:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .
second:
Although uniform scaling is the norm in the <m> multi - label classification literature </m> , single - task performance is significantly better .


class: no relation, base class: same cluster, new model class: same cluster
first:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .
second:
The KKT condition for this Lagrangian yields the desired result as section : B Additional Results on <m> Multi - label Classification </m> In this section , we present the experimental results we did not include in the main text .


class: no relation, base class: same cluster, new model class: same cluster
first:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .
second:
section : <m> Multi - Label Classification </m> To facilitate the comparison between our method and the relevant baselines , we use the exact same datasets and experimental procedure as in [ reference ][ reference ] .


class: no relation, base class: same cluster, new model class: same cluster
first:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .
second:
We demonstrate the efficacy of node2vec over existing state - of - the - art techniques on <m> multi - label classification </m> and link prediction in several real - world networks from diverse domains .


class: no relation, base class: second -> first, new model class: same cluster
first:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .
second:
In the image analysis part , we first use supervised learning to predict a set of attributes , based on words commonly found in image captions We solve this as a <m> multi - label classification problem </m> and train a corresponding deep CNN by minimizing an element - wise logistic loss function .


class: no relation, base class: second -> first, new model class: same cluster
first:
Although uniform scaling is the norm in the <m> multi - label classification literature </m> , single - task performance is significantly better .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
Although uniform scaling is the norm in the <m> multi - label classification literature </m> , single - task performance is significantly better .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
Although uniform scaling is the norm in the <m> multi - label classification literature </m> , single - task performance is significantly better .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: no relation, base class: second -> first, new model class: same cluster
first:
The KKT condition for this Lagrangian yields the desired result as section : B Additional Results on <m> Multi - label Classification </m> In this section , we present the experimental results we did not include in the main text .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
The KKT condition for this Lagrangian yields the desired result as section : B Additional Results on <m> Multi - label Classification </m> In this section , we present the experimental results we did not include in the main text .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
The KKT condition for this Lagrangian yields the desired result as section : B Additional Results on <m> Multi - label Classification </m> In this section , we present the experimental results we did not include in the main text .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: no relation, base class: second -> first, new model class: same cluster
first:
section : <m> Multi - Label Classification </m> To facilitate the comparison between our method and the relevant baselines , we use the exact same datasets and experimental procedure as in [ reference ][ reference ] .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
section : <m> Multi - Label Classification </m> To facilitate the comparison between our method and the relevant baselines , we use the exact same datasets and experimental procedure as in [ reference ][ reference ] .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
section : <m> Multi - Label Classification </m> To facilitate the comparison between our method and the relevant baselines , we use the exact same datasets and experimental procedure as in [ reference ][ reference ] .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: no relation, base class: second -> first, new model class: same cluster
first:
We demonstrate the efficacy of node2vec over existing state - of - the - art techniques on <m> multi - label classification </m> and link prediction in several real - world networks from diverse domains .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
We demonstrate the efficacy of node2vec over existing state - of - the - art techniques on <m> multi - label classification </m> and link prediction in several real - world networks from diverse domains .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: no relation, base class: same cluster, new model class: same cluster
first:
We demonstrate the efficacy of node2vec over existing state - of - the - art techniques on <m> multi - label classification </m> and link prediction in several real - world networks from diverse domains .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: no relation, base class: first -> second, new model class: same cluster
first:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: no relation, base class: first -> second, new model class: same cluster
first:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: no relation, base class: first -> second, new model class: same cluster
first:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .
second:
In the image analysis part , we first use supervised learning to predict a set of attributes , based on words commonly found in image captions We solve this as a <m> multi - label classification problem </m> and train a corresponding deep CNN by minimizing an element - wise logistic loss function .


class: no relation, base class: same cluster, new model class: same cluster
first:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .
second:
In the image analysis part , we first use supervised learning to predict a set of attributes , based on words commonly found in image captions We solve this as a <m> multi - label classification problem </m> and train a corresponding deep CNN by minimizing an element - wise logistic loss function .


class: no relation, base class: same cluster, new model class: same cluster
first:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .
second:
In the image analysis part , we first use supervised learning to predict a set of attributes , based on words commonly found in image captions We solve this as a <m> multi - label classification problem </m> and train a corresponding deep CNN by minimizing an element - wise logistic loss function .


