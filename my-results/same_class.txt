class: same cluster, base class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .


class: same cluster, base class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .


class: same cluster, base class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .


class: same cluster, base class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .


class: same cluster, base class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .


class: same cluster, base class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .


class: same cluster, base class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .


class: same cluster, base class: no relation
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .


class: same cluster, base class: no relation
first:
Two assumptions are made : 1 ) In a new pair of images the deformation is approximately the same size and has only been spatially relocated in the image , and that by a simple <m> affine transformation </m> one can identify the optimal configuration on this new pair of images ; and 2 ) The deformation is of similar size and shape on the original pair of images .
second:
Also in this group , for those vertices associated with large residual errors under <m> affine mapping </m> , we encode their motion effectively using Newtonian motion estimates .


class: same cluster, base class: no relation
first:
Two assumptions are made : 1 ) In a new pair of images the deformation is approximately the same size and has only been spatially relocated in the image , and that by a simple <m> affine transformation </m> one can identify the optimal configuration on this new pair of images ; and 2 ) The deformation is of similar size and shape on the original pair of images .
second:
GCPs derived from each source were used independently for the GCP-based geometric correction of IRS PAN sensor images by using <m> affine mapping function </m> .


class: same cluster, base class: no relation
first:
Two assumptions are made : 1 ) In a new pair of images the deformation is approximately the same size and has only been spatially relocated in the image , and that by a simple <m> affine transformation </m> one can identify the optimal configuration on this new pair of images ; and 2 ) The deformation is of similar size and shape on the original pair of images .
second:
In this paper we introduce a class of constraint logic programs such that their termination can be proved by using <m> affine level mappings </m> .


class: same cluster, base class: no relation
first:
Two assumptions are made : 1 ) In a new pair of images the deformation is approximately the same size and has only been spatially relocated in the image , and that by a simple <m> affine transformation </m> one can identify the optimal configuration on this new pair of images ; and 2 ) The deformation is of similar size and shape on the original pair of images .
second:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .


class: same cluster, base class: no relation
first:
Two assumptions are made : 1 ) In a new pair of images the deformation is approximately the same size and has only been spatially relocated in the image , and that by a simple <m> affine transformation </m> one can identify the optimal configuration on this new pair of images ; and 2 ) The deformation is of similar size and shape on the original pair of images .
second:
To be specific , we propose <m> affine mapping based schemes </m> for the problem transformation and outsourcing so that the cloud is unable to learn any key information from the transformed problem .


class: same cluster, base class: no relation
first:
Sum of Square Difference ( SSD ) and Bilinear interpolation models were used to establish the similarity measure between the images to be registered , resampling of the pixel-values and computation of non-integer coordinates respectively while Random Sampling Consensus ( RANSAC ) algorithm was used to exclude the outliers and to compute the transformation matrix using <m> affine transformation function </m> .
second:
Also in this group , for those vertices associated with large residual errors under <m> affine mapping </m> , we encode their motion effectively using Newtonian motion estimates .


class: same cluster, base class: no relation
first:
Sum of Square Difference ( SSD ) and Bilinear interpolation models were used to establish the similarity measure between the images to be registered , resampling of the pixel-values and computation of non-integer coordinates respectively while Random Sampling Consensus ( RANSAC ) algorithm was used to exclude the outliers and to compute the transformation matrix using <m> affine transformation function </m> .
second:
GCPs derived from each source were used independently for the GCP-based geometric correction of IRS PAN sensor images by using <m> affine mapping function </m> .


class: same cluster, base class: no relation
first:
Sum of Square Difference ( SSD ) and Bilinear interpolation models were used to establish the similarity measure between the images to be registered , resampling of the pixel-values and computation of non-integer coordinates respectively while Random Sampling Consensus ( RANSAC ) algorithm was used to exclude the outliers and to compute the transformation matrix using <m> affine transformation function </m> .
second:
In this paper we introduce a class of constraint logic programs such that their termination can be proved by using <m> affine level mappings </m> .


class: same cluster, base class: no relation
first:
Sum of Square Difference ( SSD ) and Bilinear interpolation models were used to establish the similarity measure between the images to be registered , resampling of the pixel-values and computation of non-integer coordinates respectively while Random Sampling Consensus ( RANSAC ) algorithm was used to exclude the outliers and to compute the transformation matrix using <m> affine transformation function </m> .
second:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .


class: same cluster, base class: no relation
first:
Sum of Square Difference ( SSD ) and Bilinear interpolation models were used to establish the similarity measure between the images to be registered , resampling of the pixel-values and computation of non-integer coordinates respectively while Random Sampling Consensus ( RANSAC ) algorithm was used to exclude the outliers and to compute the transformation matrix using <m> affine transformation function </m> .
second:
To be specific , we propose <m> affine mapping based schemes </m> for the problem transformation and outsourcing so that the cloud is unable to learn any key information from the transformed problem .


class: same cluster, base class: no relation
first:
After that , the pattern elements are extracted with the <m> affine transformation theory </m> and bilinear interpolation algorithm .
second:
Also in this group , for those vertices associated with large residual errors under <m> affine mapping </m> , we encode their motion effectively using Newtonian motion estimates .


class: same cluster, base class: no relation
first:
After that , the pattern elements are extracted with the <m> affine transformation theory </m> and bilinear interpolation algorithm .
second:
GCPs derived from each source were used independently for the GCP-based geometric correction of IRS PAN sensor images by using <m> affine mapping function </m> .


class: same cluster, base class: no relation
first:
After that , the pattern elements are extracted with the <m> affine transformation theory </m> and bilinear interpolation algorithm .
second:
In this paper we introduce a class of constraint logic programs such that their termination can be proved by using <m> affine level mappings </m> .


class: same cluster, base class: no relation
first:
After that , the pattern elements are extracted with the <m> affine transformation theory </m> and bilinear interpolation algorithm .
second:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .


class: same cluster, base class: no relation
first:
After that , the pattern elements are extracted with the <m> affine transformation theory </m> and bilinear interpolation algorithm .
second:
To be specific , we propose <m> affine mapping based schemes </m> for the problem transformation and outsourcing so that the cloud is unable to learn any key information from the transformed problem .


class: same cluster, base class: no relation
first:
The content includes : dividing an image frame including a target object , determining a target object region , using a feature information classifier to scan the determined target object region , determining central point position information of a region that feature information to be positioned occupies in the target object region , performing <m> affine transformation operation </m> on the determined central point position information , and obtaining initial position information corresponding to the region that the feature information to be positioned occupies in the target object region , thereby improving precision of initial positioning ; performing iteration processing on the obtained initial position information , obtaining position information corresponding to the region that the feature information to be positioned occupies in the target object region , integrating the obtained position information corresponding to the region that the feature information to be positioned occupies in the target object region , and obtaining the feature information of the target object , thereby improving precision of positioning the feature information of the target object .
second:
Also in this group , for those vertices associated with large residual errors under <m> affine mapping </m> , we encode their motion effectively using Newtonian motion estimates .


class: same cluster, base class: no relation
first:
The content includes : dividing an image frame including a target object , determining a target object region , using a feature information classifier to scan the determined target object region , determining central point position information of a region that feature information to be positioned occupies in the target object region , performing <m> affine transformation operation </m> on the determined central point position information , and obtaining initial position information corresponding to the region that the feature information to be positioned occupies in the target object region , thereby improving precision of initial positioning ; performing iteration processing on the obtained initial position information , obtaining position information corresponding to the region that the feature information to be positioned occupies in the target object region , integrating the obtained position information corresponding to the region that the feature information to be positioned occupies in the target object region , and obtaining the feature information of the target object , thereby improving precision of positioning the feature information of the target object .
second:
GCPs derived from each source were used independently for the GCP-based geometric correction of IRS PAN sensor images by using <m> affine mapping function </m> .


class: same cluster, base class: no relation
first:
The content includes : dividing an image frame including a target object , determining a target object region , using a feature information classifier to scan the determined target object region , determining central point position information of a region that feature information to be positioned occupies in the target object region , performing <m> affine transformation operation </m> on the determined central point position information , and obtaining initial position information corresponding to the region that the feature information to be positioned occupies in the target object region , thereby improving precision of initial positioning ; performing iteration processing on the obtained initial position information , obtaining position information corresponding to the region that the feature information to be positioned occupies in the target object region , integrating the obtained position information corresponding to the region that the feature information to be positioned occupies in the target object region , and obtaining the feature information of the target object , thereby improving precision of positioning the feature information of the target object .
second:
In this paper we introduce a class of constraint logic programs such that their termination can be proved by using <m> affine level mappings </m> .


class: same cluster, base class: no relation
first:
The content includes : dividing an image frame including a target object , determining a target object region , using a feature information classifier to scan the determined target object region , determining central point position information of a region that feature information to be positioned occupies in the target object region , performing <m> affine transformation operation </m> on the determined central point position information , and obtaining initial position information corresponding to the region that the feature information to be positioned occupies in the target object region , thereby improving precision of initial positioning ; performing iteration processing on the obtained initial position information , obtaining position information corresponding to the region that the feature information to be positioned occupies in the target object region , integrating the obtained position information corresponding to the region that the feature information to be positioned occupies in the target object region , and obtaining the feature information of the target object , thereby improving precision of positioning the feature information of the target object .
second:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .


class: same cluster, base class: no relation
first:
The content includes : dividing an image frame including a target object , determining a target object region , using a feature information classifier to scan the determined target object region , determining central point position information of a region that feature information to be positioned occupies in the target object region , performing <m> affine transformation operation </m> on the determined central point position information , and obtaining initial position information corresponding to the region that the feature information to be positioned occupies in the target object region , thereby improving precision of initial positioning ; performing iteration processing on the obtained initial position information , obtaining position information corresponding to the region that the feature information to be positioned occupies in the target object region , integrating the obtained position information corresponding to the region that the feature information to be positioned occupies in the target object region , and obtaining the feature information of the target object , thereby improving precision of positioning the feature information of the target object .
second:
To be specific , we propose <m> affine mapping based schemes </m> for the problem transformation and outsourcing so that the cloud is unable to learn any key information from the transformed problem .


class: same cluster, base class: no relation
first:
Specific topics include : Loading images from JAR files Describing shapes and the affine transformation in Java 2D Designing a class library of shapes Designing a component class for interactive drawing Designing a component class for displaying images Printing in Java 2D Implementing pan and zoom Implementing interactive rotate , shear , and flip Manipulating images at the pixel level using the Java 2D API Working with tiles in JAI to render large images Image manipulation in JAI <m> usingaffine transformation </m> , projections , and warping Image analysis in JAI , including edge detection , statistics , and region of interest computation ( ROI ) Remote imaging using RMI and JAI Internet imaging , including the use of Java 2D and JAI with servlets and JSP , the design of Web-based imaging applications , and the Internet Imaging Protocol ( IIP ) Using the new Image I/O framework to read and write images " Putting It All Together " sections help you put vital concepts and techniques into practice with interactive examples using actual applications .
second:
Also in this group , for those vertices associated with large residual errors under <m> affine mapping </m> , we encode their motion effectively using Newtonian motion estimates .


class: same cluster, base class: no relation
first:
Specific topics include : Loading images from JAR files Describing shapes and the affine transformation in Java 2D Designing a class library of shapes Designing a component class for interactive drawing Designing a component class for displaying images Printing in Java 2D Implementing pan and zoom Implementing interactive rotate , shear , and flip Manipulating images at the pixel level using the Java 2D API Working with tiles in JAI to render large images Image manipulation in JAI <m> usingaffine transformation </m> , projections , and warping Image analysis in JAI , including edge detection , statistics , and region of interest computation ( ROI ) Remote imaging using RMI and JAI Internet imaging , including the use of Java 2D and JAI with servlets and JSP , the design of Web-based imaging applications , and the Internet Imaging Protocol ( IIP ) Using the new Image I/O framework to read and write images " Putting It All Together " sections help you put vital concepts and techniques into practice with interactive examples using actual applications .
second:
GCPs derived from each source were used independently for the GCP-based geometric correction of IRS PAN sensor images by using <m> affine mapping function </m> .


class: same cluster, base class: no relation
first:
Specific topics include : Loading images from JAR files Describing shapes and the affine transformation in Java 2D Designing a class library of shapes Designing a component class for interactive drawing Designing a component class for displaying images Printing in Java 2D Implementing pan and zoom Implementing interactive rotate , shear , and flip Manipulating images at the pixel level using the Java 2D API Working with tiles in JAI to render large images Image manipulation in JAI <m> usingaffine transformation </m> , projections , and warping Image analysis in JAI , including edge detection , statistics , and region of interest computation ( ROI ) Remote imaging using RMI and JAI Internet imaging , including the use of Java 2D and JAI with servlets and JSP , the design of Web-based imaging applications , and the Internet Imaging Protocol ( IIP ) Using the new Image I/O framework to read and write images " Putting It All Together " sections help you put vital concepts and techniques into practice with interactive examples using actual applications .
second:
In this paper we introduce a class of constraint logic programs such that their termination can be proved by using <m> affine level mappings </m> .


class: same cluster, base class: no relation
first:
Specific topics include : Loading images from JAR files Describing shapes and the affine transformation in Java 2D Designing a class library of shapes Designing a component class for interactive drawing Designing a component class for displaying images Printing in Java 2D Implementing pan and zoom Implementing interactive rotate , shear , and flip Manipulating images at the pixel level using the Java 2D API Working with tiles in JAI to render large images Image manipulation in JAI <m> usingaffine transformation </m> , projections , and warping Image analysis in JAI , including edge detection , statistics , and region of interest computation ( ROI ) Remote imaging using RMI and JAI Internet imaging , including the use of Java 2D and JAI with servlets and JSP , the design of Web-based imaging applications , and the Internet Imaging Protocol ( IIP ) Using the new Image I/O framework to read and write images " Putting It All Together " sections help you put vital concepts and techniques into practice with interactive examples using actual applications .
second:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .


class: same cluster, base class: no relation
first:
Specific topics include : Loading images from JAR files Describing shapes and the affine transformation in Java 2D Designing a class library of shapes Designing a component class for interactive drawing Designing a component class for displaying images Printing in Java 2D Implementing pan and zoom Implementing interactive rotate , shear , and flip Manipulating images at the pixel level using the Java 2D API Working with tiles in JAI to render large images Image manipulation in JAI <m> usingaffine transformation </m> , projections , and warping Image analysis in JAI , including edge detection , statistics , and region of interest computation ( ROI ) Remote imaging using RMI and JAI Internet imaging , including the use of Java 2D and JAI with servlets and JSP , the design of Web-based imaging applications , and the Internet Imaging Protocol ( IIP ) Using the new Image I/O framework to read and write images " Putting It All Together " sections help you put vital concepts and techniques into practice with interactive examples using actual applications .
second:
To be specific , we propose <m> affine mapping based schemes </m> for the problem transformation and outsourcing so that the cloud is unable to learn any key information from the transformed problem .


class: same cluster, base class: no relation
first:
Since the textual and visual vectors have different dimensionality and belong to different spaces , we first map them to a mutual space using an <m> affine transformation </m> .
second:
Also in this group , for those vertices associated with large residual errors under <m> affine mapping </m> , we encode their motion effectively using Newtonian motion estimates .


class: same cluster, base class: no relation
first:
Since the textual and visual vectors have different dimensionality and belong to different spaces , we first map them to a mutual space using an <m> affine transformation </m> .
second:
GCPs derived from each source were used independently for the GCP-based geometric correction of IRS PAN sensor images by using <m> affine mapping function </m> .


class: same cluster, base class: no relation
first:
Since the textual and visual vectors have different dimensionality and belong to different spaces , we first map them to a mutual space using an <m> affine transformation </m> .
second:
In this paper we introduce a class of constraint logic programs such that their termination can be proved by using <m> affine level mappings </m> .


class: same cluster, base class: no relation
first:
Since the textual and visual vectors have different dimensionality and belong to different spaces , we first map them to a mutual space using an <m> affine transformation </m> .
second:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .


class: same cluster, base class: no relation
first:
Since the textual and visual vectors have different dimensionality and belong to different spaces , we first map them to a mutual space using an <m> affine transformation </m> .
second:
To be specific , we propose <m> affine mapping based schemes </m> for the problem transformation and outsourcing so that the cloud is unable to learn any key information from the transformed problem .


class: same cluster, base class: no relation
first:
Furthermore , the learned <m> affine transform </m> applied to these normalized activations allows the BN transform to represent the identity transformation and preserves the network capacity .
second:
Also in this group , for those vertices associated with large residual errors under <m> affine mapping </m> , we encode their motion effectively using Newtonian motion estimates .


class: same cluster, base class: no relation
first:
Furthermore , the learned <m> affine transform </m> applied to these normalized activations allows the BN transform to represent the identity transformation and preserves the network capacity .
second:
GCPs derived from each source were used independently for the GCP-based geometric correction of IRS PAN sensor images by using <m> affine mapping function </m> .


class: same cluster, base class: no relation
first:
Furthermore , the learned <m> affine transform </m> applied to these normalized activations allows the BN transform to represent the identity transformation and preserves the network capacity .
second:
In this paper we introduce a class of constraint logic programs such that their termination can be proved by using <m> affine level mappings </m> .


class: same cluster, base class: no relation
first:
Furthermore , the learned <m> affine transform </m> applied to these normalized activations allows the BN transform to represent the identity transformation and preserves the network capacity .
second:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .


class: same cluster, base class: no relation
first:
Furthermore , the learned <m> affine transform </m> applied to these normalized activations allows the BN transform to represent the identity transformation and preserves the network capacity .
second:
To be specific , we propose <m> affine mapping based schemes </m> for the problem transformation and outsourcing so that the cloud is unable to learn any key information from the transformed problem .


class: same cluster, base class: first -> second
first:
The adaptive Parallel Subgradient Projection ( PSP ) technique improves the convergence speed , in noisy environment , of <m> linear-projection-based algorithms </m> ( e.g. , NLMS and APA ) , with low computational complexity .
second:
For preserving the low rank properties the same , we proposed an algorithm , called <m> linear preserving projection </m> based on low rank representations(LLRLPP),to reduce the dimension of data .


class: same cluster, base class: first -> second
first:
We can build a discrete model of the system by using a <m> linear projection operator </m> to approximate the state space projection .
second:
For preserving the low rank properties the same , we proposed an algorithm , called <m> linear preserving projection </m> based on low rank representations(LLRLPP),to reduce the dimension of data .


class: same cluster, base class: second -> first
first:
For preserving the low rank properties the same , we proposed an algorithm , called <m> linear preserving projection </m> based on low rank representations(LLRLPP),to reduce the dimension of data .
second:
Principal component analysis(PCA ) and Singular value decomposition(SVD ) are two algebra methods used in <m> linear projection analysis </m> to reduce the dimensions in algebra space .


class: same cluster, base class: second -> first
first:
For preserving the low rank properties the same , we proposed an algorithm , called <m> linear preserving projection </m> based on low rank representations(LLRLPP),to reduce the dimension of data .
second:
In this system , sensors take samples while compressing the signal with <m> linear projection operations </m> using the idea of compressive sensing ( CS ) [ 1 ] .


class: same cluster, base class: second -> first
first:
For preserving the low rank properties the same , we proposed an algorithm , called <m> linear preserving projection </m> based on low rank representations(LLRLPP),to reduce the dimension of data .
second:
The monocular vision-based spacecraft relative navigation method is researched , and non-linear equations are established representing relationship between image characteristics and navigation information , based on <m> linear projection principle </m> .


class: same cluster, base class: second -> first
first:
For preserving the low rank properties the same , we proposed an algorithm , called <m> linear preserving projection </m> based on low rank representations(LLRLPP),to reduce the dimension of data .
second:
Specifically , the authors first present a <m> linear projection view </m> to formulate subspace learning and then develop a novel framework , called Latent Subspace Projection Pursuit ( LSPP ) , to estimate the intrinsic dimension , removing corruptions and recovering the subspace structure for observed datasets .


class: same cluster, base class: second -> first
first:
For preserving the low rank properties the same , we proposed an algorithm , called <m> linear preserving projection </m> based on low rank representations(LLRLPP),to reduce the dimension of data .
second:
This model first applies a <m> linear projection </m> to each tuple ( ) and then takes as input the set of projected tuples .


class: same cluster, base class: no relation
first:
Also in this group , for those vertices associated with large residual errors under <m> affine mapping </m> , we encode their motion effectively using Newtonian motion estimates .
second:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .


class: same cluster, base class: no relation
first:
Also in this group , for those vertices associated with large residual errors under <m> affine mapping </m> , we encode their motion effectively using Newtonian motion estimates .
second:
The algorithm estimates the 12 calibration parameters , which are elements of an <m> affine transformation mapping </m> measured , and thus , erroneous magnetometer measurements into corrected field measurements .


class: same cluster, base class: no relation
first:
GCPs derived from each source were used independently for the GCP-based geometric correction of IRS PAN sensor images by using <m> affine mapping function </m> .
second:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .


class: same cluster, base class: no relation
first:
GCPs derived from each source were used independently for the GCP-based geometric correction of IRS PAN sensor images by using <m> affine mapping function </m> .
second:
The algorithm estimates the 12 calibration parameters , which are elements of an <m> affine transformation mapping </m> measured , and thus , erroneous magnetometer measurements into corrected field measurements .


class: same cluster, base class: no relation
first:
In this paper we introduce a class of constraint logic programs such that their termination can be proved by using <m> affine level mappings </m> .
second:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .


class: same cluster, base class: no relation
first:
In this paper we introduce a class of constraint logic programs such that their termination can be proved by using <m> affine level mappings </m> .
second:
The algorithm estimates the 12 calibration parameters , which are elements of an <m> affine transformation mapping </m> measured , and thus , erroneous magnetometer measurements into corrected field measurements .


class: same cluster, base class: no relation
first:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .
second:
To be specific , we propose <m> affine mapping based schemes </m> for the problem transformation and outsourcing so that the cloud is unable to learn any key information from the transformed problem .


class: same cluster, base class: no relation
first:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .
second:
The algorithm estimates the 12 calibration parameters , which are elements of an <m> affine transformation mapping </m> measured , and thus , erroneous magnetometer measurements into corrected field measurements .


class: same cluster, base class: no relation
first:
To be specific , we propose <m> affine mapping based schemes </m> for the problem transformation and outsourcing so that the cloud is unable to learn any key information from the transformed problem .
second:
The algorithm estimates the 12 calibration parameters , which are elements of an <m> affine transformation mapping </m> measured , and thus , erroneous magnetometer measurements into corrected field measurements .


class: same cluster, base class: first -> second
first:
We use two additional <m> linear layers </m> and principal component analysis to reduce the dimension of both inputs and internal representations and to transmit the essential information .
second:
The parameters of the <m> linear neural layer </m> are determined by solving a constrained optimization problem in a regularized framework .


class: same cluster, base class: first -> second
first:
The three methods , named Reconstructing DDT and LAT According to Weight , Executing <m> Linear Layer Operations </m> in Minimal Cost and Merging Two 4-bit S-boxes into One 8-bit S-box respectively , can efficiently speed up the search process by reducing the search space as much as possible and reducing the cost of executing linear layer operations .
second:
The parameters of the <m> linear neural layer </m> are determined by solving a constrained optimization problem in a regularized framework .


class: same cluster, base class: second -> first
first:
The parameters of the <m> linear neural layer </m> are determined by solving a constrained optimization problem in a regularized framework .
second:
This method is fundamentally different from existing methods because we use the domain features to calculate a <m> linear combination of linear layers </m> .


class: same cluster, base class: second -> first
first:
The parameters of the <m> linear neural layer </m> are determined by solving a constrained optimization problem in a regularized framework .
second:
<m> Linear layer ( design ) model </m> gave best outcomes ( MSE : 0.000293366 , RMSE : 0.017127919 , R : 0.996479613 ) .


class: same cluster, base class: second -> first
first:
The parameters of the <m> linear neural layer </m> are determined by solving a constrained optimization problem in a regularized framework .
second:
The process model described is a <m> linear layered model </m> .


class: same cluster, base class: second -> first
first:
The parameters of the <m> linear neural layer </m> are determined by solving a constrained optimization problem in a regularized framework .
second:
Finally , we pass this fixed vector through a <m> linear layer </m> to obtain the prediction whether the question is answerable .


class: same cluster, base class: no relation
first:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .
second:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .


class: same cluster, base class: first -> second
first:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: same cluster, base class: no relation
first:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .
second:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .


class: same cluster, base class: first -> second
first:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: same cluster, base class: no relation
first:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .
second:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .


class: same cluster, base class: first -> second
first:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: same cluster, base class: no relation
first:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: same cluster, base class: no relation
first:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: same cluster, base class: no relation
first:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: same cluster, base class: no relation
first:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: same cluster, base class: no relation
first:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: same cluster, base class: no relation
first:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: same cluster, base class: no relation
first:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: same cluster, base class: no relation
first:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: same cluster, base class: no relation
first:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: same cluster, base class: no relation
first:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: same cluster, base class: first -> second
first:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: same cluster, base class: no relation
first:
More precisely , the contributions achieved during this thesis are : * A feature-preserving filter for the <m> downsampling of scalar volume data </m> .
second:
The third one is the <m> volumetric downsampling block </m> that is identical to a volumetric max pooling layer .


class: same cluster, base class: second -> first
first:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: same cluster, base class: second -> first
first:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: same cluster, base class: second -> first
first:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: same cluster, base class: second -> first
first:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: same cluster, base class: second -> first
first:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: same cluster, base class: first -> second
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .


class: same cluster, base class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .


class: same cluster, base class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .


class: same cluster, base class: first -> second
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .


class: same cluster, base class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .


class: same cluster, base class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .


class: same cluster, base class: second -> first
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .


class: same cluster, base class: second -> first
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .


class: same cluster, base class: second -> first
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .


class: same cluster, base class: second -> first
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .


class: same cluster, base class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .


class: same cluster, base class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .


class: same cluster, base class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .


class: same cluster, base class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .


class: same cluster, base class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .


class: same cluster, base class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .


class: same cluster, base class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .


class: same cluster, base class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .


class: same cluster, base class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .


class: same cluster, base class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .


class: same cluster, base class: first -> second
first:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: same cluster, base class: first -> second
first:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: same cluster, base class: second -> first
first:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: same cluster, base class: second -> first
first:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: same cluster, base class: second -> first
first:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: same cluster, base class: first -> second
first:
The results show the potential of the <m> word embeddings approach </m> for sentiment analysis in the social sciences .
second:
This candidate list was then compared with an expert-generated list of marijuana terms to assess the accuracy and efficacy of using <m> word-vector embeddings </m> to search for novel drug terminology .


class: same cluster, base class: first -> second
first:
We study the effectiveness of <m> word embeddings </m> to overcome this disadvantage of ROUGE .
second:
This candidate list was then compared with an expert-generated list of marijuana terms to assess the accuracy and efficacy of using <m> word-vector embeddings </m> to search for novel drug terminology .


class: same cluster, base class: first -> second
first:
We combine convolutional neural networks with subword-level information vectors , which are <m> word embedding representations </m> learned from Wikipedia that take advantage of the words morphology ; so each word is represented as a bag of their character n-grams .
second:
This candidate list was then compared with an expert-generated list of marijuana terms to assess the accuracy and efficacy of using <m> word-vector embeddings </m> to search for novel drug terminology .


class: same cluster, base class: first -> second
first:
Then , for testing sentiment similarity , we use : Similarity Measures methods between words and cosine similarity measure between the <m> word embedding representations </m> ( e.g. word2vec , GloVE ) .
second:
This candidate list was then compared with an expert-generated list of marijuana terms to assess the accuracy and efficacy of using <m> word-vector embeddings </m> to search for novel drug terminology .


class: same cluster, base class: second -> first
first:
This candidate list was then compared with an expert-generated list of marijuana terms to assess the accuracy and efficacy of using <m> word-vector embeddings </m> to search for novel drug terminology .
second:
In this paper , we utilize <m> word vector embeddings </m> along with fastText sentence classification algorithm to perform the task of classification of tweets posted during natural disasters .


class: same cluster, base class: second -> first
first:
This candidate list was then compared with an expert-generated list of marijuana terms to assess the accuracy and efficacy of using <m> word-vector embeddings </m> to search for novel drug terminology .
second:
Then , the actual input to the Transformer is the element - wise addition of the <m> word embeddings </m> and the positional encodings .


class: same cluster, base class: no relation
first:
On the basis of this principle , this study aims to verify whether the performance of <m> GloVe </m> can be improved after processing certain calibrations on the GloVe model .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: no relation
first:
On the basis of this principle , this study aims to verify whether the performance of <m> GloVe </m> can be improved after processing certain calibrations on the GloVe model .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: no relation
first:
We propose a new word embedding model , inspired by <m> GloVe </m> , which is formulated as a feasible least squares optimization problem .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: no relation
first:
We propose a new word embedding model , inspired by <m> GloVe </m> , which is formulated as a feasible least squares optimization problem .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: no relation
first:
We also show that distributed representations produced by the <m> GloVe model </m> are better than those produced by the Skip-gram model when being used in dependency parsing .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: no relation
first:
We also show that distributed representations produced by the <m> GloVe model </m> are better than those produced by the Skip-gram model when being used in dependency parsing .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: no relation
first:
For topic detection , we design a novel real-time topic model dubbed as a Cost-Effective And Scalable Embedding model ( CEASE ) based on improved <m> GloVe Models </m> and keyword frequency clustering algorithm .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: no relation
first:
For topic detection , we design a novel real-time topic model dubbed as a Cost-Effective And Scalable Embedding model ( CEASE ) based on improved <m> GloVe Models </m> and keyword frequency clustering algorithm .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: no relation
first:
The latent features consist of representations for a person learned by using a word2vec model and representations for profession/nationality values extracted from a <m> pre-trained GloVe embedding model </m> .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: no relation
first:
The latent features consist of representations for a person learned by using a word2vec model and representations for profession/nationality values extracted from a <m> pre-trained GloVe embedding model </m> .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: no relation
first:
Recent approaches to word vector representations , e.g. , ‘ w2vec ’ and ‘ <m> GloVe ’ </m> , have been shown to be powerful methods for capturing the semantics and syntax of words in a text .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: no relation
first:
Recent approaches to word vector representations , e.g. , ‘ w2vec ’ and ‘ <m> GloVe ’ </m> , have been shown to be powerful methods for capturing the semantics and syntax of words in a text .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: no relation
first:
First , we propose an online topic clustering algorithm based on keyword frequency to combine the extended TextRank with weighted titles and the improved <m> GloVe methods </m> .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: no relation
first:
First , we propose an online topic clustering algorithm based on keyword frequency to combine the extended TextRank with weighted titles and the improved <m> GloVe methods </m> .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: no relation
first:
The deep learning techniques such as long short term memory cell ( LSTM ) with and without word GloVe embeddings , a Convolution neural network ( CNN ) with or without GloVe are used , and <m> GloVe pretrained model </m> is used for classification
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: no relation
first:
The deep learning techniques such as long short term memory cell ( LSTM ) with and without word GloVe embeddings , a Convolution neural network ( CNN ) with or without GloVe are used , and <m> GloVe pretrained model </m> is used for classification
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: no relation
first:
subsection : Parsing , POS and predicate detection We first report the labeled and unlabeled attachment scores ( LAS , UAS ) of our parsing models on the CoNLL - 2005 and 2012 test sets ( Table [ reference ] ) with <m> GloVe </m> ( ) and ELMo ( ) embeddings .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: no relation
first:
subsection : Parsing , POS and predicate detection We first report the labeled and unlabeled attachment scores ( LAS , UAS ) of our parsing models on the CoNLL - 2005 and 2012 test sets ( Table [ reference ] ) with <m> GloVe </m> ( ) and ELMo ( ) embeddings .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: no relation
first:
We use open - source <m> GloVe vectors </m> trained on Common Crawl 840B with 300 dimensions as fixed word embeddings .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: no relation
first:
We use open - source <m> GloVe vectors </m> trained on Common Crawl 840B with 300 dimensions as fixed word embeddings .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: no relation
first:
<m> GloVe vectors </m> were trained on a large corpus of about 6 billion tokens pennington2014glove , and provide an important source of prior knowledge for the model .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: no relation
first:
<m> GloVe vectors </m> were trained on a large corpus of about 6 billion tokens pennington2014glove , and provide an important source of prior knowledge for the model .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: no relation
first:
The model exploits the 300 , 000 most frequent <m> pretrained GloVe embeddings </m> and improves them during the training process .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: no relation
first:
The model exploits the 300 , 000 most frequent <m> pretrained GloVe embeddings </m> and improves them during the training process .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: no relation
first:
We use pre - trained <m> word vectors GloVe </m> to obtain the fixed word embedding of each word .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: no relation
first:
We use pre - trained <m> word vectors GloVe </m> to obtain the fixed word embedding of each word .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: no relation
first:
On the textual side , we worked with <m> GloVe word embeddings </m> .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: no relation
first:
On the textual side , we worked with <m> GloVe word embeddings </m> .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: no relation
first:
<m> GloVe embeddings </m> [ Pennington et al. in : Empirical methods in natural language processing ( EMNLP ) , pp 1532–1543 , 2014 ] have been used to represent text data numerically .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: no relation
first:
<m> GloVe embeddings </m> [ Pennington et al. in : Empirical methods in natural language processing ( EMNLP ) , pp 1532–1543 , 2014 ] have been used to represent text data numerically .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: no relation
first:
For the unsupervised classifiers , we implemented several models of CNN and RNN classifiers utilizing <m> GloVe-based word embeddings </m> .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: no relation
first:
For the unsupervised classifiers , we implemented several models of CNN and RNN classifiers utilizing <m> GloVe-based word embeddings </m> .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: no relation
first:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .
second:
This model uses <m> GloVe embeddings </m> and the same optimization and procedure described above .


class: same cluster, base class: no relation
first:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .
second:
An initial embedding layer vectorises words in P and H using pretrained <m> GLoVe embeddings </m> , and passing them to a context representation layer , which uses bidirectional LSTMs ( BiLSTMs ) to encode context vectors for each time - step .


class: same cluster, base class: no relation
first:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .
second:
In particular , our experiments utilize the <m> GloVe embeddings </m> trained by pennington2014glove on 6 billion tokens of Wikipedia 2014 and Gigaword 5 .


class: same cluster, base class: no relation
first:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .
second:
We choose word embedding size , and use the - dimensional pre - trained <m> GloVe word embeddings </m> for initialization .


class: same cluster, base class: no relation
first:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .
second:
This model uses <m> GloVe embeddings </m> and the same optimization and procedure described above .


class: same cluster, base class: no relation
first:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .
second:
An initial embedding layer vectorises words in P and H using pretrained <m> GLoVe embeddings </m> , and passing them to a context representation layer , which uses bidirectional LSTMs ( BiLSTMs ) to encode context vectors for each time - step .


class: same cluster, base class: no relation
first:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .
second:
In particular , our experiments utilize the <m> GloVe embeddings </m> trained by pennington2014glove on 6 billion tokens of Wikipedia 2014 and Gigaword 5 .


class: same cluster, base class: no relation
first:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .
second:
We choose word embedding size , and use the - dimensional pre - trained <m> GloVe word embeddings </m> for initialization .


class: same cluster, base class: no relation
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: second -> first
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: second -> first
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: second -> first
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: second -> first
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: second -> first
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: first -> second
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: first -> second
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: first -> second
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: first -> second
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: first -> second
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: first -> second
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: first -> second
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: first -> second
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: second -> first
first:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: second -> first
first:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: second -> first
first:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: second -> first
first:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: second -> first
first:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: second -> first
first:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: first -> second
first:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: first -> second
first:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation
first:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation
first:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation
first:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation
first:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation
first:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation
first:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation
first:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation
first:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation
first:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation
first:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation
first:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation
first:
<m> Learning representations of words </m> is a pioneering study in this school of research .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation
first:
<m> Learning representations of words </m> is a pioneering study in this school of research .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation
first:
<m> Learning representations of words </m> is a pioneering study in this school of research .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation
first:
<m> Learning representations of words </m> is a pioneering study in this school of research .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation
first:
<m> Learning representations of words </m> is a pioneering study in this school of research .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation
first:
<m> Learning representations of words </m> is a pioneering study in this school of research .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation
first:
<m> Learning representations of words </m> is a pioneering study in this school of research .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation
first:
<m> Learning representations of words </m> is a pioneering study in this school of research .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation
first:
<m> Learning representations of words </m> is a pioneering study in this school of research .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation
first:
<m> Learning representations of words </m> is a pioneering study in this school of research .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation
first:
<m> Learning representations of words </m> is a pioneering study in this school of research .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation
first:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation
first:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation
first:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation
first:
However , <m> learning of word vectors </m> is crucial to obtain good results .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation
first:
However , <m> learning of word vectors </m> is crucial to obtain good results .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation
first:
However , <m> learning of word vectors </m> is crucial to obtain good results .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation
first:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation
first:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation
first:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation
first:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation
first:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation
first:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation
first:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation
first:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation
first:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation
first:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation
first:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation
first:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation
first:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation
first:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation
first:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation
first:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation
first:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation
first:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation
first:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation
first:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation
first:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation
first:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation
first:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation
first:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation
first:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation
first:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation
first:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: first -> second
first:
After that , <m> Conv - LSTM layers </m> with 256 channels are inserted into the single - shot TLL network .
second:
For RNNs , <m> Convolutional - LSTM </m> ( CLSTM ) also achieved a good performance in HSI classification .


class: same cluster, base class: no relation
first:
This system achieves 79.5 % correct prediction using the " hard " CASP 3-class assignment , and 81.4 % with a more lenient assignment , outperforming a sophisticated state-of-the-art <m> predictor </m> ( Porter ) trained in the same experimental conditions .
second:
Several studies have also confirmed that this philosophy of making the <m> predictor </m> robust against random and local perturbation is effective in semi - supervised learning .


class: same cluster, base class: no relation
first:
This system achieves 79.5 % correct prediction using the " hard " CASP 3-class assignment , and 81.4 % with a more lenient assignment , outperforming a sophisticated state-of-the-art <m> predictor </m> ( Porter ) trained in the same experimental conditions .
second:
As in Fig 2 ( top ) , our network has an encoder stage and a <m> predictor stage </m> .


class: same cluster, base class: no relation
first:
However , SVM has a weakness for parameter selection or suitable <m> feature </m> .
second:
This would amount to an additional <m> feature level </m> GAN loss ( see Figure [ reference ] orange portion ) : Taken together , these loss functions form our complete objective : This ultimately corresponds to solving for a target model according to the optimization problem We have introduced a method for unsupervised adaptation which generalizes adversarial objectives to be viewed as operating at the pixel or feature level .


class: same cluster, base class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .


class: same cluster, base class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .


class: same cluster, base class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .


class: same cluster, base class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .


class: same cluster, base class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .


class: same cluster, base class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .


class: same cluster, base class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .


class: same cluster, base class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .


class: same cluster, base class: no relation
first:
They are averaging , Karmarker 's linear programming algorithm , <m> MLP combiner </m> and fuzzy neural network .
second:
Further experiment shows that using <m> MLP merging </m> and decision , the problem of variation of thresholds can be solved to some extent , and the robustness of the system can be improved effectively .


class: same cluster, base class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .


class: same cluster, base class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .


class: same cluster, base class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .


class: same cluster, base class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .


class: same cluster, base class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .


class: same cluster, base class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: same cluster, base class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: same cluster, base class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: same cluster, base class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .


class: same cluster, base class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .


class: same cluster, base class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: same cluster, base class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: same cluster, base class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: same cluster, base class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .


class: same cluster, base class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .


class: same cluster, base class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: same cluster, base class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: same cluster, base class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: same cluster, base class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .


class: same cluster, base class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .


class: same cluster, base class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: same cluster, base class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: same cluster, base class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: same cluster, base class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .


class: same cluster, base class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .


class: same cluster, base class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: same cluster, base class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: same cluster, base class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: same cluster, base class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .


class: same cluster, base class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .


class: same cluster, base class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: same cluster, base class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: same cluster, base class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: same cluster, base class: no relation
first:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .
second:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .


class: same cluster, base class: no relation
first:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: same cluster, base class: no relation
first:
We evaluate the accessibility of our approach with a <m> user study </m> .
second:
The main contributions of this paper are two-fold and materialize into i ) a <m> user research study </m> that covers aspects such as perception , cognition , mental and psychosocial changes that occur with age and ii ) an example-based description of the process of creating personas .


class: same cluster, base class: no relation
first:
We evaluate the accessibility of our approach with a <m> user study </m> .
second:
In the core of our paper , we survey colormap generation techniques , including the latest advances in the field by grouping these techniques into four classes : procedural methods , <m> user-study based methods </m> , rule-based methods , and data-driven methods ; we also include a section on methods that are beyond pure data comprehension purposes .


class: same cluster, base class: no relation
first:
We evaluate the accessibility of our approach with a <m> user study </m> .
second:
The laboratory setting allows evaluators to create not only the environment , but also the scenario in which a <m> user study of system </m> is conducted .


class: same cluster, base class: no relation
first:
We evaluate the accessibility of our approach with a <m> user study </m> .
second:
In this paper , we present the results of a <m> user study of a technique </m> used to pair two devices ( such as two cell phones ) which have good quality output interfaces in the form of a display , speaker , and/or vibration .


class: same cluster, base class: no relation
first:
We evaluate the accessibility of our approach with a <m> user study </m> .
second:
System evaluation and <m> user study research </m> thus can play a significant role in infectious disease information research .


class: same cluster, base class: no relation
first:
We evaluate the accessibility of our approach with a <m> user study </m> .
second:
To evaluate the green navigation service efficiency , we conducted a <m> user subject study </m> consisting of 22 users driving different vehicles over the course of several months in Urbana-Champaign , IL .


class: same cluster, base class: no relation
first:
The main contributions of this paper are two-fold and materialize into i ) a <m> user research study </m> that covers aspects such as perception , cognition , mental and psychosocial changes that occur with age and ii ) an example-based description of the process of creating personas .
second:
In the core of our paper , we survey colormap generation techniques , including the latest advances in the field by grouping these techniques into four classes : procedural methods , <m> user-study based methods </m> , rule-based methods , and data-driven methods ; we also include a section on methods that are beyond pure data comprehension purposes .


class: same cluster, base class: no relation
first:
The main contributions of this paper are two-fold and materialize into i ) a <m> user research study </m> that covers aspects such as perception , cognition , mental and psychosocial changes that occur with age and ii ) an example-based description of the process of creating personas .
second:
The laboratory setting allows evaluators to create not only the environment , but also the scenario in which a <m> user study of system </m> is conducted .


class: same cluster, base class: no relation
first:
The main contributions of this paper are two-fold and materialize into i ) a <m> user research study </m> that covers aspects such as perception , cognition , mental and psychosocial changes that occur with age and ii ) an example-based description of the process of creating personas .
second:
In this paper , we present the results of a <m> user study of a technique </m> used to pair two devices ( such as two cell phones ) which have good quality output interfaces in the form of a display , speaker , and/or vibration .


class: same cluster, base class: no relation
first:
The main contributions of this paper are two-fold and materialize into i ) a <m> user research study </m> that covers aspects such as perception , cognition , mental and psychosocial changes that occur with age and ii ) an example-based description of the process of creating personas .
second:
To evaluate the green navigation service efficiency , we conducted a <m> user subject study </m> consisting of 22 users driving different vehicles over the course of several months in Urbana-Champaign , IL .


class: same cluster, base class: no relation
first:
The main contributions of this paper are two-fold and materialize into i ) a <m> user research study </m> that covers aspects such as perception , cognition , mental and psychosocial changes that occur with age and ii ) an example-based description of the process of creating personas .
second:
For realism , we conduct a <m> user study </m> using pairwise comparison .


class: same cluster, base class: no relation
first:
In the core of our paper , we survey colormap generation techniques , including the latest advances in the field by grouping these techniques into four classes : procedural methods , <m> user-study based methods </m> , rule-based methods , and data-driven methods ; we also include a section on methods that are beyond pure data comprehension purposes .
second:
The laboratory setting allows evaluators to create not only the environment , but also the scenario in which a <m> user study of system </m> is conducted .


class: same cluster, base class: no relation
first:
In the core of our paper , we survey colormap generation techniques , including the latest advances in the field by grouping these techniques into four classes : procedural methods , <m> user-study based methods </m> , rule-based methods , and data-driven methods ; we also include a section on methods that are beyond pure data comprehension purposes .
second:
In this paper , we present the results of a <m> user study of a technique </m> used to pair two devices ( such as two cell phones ) which have good quality output interfaces in the form of a display , speaker , and/or vibration .


class: same cluster, base class: no relation
first:
In the core of our paper , we survey colormap generation techniques , including the latest advances in the field by grouping these techniques into four classes : procedural methods , <m> user-study based methods </m> , rule-based methods , and data-driven methods ; we also include a section on methods that are beyond pure data comprehension purposes .
second:
System evaluation and <m> user study research </m> thus can play a significant role in infectious disease information research .


class: same cluster, base class: no relation
first:
In the core of our paper , we survey colormap generation techniques , including the latest advances in the field by grouping these techniques into four classes : procedural methods , <m> user-study based methods </m> , rule-based methods , and data-driven methods ; we also include a section on methods that are beyond pure data comprehension purposes .
second:
To evaluate the green navigation service efficiency , we conducted a <m> user subject study </m> consisting of 22 users driving different vehicles over the course of several months in Urbana-Champaign , IL .


class: same cluster, base class: no relation
first:
In the core of our paper , we survey colormap generation techniques , including the latest advances in the field by grouping these techniques into four classes : procedural methods , <m> user-study based methods </m> , rule-based methods , and data-driven methods ; we also include a section on methods that are beyond pure data comprehension purposes .
second:
For realism , we conduct a <m> user study </m> using pairwise comparison .


class: same cluster, base class: no relation
first:
The laboratory setting allows evaluators to create not only the environment , but also the scenario in which a <m> user study of system </m> is conducted .
second:
In this paper , we present the results of a <m> user study of a technique </m> used to pair two devices ( such as two cell phones ) which have good quality output interfaces in the form of a display , speaker , and/or vibration .


class: same cluster, base class: no relation
first:
The laboratory setting allows evaluators to create not only the environment , but also the scenario in which a <m> user study of system </m> is conducted .
second:
System evaluation and <m> user study research </m> thus can play a significant role in infectious disease information research .


class: same cluster, base class: no relation
first:
The laboratory setting allows evaluators to create not only the environment , but also the scenario in which a <m> user study of system </m> is conducted .
second:
To evaluate the green navigation service efficiency , we conducted a <m> user subject study </m> consisting of 22 users driving different vehicles over the course of several months in Urbana-Champaign , IL .


class: same cluster, base class: no relation
first:
The laboratory setting allows evaluators to create not only the environment , but also the scenario in which a <m> user study of system </m> is conducted .
second:
For realism , we conduct a <m> user study </m> using pairwise comparison .


class: same cluster, base class: no relation
first:
In this paper , we present the results of a <m> user study of a technique </m> used to pair two devices ( such as two cell phones ) which have good quality output interfaces in the form of a display , speaker , and/or vibration .
second:
System evaluation and <m> user study research </m> thus can play a significant role in infectious disease information research .


class: same cluster, base class: no relation
first:
In this paper , we present the results of a <m> user study of a technique </m> used to pair two devices ( such as two cell phones ) which have good quality output interfaces in the form of a display , speaker , and/or vibration .
second:
To evaluate the green navigation service efficiency , we conducted a <m> user subject study </m> consisting of 22 users driving different vehicles over the course of several months in Urbana-Champaign , IL .


class: same cluster, base class: no relation
first:
In this paper , we present the results of a <m> user study of a technique </m> used to pair two devices ( such as two cell phones ) which have good quality output interfaces in the form of a display , speaker , and/or vibration .
second:
For realism , we conduct a <m> user study </m> using pairwise comparison .


class: same cluster, base class: no relation
first:
System evaluation and <m> user study research </m> thus can play a significant role in infectious disease information research .
second:
To evaluate the green navigation service efficiency , we conducted a <m> user subject study </m> consisting of 22 users driving different vehicles over the course of several months in Urbana-Champaign , IL .


class: same cluster, base class: no relation
first:
System evaluation and <m> user study research </m> thus can play a significant role in infectious disease information research .
second:
For realism , we conduct a <m> user study </m> using pairwise comparison .


class: same cluster, base class: no relation
first:
To evaluate the green navigation service efficiency , we conducted a <m> user subject study </m> consisting of 22 users driving different vehicles over the course of several months in Urbana-Champaign , IL .
second:
For realism , we conduct a <m> user study </m> using pairwise comparison .


class: same cluster, base class: first -> second
first:
<m> Eye tracking </m> has long held the promise of being a useful methodology for human computer interaction .
second:
The focus of this demonstration is placed on four different algorithms : auto-initialization ( RHED ) , <m> eye position tracking </m> ( SIRAT ) , eye closure recognition ( HRA ) , driver head pose categorization .


class: same cluster, base class: first -> second
first:
<m> Eye tracking </m> has long held the promise of being a useful methodology for human computer interaction .
second:
The apparatus can also have a movable platform configured to move the pair of Scheimpflug imaging systems in accordance with eye movement detected by an <m> eye tracking imaging system </m> .


class: same cluster, base class: second -> first
first:
The focus of this demonstration is placed on four different algorithms : auto-initialization ( RHED ) , <m> eye position tracking </m> ( SIRAT ) , eye closure recognition ( HRA ) , driver head pose categorization .
second:
However , a practical <m> eye-tracking-based system </m> must also account for pupil changes due to variable lighting conditions .


class: same cluster, base class: second -> first
first:
The focus of this demonstration is placed on four different algorithms : auto-initialization ( RHED ) , <m> eye position tracking </m> ( SIRAT ) , eye closure recognition ( HRA ) , driver head pose categorization .
second:
The specific objectives are : to describe the variables that can be captured by eye tracking tools and the data that can be obtained from them ; to make a bibliographical survey of the benefits and limitations of the <m> eye tracking tool </m> ; understand how the data collected through an eye tracking tool can contribute to the decision making in advertising and possible applications in this field .


class: same cluster, base class: second -> first
first:
The focus of this demonstration is placed on four different algorithms : auto-initialization ( RHED ) , <m> eye position tracking </m> ( SIRAT ) , eye closure recognition ( HRA ) , driver head pose categorization .
second:
In this paper , an individual human computer interface system using <m> eye motion tracking </m> is introduced .


class: same cluster, base class: second -> first
first:
The focus of this demonstration is placed on four different algorithms : auto-initialization ( RHED ) , <m> eye position tracking </m> ( SIRAT ) , eye closure recognition ( HRA ) , driver head pose categorization .
second:
The evolutionary algorithm encodes an <m> eye-tracking scheme </m> as a genetic code based on image variation analysis .


class: same cluster, base class: no relation
first:
The focus of this demonstration is placed on four different algorithms : auto-initialization ( RHED ) , <m> eye position tracking </m> ( SIRAT ) , eye closure recognition ( HRA ) , driver head pose categorization .
second:
The apparatus can also have a movable platform configured to move the pair of Scheimpflug imaging systems in accordance with eye movement detected by an <m> eye tracking imaging system </m> .


class: same cluster, base class: second -> first
first:
The focus of this demonstration is placed on four different algorithms : auto-initialization ( RHED ) , <m> eye position tracking </m> ( SIRAT ) , eye closure recognition ( HRA ) , driver head pose categorization .
second:
The apparatus for driving a wiper of a vehicle by using eyes of a driver comprises : an eye tracking part for tracking eyes of a driver ; a controlling part for controlling a wiper driving part to drive the wiper when the eyes of the driver tracked by the <m> eye tracking part </m> focus on a windshield of the vehicle ; and the wiper driving part for driving the wiper under the control of the controlling part .


class: same cluster, base class: second -> first
first:
The focus of this demonstration is placed on four different algorithms : auto-initialization ( RHED ) , <m> eye position tracking </m> ( SIRAT ) , eye closure recognition ( HRA ) , driver head pose categorization .
second:
Advanced translation workbenches with detailed logging and <m> eye-tracking capabilities </m> greatly facilitate the recording of key strokes , mouse activity , or eye movement of translators and post-editors .


class: same cluster, base class: first -> second
first:
However , a practical <m> eye-tracking-based system </m> must also account for pupil changes due to variable lighting conditions .
second:
The apparatus can also have a movable platform configured to move the pair of Scheimpflug imaging systems in accordance with eye movement detected by an <m> eye tracking imaging system </m> .


class: same cluster, base class: first -> second
first:
The specific objectives are : to describe the variables that can be captured by eye tracking tools and the data that can be obtained from them ; to make a bibliographical survey of the benefits and limitations of the <m> eye tracking tool </m> ; understand how the data collected through an eye tracking tool can contribute to the decision making in advertising and possible applications in this field .
second:
The apparatus can also have a movable platform configured to move the pair of Scheimpflug imaging systems in accordance with eye movement detected by an <m> eye tracking imaging system </m> .


class: same cluster, base class: first -> second
first:
In this paper , an individual human computer interface system using <m> eye motion tracking </m> is introduced .
second:
The apparatus can also have a movable platform configured to move the pair of Scheimpflug imaging systems in accordance with eye movement detected by an <m> eye tracking imaging system </m> .


class: same cluster, base class: first -> second
first:
The evolutionary algorithm encodes an <m> eye-tracking scheme </m> as a genetic code based on image variation analysis .
second:
The apparatus can also have a movable platform configured to move the pair of Scheimpflug imaging systems in accordance with eye movement detected by an <m> eye tracking imaging system </m> .


class: same cluster, base class: second -> first
first:
The apparatus can also have a movable platform configured to move the pair of Scheimpflug imaging systems in accordance with eye movement detected by an <m> eye tracking imaging system </m> .
second:
The apparatus for driving a wiper of a vehicle by using eyes of a driver comprises : an eye tracking part for tracking eyes of a driver ; a controlling part for controlling a wiper driving part to drive the wiper when the eyes of the driver tracked by the <m> eye tracking part </m> focus on a windshield of the vehicle ; and the wiper driving part for driving the wiper under the control of the controlling part .


class: same cluster, base class: second -> first
first:
The apparatus can also have a movable platform configured to move the pair of Scheimpflug imaging systems in accordance with eye movement detected by an <m> eye tracking imaging system </m> .
second:
Advanced translation workbenches with detailed logging and <m> eye-tracking capabilities </m> greatly facilitate the recording of key strokes , mouse activity , or eye movement of translators and post-editors .


class: same cluster, base class: no relation
first:
Submitted results will also be subject to <m> human evaluation </m> .
second:
<m> Human-based evaluation </m> shows that people are generally positive towards PASS in regards to its clarity and fluency , and that the tailoring is accurately recognized in most cases .


class: same cluster, base class: no relation
first:
<m> Human-based evaluation </m> shows that people are generally positive towards PASS in regards to its clarity and fluency , and that the tailoring is accurately recognized in most cases .
second:
Hence , we conducted a <m> human evaluation </m> study on Amazon Mechanical Turk where each worker is required to make a binary choice when shown one generated and one real image .


class: same cluster, base class: no relation
first:
<m> Human-based evaluation </m> shows that people are generally positive towards PASS in regards to its clarity and fluency , and that the tailoring is accurately recognized in most cases .
second:
section : Human Evaluation We also undertook two <m> human evaluation studies </m> , using Amazon Mechanical Turk .


class: same cluster, base class: no relation
first:
The paper finally presents a <m> user experiment </m> , carried out with 30 participants according to a between-subject protocol , to evaluate the usability of ImAtHome and compare it with the official app for home automation recently released by Apple .
second:
This tutorial is for researchers and engineers , working in the field of Virtual Reality ( VR ) and Augmented Reality ( AR ) , who wish to conduct <m> user-based experiments </m> with a specific aim of promoting both traditional quantitative human-subject experiments and qualitative methods for assessing usability .


class: same cluster, base class: no relation
first:
The paper finally presents a <m> user experiment </m> , carried out with 30 participants according to a between-subject protocol , to evaluate the usability of ImAtHome and compare it with the official app for home automation recently released by Apple .
second:
We evaluate four interactive segmentation algorithms using these strategies , and compare the results with our previous <m> user experiment-based evaluation </m> .


class: same cluster, base class: no relation
first:
This tutorial is for researchers and engineers , working in the field of Virtual Reality ( VR ) and Augmented Reality ( AR ) , who wish to conduct <m> user-based experiments </m> with a specific aim of promoting both traditional quantitative human-subject experiments and qualitative methods for assessing usability .
second:
We evaluate four interactive segmentation algorithms using these strategies , and compare the results with our previous <m> user experiment-based evaluation </m> .


class: same cluster, base class: second -> first
first:
Then , it describes robust sparse PCA which can be efficiently used in <m> feature learning </m> .
second:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .


class: same cluster, base class: second -> first
first:
Then , it describes robust sparse PCA which can be efficiently used in <m> feature learning </m> .
second:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .


class: same cluster, base class: second -> first
first:
Then , it describes robust sparse PCA which can be efficiently used in <m> feature learning </m> .
second:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .


class: same cluster, base class: second -> first
first:
Then , it describes robust sparse PCA which can be efficiently used in <m> feature learning </m> .
second:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .


class: same cluster, base class: second -> first
first:
Then , it describes robust sparse PCA which can be efficiently used in <m> feature learning </m> .
second:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .


class: same cluster, base class: second -> first
first:
Then , it describes robust sparse PCA which can be efficiently used in <m> feature learning </m> .
second:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .


class: same cluster, base class: second -> first
first:
Then , it describes robust sparse PCA which can be efficiently used in <m> feature learning </m> .
second:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .


class: same cluster, base class: second -> first
first:
Then , it describes robust sparse PCA which can be efficiently used in <m> feature learning </m> .
second:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .


class: same cluster, base class: second -> first
first:
Then , it describes robust sparse PCA which can be efficiently used in <m> feature learning </m> .
second:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .


class: same cluster, base class: second -> first
first:
Then , it describes robust sparse PCA which can be efficiently used in <m> feature learning </m> .
second:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .


class: same cluster, base class: second -> first
first:
Then , it describes robust sparse PCA which can be efficiently used in <m> feature learning </m> .
second:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .


class: same cluster, base class: second -> first
first:
Then , it describes robust sparse PCA which can be efficiently used in <m> feature learning </m> .
second:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .


class: same cluster, base class: second -> first
first:
The experiments show promising results over other <m> feature based learning approaches </m> .
second:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .


class: same cluster, base class: second -> first
first:
The experiments show promising results over other <m> feature based learning approaches </m> .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: same cluster, base class: second -> first
first:
Specifically , it employs an adversarial learning guided multi-label attention module to enhance the <m> feature learning part </m> which can learn discriminative feature representations and keep the cross-modal invariability .
second:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .


class: same cluster, base class: second -> first
first:
Specifically , it employs an adversarial learning guided multi-label attention module to enhance the <m> feature learning part </m> which can learn discriminative feature representations and keep the cross-modal invariability .
second:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .


class: same cluster, base class: second -> first
first:
Specifically , it employs an adversarial learning guided multi-label attention module to enhance the <m> feature learning part </m> which can learn discriminative feature representations and keep the cross-modal invariability .
second:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .


class: same cluster, base class: second -> first
first:
Specifically , it employs an adversarial learning guided multi-label attention module to enhance the <m> feature learning part </m> which can learn discriminative feature representations and keep the cross-modal invariability .
second:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .


class: same cluster, base class: second -> first
first:
Specifically , it employs an adversarial learning guided multi-label attention module to enhance the <m> feature learning part </m> which can learn discriminative feature representations and keep the cross-modal invariability .
second:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .


class: same cluster, base class: second -> first
first:
Specifically , it employs an adversarial learning guided multi-label attention module to enhance the <m> feature learning part </m> which can learn discriminative feature representations and keep the cross-modal invariability .
second:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .


class: same cluster, base class: second -> first
first:
Specifically , it employs an adversarial learning guided multi-label attention module to enhance the <m> feature learning part </m> which can learn discriminative feature representations and keep the cross-modal invariability .
second:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .


class: same cluster, base class: second -> first
first:
Specifically , it employs an adversarial learning guided multi-label attention module to enhance the <m> feature learning part </m> which can learn discriminative feature representations and keep the cross-modal invariability .
second:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .


class: same cluster, base class: second -> first
first:
Specifically , it employs an adversarial learning guided multi-label attention module to enhance the <m> feature learning part </m> which can learn discriminative feature representations and keep the cross-modal invariability .
second:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .


class: same cluster, base class: second -> first
first:
Specifically , it employs an adversarial learning guided multi-label attention module to enhance the <m> feature learning part </m> which can learn discriminative feature representations and keep the cross-modal invariability .
second:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .


class: same cluster, base class: second -> first
first:
Specifically , it employs an adversarial learning guided multi-label attention module to enhance the <m> feature learning part </m> which can learn discriminative feature representations and keep the cross-modal invariability .
second:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .


class: same cluster, base class: second -> first
first:
Specifically , it employs an adversarial learning guided multi-label attention module to enhance the <m> feature learning part </m> which can learn discriminative feature representations and keep the cross-modal invariability .
second:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .


class: same cluster, base class: second -> first
first:
The big data trend has inspired <m> feature-driven learning tasks </m> , which can not be handled by conventional machine learning models .
second:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .


class: same cluster, base class: second -> first
first:
The big data trend has inspired <m> feature-driven learning tasks </m> , which can not be handled by conventional machine learning models .
second:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .


class: same cluster, base class: second -> first
first:
<m> Feature learning methods </m> based on the Skip - gram architecture have been originally developed in the context of natural language .
second:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .


class: same cluster, base class: second -> first
first:
<m> Feature learning methods </m> based on the Skip - gram architecture have been originally developed in the context of natural language .
second:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .


class: same cluster, base class: second -> first
first:
<m> Feature learning methods </m> based on the Skip - gram architecture have been originally developed in the context of natural language .
second:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .


class: same cluster, base class: second -> first
first:
<m> Feature learning methods </m> based on the Skip - gram architecture have been originally developed in the context of natural language .
second:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .


class: same cluster, base class: second -> first
first:
<m> Feature learning methods </m> based on the Skip - gram architecture have been originally developed in the context of natural language .
second:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .


class: same cluster, base class: second -> first
first:
<m> Feature learning methods </m> based on the Skip - gram architecture have been originally developed in the context of natural language .
second:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .


class: same cluster, base class: second -> first
first:
<m> Feature learning methods </m> based on the Skip - gram architecture have been originally developed in the context of natural language .
second:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .


class: same cluster, base class: second -> first
first:
<m> Feature learning methods </m> based on the Skip - gram architecture have been originally developed in the context of natural language .
second:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .


class: same cluster, base class: second -> first
first:
<m> Feature learning methods </m> based on the Skip - gram architecture have been originally developed in the context of natural language .
second:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .


class: same cluster, base class: second -> first
first:
<m> Feature learning methods </m> based on the Skip - gram architecture have been originally developed in the context of natural language .
second:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .


class: same cluster, base class: second -> first
first:
<m> Feature learning methods </m> based on the Skip - gram architecture have been originally developed in the context of natural language .
second:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .


class: same cluster, base class: second -> first
first:
<m> Feature learning methods </m> based on the Skip - gram architecture have been originally developed in the context of natural language .
second:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .


class: same cluster, base class: second -> first
first:
We find that this simple <m> feature learning algorithm </m> is surprisingly successful when applied to visual object recognition .
second:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .


class: same cluster, base class: second -> first
first:
We find that this simple <m> feature learning algorithm </m> is surprisingly successful when applied to visual object recognition .
second:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .


class: same cluster, base class: second -> first
first:
We find that this simple <m> feature learning algorithm </m> is surprisingly successful when applied to visual object recognition .
second:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .


class: same cluster, base class: second -> first
first:
We find that this simple <m> feature learning algorithm </m> is surprisingly successful when applied to visual object recognition .
second:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .


class: same cluster, base class: second -> first
first:
We find that this simple <m> feature learning algorithm </m> is surprisingly successful when applied to visual object recognition .
second:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .


class: same cluster, base class: second -> first
first:
We find that this simple <m> feature learning algorithm </m> is surprisingly successful when applied to visual object recognition .
second:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .


class: same cluster, base class: second -> first
first:
We find that this simple <m> feature learning algorithm </m> is surprisingly successful when applied to visual object recognition .
second:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .


class: same cluster, base class: second -> first
first:
We find that this simple <m> feature learning algorithm </m> is surprisingly successful when applied to visual object recognition .
second:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .


class: same cluster, base class: second -> first
first:
We find that this simple <m> feature learning algorithm </m> is surprisingly successful when applied to visual object recognition .
second:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .


class: same cluster, base class: second -> first
first:
We find that this simple <m> feature learning algorithm </m> is surprisingly successful when applied to visual object recognition .
second:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .


class: same cluster, base class: second -> first
first:
We find that this simple <m> feature learning algorithm </m> is surprisingly successful when applied to visual object recognition .
second:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .


class: same cluster, base class: second -> first
first:
We find that this simple <m> feature learning algorithm </m> is surprisingly successful when applied to visual object recognition .
second:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .


class: same cluster, base class: second -> first
first:
We formulate such a dual path architecture as follows : where and denote the extracted information at - th step from individual path , is a <m> feature learning function </m> as .
second:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .


class: same cluster, base class: second -> first
first:
We formulate such a dual path architecture as follows : where and denote the extracted information at - th step from individual path , is a <m> feature learning function </m> as .
second:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .


class: same cluster, base class: second -> first
first:
We formulate such a dual path architecture as follows : where and denote the extracted information at - th step from individual path , is a <m> feature learning function </m> as .
second:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .


class: same cluster, base class: second -> first
first:
We formulate such a dual path architecture as follows : where and denote the extracted information at - th step from individual path , is a <m> feature learning function </m> as .
second:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .


class: same cluster, base class: second -> first
first:
We formulate such a dual path architecture as follows : where and denote the extracted information at - th step from individual path , is a <m> feature learning function </m> as .
second:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .


class: same cluster, base class: second -> first
first:
We formulate such a dual path architecture as follows : where and denote the extracted information at - th step from individual path , is a <m> feature learning function </m> as .
second:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .


class: same cluster, base class: second -> first
first:
We formulate such a dual path architecture as follows : where and denote the extracted information at - th step from individual path , is a <m> feature learning function </m> as .
second:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .


class: same cluster, base class: second -> first
first:
We formulate such a dual path architecture as follows : where and denote the extracted information at - th step from individual path , is a <m> feature learning function </m> as .
second:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .


class: same cluster, base class: second -> first
first:
We formulate such a dual path architecture as follows : where and denote the extracted information at - th step from individual path , is a <m> feature learning function </m> as .
second:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .


class: same cluster, base class: second -> first
first:
We formulate such a dual path architecture as follows : where and denote the extracted information at - th step from individual path , is a <m> feature learning function </m> as .
second:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .


class: same cluster, base class: second -> first
first:
We formulate such a dual path architecture as follows : where and denote the extracted information at - th step from individual path , is a <m> feature learning function </m> as .
second:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .


class: same cluster, base class: second -> first
first:
We formulate such a dual path architecture as follows : where and denote the extracted information at - th step from individual path , is a <m> feature learning function </m> as .
second:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .


class: same cluster, base class: second -> first
first:
In this paper , we address the <m> feature learning problem </m> in DL by presenting a new algorithm , deeply - supervised nets ( DSN ) , which enforces direct and early supervision for both the hidden layers and the output layer .
second:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .


class: same cluster, base class: second -> first
first:
In this paper , we address the <m> feature learning problem </m> in DL by presenting a new algorithm , deeply - supervised nets ( DSN ) , which enforces direct and early supervision for both the hidden layers and the output layer .
second:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .


class: same cluster, base class: second -> first
first:
In this paper , we address the <m> feature learning problem </m> in DL by presenting a new algorithm , deeply - supervised nets ( DSN ) , which enforces direct and early supervision for both the hidden layers and the output layer .
second:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .


class: same cluster, base class: second -> first
first:
In this paper , we address the <m> feature learning problem </m> in DL by presenting a new algorithm , deeply - supervised nets ( DSN ) , which enforces direct and early supervision for both the hidden layers and the output layer .
second:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .


class: same cluster, base class: second -> first
first:
In this paper , we address the <m> feature learning problem </m> in DL by presenting a new algorithm , deeply - supervised nets ( DSN ) , which enforces direct and early supervision for both the hidden layers and the output layer .
second:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .


class: same cluster, base class: second -> first
first:
In this paper , we address the <m> feature learning problem </m> in DL by presenting a new algorithm , deeply - supervised nets ( DSN ) , which enforces direct and early supervision for both the hidden layers and the output layer .
second:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .


class: same cluster, base class: second -> first
first:
In this paper , we address the <m> feature learning problem </m> in DL by presenting a new algorithm , deeply - supervised nets ( DSN ) , which enforces direct and early supervision for both the hidden layers and the output layer .
second:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .


class: same cluster, base class: second -> first
first:
In this paper , we address the <m> feature learning problem </m> in DL by presenting a new algorithm , deeply - supervised nets ( DSN ) , which enforces direct and early supervision for both the hidden layers and the output layer .
second:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .


class: same cluster, base class: second -> first
first:
In this paper , we address the <m> feature learning problem </m> in DL by presenting a new algorithm , deeply - supervised nets ( DSN ) , which enforces direct and early supervision for both the hidden layers and the output layer .
second:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .


class: same cluster, base class: second -> first
first:
In this paper , we address the <m> feature learning problem </m> in DL by presenting a new algorithm , deeply - supervised nets ( DSN ) , which enforces direct and early supervision for both the hidden layers and the output layer .
second:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .


class: same cluster, base class: second -> first
first:
In this paper , we address the <m> feature learning problem </m> in DL by presenting a new algorithm , deeply - supervised nets ( DSN ) , which enforces direct and early supervision for both the hidden layers and the output layer .
second:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .


class: same cluster, base class: second -> first
first:
In this paper , we address the <m> feature learning problem </m> in DL by presenting a new algorithm , deeply - supervised nets ( DSN ) , which enforces direct and early supervision for both the hidden layers and the output layer .
second:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .


class: same cluster, base class: second -> first
first:
The process of training a model 's parameters such that a few gradient steps , or even a single gradient step , can produce good results on a new task can be viewed from a <m> feature learning standpoint </m> as building an internal representation that is broadly suitable for many tasks .
second:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .


class: same cluster, base class: second -> first
first:
The process of training a model 's parameters such that a few gradient steps , or even a single gradient step , can produce good results on a new task can be viewed from a <m> feature learning standpoint </m> as building an internal representation that is broadly suitable for many tasks .
second:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .


class: same cluster, base class: second -> first
first:
The process of training a model 's parameters such that a few gradient steps , or even a single gradient step , can produce good results on a new task can be viewed from a <m> feature learning standpoint </m> as building an internal representation that is broadly suitable for many tasks .
second:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .


class: same cluster, base class: second -> first
first:
The process of training a model 's parameters such that a few gradient steps , or even a single gradient step , can produce good results on a new task can be viewed from a <m> feature learning standpoint </m> as building an internal representation that is broadly suitable for many tasks .
second:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .


class: same cluster, base class: second -> first
first:
The process of training a model 's parameters such that a few gradient steps , or even a single gradient step , can produce good results on a new task can be viewed from a <m> feature learning standpoint </m> as building an internal representation that is broadly suitable for many tasks .
second:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .


class: same cluster, base class: second -> first
first:
The process of training a model 's parameters such that a few gradient steps , or even a single gradient step , can produce good results on a new task can be viewed from a <m> feature learning standpoint </m> as building an internal representation that is broadly suitable for many tasks .
second:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .


class: same cluster, base class: second -> first
first:
The process of training a model 's parameters such that a few gradient steps , or even a single gradient step , can produce good results on a new task can be viewed from a <m> feature learning standpoint </m> as building an internal representation that is broadly suitable for many tasks .
second:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .


class: same cluster, base class: second -> first
first:
The process of training a model 's parameters such that a few gradient steps , or even a single gradient step , can produce good results on a new task can be viewed from a <m> feature learning standpoint </m> as building an internal representation that is broadly suitable for many tasks .
second:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .


class: same cluster, base class: second -> first
first:
The process of training a model 's parameters such that a few gradient steps , or even a single gradient step , can produce good results on a new task can be viewed from a <m> feature learning standpoint </m> as building an internal representation that is broadly suitable for many tasks .
second:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .


class: same cluster, base class: second -> first
first:
The process of training a model 's parameters such that a few gradient steps , or even a single gradient step , can produce good results on a new task can be viewed from a <m> feature learning standpoint </m> as building an internal representation that is broadly suitable for many tasks .
second:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .


class: same cluster, base class: second -> first
first:
The process of training a model 's parameters such that a few gradient steps , or even a single gradient step , can produce good results on a new task can be viewed from a <m> feature learning standpoint </m> as building an internal representation that is broadly suitable for many tasks .
second:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .


class: same cluster, base class: second -> first
first:
The process of training a model 's parameters such that a few gradient steps , or even a single gradient step , can produce good results on a new task can be viewed from a <m> feature learning standpoint </m> as building an internal representation that is broadly suitable for many tasks .
second:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .


class: same cluster, base class: second -> first
first:
In recent years , deep neural networks ( DNNs ) have become successful in computer vision , speech recognition , and natural language processing with their great power of <m> feature representation learning </m> .
second:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .


class: same cluster, base class: second -> first
first:
In recent years , deep neural networks ( DNNs ) have become successful in computer vision , speech recognition , and natural language processing with their great power of <m> feature representation learning </m> .
second:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .


class: same cluster, base class: second -> first
first:
In recent years , deep neural networks ( DNNs ) have become successful in computer vision , speech recognition , and natural language processing with their great power of <m> feature representation learning </m> .
second:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .


class: same cluster, base class: second -> first
first:
In recent years , deep neural networks ( DNNs ) have become successful in computer vision , speech recognition , and natural language processing with their great power of <m> feature representation learning </m> .
second:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .


class: same cluster, base class: second -> first
first:
In recent years , deep neural networks ( DNNs ) have become successful in computer vision , speech recognition , and natural language processing with their great power of <m> feature representation learning </m> .
second:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .


class: same cluster, base class: second -> first
first:
In recent years , deep neural networks ( DNNs ) have become successful in computer vision , speech recognition , and natural language processing with their great power of <m> feature representation learning </m> .
second:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .


class: same cluster, base class: second -> first
first:
In recent years , deep neural networks ( DNNs ) have become successful in computer vision , speech recognition , and natural language processing with their great power of <m> feature representation learning </m> .
second:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .


class: same cluster, base class: second -> first
first:
In recent years , deep neural networks ( DNNs ) have become successful in computer vision , speech recognition , and natural language processing with their great power of <m> feature representation learning </m> .
second:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .


class: same cluster, base class: second -> first
first:
In recent years , deep neural networks ( DNNs ) have become successful in computer vision , speech recognition , and natural language processing with their great power of <m> feature representation learning </m> .
second:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .


class: same cluster, base class: second -> first
first:
In recent years , deep neural networks ( DNNs ) have become successful in computer vision , speech recognition , and natural language processing with their great power of <m> feature representation learning </m> .
second:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .


class: same cluster, base class: second -> first
first:
In recent years , deep neural networks ( DNNs ) have become successful in computer vision , speech recognition , and natural language processing with their great power of <m> feature representation learning </m> .
second:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .


class: same cluster, base class: second -> first
first:
In recent years , deep neural networks ( DNNs ) have become successful in computer vision , speech recognition , and natural language processing with their great power of <m> feature representation learning </m> .
second:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .


class: same cluster, base class: second -> first
first:
With the increasing sample size of ReID dataset , the <m> learning of features </m> from multi - class person identification tasks , denoted as ID - discriminative Embedding ( IDE ) , has shown great potentials on current large - scale person ReID datasets , such as MARS and PRW , where the IDE features are taken from the last hidden layer of Deep Convolutional Neural Networks ( DCNN ) .
second:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .


class: same cluster, base class: second -> first
first:
With the increasing sample size of ReID dataset , the <m> learning of features </m> from multi - class person identification tasks , denoted as ID - discriminative Embedding ( IDE ) , has shown great potentials on current large - scale person ReID datasets , such as MARS and PRW , where the IDE features are taken from the last hidden layer of Deep Convolutional Neural Networks ( DCNN ) .
second:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .


class: same cluster, base class: second -> first
first:
With the increasing sample size of ReID dataset , the <m> learning of features </m> from multi - class person identification tasks , denoted as ID - discriminative Embedding ( IDE ) , has shown great potentials on current large - scale person ReID datasets , such as MARS and PRW , where the IDE features are taken from the last hidden layer of Deep Convolutional Neural Networks ( DCNN ) .
second:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .


class: same cluster, base class: second -> first
first:
With the increasing sample size of ReID dataset , the <m> learning of features </m> from multi - class person identification tasks , denoted as ID - discriminative Embedding ( IDE ) , has shown great potentials on current large - scale person ReID datasets , such as MARS and PRW , where the IDE features are taken from the last hidden layer of Deep Convolutional Neural Networks ( DCNN ) .
second:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .


class: same cluster, base class: second -> first
first:
With the increasing sample size of ReID dataset , the <m> learning of features </m> from multi - class person identification tasks , denoted as ID - discriminative Embedding ( IDE ) , has shown great potentials on current large - scale person ReID datasets , such as MARS and PRW , where the IDE features are taken from the last hidden layer of Deep Convolutional Neural Networks ( DCNN ) .
second:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .


class: same cluster, base class: second -> first
first:
With the increasing sample size of ReID dataset , the <m> learning of features </m> from multi - class person identification tasks , denoted as ID - discriminative Embedding ( IDE ) , has shown great potentials on current large - scale person ReID datasets , such as MARS and PRW , where the IDE features are taken from the last hidden layer of Deep Convolutional Neural Networks ( DCNN ) .
second:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .


class: same cluster, base class: second -> first
first:
With the increasing sample size of ReID dataset , the <m> learning of features </m> from multi - class person identification tasks , denoted as ID - discriminative Embedding ( IDE ) , has shown great potentials on current large - scale person ReID datasets , such as MARS and PRW , where the IDE features are taken from the last hidden layer of Deep Convolutional Neural Networks ( DCNN ) .
second:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .


class: same cluster, base class: second -> first
first:
With the increasing sample size of ReID dataset , the <m> learning of features </m> from multi - class person identification tasks , denoted as ID - discriminative Embedding ( IDE ) , has shown great potentials on current large - scale person ReID datasets , such as MARS and PRW , where the IDE features are taken from the last hidden layer of Deep Convolutional Neural Networks ( DCNN ) .
second:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .


class: same cluster, base class: second -> first
first:
With the increasing sample size of ReID dataset , the <m> learning of features </m> from multi - class person identification tasks , denoted as ID - discriminative Embedding ( IDE ) , has shown great potentials on current large - scale person ReID datasets , such as MARS and PRW , where the IDE features are taken from the last hidden layer of Deep Convolutional Neural Networks ( DCNN ) .
second:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .


class: same cluster, base class: second -> first
first:
With the increasing sample size of ReID dataset , the <m> learning of features </m> from multi - class person identification tasks , denoted as ID - discriminative Embedding ( IDE ) , has shown great potentials on current large - scale person ReID datasets , such as MARS and PRW , where the IDE features are taken from the last hidden layer of Deep Convolutional Neural Networks ( DCNN ) .
second:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .


class: same cluster, base class: second -> first
first:
With the increasing sample size of ReID dataset , the <m> learning of features </m> from multi - class person identification tasks , denoted as ID - discriminative Embedding ( IDE ) , has shown great potentials on current large - scale person ReID datasets , such as MARS and PRW , where the IDE features are taken from the last hidden layer of Deep Convolutional Neural Networks ( DCNN ) .
second:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .


class: same cluster, base class: second -> first
first:
With the increasing sample size of ReID dataset , the <m> learning of features </m> from multi - class person identification tasks , denoted as ID - discriminative Embedding ( IDE ) , has shown great potentials on current large - scale person ReID datasets , such as MARS and PRW , where the IDE features are taken from the last hidden layer of Deep Convolutional Neural Networks ( DCNN ) .
second:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .


class: same cluster, base class: second -> first
first:
Thus , it is significant to explicitly involve the semantic boundary to guide the <m> learning of the features </m> .
second:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .


class: same cluster, base class: second -> first
first:
Thus , it is significant to explicitly involve the semantic boundary to guide the <m> learning of the features </m> .
second:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .


class: same cluster, base class: second -> first
first:
Thus , it is significant to explicitly involve the semantic boundary to guide the <m> learning of the features </m> .
second:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .


class: same cluster, base class: second -> first
first:
Thus , it is significant to explicitly involve the semantic boundary to guide the <m> learning of the features </m> .
second:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .


class: same cluster, base class: second -> first
first:
Thus , it is significant to explicitly involve the semantic boundary to guide the <m> learning of the features </m> .
second:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .


class: same cluster, base class: second -> first
first:
Thus , it is significant to explicitly involve the semantic boundary to guide the <m> learning of the features </m> .
second:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .


class: same cluster, base class: second -> first
first:
Thus , it is significant to explicitly involve the semantic boundary to guide the <m> learning of the features </m> .
second:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .


class: same cluster, base class: second -> first
first:
Thus , it is significant to explicitly involve the semantic boundary to guide the <m> learning of the features </m> .
second:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .


class: same cluster, base class: second -> first
first:
Thus , it is significant to explicitly involve the semantic boundary to guide the <m> learning of the features </m> .
second:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .


class: same cluster, base class: second -> first
first:
Thus , it is significant to explicitly involve the semantic boundary to guide the <m> learning of the features </m> .
second:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .


class: same cluster, base class: second -> first
first:
Thus , it is significant to explicitly involve the semantic boundary to guide the <m> learning of the features </m> .
second:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .


class: same cluster, base class: second -> first
first:
Thus , it is significant to explicitly involve the semantic boundary to guide the <m> learning of the features </m> .
second:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .


class: same cluster, base class: no relation
first:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .
second:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .


class: same cluster, base class: no relation
first:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .
second:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .


class: same cluster, base class: no relation
first:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: same cluster, base class: no relation
first:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: same cluster, base class: no relation
first:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: same cluster, base class: no relation
first:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: same cluster, base class: no relation
first:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .
second:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .


class: same cluster, base class: no relation
first:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .
second:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .


class: same cluster, base class: no relation
first:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: same cluster, base class: no relation
first:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: same cluster, base class: no relation
first:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: same cluster, base class: no relation
first:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: same cluster, base class: no relation
first:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .
second:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .


class: same cluster, base class: no relation
first:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .
second:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .


class: same cluster, base class: no relation
first:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: same cluster, base class: no relation
first:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: same cluster, base class: no relation
first:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: same cluster, base class: no relation
first:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: same cluster, base class: no relation
first:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: same cluster, base class: no relation
first:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: same cluster, base class: no relation
first:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .
second:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .


class: same cluster, base class: no relation
first:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .
second:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .


class: same cluster, base class: no relation
first:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: same cluster, base class: no relation
first:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: same cluster, base class: no relation
first:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: same cluster, base class: no relation
first:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: same cluster, base class: no relation
first:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: same cluster, base class: first -> second
first:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: same cluster, base class: no relation
first:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: same cluster, base class: first -> second
first:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: same cluster, base class: no relation
first:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: same cluster, base class: first -> second
first:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: same cluster, base class: no relation
first:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: same cluster, base class: first -> second
first:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: same cluster, base class: no relation
first:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: same cluster, base class: first -> second
first:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: same cluster, base class: no relation
first:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: same cluster, base class: first -> second
first:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: same cluster, base class: no relation
first:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: same cluster, base class: no relation
first:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: same cluster, base class: no relation
first:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: same cluster, base class: no relation
first:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: same cluster, base class: no relation
first:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: same cluster, base class: no relation
first:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: same cluster, base class: no relation
first:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: same cluster, base class: no relation
first:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: same cluster, base class: no relation
first:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: same cluster, base class: second -> first
first:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: same cluster, base class: no relation
first:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: same cluster, base class: no relation
first:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: same cluster, base class: no relation
first:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: same cluster, base class: first -> second
first:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: same cluster, base class: first -> second
first:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: same cluster, base class: first -> second
first:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: same cluster, base class: first -> second
first:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: same cluster, base class: first -> second
first:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: same cluster, base class: first -> second
first:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: same cluster, base class: first -> second
first:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: same cluster, base class: first -> second
first:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: same cluster, base class: first -> second
first:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: same cluster, base class: second -> first
first:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: same cluster, base class: second -> first
first:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: same cluster, base class: second -> first
first:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: same cluster, base class: second -> first
first:
Furthermore , the optimal organization of <m> numerical data processing </m> in enterprises is analyzed on the example of predicting demand in the firm .
second:
The rapidly increasing power and availability of small computers have made sophisticated <m> numerical analysis of data </m> a routine feature of modem chemistry .


class: same cluster, base class: second -> first
first:
It is designed to support distributed computing and data sharing on the Internet through the use of distributed objects and a very general <m> numerical data model </m> .
second:
The rapidly increasing power and availability of small computers have made sophisticated <m> numerical analysis of data </m> a routine feature of modem chemistry .


class: same cluster, base class: first -> second
first:
The rapidly increasing power and availability of small computers have made sophisticated <m> numerical analysis of data </m> a routine feature of modem chemistry .
second:
Formation features of the <m> numerical data source model </m> were examined .


class: same cluster, base class: second -> first
first:
The fourth and last module , entitled Backtesting and <m> Numerical Results </m> , evaluates the accuracy of the trend predictive numerical models over a " significant " test set via two generic backtesting plans .
second:
On the downside , todayâs computing infrastructures are very inefficient when it comes to <m> numerical calculation </m> on non - uniform sparse data structures .


class: same cluster, base class: second -> first
first:
The fourth and last module , entitled Backtesting and <m> Numerical Results </m> , evaluates the accuracy of the trend predictive numerical models over a " significant " test set via two generic backtesting plans .
second:
Because sea surface propagation causes multipath effect , it 's necessary to study the multipath ray model and the <m> numerical characteristics </m> .


class: same cluster, base class: no relation
first:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .
second:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .


class: same cluster, base class: no relation
first:
Basic methods of the research : <m> categorical analysis </m> , determining of statistic parameters of word-forming models .
second:
Analysis of quantitative ( Bias , Relative RMSE ) and <m> categorical statistics </m> ( POD , FAR ) for the whole period show a more accurate spatial distribution of mean daily rainfall estimations in the lowlands than in the Andean regions .


class: same cluster, base class: second -> first
first:
Conventional analyses address discriminative tasks of behaviors , e.g. , classification and clustering typically using the subsequences extracted from the trajectory of an object as a <m> numerical feature representation </m> .
second:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .


class: same cluster, base class: second -> first
first:
Here , we present iFeature , a versatile Python‐based toolkit for generating various <m> numerical feature representation schemes </m> for both protein and peptide sequences .
second:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .


class: same cluster, base class: second -> first
first:
Experimental comparisons also show the new method can significantly outperform standard <m> numerical feature-type methods </m> in terms of agreement with the clinical diagnosis gold standard .
second:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .


class: same cluster, base class: second -> first
first:
MPEG-7 visual descriptors are <m> numerical representations of features </m> - such as : texture , shape and color - extracted from an image .
second:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .


class: same cluster, base class: first -> second
first:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .
second:
The main goal of this dissertation work is to provide syntactically and semantically related words based on <m> continuous feature vector representation </m> .


class: same cluster, base class: first -> second
first:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .
second:
The main goal of this dissertation work is to provide syntactically and semantically related words based on <m> continuous feature vector representation </m> .


class: same cluster, base class: first -> second
first:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .
second:
The main goal of this dissertation work is to provide syntactically and semantically related words based on <m> continuous feature vector representation </m> .


class: same cluster, base class: second -> first
first:
The main goal of this dissertation work is to provide syntactically and semantically related words based on <m> continuous feature vector representation </m> .
second:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: same cluster, base class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .


class: same cluster, base class: no relation
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: same cluster, base class: no relation
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: same cluster, base class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .


class: same cluster, base class: no relation
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: same cluster, base class: no relation
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: same cluster, base class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .


class: same cluster, base class: no relation
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: same cluster, base class: no relation
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: same cluster, base class: second -> first
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .


class: same cluster, base class: second -> first
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .


class: same cluster, base class: second -> first
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .


class: same cluster, base class: second -> first
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .


class: same cluster, base class: second -> first
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .


class: same cluster, base class: second -> first
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .


class: same cluster, base class: second -> first
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .


class: same cluster, base class: second -> first
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup


class: same cluster, base class: second -> first
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .


class: same cluster, base class: second -> first
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .


class: same cluster, base class: second -> first
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .


class: same cluster, base class: second -> first
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .


class: same cluster, base class: second -> first
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .


class: same cluster, base class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: same cluster, base class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: same cluster, base class: no relation
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: same cluster, base class: no relation
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: same cluster, base class: no relation
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: same cluster, base class: no relation
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: same cluster, base class: no relation
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: same cluster, base class: no relation
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: same cluster, base class: no relation
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: same cluster, base class: no relation
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: same cluster, base class: no relation
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: same cluster, base class: no relation
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: same cluster, base class: no relation
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: same cluster, base class: no relation
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: same cluster, base class: no relation
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: same cluster, base class: no relation
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: same cluster, base class: no relation
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: same cluster, base class: no relation
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: same cluster, base class: no relation
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: same cluster, base class: no relation
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: same cluster, base class: no relation
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: same cluster, base class: no relation
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: same cluster, base class: no relation
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: same cluster, base class: no relation
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: same cluster, base class: no relation
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: same cluster, base class: no relation
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: same cluster, base class: no relation
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: same cluster, base class: no relation
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: same cluster, base class: no relation
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .


class: same cluster, base class: no relation
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .


class: same cluster, base class: no relation
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .


class: same cluster, base class: no relation
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .


class: same cluster, base class: no relation
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .


class: same cluster, base class: no relation
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ). </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .


class: same cluster, base class: no relation
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .


class: same cluster, base class: no relation
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: same cluster, base class: no relation
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: same cluster, base class: no relation
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: same cluster, base class: no relation
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: same cluster, base class: no relation
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .


class: same cluster, base class: no relation
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .


class: same cluster, base class: no relation
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .


class: same cluster, base class: no relation
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .


class: same cluster, base class: no relation
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: same cluster, base class: no relation
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: same cluster, base class: no relation
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: same cluster, base class: no relation
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: same cluster, base class: no relation
first:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .
second:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .


class: same cluster, base class: no relation
first:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .
second:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .


class: same cluster, base class: no relation
first:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .
second:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .


class: same cluster, base class: no relation
first:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .
second:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .


class: same cluster, base class: no relation
first:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .
second:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .


class: same cluster, base class: no relation
first:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .
second:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .


class: same cluster, base class: no relation
first:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .
second:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .


class: same cluster, base class: no relation
first:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: same cluster, base class: no relation
first:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: same cluster, base class: no relation
first:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: same cluster, base class: no relation
first:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: same cluster, base class: no relation
first:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .
second:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .


class: same cluster, base class: no relation
first:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .
second:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .


class: same cluster, base class: no relation
first:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .
second:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .


class: same cluster, base class: no relation
first:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: same cluster, base class: no relation
first:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: same cluster, base class: no relation
first:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: same cluster, base class: no relation
first:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: same cluster, base class: no relation
first:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .
second:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .


class: same cluster, base class: no relation
first:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .
second:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .


class: same cluster, base class: no relation
first:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: same cluster, base class: no relation
first:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: same cluster, base class: no relation
first:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: same cluster, base class: no relation
first:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: same cluster, base class: first -> second
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .


class: same cluster, base class: first -> second
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .


class: same cluster, base class: first -> second
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .


class: same cluster, base class: first -> second
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .


class: same cluster, base class: first -> second
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .


class: same cluster, base class: first -> second
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .


class: same cluster, base class: first -> second
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .


class: same cluster, base class: first -> second
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .


class: same cluster, base class: first -> second
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .


class: same cluster, base class: no relation
first:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .
second:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .


class: same cluster, base class: no relation
first:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .
second:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .


class: same cluster, base class: no relation
first:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .
second:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .


class: same cluster, base class: no relation
first:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .
second:
Here we investigate the network properties of the third party referral structures that facilitate gathering of user information for the <m> delivery of personalized ads </m> .


class: same cluster, base class: no relation
first:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: same cluster, base class: no relation
first:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .
second:
In many e-commerce applications , ranging from dynamic Web content presentation , to <m> personalized ad targeting </m> , to individual recommendations to the customers , it is important to build personalized profiles of individual users from their transactional histories .


class: same cluster, base class: no relation
first:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .
second:
Driven by the rapid growth of large scale real-time data mining applications for <m> personalized ads </m> and content recommendations , distributed stream processing systems are widely applied in modern big-data architectures .


class: same cluster, base class: no relation
first:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .
second:
In this paper we present an approach for the presentation of <m> personalized advertisements </m> on mobile devices , which is based on a user model that takes into account user ’s interests over time .


class: same cluster, base class: no relation
first:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .
second:
This work takes the first step toward the <m> personalized positioning of ads </m> on article pages .


class: same cluster, base class: no relation
first:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .
second:
Mobile devices provide a continuous data stream of contextual behavioral information which can be leveraged for a variety of user services , such as in <m> personalizing ads </m> and customizing home screens .


class: same cluster, base class: no relation
first:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .
second:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .


class: same cluster, base class: no relation
first:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .
second:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .


class: same cluster, base class: no relation
first:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .
second:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .


class: same cluster, base class: no relation
first:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .
second:
Here we investigate the network properties of the third party referral structures that facilitate gathering of user information for the <m> delivery of personalized ads </m> .


class: same cluster, base class: no relation
first:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: same cluster, base class: no relation
first:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .
second:
In many e-commerce applications , ranging from dynamic Web content presentation , to <m> personalized ad targeting </m> , to individual recommendations to the customers , it is important to build personalized profiles of individual users from their transactional histories .


class: same cluster, base class: no relation
first:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .
second:
Driven by the rapid growth of large scale real-time data mining applications for <m> personalized ads </m> and content recommendations , distributed stream processing systems are widely applied in modern big-data architectures .


class: same cluster, base class: no relation
first:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .
second:
In this paper we present an approach for the presentation of <m> personalized advertisements </m> on mobile devices , which is based on a user model that takes into account user ’s interests over time .


class: same cluster, base class: no relation
first:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .
second:
This work takes the first step toward the <m> personalized positioning of ads </m> on article pages .


class: same cluster, base class: no relation
first:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .
second:
Mobile devices provide a continuous data stream of contextual behavioral information which can be leveraged for a variety of user services , such as in <m> personalizing ads </m> and customizing home screens .


class: same cluster, base class: no relation
first:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .
second:
Here we investigate the network properties of the third party referral structures that facilitate gathering of user information for the <m> delivery of personalized ads </m> .


class: same cluster, base class: no relation
first:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: same cluster, base class: no relation
first:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .
second:
In many e-commerce applications , ranging from dynamic Web content presentation , to <m> personalized ad targeting </m> , to individual recommendations to the customers , it is important to build personalized profiles of individual users from their transactional histories .


class: same cluster, base class: no relation
first:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .
second:
Driven by the rapid growth of large scale real-time data mining applications for <m> personalized ads </m> and content recommendations , distributed stream processing systems are widely applied in modern big-data architectures .


class: same cluster, base class: no relation
first:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .
second:
In this paper we present an approach for the presentation of <m> personalized advertisements </m> on mobile devices , which is based on a user model that takes into account user ’s interests over time .


class: same cluster, base class: no relation
first:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .
second:
This work takes the first step toward the <m> personalized positioning of ads </m> on article pages .


class: same cluster, base class: no relation
first:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .
second:
Mobile devices provide a continuous data stream of contextual behavioral information which can be leveraged for a variety of user services , such as in <m> personalizing ads </m> and customizing home screens .


class: same cluster, base class: no relation
first:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .
second:
Here we investigate the network properties of the third party referral structures that facilitate gathering of user information for the <m> delivery of personalized ads </m> .


class: same cluster, base class: no relation
first:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: same cluster, base class: no relation
first:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .
second:
In many e-commerce applications , ranging from dynamic Web content presentation , to <m> personalized ad targeting </m> , to individual recommendations to the customers , it is important to build personalized profiles of individual users from their transactional histories .


class: same cluster, base class: no relation
first:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .
second:
Driven by the rapid growth of large scale real-time data mining applications for <m> personalized ads </m> and content recommendations , distributed stream processing systems are widely applied in modern big-data architectures .


class: same cluster, base class: no relation
first:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .
second:
In this paper we present an approach for the presentation of <m> personalized advertisements </m> on mobile devices , which is based on a user model that takes into account user ’s interests over time .


class: same cluster, base class: no relation
first:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .
second:
This work takes the first step toward the <m> personalized positioning of ads </m> on article pages .


class: same cluster, base class: no relation
first:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .
second:
Mobile devices provide a continuous data stream of contextual behavioral information which can be leveraged for a variety of user services , such as in <m> personalizing ads </m> and customizing home screens .


class: same cluster, base class: no relation
first:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .
second:
Here we investigate the network properties of the third party referral structures that facilitate gathering of user information for the <m> delivery of personalized ads </m> .


class: same cluster, base class: no relation
first:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: same cluster, base class: no relation
first:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .
second:
In many e-commerce applications , ranging from dynamic Web content presentation , to <m> personalized ad targeting </m> , to individual recommendations to the customers , it is important to build personalized profiles of individual users from their transactional histories .


class: same cluster, base class: no relation
first:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .
second:
Driven by the rapid growth of large scale real-time data mining applications for <m> personalized ads </m> and content recommendations , distributed stream processing systems are widely applied in modern big-data architectures .


class: same cluster, base class: no relation
first:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .
second:
In this paper we present an approach for the presentation of <m> personalized advertisements </m> on mobile devices , which is based on a user model that takes into account user ’s interests over time .


class: same cluster, base class: no relation
first:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .
second:
This work takes the first step toward the <m> personalized positioning of ads </m> on article pages .


class: same cluster, base class: no relation
first:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .
second:
Mobile devices provide a continuous data stream of contextual behavioral information which can be leveraged for a variety of user services , such as in <m> personalizing ads </m> and customizing home screens .


class: same cluster, base class: no relation
first:
Here we investigate the network properties of the third party referral structures that facilitate gathering of user information for the <m> delivery of personalized ads </m> .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: same cluster, base class: no relation
first:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .
second:
In many e-commerce applications , ranging from dynamic Web content presentation , to <m> personalized ad targeting </m> , to individual recommendations to the customers , it is important to build personalized profiles of individual users from their transactional histories .


class: same cluster, base class: no relation
first:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .
second:
Driven by the rapid growth of large scale real-time data mining applications for <m> personalized ads </m> and content recommendations , distributed stream processing systems are widely applied in modern big-data architectures .


class: same cluster, base class: no relation
first:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .
second:
In this paper we present an approach for the presentation of <m> personalized advertisements </m> on mobile devices , which is based on a user model that takes into account user ’s interests over time .


class: same cluster, base class: no relation
first:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .
second:
This work takes the first step toward the <m> personalized positioning of ads </m> on article pages .


class: same cluster, base class: no relation
first:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .
second:
Mobile devices provide a continuous data stream of contextual behavioral information which can be leveraged for a variety of user services , such as in <m> personalizing ads </m> and customizing home screens .


class: same cluster, base class: no relation
first:
In the first paper , Adany , Kraus and Ordonez investigate the area of <m> personalized TV ads </m> to improve the effectiveness of ads distribution to targeted viewers .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: same cluster, base class: no relation
first:
We discuss in detail the effects of roles on each NE and on semantic categories of noun and verb features in terms of accuracy , precision/recall and F-score measures for the <m> text classification task </m> .
second:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .


class: same cluster, base class: no relation
first:
In this paper , we address the <m> text classification problem </m> that a period of time created test data is different from the training data , and present a method for text classification based on temporal adaptation .
second:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .


class: same cluster, base class: no relation
first:
Given a small subset of relations between the individuals , the problem of learning social network is translated into a <m> text classification problem </m> .
second:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .


class: same cluster, base class: no relation
first:
We present a geo-enriched classifier joining established methods for <m> text-based classification </m> with location-based topic prediction .
second:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .


class: same cluster, base class: no relation
first:
Using a matrix of closeness and a set of additional fusion rules , the proposed method improves the classification performance by only subjecting likely misclassified samples to a text-based classifier followed by additional fusion of both image-based classification and <m> text-based classification </m> results .
second:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .


class: same cluster, base class: no relation
first:
The model of <m> text classifying </m> using non-labeled training sample with more extensive application is realized .
second:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .


class: same cluster, base class: no relation
first:
For example , in <m> text data classification </m> , groups of documents of different types are categorized by different subsets of terms .
second:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .


class: same cluster, base class: no relation
first:
This study contributes to the field of network troubleshooting , and the field of <m> text data classification </m> .
second:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .


class: same cluster, base class: no relation
first:
Abstract The genetic selection of keyword sets , the text frequencies of which are considered as attributes in <m> text classification analysis </m> , has been analysed .
second:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .


class: same cluster, base class: no relation
first:
In <m> Text Classification of Methods </m> , there is a sentence “ Where P(B|A ) is the probability of B appearance when A is known ?
second:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .


class: same cluster, base class: no relation
first:
<m> TEXT CLASSIFICATION Text classification </m> is a commonly used technique as a basis for applications in document processing and Visualization , web mining , surveillance technology , patent Analysis , etc .
second:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
However , how well do these models perform on practical <m> text classification problems </m> , with real world data ?


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
It is difficult to fit a single model for <m> text classification </m> across domains , due to unknown words , specialized context , colloquial language , and other differences between domains .


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
section : Conclusion In this work we demonstrate that unsupervised pretraining and finetuning provides a flexible framework that is effective for difficult <m> text classification tasks </m> .


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
This paper studies CNN on <m> text categorization </m> to exploit the 1D structure ( namely , word order ) of text data for accurate prediction .


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
Learning distributed sentence representation through neural network models requires little external domain knowledge and can reach satisfactory results in related tasks like sentiment classification , <m> text categorization </m> .


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
Meanwhile , linear classifiers are often considered as strong baselines for <m> text classification problems </m> .


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
In order to test the applicability of the obtained ontologies , a <m> text categorization </m> experiment has been proposed and the obtained results indicate that the approach can be applied with satisfactory results and warrants further research .


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
This paper describes a study on <m> text categorization </m> using a character n­grams approach for the morphological normalization .


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
In contrast to previous <m> text categorization research </m> in e-rulemaking [ 5 , 6 ] , and in an attempt to more closely duplicate the comment analysis process in federal agencies , we employ a set of rule-specific categories , each of which corresponds to a significant issue raised in the comments .


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
Therefore , <m> text categorization research </m> has become more important .


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
This method can improve the accuracy of <m> text categorizing </m> efficiently , which is proved by experiments .


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
Making authoritative corpus and standard of <m> text categorizing </m> are suggested for evaluation and application of text categorizing model .


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
In direct proportion to the heavy increase of online information data , the attention to <m> text categorization ( classification </m> ) has also increased .


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
While the former problem may be addressed using one of the well known techniques of <m> text categorization ( classification </m> ) , the latter seems to require some distinct approaches due to the fact that the set of cases is unknown in advance , as well as due to the assumed limited number of training documents , if a case should be interpreted as a classic category .


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
The use of word embedding models and deep learning algorithms are currently the most common and popular trends to enhance the overall performance of a <m> text classification/categorization system </m> .


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
For <m> text classification/categorization </m> , we investigate a steepest descent induction algorithm combined with a two-level preference relation on user ranking .


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
There are two approaches for this <m> text-based categorization </m> : manual and automatic .


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
<m> Text based categorization </m> is made use of for document classification with pattern recognition and machine learning .


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
The results and the statistical evaluation of this procedure showed that the proposed method may be characterized as highly accurate for <m> text categorization purposes </m> .


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
In this particular paper we describe the design and implementation of a distributed tree-based algorithm for <m> text categorization purposes </m> .


class: same cluster, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
To date , our focus has been on <m> text information categorization </m> using both existing text classification techniques ( e.g. Weighted Feature Vector ( WFV ) classification ) and machine learning algorithms based on inductive logic programming ( ILP ) and natural language processing techniques .


class: same cluster, base class: first -> second
first:
To illustrate the effectiveness of the proposed page signature , a detailed experimental evaluation has been conducted in this article for assessing two possible categorization applications , unsupervised page classification and <m> page stream segmentation </m> .
second:
In this manuscript we propose a novel method for <m> jointly page stream segmentation </m> and multi-page document classification .


class: same cluster, base class: first -> second
first:
To facilitate workflows involving very large amounts of paper scans , <m> page stream segmentation ( PSS </m> ) is the task to automatically separate a stream of scanned images into coherent multi-page documents .
second:
In this manuscript we propose a novel method for <m> jointly page stream segmentation </m> and multi-page document classification .


