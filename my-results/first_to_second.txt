class: first -> second, base class: no relation
first:
We use two additional <m> linear layers </m> and principal component analysis to reduce the dimension of both inputs and internal representations and to transmit the essential information .
second:
This amounts to adding a <m> bottleneck linear layer </m> , and brings the CNN Softmax much closer to our best result , as can be seen in Table [ reference ] , where adding a 128 - dim correction halves the gap between regular and the CNN Softmax .


class: first -> second, base class: no relation
first:
We use two additional <m> linear layers </m> and principal component analysis to reduce the dimension of both inputs and internal representations and to transmit the essential information .
second:
subsection : <m> Layer - Wise Linear Model </m> A neural network model based on graph convolutions can therefore be built by stacking multiple convolutional layers of the form of Eq .


class: first -> second, base class: no relation
first:
We use two additional <m> linear layers </m> and principal component analysis to reduce the dimension of both inputs and internal representations and to transmit the essential information .
second:
Considering that a single <m> linear Weight Layer </m> might cause excessive response on some specific dimensions of the part vector , we add a nonlinear function to equalize the response of part feature vector , and the fused feature representation is where the and the are the global and part feature vectors .


class: first -> second, base class: no relation
first:
The three methods , named Reconstructing DDT and LAT According to Weight , Executing <m> Linear Layer Operations </m> in Minimal Cost and Merging Two 4-bit S-boxes into One 8-bit S-box respectively , can efficiently speed up the search process by reducing the search space as much as possible and reducing the cost of executing linear layer operations .
second:
This amounts to adding a <m> bottleneck linear layer </m> , and brings the CNN Softmax much closer to our best result , as can be seen in Table [ reference ] , where adding a 128 - dim correction halves the gap between regular and the CNN Softmax .


class: first -> second, base class: no relation
first:
The three methods , named Reconstructing DDT and LAT According to Weight , Executing <m> Linear Layer Operations </m> in Minimal Cost and Merging Two 4-bit S-boxes into One 8-bit S-box respectively , can efficiently speed up the search process by reducing the search space as much as possible and reducing the cost of executing linear layer operations .
second:
subsection : <m> Layer - Wise Linear Model </m> A neural network model based on graph convolutions can therefore be built by stacking multiple convolutional layers of the form of Eq .


class: first -> second, base class: no relation
first:
The three methods , named Reconstructing DDT and LAT According to Weight , Executing <m> Linear Layer Operations </m> in Minimal Cost and Merging Two 4-bit S-boxes into One 8-bit S-box respectively , can efficiently speed up the search process by reducing the search space as much as possible and reducing the cost of executing linear layer operations .
second:
Considering that a single <m> linear Weight Layer </m> might cause excessive response on some specific dimensions of the part vector , we add a nonlinear function to equalize the response of part feature vector , and the fused feature representation is where the and the are the global and part feature vectors .


class: first -> second, base class: no relation
first:
The parameters of the <m> linear neural layer </m> are determined by solving a constrained optimization problem in a regularized framework .
second:
This amounts to adding a <m> bottleneck linear layer </m> , and brings the CNN Softmax much closer to our best result , as can be seen in Table [ reference ] , where adding a 128 - dim correction halves the gap between regular and the CNN Softmax .


class: first -> second, base class: no relation
first:
This method is fundamentally different from existing methods because we use the domain features to calculate a <m> linear combination of linear layers </m> .
second:
This amounts to adding a <m> bottleneck linear layer </m> , and brings the CNN Softmax much closer to our best result , as can be seen in Table [ reference ] , where adding a 128 - dim correction halves the gap between regular and the CNN Softmax .


class: first -> second, base class: no relation
first:
This method is fundamentally different from existing methods because we use the domain features to calculate a <m> linear combination of linear layers </m> .
second:
subsection : <m> Layer - Wise Linear Model </m> A neural network model based on graph convolutions can therefore be built by stacking multiple convolutional layers of the form of Eq .


class: first -> second, base class: no relation
first:
This method is fundamentally different from existing methods because we use the domain features to calculate a <m> linear combination of linear layers </m> .
second:
Considering that a single <m> linear Weight Layer </m> might cause excessive response on some specific dimensions of the part vector , we add a nonlinear function to equalize the response of part feature vector , and the fused feature representation is where the and the are the global and part feature vectors .


class: first -> second, base class: no relation
first:
<m> Linear layer ( design ) model </m> gave best outcomes ( MSE : 0.000293366 , RMSE : 0.017127919 , R : 0.996479613 ) .
second:
This amounts to adding a <m> bottleneck linear layer </m> , and brings the CNN Softmax much closer to our best result , as can be seen in Table [ reference ] , where adding a 128 - dim correction halves the gap between regular and the CNN Softmax .


class: first -> second, base class: no relation
first:
<m> Linear layer ( design ) model </m> gave best outcomes ( MSE : 0.000293366 , RMSE : 0.017127919 , R : 0.996479613 ) .
second:
subsection : <m> Layer - Wise Linear Model </m> A neural network model based on graph convolutions can therefore be built by stacking multiple convolutional layers of the form of Eq .


class: first -> second, base class: no relation
first:
<m> Linear layer ( design ) model </m> gave best outcomes ( MSE : 0.000293366 , RMSE : 0.017127919 , R : 0.996479613 ) .
second:
Considering that a single <m> linear Weight Layer </m> might cause excessive response on some specific dimensions of the part vector , we add a nonlinear function to equalize the response of part feature vector , and the fused feature representation is where the and the are the global and part feature vectors .


class: first -> second, base class: no relation
first:
The process model described is a <m> linear layered model </m> .
second:
This amounts to adding a <m> bottleneck linear layer </m> , and brings the CNN Softmax much closer to our best result , as can be seen in Table [ reference ] , where adding a 128 - dim correction halves the gap between regular and the CNN Softmax .


class: first -> second, base class: no relation
first:
The process model described is a <m> linear layered model </m> .
second:
subsection : <m> Layer - Wise Linear Model </m> A neural network model based on graph convolutions can therefore be built by stacking multiple convolutional layers of the form of Eq .


class: first -> second, base class: no relation
first:
The process model described is a <m> linear layered model </m> .
second:
Considering that a single <m> linear Weight Layer </m> might cause excessive response on some specific dimensions of the part vector , we add a nonlinear function to equalize the response of part feature vector , and the fused feature representation is where the and the are the global and part feature vectors .


class: first -> second, base class: no relation
first:
Finally , we pass this fixed vector through a <m> linear layer </m> to obtain the prediction whether the question is answerable .
second:
This amounts to adding a <m> bottleneck linear layer </m> , and brings the CNN Softmax much closer to our best result , as can be seen in Table [ reference ] , where adding a 128 - dim correction halves the gap between regular and the CNN Softmax .


class: first -> second, base class: no relation
first:
Finally , we pass this fixed vector through a <m> linear layer </m> to obtain the prediction whether the question is answerable .
second:
subsection : <m> Layer - Wise Linear Model </m> A neural network model based on graph convolutions can therefore be built by stacking multiple convolutional layers of the form of Eq .


class: first -> second, base class: no relation
first:
Finally , we pass this fixed vector through a <m> linear layer </m> to obtain the prediction whether the question is answerable .
second:
Considering that a single <m> linear Weight Layer </m> might cause excessive response on some specific dimensions of the part vector , we add a nonlinear function to equalize the response of part feature vector , and the fused feature representation is where the and the are the global and part feature vectors .


class: first -> second, base class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
<m> Anti mutual-information subsampling </m> does not require to remove from the brain signals the shared variance between aging and fluid intelligence , and hence does not display this pessimistic behavior .


class: first -> second, base class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
A <m> subsampling stochastic gradient descent algorithm </m> has been proposed to accelerate the training of recurrent neural network language models .


class: first -> second, base class: same cluster
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .


class: first -> second, base class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
Subsampling in CSXI is accomplished by either <m> view angle spectral subsampling </m> , spatial subsampling enabled by block-unblock coded apertures placed at the source or detector side , or both .


class: first -> second, base class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .


class: first -> second, base class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
Note that SNIPER also performs <m> adaptive downsampling </m> .


class: first -> second, base class: same cluster
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .


class: first -> second, base class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
Subsampling in CSXI is accomplished by either <m> view angle spectral subsampling </m> , spatial subsampling enabled by block-unblock coded apertures placed at the source or detector side , or both .


class: first -> second, base class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .


class: first -> second, base class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
Note that SNIPER also performs <m> adaptive downsampling </m> .


class: first -> second, base class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
subsection : Results of Hierarchical Subsampling We first demonstrate the results of the <m> hierarchical subsampling recurrent network </m> , which is the key to speed up our experiments .


class: first -> second, base class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
subsection : Results of Hierarchical Subsampling We first demonstrate the results of the <m> hierarchical subsampling recurrent network </m> , which is the key to speed up our experiments .


class: first -> second, base class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .


class: first -> second, base class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
Note that SNIPER also performs <m> adaptive downsampling </m> .


class: first -> second, base class: no relation
first:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .
second:
The overall best performance was obtained with a normalized correlation metric coupled with <m> HFUS downsampling </m> and a one-plus-one evolutionary optimizer , yielding a mean registration error of 0.05 mm .


class: first -> second, base class: same cluster
first:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .
second:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .


class: first -> second, base class: no relation
first:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: first -> second, base class: no relation
first:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: first -> second, base class: no relation
first:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .
second:
The overall best performance was obtained with a normalized correlation metric coupled with <m> HFUS downsampling </m> and a one-plus-one evolutionary optimizer , yielding a mean registration error of 0.05 mm .


class: first -> second, base class: same cluster
first:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .
second:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .


class: first -> second, base class: no relation
first:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: first -> second, base class: no relation
first:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: first -> second, base class: no relation
first:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .
second:
The overall best performance was obtained with a normalized correlation metric coupled with <m> HFUS downsampling </m> and a one-plus-one evolutionary optimizer , yielding a mean registration error of 0.05 mm .


class: first -> second, base class: same cluster
first:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .
second:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .


class: first -> second, base class: no relation
first:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: first -> second, base class: no relation
first:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: first -> second, base class: no relation
first:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .
second:
This paper explores the problem <m> oftask-oriented downsampling </m> , which aims to downsample a pointcloud and maintain the performance of subsequent applications asmuch as possible .


class: first -> second, base class: no relation
first:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .
second:
We further evaluate three potential defense methods to mitigate such attacks , including adversarial training , <m> audio downsampling </m> , and moving average filtering , which leads to promising directions for further research .


class: first -> second, base class: no relation
first:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .
second:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .


class: first -> second, base class: no relation
first:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: first -> second, base class: no relation
first:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: first -> second, base class: same cluster
first:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .
second:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .


class: first -> second, base class: no relation
first:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: first -> second, base class: no relation
first:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: first -> second, base class: no relation
first:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: first -> second, base class: no relation
first:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: first -> second, base class: no relation
first:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: first -> second, base class: no relation
first:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: first -> second, base class: no relation
first:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: first -> second, base class: no relation
first:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: first -> second, base class: no relation
first:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: first -> second, base class: no relation
first:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: first -> second, base class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .


class: first -> second, base class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .


class: first -> second, base class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .


class: first -> second, base class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .


class: first -> second, base class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .


class: first -> second, base class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .


class: first -> second, base class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .


class: first -> second, base class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .


class: first -> second, base class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .


class: first -> second, base class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .


class: first -> second, base class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .


class: first -> second, base class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .


class: first -> second, base class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .


class: first -> second, base class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .


class: first -> second, base class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .


class: first -> second, base class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .


class: first -> second, base class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .


class: first -> second, base class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .


class: first -> second, base class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .


class: first -> second, base class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .


class: first -> second, base class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .


class: first -> second, base class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .


class: first -> second, base class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .


class: first -> second, base class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .


class: first -> second, base class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .


class: first -> second, base class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .


class: first -> second, base class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .


class: first -> second, base class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .


class: first -> second, base class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .


class: first -> second, base class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .


class: first -> second, base class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
The <m> Virtual Assistant Designer </m> .


class: first -> second, base class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .


class: first -> second, base class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .


class: first -> second, base class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .


class: first -> second, base class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .


class: first -> second, base class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .


class: first -> second, base class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .


class: first -> second, base class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .


class: first -> second, base class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: first -> second, base class: no relation
first:
Therefore , website ’s ranking could be figured out via the rates of computers with the <m> Alexa tool bar </m> and Internet traffic data collected .
second:
Certified metrics are available with <m> Alexa Pro plans </m> , which gives traffic data accurately by the code installed in the server of the website .


class: first -> second, base class: no relation
first:
Moreover , the scheme incorporates several source coding techniques , such as Wyner-Ziv coding , <m> binning </m> and superposition coding .
second:
This paper demonstrates and compares different approaches for incorporating deterministic models of physical parameters into probabilistic models ; <m> parameter range binning </m> , response curves , and integral deterministic models .


class: first -> second, base class: no relation
first:
Moreover , the scheme incorporates several source coding techniques , such as Wyner-Ziv coding , <m> binning </m> and superposition coding .
second:
As in previous versions , a filtering and a <m> weighted binning </m> based on the uncertainty of the SM retrievals by means of the Data Quality Index ( DQX ) was applied for the L3 production .


class: first -> second, base class: no relation
first:
The central task of automatic data visualization is , given a dataset , to visualize its compelling stories by transforming the data ( e.g. , selecting attributes , <m> grouping and binning values </m> ) and deciding the right type of visualization ( e.g. , bar or line charts ) .
second:
Obtaining a coherent binned representation in higher dimensions is challenging because <m> multidimensional binning </m> can suffer from the curse of dimensionality .


class: first -> second, base class: no relation
first:
This candidate list was then compared with an expert-generated list of marijuana terms to assess the accuracy and efficacy of using <m> word-vector embeddings </m> to search for novel drug terminology .
second:
Through large scale human evaluation , we report that our resulting word embedddings are much more interpretable than the original <m> GloVe and word2vec embeddings </m> .


class: first -> second, base class: no relation
first:
Through large scale human evaluation , we report that our resulting word embedddings are much more interpretable than the original <m> GloVe and word2vec embeddings </m> .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: first -> second, base class: no relation
first:
Through large scale human evaluation , we report that our resulting word embedddings are much more interpretable than the original <m> GloVe and word2vec embeddings </m> .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: first -> second, base class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: first -> second, base class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: first -> second, base class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: first -> second, base class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: first -> second, base class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: first -> second, base class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: first -> second, base class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: first -> second, base class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: first -> second, base class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: first -> second, base class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: first -> second, base class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: first -> second, base class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: first -> second, base class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: first -> second, base class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: first -> second, base class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: first -> second, base class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: first -> second, base class: no relation
first:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: first -> second, base class: no relation
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: first -> second, base class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: first -> second, base class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: first -> second, base class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: first -> second, base class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: first -> second, base class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: first -> second, base class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: first -> second, base class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: first -> second, base class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: first -> second, base class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: first -> second, base class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: first -> second, base class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: first -> second, base class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: first -> second, base class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: first -> second, base class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: first -> second, base class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: first -> second, base class: no relation
first:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: first -> second, base class: no relation
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .


class: first -> second, base class: no relation
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: first -> second, base class: no relation
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: first -> second, base class: no relation
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: first -> second, base class: no relation
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: first -> second, base class: no relation
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: first -> second, base class: no relation
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: first -> second, base class: no relation
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: first -> second, base class: no relation
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: first -> second, base class: no relation
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: first -> second, base class: no relation
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: first -> second, base class: no relation
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: first -> second, base class: no relation
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: first -> second, base class: no relation
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: first -> second, base class: no relation
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: first -> second, base class: no relation
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: first -> second, base class: no relation
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: first -> second, base class: no relation
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: first -> second, base class: no relation
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: first -> second, base class: no relation
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: first -> second, base class: no relation
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: first -> second, base class: no relation
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: first -> second, base class: no relation
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: first -> second, base class: no relation
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
The DeepLab - ASPP - L model has four parallel branches after conv5 block of <m> ResNet101 architecture </m> .


class: first -> second, base class: no relation
first:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: first -> second, base class: no relation
first:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: first -> second, base class: no relation
first:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: first -> second, base class: no relation
first:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: first -> second, base class: no relation
first:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: first -> second, base class: no relation
first:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: first -> second, base class: no relation
first:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: first -> second, base class: no relation
first:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .
second:
We employ the <m> attentional architecture </m> from Luong2015 , which achieved state - of - the - art results on English German translation .


class: first -> second, base class: no relation
first:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .
second:
Character - level language modelling is a representative example of discrete [ reference ] 1.67 MRNN [ reference ] 1.60 <m> GF - LSTM </m> [ reference ] 1.58 Grid - LSTM [ reference ] 1.47 MI - LSTM 1.44 Recurrent Memory Array Structures ( Rocki , 2016a ) 1.40 SF - LSTM


class: first -> second, base class: no relation
first:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: first -> second, base class: no relation
first:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .
second:
Deep Bi-LSTM has great performance in the cases with relative long latency , average MAE of 0.074 mm , RMSE of 0.097 mm , and normalized root mean square error ( nRMSE ) of 0.081 with latency about 400 ms are obtained from predictive results of <m> Deep Bi-LSTM </m> .


class: first -> second, base class: no relation
first:
By comparing the three model experiments with Word2vec , CNN and <m> LSTM </m> , the experimental results on open datasets show that it is effective to use these three models , of which the LSTM model works best .
second:
By comparing with other two types deep learning approaches ( DCNN and LSTM ) , the presented <m> deep C-LSTM </m> obtains the best performance for classifying these five classes .


class: first -> second, base class: no relation
first:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .
second:
Character - level language modelling is a representative example of discrete [ reference ] 1.67 MRNN [ reference ] 1.60 <m> GF - LSTM </m> [ reference ] 1.58 Grid - LSTM [ reference ] 1.47 MI - LSTM 1.44 Recurrent Memory Array Structures ( Rocki , 2016a ) 1.40 SF - LSTM


class: first -> second, base class: no relation
first:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: first -> second, base class: no relation
first:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .
second:
Deep Bi-LSTM has great performance in the cases with relative long latency , average MAE of 0.074 mm , RMSE of 0.097 mm , and normalized root mean square error ( nRMSE ) of 0.081 with latency about 400 ms are obtained from predictive results of <m> Deep Bi-LSTM </m> .


class: first -> second, base class: no relation
first:
The “ concatenation ” alignment function adds one input vector ( e.g. hidden state vector of the <m> LSTM </m> ) to each candidate feature vector , embeds the resulting vectors into scalar values , and then applies the softmax function to generate the attention weight for each candidate .
second:
By comparing with other two types deep learning approaches ( DCNN and LSTM ) , the presented <m> deep C-LSTM </m> obtains the best performance for classifying these five classes .


class: first -> second, base class: no relation
first:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .
second:
Character - level language modelling is a representative example of discrete [ reference ] 1.67 MRNN [ reference ] 1.60 <m> GF - LSTM </m> [ reference ] 1.58 Grid - LSTM [ reference ] 1.47 MI - LSTM 1.44 Recurrent Memory Array Structures ( Rocki , 2016a ) 1.40 SF - LSTM


class: first -> second, base class: no relation
first:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: first -> second, base class: no relation
first:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .
second:
Deep Bi-LSTM has great performance in the cases with relative long latency , average MAE of 0.074 mm , RMSE of 0.097 mm , and normalized root mean square error ( nRMSE ) of 0.081 with latency about 400 ms are obtained from predictive results of <m> Deep Bi-LSTM </m> .


class: first -> second, base class: no relation
first:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .
second:
By comparing with other two types deep learning approaches ( DCNN and LSTM ) , the presented <m> deep C-LSTM </m> obtains the best performance for classifying these five classes .


class: first -> second, base class: no relation
first:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .
second:
subsection : Using biLMs for supervised NLP tasks Given a pre - trained biLM and a <m> supervised architecture </m> for a target NLP task , it is a simple process to use the biLM to improve the task model .


class: first -> second, base class: no relation
first:
This system achieves 79.5 % correct prediction using the " hard " CASP 3-class assignment , and 81.4 % with a more lenient assignment , outperforming a sophisticated state-of-the-art <m> predictor </m> ( Porter ) trained in the same experimental conditions .
second:
The paper presents an extension of the <m> modified Smith predictor controller </m> referred to as the flexible Smith predictor control scheme .


class: first -> second, base class: no relation
first:
This system achieves 79.5 % correct prediction using the " hard " CASP 3-class assignment , and 81.4 % with a more lenient assignment , outperforming a sophisticated state-of-the-art <m> predictor </m> ( Porter ) trained in the same experimental conditions .
second:
Using our techniques , the <m> hardware choice predictor </m> of a hybrid predictor can be completely eliminated from the processor and replaced with our off-line profiling schemes .


class: first -> second, base class: no relation
first:
Several studies have also confirmed that this philosophy of making the <m> predictor </m> robust against random and local perturbation is effective in semi - supervised learning .
second:
Formally , let denote the encoding of the state , and let be a <m> dynamics predictor </m> parameterized by .


class: first -> second, base class: no relation
first:
As in Fig 2 ( top ) , our network has an encoder stage and a <m> predictor stage </m> .
second:
Formally , let denote the encoding of the state , and let be a <m> dynamics predictor </m> parameterized by .


class: first -> second, base class: same cluster
first:
However , SVM has a weakness for parameter selection or suitable <m> feature </m> .
second:
subsection : Locality Sensitive Hashing ( LSH ) An alternative approach to BASS and DISCO is to use well - established <m> feature generation methods </m> that are agnostic about the type of input they receive .


class: first -> second, base class: same cluster
first:
However , SVM has a weakness for parameter selection or suitable <m> feature </m> .
second:
subsection : Unimodal Feature Extraction For a fair comparison with the state - of - the - art method , conversational memory networks ( CMN ) , we follow identical <m> feature extraction procedures </m> .


class: first -> second, base class: no relation
first:
Fang et al . won the 2015 COCO Captioning Challenge with an approach that is similar to ours in as much as it applies a visual concept ( i.e. , <m> attribute ) detection process </m> before generating sentences .
second:
<m> Adversarial attribute loss </m> .


class: first -> second, base class: no relation
first:
Compared with conventional perceptual loss , ours is more effective in computation , tailored to our specific <m> attribute transfer task </m> , and can serve as a kind of hidden - layer supervision or regularization to ease the training of the DIAT model .
second:
Shen et al . learn the residual image in the GAN framework , and adopt dual learning to learn two <m> reverse attribute transfer models </m> simultaneously .


class: first -> second, base class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .


class: first -> second, base class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :


class: first -> second, base class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .


class: first -> second, base class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
We evaluate the proposed learning rule by training <m> deep spiking multi-layer perceptron ( MLP ) </m> and spiking convolutional neural network ( CNN ) on the UCI machine learning and MNIST handwritten digit datasets .


class: first -> second, base class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .


class: first -> second, base class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .


class: first -> second, base class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :


class: first -> second, base class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .


class: first -> second, base class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .


class: first -> second, base class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :


class: first -> second, base class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .


class: first -> second, base class: no relation
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :


class: first -> second, base class: no relation
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .


class: first -> second, base class: no relation
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :


class: first -> second, base class: no relation
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .


class: first -> second, base class: no relation
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :


class: first -> second, base class: no relation
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .


class: first -> second, base class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: first -> second, base class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: first -> second, base class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: first -> second, base class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: first -> second, base class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: first -> second, base class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: first -> second, base class: no relation
first:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: first -> second, base class: no relation
first:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: first -> second, base class: no relation
first:
The focus of this demonstration is placed on four different algorithms : auto-initialization ( RHED ) , <m> eye position tracking </m> ( SIRAT ) , eye closure recognition ( HRA ) , driver head pose categorization .
second:
<m> Eye state tracking </m> for driving is the key problem in fatigue detection , and it has been proved to be effective and popular method in fatigue driving .


class: first -> second, base class: no relation
first:
The apparatus can also have a movable platform configured to move the pair of Scheimpflug imaging systems in accordance with eye movement detected by an <m> eye tracking imaging system </m> .
second:
<m> Eye state tracking </m> for driving is the key problem in fatigue detection , and it has been proved to be effective and popular method in fatigue driving .


class: first -> second, base class: no relation
first:
A standard <m> survey </m> was applied in five experiments .
second:
The framework passed by an evaluation process and went available for collecting data of small enterprises through <m> digital survey platform </m> , sent by e-mail .


class: first -> second, base class: no relation
first:
A standard <m> survey </m> was applied in five experiments .
second:
<m> Shape survey </m> ( survey study ) .


class: first -> second, base class: no relation
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
The added advantage of <m> hidden layer </m> for lip reading is that it takes into account the nonlinear variation of lip features while speaking .


class: first -> second, base class: no relation
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
The experiment showed that the best <m> hidden layer architecture </m> ( HLA ) is 5 - 10 - 11 - 12 - 13 - 1 with learning function ( LF ) of trainlm , activation function ( AF ) of logsig and purelin , and learning rate ( LR ) of 0.5 .


class: first -> second, base class: no relation
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
Further <m> Hidden layer 1 </m> helps to extract data to estimate the forecasting equation and also it helps for evaluating estimated Model using different measures .


class: first -> second, base class: no relation
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
where and are the question and the answer encoded vectors and denotes the output of the <m> hidden layer </m> of the MLP .


class: first -> second, base class: no relation
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
Similar to our LoNGAE model , the GCN model can learn <m> hidden layer representations </m> that encode both local graph structure and features of nodes .


class: first -> second, base class: no relation
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
PNN : For the purpose of capturing high - order feature interactions , PNN imposes a product layer between the embedding layer and the <m> first hidden layer </m> .


class: first -> second, base class: no relation
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
Their method simultaneously minimizes classification error while improving the directness and transparency of the <m> hidden layer learning process </m> .


class: first -> second, base class: no relation
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
This was found sufficient , with the following <m> hidden layers learning </m> to combine the multi - scale features .


class: first -> second, base class: no relation
first:
The added advantage of <m> hidden layer </m> for lip reading is that it takes into account the nonlinear variation of lip features while speaking .
second:
They demonstrate large improvements in reducing the number of parameters of the output softmax layer , but only modest improvements for the <m> hidden fully connected layers </m> .


class: first -> second, base class: no relation
first:
The experiment showed that the best <m> hidden layer architecture </m> ( HLA ) is 5 - 10 - 11 - 12 - 13 - 1 with learning function ( LF ) of trainlm , activation function ( AF ) of logsig and purelin , and learning rate ( LR ) of 0.5 .
second:
They demonstrate large improvements in reducing the number of parameters of the output softmax layer , but only modest improvements for the <m> hidden fully connected layers </m> .


class: first -> second, base class: no relation
first:
Further <m> Hidden layer 1 </m> helps to extract data to estimate the forecasting equation and also it helps for evaluating estimated Model using different measures .
second:
They demonstrate large improvements in reducing the number of parameters of the output softmax layer , but only modest improvements for the <m> hidden fully connected layers </m> .


class: first -> second, base class: no relation
first:
where and are the question and the answer encoded vectors and denotes the output of the <m> hidden layer </m> of the MLP .
second:
They demonstrate large improvements in reducing the number of parameters of the output softmax layer , but only modest improvements for the <m> hidden fully connected layers </m> .


class: first -> second, base class: no relation
first:
Similar to our LoNGAE model , the GCN model can learn <m> hidden layer representations </m> that encode both local graph structure and features of nodes .
second:
They demonstrate large improvements in reducing the number of parameters of the output softmax layer , but only modest improvements for the <m> hidden fully connected layers </m> .


class: first -> second, base class: no relation
first:
PNN : For the purpose of capturing high - order feature interactions , PNN imposes a product layer between the embedding layer and the <m> first hidden layer </m> .
second:
Very recently , Ma et al. ( ICLR 2018 ) proposed to use local intrinsic dimensionality ( LID ) in <m> layer-wise hidden representations of DNNs </m> to study adversarial subspaces .


class: first -> second, base class: same cluster
first:
We review how data-analysis concepts and techniques developed by computational neuroscientists can be useful for analyzing representations in DNNs , and in turn , how recently developed techniques for <m> analysis of DNNs </m> can be useful for understanding representations in biological neural networks .
second:
This dissertation explores model compression and acceleration methods for deep neural networks to save both memory and computation resources for the <m> hardware implementation of DNNs </m> .


class: first -> second, base class: same cluster
first:
Accordingly , techniques that enable efficient <m> processing of DNNs </m> to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI system .
second:
This dissertation explores model compression and acceleration methods for deep neural networks to save both memory and computation resources for the <m> hardware implementation of DNNs </m> .


class: first -> second, base class: no relation
first:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .
second:
subsection : PixelGAN Autoencoders with <m> Categorical Priors </m> In this section , we present an architecture of the PixelGAN autoencoder that can separate the discrete information ( e.g. , class label ) from the continuous information ( e.g. , style information ) in the images .


class: first -> second, base class: no relation
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
A classic problem , the spatial movement to goal problem , is used to illustrate the power of causal learning in vastly reducing the problem solving search space involved , and this is contrasted with the traditional <m> AI A * algorithm </m> which requires a huge search space .


class: first -> second, base class: no relation
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .


class: first -> second, base class: no relation
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: first -> second, base class: no relation
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .


class: first -> second, base class: no relation
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
We choose 7 popular conferences including AAAI , CIKM , <m> ICML </m> , KDD , NIPS , SIGIR , and WWW as the classification categories .


class: first -> second, base class: no relation
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: first -> second, base class: no relation
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
A classic problem , the spatial movement to goal problem , is used to illustrate the power of causal learning in vastly reducing the problem solving search space involved , and this is contrasted with the traditional <m> AI A * algorithm </m> which requires a huge search space .


class: first -> second, base class: no relation
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .


class: first -> second, base class: no relation
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: first -> second, base class: no relation
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .


class: first -> second, base class: no relation
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
We choose 7 popular conferences including AAAI , CIKM , <m> ICML </m> , KDD , NIPS , SIGIR , and WWW as the classification categories .


class: first -> second, base class: no relation
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: first -> second, base class: no relation
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
A classic problem , the spatial movement to goal problem , is used to illustrate the power of causal learning in vastly reducing the problem solving search space involved , and this is contrasted with the traditional <m> AI A * algorithm </m> which requires a huge search space .


class: first -> second, base class: no relation
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .


class: first -> second, base class: no relation
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: first -> second, base class: no relation
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .


class: first -> second, base class: no relation
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
We choose 7 popular conferences including AAAI , CIKM , <m> ICML </m> , KDD , NIPS , SIGIR , and WWW as the classification categories .


class: first -> second, base class: no relation
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: first -> second, base class: no relation
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
A classic problem , the spatial movement to goal problem , is used to illustrate the power of causal learning in vastly reducing the problem solving search space involved , and this is contrasted with the traditional <m> AI A * algorithm </m> which requires a huge search space .


class: first -> second, base class: no relation
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .


class: first -> second, base class: no relation
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .


class: first -> second, base class: no relation
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
We choose 7 popular conferences including AAAI , CIKM , <m> ICML </m> , KDD , NIPS , SIGIR , and WWW as the classification categories .


class: first -> second, base class: no relation
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: first -> second, base class: no relation
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
A classic problem , the spatial movement to goal problem , is used to illustrate the power of causal learning in vastly reducing the problem solving search space involved , and this is contrasted with the traditional <m> AI A * algorithm </m> which requires a huge search space .


class: first -> second, base class: no relation
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .


class: first -> second, base class: no relation
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: first -> second, base class: no relation
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .


class: first -> second, base class: no relation
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
We choose 7 popular conferences including AAAI , CIKM , <m> ICML </m> , KDD , NIPS , SIGIR , and WWW as the classification categories .


class: first -> second, base class: no relation
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: first -> second, base class: no relation
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
A classic problem , the spatial movement to goal problem , is used to illustrate the power of causal learning in vastly reducing the problem solving search space involved , and this is contrasted with the traditional <m> AI A * algorithm </m> which requires a huge search space .


class: first -> second, base class: no relation
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .


class: first -> second, base class: no relation
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: first -> second, base class: no relation
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .


class: first -> second, base class: no relation
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
We choose 7 popular conferences including AAAI , CIKM , <m> ICML </m> , KDD , NIPS , SIGIR , and WWW as the classification categories .


class: first -> second, base class: no relation
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: first -> second, base class: no relation
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
A classic problem , the spatial movement to goal problem , is used to illustrate the power of causal learning in vastly reducing the problem solving search space involved , and this is contrasted with the traditional <m> AI A * algorithm </m> which requires a huge search space .


class: first -> second, base class: no relation
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .


class: first -> second, base class: no relation
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: first -> second, base class: no relation
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .


class: first -> second, base class: no relation
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
We choose 7 popular conferences including AAAI , CIKM , <m> ICML </m> , KDD , NIPS , SIGIR , and WWW as the classification categories .


class: first -> second, base class: no relation
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: first -> second, base class: no relation
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
A classic problem , the spatial movement to goal problem , is used to illustrate the power of causal learning in vastly reducing the problem solving search space involved , and this is contrasted with the traditional <m> AI A * algorithm </m> which requires a huge search space .


class: first -> second, base class: no relation
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .


class: first -> second, base class: no relation
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: first -> second, base class: no relation
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .


class: first -> second, base class: no relation
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
We choose 7 popular conferences including AAAI , CIKM , <m> ICML </m> , KDD , NIPS , SIGIR , and WWW as the classification categories .


class: first -> second, base class: no relation
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: first -> second, base class: no relation
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
A classic problem , the spatial movement to goal problem , is used to illustrate the power of causal learning in vastly reducing the problem solving search space involved , and this is contrasted with the traditional <m> AI A * algorithm </m> which requires a huge search space .


class: first -> second, base class: no relation
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .


class: first -> second, base class: no relation
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: first -> second, base class: no relation
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .


class: first -> second, base class: no relation
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
We choose 7 popular conferences including AAAI , CIKM , <m> ICML </m> , KDD , NIPS , SIGIR , and WWW as the classification categories .


class: first -> second, base class: no relation
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: first -> second, base class: no relation
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
A classic problem , the spatial movement to goal problem , is used to illustrate the power of causal learning in vastly reducing the problem solving search space involved , and this is contrasted with the traditional <m> AI A * algorithm </m> which requires a huge search space .


class: first -> second, base class: no relation
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .


class: first -> second, base class: no relation
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: first -> second, base class: no relation
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .


class: first -> second, base class: no relation
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
We choose 7 popular conferences including AAAI , CIKM , <m> ICML </m> , KDD , NIPS , SIGIR , and WWW as the classification categories .


class: first -> second, base class: no relation
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: first -> second, base class: no relation
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
A classic problem , the spatial movement to goal problem , is used to illustrate the power of causal learning in vastly reducing the problem solving search space involved , and this is contrasted with the traditional <m> AI A * algorithm </m> which requires a huge search space .


class: first -> second, base class: no relation
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .


class: first -> second, base class: no relation
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: first -> second, base class: no relation
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .


class: first -> second, base class: no relation
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
We choose 7 popular conferences including AAAI , CIKM , <m> ICML </m> , KDD , NIPS , SIGIR , and WWW as the classification categories .


class: first -> second, base class: no relation
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: first -> second, base class: no relation
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
A classic problem , the spatial movement to goal problem , is used to illustrate the power of causal learning in vastly reducing the problem solving search space involved , and this is contrasted with the traditional <m> AI A * algorithm </m> which requires a huge search space .


class: first -> second, base class: no relation
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .


class: first -> second, base class: no relation
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: first -> second, base class: no relation
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .


class: first -> second, base class: no relation
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
We choose 7 popular conferences including AAAI , CIKM , <m> ICML </m> , KDD , NIPS , SIGIR , and WWW as the classification categories .


class: first -> second, base class: no relation
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: first -> second, base class: no relation
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
A classic problem , the spatial movement to goal problem , is used to illustrate the power of causal learning in vastly reducing the problem solving search space involved , and this is contrasted with the traditional <m> AI A * algorithm </m> which requires a huge search space .


class: first -> second, base class: no relation
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .


class: first -> second, base class: no relation
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: first -> second, base class: no relation
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .


class: first -> second, base class: no relation
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
We choose 7 popular conferences including AAAI , CIKM , <m> ICML </m> , KDD , NIPS , SIGIR , and WWW as the classification categories .


class: first -> second, base class: no relation
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: first -> second, base class: no relation
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
A classic problem , the spatial movement to goal problem , is used to illustrate the power of causal learning in vastly reducing the problem solving search space involved , and this is contrasted with the traditional <m> AI A * algorithm </m> which requires a huge search space .


class: first -> second, base class: no relation
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .


class: first -> second, base class: no relation
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: first -> second, base class: no relation
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .


class: first -> second, base class: no relation
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
We choose 7 popular conferences including AAAI , CIKM , <m> ICML </m> , KDD , NIPS , SIGIR , and WWW as the classification categories .


class: first -> second, base class: no relation
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: first -> second, base class: no relation
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
A classic problem , the spatial movement to goal problem , is used to illustrate the power of causal learning in vastly reducing the problem solving search space involved , and this is contrasted with the traditional <m> AI A * algorithm </m> which requires a huge search space .


class: first -> second, base class: no relation
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .


class: first -> second, base class: no relation
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: first -> second, base class: no relation
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .


class: first -> second, base class: no relation
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
We choose 7 popular conferences including AAAI , CIKM , <m> ICML </m> , KDD , NIPS , SIGIR , and WWW as the classification categories .


class: first -> second, base class: no relation
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: first -> second, base class: no relation
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
A classic problem , the spatial movement to goal problem , is used to illustrate the power of causal learning in vastly reducing the problem solving search space involved , and this is contrasted with the traditional <m> AI A * algorithm </m> which requires a huge search space .


class: first -> second, base class: no relation
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .


class: first -> second, base class: no relation
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: first -> second, base class: no relation
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .


class: first -> second, base class: no relation
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
We choose 7 popular conferences including AAAI , CIKM , <m> ICML </m> , KDD , NIPS , SIGIR , and WWW as the classification categories .


class: first -> second, base class: no relation
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: first -> second, base class: no relation
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
A classic problem , the spatial movement to goal problem , is used to illustrate the power of causal learning in vastly reducing the problem solving search space involved , and this is contrasted with the traditional <m> AI A * algorithm </m> which requires a huge search space .


class: first -> second, base class: no relation
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .


class: first -> second, base class: no relation
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: first -> second, base class: no relation
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
This work focuses on the problem of multi-label learning with missing labels ( <m> MLML </m> ) , which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels ( i.e. some of their labels are missing ) .


class: first -> second, base class: no relation
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
We choose 7 popular conferences including AAAI , CIKM , <m> ICML </m> , KDD , NIPS , SIGIR , and WWW as the classification categories .


class: first -> second, base class: no relation
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
We adopted state - of - art graphs kernels for comparison namely : Random Walk ( RW ) , Shortest Path Kernel ( SP ) , Graphlet Kernel ( GK ) , Weisfeiler - Lehman Sub - tree Kernel ( WL ) , Deep Graph Kernels ( DGK ) and Multiscale Laplacian Graph Kernels ( <m> MLK </m> ) .


class: first -> second, base class: no relation
first:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .
second:
Based on the simple and well specified autonomous driving and navigation environment called ' Duckietown ' , <m> AI-DO </m> includes a series of tasks of increasing complexity -- from simple lane-following to fleet management .


class: first -> second, base class: no relation
first:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .
second:
Based on the simple and well specified autonomous driving and navigation environment called ' Duckietown ' , <m> AI-DO </m> includes a series of tasks of increasing complexity -- from simple lane-following to fleet management .


class: first -> second, base class: no relation
first:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .
second:
Based on the simple and well specified autonomous driving and navigation environment called ' Duckietown ' , <m> AI-DO </m> includes a series of tasks of increasing complexity -- from simple lane-following to fleet management .


class: first -> second, base class: no relation
first:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .
second:
Based on the simple and well specified autonomous driving and navigation environment called ' Duckietown ' , <m> AI-DO </m> includes a series of tasks of increasing complexity -- from simple lane-following to fleet management .


class: first -> second, base class: no relation
first:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .
second:
Based on the simple and well specified autonomous driving and navigation environment called ' Duckietown ' , <m> AI-DO </m> includes a series of tasks of increasing complexity -- from simple lane-following to fleet management .


class: first -> second, base class: no relation
first:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .
second:
Based on the simple and well specified autonomous driving and navigation environment called ' Duckietown ' , <m> AI-DO </m> includes a series of tasks of increasing complexity -- from simple lane-following to fleet management .


class: first -> second, base class: no relation
first:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .
second:
Based on the simple and well specified autonomous driving and navigation environment called ' Duckietown ' , <m> AI-DO </m> includes a series of tasks of increasing complexity -- from simple lane-following to fleet management .


class: first -> second, base class: no relation
first:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .
second:
Based on the simple and well specified autonomous driving and navigation environment called ' Duckietown ' , <m> AI-DO </m> includes a series of tasks of increasing complexity -- from simple lane-following to fleet management .


class: first -> second, base class: no relation
first:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .
second:
Based on the simple and well specified autonomous driving and navigation environment called ' Duckietown ' , <m> AI-DO </m> includes a series of tasks of increasing complexity -- from simple lane-following to fleet management .


class: first -> second, base class: no relation
first:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .
second:
Based on the simple and well specified autonomous driving and navigation environment called ' Duckietown ' , <m> AI-DO </m> includes a series of tasks of increasing complexity -- from simple lane-following to fleet management .


class: first -> second, base class: no relation
first:
Behind the API it consists of a <m> kernel </m> implementing a reference monitor which controls access to security-relevant objects and attributes based on a configurable security policy .
second:
Realistic software research environments will be provided by connecting PSIs to encourage <m> kernel language implementation </m> and parallel operating system development .


class: first -> second, base class: no relation
first:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .
second:
They can be used for <m> highest order Volterra kernel realization </m> as well .


class: first -> second, base class: no relation
first:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .
second:
In this paper , we present an algorithm that allows for different memory lengths of the <m> linear and quadratic Volterra kernel </m> while preserving the advantages of fast convolution techniques in the frequency domain for a second-order Volterra filter .


class: first -> second, base class: no relation
first:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .
second:
Subsequently , a general model that computes the ratio of two non-linear functionals , each comprising <m> linear ( first order Volterra kernel </m> ) and quadratic ( second-order Volterra kernel ) filters , is proposed .


class: first -> second, base class: no relation
first:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .
second:
It provides a set of diagnostic features formed on base of <m> multidimensional Volterra kernels </m> : discrete values of Volterra kernels , heuristic features , moments and wavelet transform coefficients .


class: first -> second, base class: no relation
first:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .
second:
Subsequently , we improved the RTLSR model into a four-parameter version ( RTLSRV4p ) with a new <m> volumetric scattering kernel </m> derived from the assumption of vertical leaf inclination .


class: first -> second, base class: no relation
first:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .
second:
The learned kernel features include : Spatio_dim kernel feature , Grad_nor kernel feature , <m> Volum_shape kernel feature </m> , and KPCA kernel feature .


class: first -> second, base class: no relation
first:
This thesis addresses TCP/IP 's application to wireless and secure area to discuss , it analyzes several problems about a few of critical parts within TCP/IP in details , simulaneously delves into slow start algorithm , initial window size , increasing policy of ini- tial window , RTO estimator , delay acknowledgement , deficiency of <m> kernel implementation </m> and security leakage and gives some constructive solutions in order to further research to pave a basis .
second:
Realistic software research environments will be provided by connecting PSIs to encourage <m> kernel language implementation </m> and parallel operating system development .


class: first -> second, base class: no relation
first:
The <m> kernel-level implementation framework </m> of VMDFS is illustrated based on Linux 2.4.22 .
second:
Realistic software research environments will be provided by connecting PSIs to encourage <m> kernel language implementation </m> and parallel operating system development .


class: first -> second, base class: no relation
first:
From the implementation perspective , we show that using application level measurements is highly CPU intensive , while a <m> kernel based implementation </m> has comparably a very low CPU usage .
second:
Realistic software research environments will be provided by connecting PSIs to encourage <m> kernel language implementation </m> and parallel operating system development .


class: first -> second, base class: no relation
first:
This paper describes a <m> kernel-mode implementation </m> and compares two different design alternatives .
second:
Realistic software research environments will be provided by connecting PSIs to encourage <m> kernel language implementation </m> and parallel operating system development .


class: first -> second, base class: no relation
first:
The principle and <m> kernel implementing technology </m> of Sybase web solution are introduced in detail .
second:
Realistic software research environments will be provided by connecting PSIs to encourage <m> kernel language implementation </m> and parallel operating system development .


class: first -> second, base class: no relation
first:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .
second:
Subsequently , we improved the RTLSR model into a four-parameter version ( RTLSRV4p ) with a new <m> volumetric scattering kernel </m> derived from the assumption of vertical leaf inclination .


class: first -> second, base class: no relation
first:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .
second:
The learned kernel features include : Spatio_dim kernel feature , Grad_nor kernel feature , <m> Volum_shape kernel feature </m> , and KPCA kernel feature .


class: first -> second, base class: no relation
first:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .
second:
Subsequently , we improved the RTLSR model into a four-parameter version ( RTLSRV4p ) with a new <m> volumetric scattering kernel </m> derived from the assumption of vertical leaf inclination .


class: first -> second, base class: no relation
first:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .
second:
The learned kernel features include : Spatio_dim kernel feature , Grad_nor kernel feature , <m> Volum_shape kernel feature </m> , and KPCA kernel feature .


class: first -> second, base class: no relation
first:
Moreover , since it does not need <m> word segmentation </m> , it can be applied to Eastern languages where they do not put clear spacing between words .
second:
The improved Chinese word segmentation is made of a bigram segmentation and a segmentation correction , new words recognition and disambiguation through the <m> bigram segmentation </m> , check the accuracy of segmentation results using the segmentation correction from the perspective of syntax .


class: first -> second, base class: second -> first
first:
Inspired by three characteristics , we develop three steps to <m> word and subword segmentation approach </m> : in the word or sub-words segmentation step , a peaks detection function is adopted to model the maximum and minimum peaks .
second:
Therefore , in this paper , we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit <m> word segmentation </m> .


class: first -> second, base class: second -> first
first:
Inspired by three characteristics , we develop three steps to <m> word and subword segmentation approach </m> : in the word or sub-words segmentation step , a peaks detection function is adopted to model the maximum and minimum peaks .
second:
This <m> word segmentation procedure </m> can be as simple as tokenization followed by some punctuation normalization , but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance .


class: first -> second, base class: second -> first
first:
Inspired by three characteristics , we develop three steps to <m> word and subword segmentation approach </m> : in the word or sub-words segmentation step , a peaks detection function is adopted to model the maximum and minimum peaks .
second:
paragraph : Why not Word - Level Translation ? The most pressing issue with word - level processing is that we do not have a perfect <m> word segmentation algorithm </m> for any one language .


class: first -> second, base class: second -> first
first:
Inspired by three characteristics , we develop three steps to <m> word and subword segmentation approach </m> : in the word or sub-words segmentation step , a peaks detection function is adopted to model the maximum and minimum peaks .
second:
We discuss the suitability of different <m> word segmentation techniques </m> , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: first -> second, base class: no relation
first:
Therefore , in this paper , we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit <m> word segmentation </m> .
second:
The improved Chinese word segmentation is made of a bigram segmentation and a segmentation correction , new words recognition and disambiguation through the <m> bigram segmentation </m> , check the accuracy of segmentation results using the segmentation correction from the perspective of syntax .


class: first -> second, base class: no relation
first:
This <m> word segmentation procedure </m> can be as simple as tokenization followed by some punctuation normalization , but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance .
second:
The improved Chinese word segmentation is made of a bigram segmentation and a segmentation correction , new words recognition and disambiguation through the <m> bigram segmentation </m> , check the accuracy of segmentation results using the segmentation correction from the perspective of syntax .


class: first -> second, base class: no relation
first:
paragraph : Why not Word - Level Translation ? The most pressing issue with word - level processing is that we do not have a perfect <m> word segmentation algorithm </m> for any one language .
second:
The improved Chinese word segmentation is made of a bigram segmentation and a segmentation correction , new words recognition and disambiguation through the <m> bigram segmentation </m> , check the accuracy of segmentation results using the segmentation correction from the perspective of syntax .


class: first -> second, base class: no relation
first:
We discuss the suitability of different <m> word segmentation techniques </m> , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .
second:
The improved Chinese word segmentation is made of a bigram segmentation and a segmentation correction , new words recognition and disambiguation through the <m> bigram segmentation </m> , check the accuracy of segmentation results using the segmentation correction from the perspective of syntax .


class: first -> second, base class: no relation
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
Additionally , we present <m> AST BPE </m> , which is a version of BPE that uses the Abstract Syntax Tree ( AST ) of the SQL statement to guide BPE merges and therefore produce BPE encodings that generalize better .


class: first -> second, base class: no relation
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
Additionally , we present <m> AST BPE </m> , which is a version of BPE that uses the Abstract Syntax Tree ( AST ) of the SQL statement to guide BPE merges and therefore produce BPE encodings that generalize better .


class: first -> second, base class: same cluster
first:
The results show that combining unigrams and PoS-filtered skipgrams leads to a significant improvement in classification scores over the <m> unigram baseline </m> .
second:
We evaluate the effect of reordering using three simple methods : unigram count ( UC ) , <m> unigram ratio </m> ( UR ) , and first four characters match ( FFCM ) .


class: first -> second, base class: same cluster
first:
Traditional approaches used <m> unigram based models </m> for text classification .
second:
We evaluate the effect of reordering using three simple methods : unigram count ( UC ) , <m> unigram ratio </m> ( UR ) , and first four characters match ( FFCM ) .


class: first -> second, base class: same cluster
first:
A <m> unigram analysis </m> of our annotated data shows that student learning correlates both with the tutor 's dialogue acts and with the student 's dialogue acts .
second:
We evaluate the effect of reordering using three simple methods : unigram count ( UC ) , <m> unigram ratio </m> ( UR ) , and first four characters match ( FFCM ) .


class: first -> second, base class: same cluster
first:
By augmenting conventional techniques of topic modeling with <m> unigram analysis </m> and community detection , we establish an automated method that generates a comprehensive and meaningful summary of forum conversations over time that also sheds light on patterns of user behavior .
second:
We evaluate the effect of reordering using three simple methods : unigram count ( UC ) , <m> unigram ratio </m> ( UR ) , and first four characters match ( FFCM ) .


class: first -> second, base class: same cluster
first:
Unbiased tweets were extracted from Twitter related to this specific campaign , and on comparing with manual tagging we were able to achieve 84.47 % accuracy using <m> unigram machine learning approach </m> .
second:
We evaluate the effect of reordering using three simple methods : unigram count ( UC ) , <m> unigram ratio </m> ( UR ) , and first four characters match ( FFCM ) .


class: first -> second, base class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .


class: first -> second, base class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .


class: first -> second, base class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .


class: first -> second, base class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .


class: first -> second, base class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .


class: first -> second, base class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
Here we investigate the network properties of the third party referral structures that facilitate gathering of user information for the <m> delivery of personalized ads </m> .


class: first -> second, base class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .


class: first -> second, base class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: first -> second, base class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
In many e-commerce applications , ranging from dynamic Web content presentation , to <m> personalized ad targeting </m> , to individual recommendations to the customers , it is important to build personalized profiles of individual users from their transactional histories .


class: first -> second, base class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
Driven by the rapid growth of large scale real-time data mining applications for <m> personalized ads </m> and content recommendations , distributed stream processing systems are widely applied in modern big-data architectures .


class: first -> second, base class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
In this paper we present an approach for the presentation of <m> personalized advertisements </m> on mobile devices , which is based on a user model that takes into account user ’s interests over time .


class: first -> second, base class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
This work takes the first step toward the <m> personalized positioning of ads </m> on article pages .


class: first -> second, base class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
Mobile devices provide a continuous data stream of contextual behavioral information which can be leveraged for a variety of user services , such as in <m> personalizing ads </m> and customizing home screens .


class: first -> second, base class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .


class: first -> second, base class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .


class: first -> second, base class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .


class: first -> second, base class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .


class: first -> second, base class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .


class: first -> second, base class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
Here we investigate the network properties of the third party referral structures that facilitate gathering of user information for the <m> delivery of personalized ads </m> .


class: first -> second, base class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .


class: first -> second, base class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: first -> second, base class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
In many e-commerce applications , ranging from dynamic Web content presentation , to <m> personalized ad targeting </m> , to individual recommendations to the customers , it is important to build personalized profiles of individual users from their transactional histories .


class: first -> second, base class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
Driven by the rapid growth of large scale real-time data mining applications for <m> personalized ads </m> and content recommendations , distributed stream processing systems are widely applied in modern big-data architectures .


class: first -> second, base class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
In this paper we present an approach for the presentation of <m> personalized advertisements </m> on mobile devices , which is based on a user model that takes into account user ’s interests over time .


class: first -> second, base class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
This work takes the first step toward the <m> personalized positioning of ads </m> on article pages .


class: first -> second, base class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
Mobile devices provide a continuous data stream of contextual behavioral information which can be leveraged for a variety of user services , such as in <m> personalizing ads </m> and customizing home screens .


class: first -> second, base class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .


class: first -> second, base class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .


class: first -> second, base class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .


class: first -> second, base class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .


class: first -> second, base class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .


class: first -> second, base class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
Here we investigate the network properties of the third party referral structures that facilitate gathering of user information for the <m> delivery of personalized ads </m> .


class: first -> second, base class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .


class: first -> second, base class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: first -> second, base class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
In many e-commerce applications , ranging from dynamic Web content presentation , to <m> personalized ad targeting </m> , to individual recommendations to the customers , it is important to build personalized profiles of individual users from their transactional histories .


class: first -> second, base class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
Driven by the rapid growth of large scale real-time data mining applications for <m> personalized ads </m> and content recommendations , distributed stream processing systems are widely applied in modern big-data architectures .


class: first -> second, base class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
In this paper we present an approach for the presentation of <m> personalized advertisements </m> on mobile devices , which is based on a user model that takes into account user ’s interests over time .


class: first -> second, base class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
This work takes the first step toward the <m> personalized positioning of ads </m> on article pages .


class: first -> second, base class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
Mobile devices provide a continuous data stream of contextual behavioral information which can be leveraged for a variety of user services , such as in <m> personalizing ads </m> and customizing home screens .


class: first -> second, base class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .


class: first -> second, base class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .


class: first -> second, base class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .


class: first -> second, base class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .


class: first -> second, base class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .


class: first -> second, base class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
Here we investigate the network properties of the third party referral structures that facilitate gathering of user information for the <m> delivery of personalized ads </m> .


class: first -> second, base class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .


class: first -> second, base class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: first -> second, base class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
In many e-commerce applications , ranging from dynamic Web content presentation , to <m> personalized ad targeting </m> , to individual recommendations to the customers , it is important to build personalized profiles of individual users from their transactional histories .


class: first -> second, base class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
Driven by the rapid growth of large scale real-time data mining applications for <m> personalized ads </m> and content recommendations , distributed stream processing systems are widely applied in modern big-data architectures .


class: first -> second, base class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
In this paper we present an approach for the presentation of <m> personalized advertisements </m> on mobile devices , which is based on a user model that takes into account user ’s interests over time .


class: first -> second, base class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
This work takes the first step toward the <m> personalized positioning of ads </m> on article pages .


class: first -> second, base class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
Mobile devices provide a continuous data stream of contextual behavioral information which can be leveraged for a variety of user services , such as in <m> personalizing ads </m> and customizing home screens .


class: first -> second, base class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .


class: first -> second, base class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .


class: first -> second, base class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .


class: first -> second, base class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .


class: first -> second, base class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .


class: first -> second, base class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
Here we investigate the network properties of the third party referral structures that facilitate gathering of user information for the <m> delivery of personalized ads </m> .


class: first -> second, base class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .


class: first -> second, base class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: first -> second, base class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
In many e-commerce applications , ranging from dynamic Web content presentation , to <m> personalized ad targeting </m> , to individual recommendations to the customers , it is important to build personalized profiles of individual users from their transactional histories .


class: first -> second, base class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
Driven by the rapid growth of large scale real-time data mining applications for <m> personalized ads </m> and content recommendations , distributed stream processing systems are widely applied in modern big-data architectures .


class: first -> second, base class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
In this paper we present an approach for the presentation of <m> personalized advertisements </m> on mobile devices , which is based on a user model that takes into account user ’s interests over time .


class: first -> second, base class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
This work takes the first step toward the <m> personalized positioning of ads </m> on article pages .


class: first -> second, base class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
Mobile devices provide a continuous data stream of contextual behavioral information which can be leveraged for a variety of user services , such as in <m> personalizing ads </m> and customizing home screens .


class: first -> second, base class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .


class: first -> second, base class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .


class: first -> second, base class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .


class: first -> second, base class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .


class: first -> second, base class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .


class: first -> second, base class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
Here we investigate the network properties of the third party referral structures that facilitate gathering of user information for the <m> delivery of personalized ads </m> .


class: first -> second, base class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .


class: first -> second, base class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: first -> second, base class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
In many e-commerce applications , ranging from dynamic Web content presentation , to <m> personalized ad targeting </m> , to individual recommendations to the customers , it is important to build personalized profiles of individual users from their transactional histories .


class: first -> second, base class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
Driven by the rapid growth of large scale real-time data mining applications for <m> personalized ads </m> and content recommendations , distributed stream processing systems are widely applied in modern big-data architectures .


class: first -> second, base class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
In this paper we present an approach for the presentation of <m> personalized advertisements </m> on mobile devices , which is based on a user model that takes into account user ’s interests over time .


class: first -> second, base class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
This work takes the first step toward the <m> personalized positioning of ads </m> on article pages .


class: first -> second, base class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
Mobile devices provide a continuous data stream of contextual behavioral information which can be leveraged for a variety of user services , such as in <m> personalizing ads </m> and customizing home screens .


class: first -> second, base class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .


class: first -> second, base class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .


class: first -> second, base class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .


class: first -> second, base class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .


class: first -> second, base class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .


class: first -> second, base class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
Here we investigate the network properties of the third party referral structures that facilitate gathering of user information for the <m> delivery of personalized ads </m> .


class: first -> second, base class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .


class: first -> second, base class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: first -> second, base class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
In many e-commerce applications , ranging from dynamic Web content presentation , to <m> personalized ad targeting </m> , to individual recommendations to the customers , it is important to build personalized profiles of individual users from their transactional histories .


class: first -> second, base class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
Driven by the rapid growth of large scale real-time data mining applications for <m> personalized ads </m> and content recommendations , distributed stream processing systems are widely applied in modern big-data architectures .


class: first -> second, base class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
In this paper we present an approach for the presentation of <m> personalized advertisements </m> on mobile devices , which is based on a user model that takes into account user ’s interests over time .


class: first -> second, base class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
This work takes the first step toward the <m> personalized positioning of ads </m> on article pages .


class: first -> second, base class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
Mobile devices provide a continuous data stream of contextual behavioral information which can be leveraged for a variety of user services , such as in <m> personalizing ads </m> and customizing home screens .


class: first -> second, base class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .


class: first -> second, base class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .


class: first -> second, base class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .


class: first -> second, base class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .


class: first -> second, base class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .


class: first -> second, base class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
Here we investigate the network properties of the third party referral structures that facilitate gathering of user information for the <m> delivery of personalized ads </m> .


class: first -> second, base class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .


class: first -> second, base class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: first -> second, base class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
In many e-commerce applications , ranging from dynamic Web content presentation , to <m> personalized ad targeting </m> , to individual recommendations to the customers , it is important to build personalized profiles of individual users from their transactional histories .


class: first -> second, base class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
Driven by the rapid growth of large scale real-time data mining applications for <m> personalized ads </m> and content recommendations , distributed stream processing systems are widely applied in modern big-data architectures .


class: first -> second, base class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
In this paper we present an approach for the presentation of <m> personalized advertisements </m> on mobile devices , which is based on a user model that takes into account user ’s interests over time .


class: first -> second, base class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
This work takes the first step toward the <m> personalized positioning of ads </m> on article pages .


class: first -> second, base class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
Mobile devices provide a continuous data stream of contextual behavioral information which can be leveraged for a variety of user services , such as in <m> personalizing ads </m> and customizing home screens .


class: first -> second, base class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .


class: first -> second, base class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .


class: first -> second, base class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .


class: first -> second, base class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .


class: first -> second, base class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .


class: first -> second, base class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
Here we investigate the network properties of the third party referral structures that facilitate gathering of user information for the <m> delivery of personalized ads </m> .


class: first -> second, base class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .


class: first -> second, base class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: first -> second, base class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
In many e-commerce applications , ranging from dynamic Web content presentation , to <m> personalized ad targeting </m> , to individual recommendations to the customers , it is important to build personalized profiles of individual users from their transactional histories .


class: first -> second, base class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
Driven by the rapid growth of large scale real-time data mining applications for <m> personalized ads </m> and content recommendations , distributed stream processing systems are widely applied in modern big-data architectures .


class: first -> second, base class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
In this paper we present an approach for the presentation of <m> personalized advertisements </m> on mobile devices , which is based on a user model that takes into account user ’s interests over time .


class: first -> second, base class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
This work takes the first step toward the <m> personalized positioning of ads </m> on article pages .


class: first -> second, base class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
Mobile devices provide a continuous data stream of contextual behavioral information which can be leveraged for a variety of user services , such as in <m> personalizing ads </m> and customizing home screens .


class: first -> second, base class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .


class: first -> second, base class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .


class: first -> second, base class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .


class: first -> second, base class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .


class: first -> second, base class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .


class: first -> second, base class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
Here we investigate the network properties of the third party referral structures that facilitate gathering of user information for the <m> delivery of personalized ads </m> .


class: first -> second, base class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .


class: first -> second, base class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: first -> second, base class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
In many e-commerce applications , ranging from dynamic Web content presentation , to <m> personalized ad targeting </m> , to individual recommendations to the customers , it is important to build personalized profiles of individual users from their transactional histories .


class: first -> second, base class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
Driven by the rapid growth of large scale real-time data mining applications for <m> personalized ads </m> and content recommendations , distributed stream processing systems are widely applied in modern big-data architectures .


class: first -> second, base class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
In this paper we present an approach for the presentation of <m> personalized advertisements </m> on mobile devices , which is based on a user model that takes into account user ’s interests over time .


class: first -> second, base class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
This work takes the first step toward the <m> personalized positioning of ads </m> on article pages .


class: first -> second, base class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
Mobile devices provide a continuous data stream of contextual behavioral information which can be leveraged for a variety of user services , such as in <m> personalizing ads </m> and customizing home screens .


class: first -> second, base class: no relation
first:
We discuss in detail the effects of roles on each NE and on semantic categories of noun and verb features in terms of accuracy , precision/recall and F-score measures for the <m> text classification task </m> .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
We discuss in detail the effects of roles on each NE and on semantic categories of noun and verb features in terms of accuracy , precision/recall and F-score measures for the <m> text classification task </m> .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
We discuss in detail the effects of roles on each NE and on semantic categories of noun and verb features in terms of accuracy , precision/recall and F-score measures for the <m> text classification task </m> .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
We discuss in detail the effects of roles on each NE and on semantic categories of noun and verb features in terms of accuracy , precision/recall and F-score measures for the <m> text classification task </m> .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
We discuss in detail the effects of roles on each NE and on semantic categories of noun and verb features in terms of accuracy , precision/recall and F-score measures for the <m> text classification task </m> .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
We discuss in detail the effects of roles on each NE and on semantic categories of noun and verb features in terms of accuracy , precision/recall and F-score measures for the <m> text classification task </m> .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
We discuss in detail the effects of roles on each NE and on semantic categories of noun and verb features in terms of accuracy , precision/recall and F-score measures for the <m> text classification task </m> .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
We discuss in detail the effects of roles on each NE and on semantic categories of noun and verb features in terms of accuracy , precision/recall and F-score measures for the <m> text classification task </m> .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
We discuss in detail the effects of roles on each NE and on semantic categories of noun and verb features in terms of accuracy , precision/recall and F-score measures for the <m> text classification task </m> .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
In this paper , we address the <m> text classification problem </m> that a period of time created test data is different from the training data , and present a method for text classification based on temporal adaptation .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
In this paper , we address the <m> text classification problem </m> that a period of time created test data is different from the training data , and present a method for text classification based on temporal adaptation .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
In this paper , we address the <m> text classification problem </m> that a period of time created test data is different from the training data , and present a method for text classification based on temporal adaptation .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
In this paper , we address the <m> text classification problem </m> that a period of time created test data is different from the training data , and present a method for text classification based on temporal adaptation .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
In this paper , we address the <m> text classification problem </m> that a period of time created test data is different from the training data , and present a method for text classification based on temporal adaptation .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
In this paper , we address the <m> text classification problem </m> that a period of time created test data is different from the training data , and present a method for text classification based on temporal adaptation .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
In this paper , we address the <m> text classification problem </m> that a period of time created test data is different from the training data , and present a method for text classification based on temporal adaptation .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
In this paper , we address the <m> text classification problem </m> that a period of time created test data is different from the training data , and present a method for text classification based on temporal adaptation .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
In this paper , we address the <m> text classification problem </m> that a period of time created test data is different from the training data , and present a method for text classification based on temporal adaptation .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
Given a small subset of relations between the individuals , the problem of learning social network is translated into a <m> text classification problem </m> .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
Given a small subset of relations between the individuals , the problem of learning social network is translated into a <m> text classification problem </m> .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
Given a small subset of relations between the individuals , the problem of learning social network is translated into a <m> text classification problem </m> .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
Given a small subset of relations between the individuals , the problem of learning social network is translated into a <m> text classification problem </m> .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
Given a small subset of relations between the individuals , the problem of learning social network is translated into a <m> text classification problem </m> .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
Given a small subset of relations between the individuals , the problem of learning social network is translated into a <m> text classification problem </m> .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
Given a small subset of relations between the individuals , the problem of learning social network is translated into a <m> text classification problem </m> .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
Given a small subset of relations between the individuals , the problem of learning social network is translated into a <m> text classification problem </m> .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
Given a small subset of relations between the individuals , the problem of learning social network is translated into a <m> text classification problem </m> .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
We present a geo-enriched classifier joining established methods for <m> text-based classification </m> with location-based topic prediction .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
We present a geo-enriched classifier joining established methods for <m> text-based classification </m> with location-based topic prediction .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
We present a geo-enriched classifier joining established methods for <m> text-based classification </m> with location-based topic prediction .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
We present a geo-enriched classifier joining established methods for <m> text-based classification </m> with location-based topic prediction .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
We present a geo-enriched classifier joining established methods for <m> text-based classification </m> with location-based topic prediction .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
We present a geo-enriched classifier joining established methods for <m> text-based classification </m> with location-based topic prediction .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
We present a geo-enriched classifier joining established methods for <m> text-based classification </m> with location-based topic prediction .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
We present a geo-enriched classifier joining established methods for <m> text-based classification </m> with location-based topic prediction .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
We present a geo-enriched classifier joining established methods for <m> text-based classification </m> with location-based topic prediction .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
Using a matrix of closeness and a set of additional fusion rules , the proposed method improves the classification performance by only subjecting likely misclassified samples to a text-based classifier followed by additional fusion of both image-based classification and <m> text-based classification </m> results .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
Using a matrix of closeness and a set of additional fusion rules , the proposed method improves the classification performance by only subjecting likely misclassified samples to a text-based classifier followed by additional fusion of both image-based classification and <m> text-based classification </m> results .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
Using a matrix of closeness and a set of additional fusion rules , the proposed method improves the classification performance by only subjecting likely misclassified samples to a text-based classifier followed by additional fusion of both image-based classification and <m> text-based classification </m> results .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
Using a matrix of closeness and a set of additional fusion rules , the proposed method improves the classification performance by only subjecting likely misclassified samples to a text-based classifier followed by additional fusion of both image-based classification and <m> text-based classification </m> results .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
Using a matrix of closeness and a set of additional fusion rules , the proposed method improves the classification performance by only subjecting likely misclassified samples to a text-based classifier followed by additional fusion of both image-based classification and <m> text-based classification </m> results .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
Using a matrix of closeness and a set of additional fusion rules , the proposed method improves the classification performance by only subjecting likely misclassified samples to a text-based classifier followed by additional fusion of both image-based classification and <m> text-based classification </m> results .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
Using a matrix of closeness and a set of additional fusion rules , the proposed method improves the classification performance by only subjecting likely misclassified samples to a text-based classifier followed by additional fusion of both image-based classification and <m> text-based classification </m> results .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
Using a matrix of closeness and a set of additional fusion rules , the proposed method improves the classification performance by only subjecting likely misclassified samples to a text-based classifier followed by additional fusion of both image-based classification and <m> text-based classification </m> results .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
Using a matrix of closeness and a set of additional fusion rules , the proposed method improves the classification performance by only subjecting likely misclassified samples to a text-based classifier followed by additional fusion of both image-based classification and <m> text-based classification </m> results .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
The model of <m> text classifying </m> using non-labeled training sample with more extensive application is realized .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
The model of <m> text classifying </m> using non-labeled training sample with more extensive application is realized .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
The model of <m> text classifying </m> using non-labeled training sample with more extensive application is realized .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
The model of <m> text classifying </m> using non-labeled training sample with more extensive application is realized .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
The model of <m> text classifying </m> using non-labeled training sample with more extensive application is realized .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
The model of <m> text classifying </m> using non-labeled training sample with more extensive application is realized .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
The model of <m> text classifying </m> using non-labeled training sample with more extensive application is realized .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
The model of <m> text classifying </m> using non-labeled training sample with more extensive application is realized .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
The model of <m> text classifying </m> using non-labeled training sample with more extensive application is realized .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
For example , in <m> text data classification </m> , groups of documents of different types are categorized by different subsets of terms .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
For example , in <m> text data classification </m> , groups of documents of different types are categorized by different subsets of terms .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
For example , in <m> text data classification </m> , groups of documents of different types are categorized by different subsets of terms .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
For example , in <m> text data classification </m> , groups of documents of different types are categorized by different subsets of terms .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
For example , in <m> text data classification </m> , groups of documents of different types are categorized by different subsets of terms .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
For example , in <m> text data classification </m> , groups of documents of different types are categorized by different subsets of terms .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
For example , in <m> text data classification </m> , groups of documents of different types are categorized by different subsets of terms .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
For example , in <m> text data classification </m> , groups of documents of different types are categorized by different subsets of terms .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
For example , in <m> text data classification </m> , groups of documents of different types are categorized by different subsets of terms .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
This study contributes to the field of network troubleshooting , and the field of <m> text data classification </m> .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
This study contributes to the field of network troubleshooting , and the field of <m> text data classification </m> .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
This study contributes to the field of network troubleshooting , and the field of <m> text data classification </m> .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
This study contributes to the field of network troubleshooting , and the field of <m> text data classification </m> .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
This study contributes to the field of network troubleshooting , and the field of <m> text data classification </m> .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
This study contributes to the field of network troubleshooting , and the field of <m> text data classification </m> .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
This study contributes to the field of network troubleshooting , and the field of <m> text data classification </m> .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
This study contributes to the field of network troubleshooting , and the field of <m> text data classification </m> .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
This study contributes to the field of network troubleshooting , and the field of <m> text data classification </m> .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
Abstract The genetic selection of keyword sets , the text frequencies of which are considered as attributes in <m> text classification analysis </m> , has been analysed .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
Abstract The genetic selection of keyword sets , the text frequencies of which are considered as attributes in <m> text classification analysis </m> , has been analysed .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
Abstract The genetic selection of keyword sets , the text frequencies of which are considered as attributes in <m> text classification analysis </m> , has been analysed .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
Abstract The genetic selection of keyword sets , the text frequencies of which are considered as attributes in <m> text classification analysis </m> , has been analysed .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
Abstract The genetic selection of keyword sets , the text frequencies of which are considered as attributes in <m> text classification analysis </m> , has been analysed .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
Abstract The genetic selection of keyword sets , the text frequencies of which are considered as attributes in <m> text classification analysis </m> , has been analysed .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
Abstract The genetic selection of keyword sets , the text frequencies of which are considered as attributes in <m> text classification analysis </m> , has been analysed .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
Abstract The genetic selection of keyword sets , the text frequencies of which are considered as attributes in <m> text classification analysis </m> , has been analysed .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
Abstract The genetic selection of keyword sets , the text frequencies of which are considered as attributes in <m> text classification analysis </m> , has been analysed .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
In <m> Text Classification of Methods </m> , there is a sentence “ Where P(B|A ) is the probability of B appearance when A is known ?
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
In <m> Text Classification of Methods </m> , there is a sentence “ Where P(B|A ) is the probability of B appearance when A is known ?
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
In <m> Text Classification of Methods </m> , there is a sentence “ Where P(B|A ) is the probability of B appearance when A is known ?
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
In <m> Text Classification of Methods </m> , there is a sentence “ Where P(B|A ) is the probability of B appearance when A is known ?
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
In <m> Text Classification of Methods </m> , there is a sentence “ Where P(B|A ) is the probability of B appearance when A is known ?
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
In <m> Text Classification of Methods </m> , there is a sentence “ Where P(B|A ) is the probability of B appearance when A is known ?
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
In <m> Text Classification of Methods </m> , there is a sentence “ Where P(B|A ) is the probability of B appearance when A is known ?
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
In <m> Text Classification of Methods </m> , there is a sentence “ Where P(B|A ) is the probability of B appearance when A is known ?
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
In <m> Text Classification of Methods </m> , there is a sentence “ Where P(B|A ) is the probability of B appearance when A is known ?
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
<m> TEXT CLASSIFICATION Text classification </m> is a commonly used technique as a basis for applications in document processing and Visualization , web mining , surveillance technology , patent Analysis , etc .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
<m> TEXT CLASSIFICATION Text classification </m> is a commonly used technique as a basis for applications in document processing and Visualization , web mining , surveillance technology , patent Analysis , etc .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
<m> TEXT CLASSIFICATION Text classification </m> is a commonly used technique as a basis for applications in document processing and Visualization , web mining , surveillance technology , patent Analysis , etc .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
<m> TEXT CLASSIFICATION Text classification </m> is a commonly used technique as a basis for applications in document processing and Visualization , web mining , surveillance technology , patent Analysis , etc .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
<m> TEXT CLASSIFICATION Text classification </m> is a commonly used technique as a basis for applications in document processing and Visualization , web mining , surveillance technology , patent Analysis , etc .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
<m> TEXT CLASSIFICATION Text classification </m> is a commonly used technique as a basis for applications in document processing and Visualization , web mining , surveillance technology , patent Analysis , etc .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
<m> TEXT CLASSIFICATION Text classification </m> is a commonly used technique as a basis for applications in document processing and Visualization , web mining , surveillance technology , patent Analysis , etc .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
<m> TEXT CLASSIFICATION Text classification </m> is a commonly used technique as a basis for applications in document processing and Visualization , web mining , surveillance technology , patent Analysis , etc .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
<m> TEXT CLASSIFICATION Text classification </m> is a commonly used technique as a basis for applications in document processing and Visualization , web mining , surveillance technology , patent Analysis , etc .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
However , how well do these models perform on practical <m> text classification problems </m> , with real world data ?
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
However , how well do these models perform on practical <m> text classification problems </m> , with real world data ?
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
However , how well do these models perform on practical <m> text classification problems </m> , with real world data ?
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
However , how well do these models perform on practical <m> text classification problems </m> , with real world data ?
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
However , how well do these models perform on practical <m> text classification problems </m> , with real world data ?
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
However , how well do these models perform on practical <m> text classification problems </m> , with real world data ?
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
However , how well do these models perform on practical <m> text classification problems </m> , with real world data ?
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
However , how well do these models perform on practical <m> text classification problems </m> , with real world data ?
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
However , how well do these models perform on practical <m> text classification problems </m> , with real world data ?
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
It is difficult to fit a single model for <m> text classification </m> across domains , due to unknown words , specialized context , colloquial language , and other differences between domains .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
It is difficult to fit a single model for <m> text classification </m> across domains , due to unknown words , specialized context , colloquial language , and other differences between domains .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
It is difficult to fit a single model for <m> text classification </m> across domains , due to unknown words , specialized context , colloquial language , and other differences between domains .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
It is difficult to fit a single model for <m> text classification </m> across domains , due to unknown words , specialized context , colloquial language , and other differences between domains .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
It is difficult to fit a single model for <m> text classification </m> across domains , due to unknown words , specialized context , colloquial language , and other differences between domains .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
It is difficult to fit a single model for <m> text classification </m> across domains , due to unknown words , specialized context , colloquial language , and other differences between domains .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
It is difficult to fit a single model for <m> text classification </m> across domains , due to unknown words , specialized context , colloquial language , and other differences between domains .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
It is difficult to fit a single model for <m> text classification </m> across domains , due to unknown words , specialized context , colloquial language , and other differences between domains .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
It is difficult to fit a single model for <m> text classification </m> across domains , due to unknown words , specialized context , colloquial language , and other differences between domains .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
section : Conclusion In this work we demonstrate that unsupervised pretraining and finetuning provides a flexible framework that is effective for difficult <m> text classification tasks </m> .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
section : Conclusion In this work we demonstrate that unsupervised pretraining and finetuning provides a flexible framework that is effective for difficult <m> text classification tasks </m> .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
section : Conclusion In this work we demonstrate that unsupervised pretraining and finetuning provides a flexible framework that is effective for difficult <m> text classification tasks </m> .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
section : Conclusion In this work we demonstrate that unsupervised pretraining and finetuning provides a flexible framework that is effective for difficult <m> text classification tasks </m> .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
section : Conclusion In this work we demonstrate that unsupervised pretraining and finetuning provides a flexible framework that is effective for difficult <m> text classification tasks </m> .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
section : Conclusion In this work we demonstrate that unsupervised pretraining and finetuning provides a flexible framework that is effective for difficult <m> text classification tasks </m> .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
section : Conclusion In this work we demonstrate that unsupervised pretraining and finetuning provides a flexible framework that is effective for difficult <m> text classification tasks </m> .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
section : Conclusion In this work we demonstrate that unsupervised pretraining and finetuning provides a flexible framework that is effective for difficult <m> text classification tasks </m> .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
section : Conclusion In this work we demonstrate that unsupervised pretraining and finetuning provides a flexible framework that is effective for difficult <m> text classification tasks </m> .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
This paper studies CNN on <m> text categorization </m> to exploit the 1D structure ( namely , word order ) of text data for accurate prediction .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
This paper studies CNN on <m> text categorization </m> to exploit the 1D structure ( namely , word order ) of text data for accurate prediction .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
This paper studies CNN on <m> text categorization </m> to exploit the 1D structure ( namely , word order ) of text data for accurate prediction .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
This paper studies CNN on <m> text categorization </m> to exploit the 1D structure ( namely , word order ) of text data for accurate prediction .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
This paper studies CNN on <m> text categorization </m> to exploit the 1D structure ( namely , word order ) of text data for accurate prediction .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
This paper studies CNN on <m> text categorization </m> to exploit the 1D structure ( namely , word order ) of text data for accurate prediction .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
This paper studies CNN on <m> text categorization </m> to exploit the 1D structure ( namely , word order ) of text data for accurate prediction .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
This paper studies CNN on <m> text categorization </m> to exploit the 1D structure ( namely , word order ) of text data for accurate prediction .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
This paper studies CNN on <m> text categorization </m> to exploit the 1D structure ( namely , word order ) of text data for accurate prediction .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
Learning distributed sentence representation through neural network models requires little external domain knowledge and can reach satisfactory results in related tasks like sentiment classification , <m> text categorization </m> .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
Learning distributed sentence representation through neural network models requires little external domain knowledge and can reach satisfactory results in related tasks like sentiment classification , <m> text categorization </m> .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
Learning distributed sentence representation through neural network models requires little external domain knowledge and can reach satisfactory results in related tasks like sentiment classification , <m> text categorization </m> .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
Learning distributed sentence representation through neural network models requires little external domain knowledge and can reach satisfactory results in related tasks like sentiment classification , <m> text categorization </m> .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
Learning distributed sentence representation through neural network models requires little external domain knowledge and can reach satisfactory results in related tasks like sentiment classification , <m> text categorization </m> .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
Learning distributed sentence representation through neural network models requires little external domain knowledge and can reach satisfactory results in related tasks like sentiment classification , <m> text categorization </m> .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
Learning distributed sentence representation through neural network models requires little external domain knowledge and can reach satisfactory results in related tasks like sentiment classification , <m> text categorization </m> .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
Learning distributed sentence representation through neural network models requires little external domain knowledge and can reach satisfactory results in related tasks like sentiment classification , <m> text categorization </m> .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
Learning distributed sentence representation through neural network models requires little external domain knowledge and can reach satisfactory results in related tasks like sentiment classification , <m> text categorization </m> .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
Meanwhile , linear classifiers are often considered as strong baselines for <m> text classification problems </m> .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
Meanwhile , linear classifiers are often considered as strong baselines for <m> text classification problems </m> .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
Meanwhile , linear classifiers are often considered as strong baselines for <m> text classification problems </m> .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
Meanwhile , linear classifiers are often considered as strong baselines for <m> text classification problems </m> .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
Meanwhile , linear classifiers are often considered as strong baselines for <m> text classification problems </m> .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
Meanwhile , linear classifiers are often considered as strong baselines for <m> text classification problems </m> .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
Meanwhile , linear classifiers are often considered as strong baselines for <m> text classification problems </m> .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
Meanwhile , linear classifiers are often considered as strong baselines for <m> text classification problems </m> .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
Meanwhile , linear classifiers are often considered as strong baselines for <m> text classification problems </m> .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
In order to test the applicability of the obtained ontologies , a <m> text categorization </m> experiment has been proposed and the obtained results indicate that the approach can be applied with satisfactory results and warrants further research .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
In order to test the applicability of the obtained ontologies , a <m> text categorization </m> experiment has been proposed and the obtained results indicate that the approach can be applied with satisfactory results and warrants further research .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
In order to test the applicability of the obtained ontologies , a <m> text categorization </m> experiment has been proposed and the obtained results indicate that the approach can be applied with satisfactory results and warrants further research .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
In order to test the applicability of the obtained ontologies , a <m> text categorization </m> experiment has been proposed and the obtained results indicate that the approach can be applied with satisfactory results and warrants further research .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
In order to test the applicability of the obtained ontologies , a <m> text categorization </m> experiment has been proposed and the obtained results indicate that the approach can be applied with satisfactory results and warrants further research .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
In order to test the applicability of the obtained ontologies , a <m> text categorization </m> experiment has been proposed and the obtained results indicate that the approach can be applied with satisfactory results and warrants further research .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
In order to test the applicability of the obtained ontologies , a <m> text categorization </m> experiment has been proposed and the obtained results indicate that the approach can be applied with satisfactory results and warrants further research .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
In order to test the applicability of the obtained ontologies , a <m> text categorization </m> experiment has been proposed and the obtained results indicate that the approach can be applied with satisfactory results and warrants further research .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
In order to test the applicability of the obtained ontologies , a <m> text categorization </m> experiment has been proposed and the obtained results indicate that the approach can be applied with satisfactory results and warrants further research .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
This paper describes a study on <m> text categorization </m> using a character n­grams approach for the morphological normalization .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
This paper describes a study on <m> text categorization </m> using a character n­grams approach for the morphological normalization .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
This paper describes a study on <m> text categorization </m> using a character n­grams approach for the morphological normalization .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
This paper describes a study on <m> text categorization </m> using a character n­grams approach for the morphological normalization .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
This paper describes a study on <m> text categorization </m> using a character n­grams approach for the morphological normalization .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
This paper describes a study on <m> text categorization </m> using a character n­grams approach for the morphological normalization .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
This paper describes a study on <m> text categorization </m> using a character n­grams approach for the morphological normalization .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
This paper describes a study on <m> text categorization </m> using a character n­grams approach for the morphological normalization .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
This paper describes a study on <m> text categorization </m> using a character n­grams approach for the morphological normalization .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
In contrast to previous <m> text categorization research </m> in e-rulemaking [ 5 , 6 ] , and in an attempt to more closely duplicate the comment analysis process in federal agencies , we employ a set of rule-specific categories , each of which corresponds to a significant issue raised in the comments .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
In contrast to previous <m> text categorization research </m> in e-rulemaking [ 5 , 6 ] , and in an attempt to more closely duplicate the comment analysis process in federal agencies , we employ a set of rule-specific categories , each of which corresponds to a significant issue raised in the comments .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
In contrast to previous <m> text categorization research </m> in e-rulemaking [ 5 , 6 ] , and in an attempt to more closely duplicate the comment analysis process in federal agencies , we employ a set of rule-specific categories , each of which corresponds to a significant issue raised in the comments .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
In contrast to previous <m> text categorization research </m> in e-rulemaking [ 5 , 6 ] , and in an attempt to more closely duplicate the comment analysis process in federal agencies , we employ a set of rule-specific categories , each of which corresponds to a significant issue raised in the comments .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
In contrast to previous <m> text categorization research </m> in e-rulemaking [ 5 , 6 ] , and in an attempt to more closely duplicate the comment analysis process in federal agencies , we employ a set of rule-specific categories , each of which corresponds to a significant issue raised in the comments .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
In contrast to previous <m> text categorization research </m> in e-rulemaking [ 5 , 6 ] , and in an attempt to more closely duplicate the comment analysis process in federal agencies , we employ a set of rule-specific categories , each of which corresponds to a significant issue raised in the comments .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
In contrast to previous <m> text categorization research </m> in e-rulemaking [ 5 , 6 ] , and in an attempt to more closely duplicate the comment analysis process in federal agencies , we employ a set of rule-specific categories , each of which corresponds to a significant issue raised in the comments .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
In contrast to previous <m> text categorization research </m> in e-rulemaking [ 5 , 6 ] , and in an attempt to more closely duplicate the comment analysis process in federal agencies , we employ a set of rule-specific categories , each of which corresponds to a significant issue raised in the comments .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
In contrast to previous <m> text categorization research </m> in e-rulemaking [ 5 , 6 ] , and in an attempt to more closely duplicate the comment analysis process in federal agencies , we employ a set of rule-specific categories , each of which corresponds to a significant issue raised in the comments .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
Therefore , <m> text categorization research </m> has become more important .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
Therefore , <m> text categorization research </m> has become more important .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
Therefore , <m> text categorization research </m> has become more important .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
Therefore , <m> text categorization research </m> has become more important .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
Therefore , <m> text categorization research </m> has become more important .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
Therefore , <m> text categorization research </m> has become more important .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
Therefore , <m> text categorization research </m> has become more important .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
Therefore , <m> text categorization research </m> has become more important .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
Therefore , <m> text categorization research </m> has become more important .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
This method can improve the accuracy of <m> text categorizing </m> efficiently , which is proved by experiments .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
This method can improve the accuracy of <m> text categorizing </m> efficiently , which is proved by experiments .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
This method can improve the accuracy of <m> text categorizing </m> efficiently , which is proved by experiments .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
This method can improve the accuracy of <m> text categorizing </m> efficiently , which is proved by experiments .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
This method can improve the accuracy of <m> text categorizing </m> efficiently , which is proved by experiments .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
This method can improve the accuracy of <m> text categorizing </m> efficiently , which is proved by experiments .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
This method can improve the accuracy of <m> text categorizing </m> efficiently , which is proved by experiments .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
This method can improve the accuracy of <m> text categorizing </m> efficiently , which is proved by experiments .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
This method can improve the accuracy of <m> text categorizing </m> efficiently , which is proved by experiments .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
Making authoritative corpus and standard of <m> text categorizing </m> are suggested for evaluation and application of text categorizing model .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
Making authoritative corpus and standard of <m> text categorizing </m> are suggested for evaluation and application of text categorizing model .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
Making authoritative corpus and standard of <m> text categorizing </m> are suggested for evaluation and application of text categorizing model .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
Making authoritative corpus and standard of <m> text categorizing </m> are suggested for evaluation and application of text categorizing model .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
Making authoritative corpus and standard of <m> text categorizing </m> are suggested for evaluation and application of text categorizing model .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
Making authoritative corpus and standard of <m> text categorizing </m> are suggested for evaluation and application of text categorizing model .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
Making authoritative corpus and standard of <m> text categorizing </m> are suggested for evaluation and application of text categorizing model .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
Making authoritative corpus and standard of <m> text categorizing </m> are suggested for evaluation and application of text categorizing model .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
Making authoritative corpus and standard of <m> text categorizing </m> are suggested for evaluation and application of text categorizing model .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
In direct proportion to the heavy increase of online information data , the attention to <m> text categorization ( classification </m> ) has also increased .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
In direct proportion to the heavy increase of online information data , the attention to <m> text categorization ( classification </m> ) has also increased .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
In direct proportion to the heavy increase of online information data , the attention to <m> text categorization ( classification </m> ) has also increased .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
In direct proportion to the heavy increase of online information data , the attention to <m> text categorization ( classification </m> ) has also increased .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
In direct proportion to the heavy increase of online information data , the attention to <m> text categorization ( classification </m> ) has also increased .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
In direct proportion to the heavy increase of online information data , the attention to <m> text categorization ( classification </m> ) has also increased .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
In direct proportion to the heavy increase of online information data , the attention to <m> text categorization ( classification </m> ) has also increased .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
In direct proportion to the heavy increase of online information data , the attention to <m> text categorization ( classification </m> ) has also increased .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
In direct proportion to the heavy increase of online information data , the attention to <m> text categorization ( classification </m> ) has also increased .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
While the former problem may be addressed using one of the well known techniques of <m> text categorization ( classification </m> ) , the latter seems to require some distinct approaches due to the fact that the set of cases is unknown in advance , as well as due to the assumed limited number of training documents , if a case should be interpreted as a classic category .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
While the former problem may be addressed using one of the well known techniques of <m> text categorization ( classification </m> ) , the latter seems to require some distinct approaches due to the fact that the set of cases is unknown in advance , as well as due to the assumed limited number of training documents , if a case should be interpreted as a classic category .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
While the former problem may be addressed using one of the well known techniques of <m> text categorization ( classification </m> ) , the latter seems to require some distinct approaches due to the fact that the set of cases is unknown in advance , as well as due to the assumed limited number of training documents , if a case should be interpreted as a classic category .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
While the former problem may be addressed using one of the well known techniques of <m> text categorization ( classification </m> ) , the latter seems to require some distinct approaches due to the fact that the set of cases is unknown in advance , as well as due to the assumed limited number of training documents , if a case should be interpreted as a classic category .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
While the former problem may be addressed using one of the well known techniques of <m> text categorization ( classification </m> ) , the latter seems to require some distinct approaches due to the fact that the set of cases is unknown in advance , as well as due to the assumed limited number of training documents , if a case should be interpreted as a classic category .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
While the former problem may be addressed using one of the well known techniques of <m> text categorization ( classification </m> ) , the latter seems to require some distinct approaches due to the fact that the set of cases is unknown in advance , as well as due to the assumed limited number of training documents , if a case should be interpreted as a classic category .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
While the former problem may be addressed using one of the well known techniques of <m> text categorization ( classification </m> ) , the latter seems to require some distinct approaches due to the fact that the set of cases is unknown in advance , as well as due to the assumed limited number of training documents , if a case should be interpreted as a classic category .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
While the former problem may be addressed using one of the well known techniques of <m> text categorization ( classification </m> ) , the latter seems to require some distinct approaches due to the fact that the set of cases is unknown in advance , as well as due to the assumed limited number of training documents , if a case should be interpreted as a classic category .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
While the former problem may be addressed using one of the well known techniques of <m> text categorization ( classification </m> ) , the latter seems to require some distinct approaches due to the fact that the set of cases is unknown in advance , as well as due to the assumed limited number of training documents , if a case should be interpreted as a classic category .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
The use of word embedding models and deep learning algorithms are currently the most common and popular trends to enhance the overall performance of a <m> text classification/categorization system </m> .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
The use of word embedding models and deep learning algorithms are currently the most common and popular trends to enhance the overall performance of a <m> text classification/categorization system </m> .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
The use of word embedding models and deep learning algorithms are currently the most common and popular trends to enhance the overall performance of a <m> text classification/categorization system </m> .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
The use of word embedding models and deep learning algorithms are currently the most common and popular trends to enhance the overall performance of a <m> text classification/categorization system </m> .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
The use of word embedding models and deep learning algorithms are currently the most common and popular trends to enhance the overall performance of a <m> text classification/categorization system </m> .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
The use of word embedding models and deep learning algorithms are currently the most common and popular trends to enhance the overall performance of a <m> text classification/categorization system </m> .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
The use of word embedding models and deep learning algorithms are currently the most common and popular trends to enhance the overall performance of a <m> text classification/categorization system </m> .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
The use of word embedding models and deep learning algorithms are currently the most common and popular trends to enhance the overall performance of a <m> text classification/categorization system </m> .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
The use of word embedding models and deep learning algorithms are currently the most common and popular trends to enhance the overall performance of a <m> text classification/categorization system </m> .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
For <m> text classification/categorization </m> , we investigate a steepest descent induction algorithm combined with a two-level preference relation on user ranking .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
For <m> text classification/categorization </m> , we investigate a steepest descent induction algorithm combined with a two-level preference relation on user ranking .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
For <m> text classification/categorization </m> , we investigate a steepest descent induction algorithm combined with a two-level preference relation on user ranking .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
For <m> text classification/categorization </m> , we investigate a steepest descent induction algorithm combined with a two-level preference relation on user ranking .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
For <m> text classification/categorization </m> , we investigate a steepest descent induction algorithm combined with a two-level preference relation on user ranking .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
For <m> text classification/categorization </m> , we investigate a steepest descent induction algorithm combined with a two-level preference relation on user ranking .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
For <m> text classification/categorization </m> , we investigate a steepest descent induction algorithm combined with a two-level preference relation on user ranking .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
For <m> text classification/categorization </m> , we investigate a steepest descent induction algorithm combined with a two-level preference relation on user ranking .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
For <m> text classification/categorization </m> , we investigate a steepest descent induction algorithm combined with a two-level preference relation on user ranking .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
There are two approaches for this <m> text-based categorization </m> : manual and automatic .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
There are two approaches for this <m> text-based categorization </m> : manual and automatic .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
There are two approaches for this <m> text-based categorization </m> : manual and automatic .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
There are two approaches for this <m> text-based categorization </m> : manual and automatic .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
There are two approaches for this <m> text-based categorization </m> : manual and automatic .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
There are two approaches for this <m> text-based categorization </m> : manual and automatic .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
There are two approaches for this <m> text-based categorization </m> : manual and automatic .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
There are two approaches for this <m> text-based categorization </m> : manual and automatic .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
There are two approaches for this <m> text-based categorization </m> : manual and automatic .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
<m> Text based categorization </m> is made use of for document classification with pattern recognition and machine learning .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
<m> Text based categorization </m> is made use of for document classification with pattern recognition and machine learning .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
<m> Text based categorization </m> is made use of for document classification with pattern recognition and machine learning .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
<m> Text based categorization </m> is made use of for document classification with pattern recognition and machine learning .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
<m> Text based categorization </m> is made use of for document classification with pattern recognition and machine learning .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
<m> Text based categorization </m> is made use of for document classification with pattern recognition and machine learning .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
<m> Text based categorization </m> is made use of for document classification with pattern recognition and machine learning .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
<m> Text based categorization </m> is made use of for document classification with pattern recognition and machine learning .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
<m> Text based categorization </m> is made use of for document classification with pattern recognition and machine learning .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
The results and the statistical evaluation of this procedure showed that the proposed method may be characterized as highly accurate for <m> text categorization purposes </m> .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
The results and the statistical evaluation of this procedure showed that the proposed method may be characterized as highly accurate for <m> text categorization purposes </m> .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
The results and the statistical evaluation of this procedure showed that the proposed method may be characterized as highly accurate for <m> text categorization purposes </m> .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
The results and the statistical evaluation of this procedure showed that the proposed method may be characterized as highly accurate for <m> text categorization purposes </m> .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
The results and the statistical evaluation of this procedure showed that the proposed method may be characterized as highly accurate for <m> text categorization purposes </m> .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
The results and the statistical evaluation of this procedure showed that the proposed method may be characterized as highly accurate for <m> text categorization purposes </m> .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
The results and the statistical evaluation of this procedure showed that the proposed method may be characterized as highly accurate for <m> text categorization purposes </m> .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
The results and the statistical evaluation of this procedure showed that the proposed method may be characterized as highly accurate for <m> text categorization purposes </m> .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
The results and the statistical evaluation of this procedure showed that the proposed method may be characterized as highly accurate for <m> text categorization purposes </m> .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
In this particular paper we describe the design and implementation of a distributed tree-based algorithm for <m> text categorization purposes </m> .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
In this particular paper we describe the design and implementation of a distributed tree-based algorithm for <m> text categorization purposes </m> .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
In this particular paper we describe the design and implementation of a distributed tree-based algorithm for <m> text categorization purposes </m> .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
In this particular paper we describe the design and implementation of a distributed tree-based algorithm for <m> text categorization purposes </m> .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
In this particular paper we describe the design and implementation of a distributed tree-based algorithm for <m> text categorization purposes </m> .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
In this particular paper we describe the design and implementation of a distributed tree-based algorithm for <m> text categorization purposes </m> .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
In this particular paper we describe the design and implementation of a distributed tree-based algorithm for <m> text categorization purposes </m> .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
In this particular paper we describe the design and implementation of a distributed tree-based algorithm for <m> text categorization purposes </m> .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
In this particular paper we describe the design and implementation of a distributed tree-based algorithm for <m> text categorization purposes </m> .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


class: first -> second, base class: no relation
first:
To date , our focus has been on <m> text information categorization </m> using both existing text classification techniques ( e.g. Weighted Feature Vector ( WFV ) classification ) and machine learning algorithms based on inductive logic programming ( ILP ) and natural language processing techniques .
second:
Abstract In <m> multi-label text classification </m> , considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially .


class: first -> second, base class: no relation
first:
To date , our focus has been on <m> text information categorization </m> using both existing text classification techniques ( e.g. Weighted Feature Vector ( WFV ) classification ) and machine learning algorithms based on inductive logic programming ( ILP ) and natural language processing techniques .
second:
Then , we present a <m> multi-label text classification </m> by combining Binary Relevance and Gradient Boosting algorithm .


class: first -> second, base class: no relation
first:
To date , our focus has been on <m> text information categorization </m> using both existing text classification techniques ( e.g. Weighted Feature Vector ( WFV ) classification ) and machine learning algorithms based on inductive logic programming ( ILP ) and natural language processing techniques .
second:
We model tag recommendation task as <m> multi-label text classification problem </m> and use Naive Bayes classifier as the base learner of the multilabel classifier .


class: first -> second, base class: no relation
first:
To date , our focus has been on <m> text information categorization </m> using both existing text classification techniques ( e.g. Weighted Feature Vector ( WFV ) classification ) and machine learning algorithms based on inductive logic programming ( ILP ) and natural language processing techniques .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: first -> second, base class: no relation
first:
To date , our focus has been on <m> text information categorization </m> using both existing text classification techniques ( e.g. Weighted Feature Vector ( WFV ) classification ) and machine learning algorithms based on inductive logic programming ( ILP ) and natural language processing techniques .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: first -> second, base class: no relation
first:
To date , our focus has been on <m> text information categorization </m> using both existing text classification techniques ( e.g. Weighted Feature Vector ( WFV ) classification ) and machine learning algorithms based on inductive logic programming ( ILP ) and natural language processing techniques .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: first -> second, base class: no relation
first:
To date , our focus has been on <m> text information categorization </m> using both existing text classification techniques ( e.g. Weighted Feature Vector ( WFV ) classification ) and machine learning algorithms based on inductive logic programming ( ILP ) and natural language processing techniques .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: first -> second, base class: no relation
first:
To date , our focus has been on <m> text information categorization </m> using both existing text classification techniques ( e.g. Weighted Feature Vector ( WFV ) classification ) and machine learning algorithms based on inductive logic programming ( ILP ) and natural language processing techniques .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: first -> second, base class: no relation
first:
To date , our focus has been on <m> text information categorization </m> using both existing text classification techniques ( e.g. Weighted Feature Vector ( WFV ) classification ) and machine learning algorithms based on inductive logic programming ( ILP ) and natural language processing techniques .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


