class: no relation, base class: first -> second
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
The basic architecture of our neural network is similar to the fully convolutional segmentation network of , namely , we repeat the operation of convolving a vertex function by kernels and applying a <m> non - linear transformation </m> .


class: no relation, base class: first -> second
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
The basic architecture of our neural network is similar to the fully convolutional segmentation network of , namely , we repeat the operation of convolving a vertex function by kernels and applying a <m> non - linear transformation </m> .


class: no relation, base class: first -> second
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
The basic architecture of our neural network is similar to the fully convolutional segmentation network of , namely , we repeat the operation of convolving a vertex function by kernels and applying a <m> non - linear transformation </m> .


class: no relation, base class: first -> second
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
The basic architecture of our neural network is similar to the fully convolutional segmentation network of , namely , we repeat the operation of convolving a vertex function by kernels and applying a <m> non - linear transformation </m> .


class: no relation, base class: first -> second
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
The basic architecture of our neural network is similar to the fully convolutional segmentation network of , namely , we repeat the operation of convolving a vertex function by kernels and applying a <m> non - linear transformation </m> .


class: no relation, base class: second -> first
first:
The model is described by a continuous time three-dimensional autonomous system with hyperbolic sine nonlinearity , and may be viewed as a <m> linear transformation of model </m> MO15 previously introduced in [ Sprott , 2010 ] .
second:
We use two additional <m> linear layers </m> and principal component analysis to reduce the dimension of both inputs and internal representations and to transmit the essential information .


class: no relation, base class: second -> first
first:
The model is described by a continuous time three-dimensional autonomous system with hyperbolic sine nonlinearity , and may be viewed as a <m> linear transformation of model </m> MO15 previously introduced in [ Sprott , 2010 ] .
second:
The three methods , named Reconstructing DDT and LAT According to Weight , Executing <m> Linear Layer Operations </m> in Minimal Cost and Merging Two 4-bit S-boxes into One 8-bit S-box respectively , can efficiently speed up the search process by reducing the search space as much as possible and reducing the cost of executing linear layer operations .


class: no relation, base class: second -> first
first:
The model is described by a continuous time three-dimensional autonomous system with hyperbolic sine nonlinearity , and may be viewed as a <m> linear transformation of model </m> MO15 previously introduced in [ Sprott , 2010 ] .
second:
This method is fundamentally different from existing methods because we use the domain features to calculate a <m> linear combination of linear layers </m> .


class: no relation, base class: second -> first
first:
The model is described by a continuous time three-dimensional autonomous system with hyperbolic sine nonlinearity , and may be viewed as a <m> linear transformation of model </m> MO15 previously introduced in [ Sprott , 2010 ] .
second:
<m> Linear layer ( design ) model </m> gave best outcomes ( MSE : 0.000293366 , RMSE : 0.017127919 , R : 0.996479613 ) .


class: no relation, base class: second -> first
first:
The model is described by a continuous time three-dimensional autonomous system with hyperbolic sine nonlinearity , and may be viewed as a <m> linear transformation of model </m> MO15 previously introduced in [ Sprott , 2010 ] .
second:
The process model described is a <m> linear layered model </m> .


class: no relation, base class: second -> first
first:
The model is described by a continuous time three-dimensional autonomous system with hyperbolic sine nonlinearity , and may be viewed as a <m> linear transformation of model </m> MO15 previously introduced in [ Sprott , 2010 ] .
second:
However , being a <m> linear model </m> , LR can only ascribe a uniform policy - in this case , “ treat everyone ” .


class: no relation, base class: second -> first
first:
The model is described by a continuous time three-dimensional autonomous system with hyperbolic sine nonlinearity , and may be viewed as a <m> linear transformation of model </m> MO15 previously introduced in [ Sprott , 2010 ] .
second:
Finally , we pass this fixed vector through a <m> linear layer </m> to obtain the prediction whether the question is answerable .


class: no relation, base class: first -> second
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
The basic architecture of our neural network is similar to the fully convolutional segmentation network of , namely , we repeat the operation of convolving a vertex function by kernels and applying a <m> non - linear transformation </m> .


class: no relation, base class: first -> second
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
The basic architecture of our neural network is similar to the fully convolutional segmentation network of , namely , we repeat the operation of convolving a vertex function by kernels and applying a <m> non - linear transformation </m> .


class: no relation, base class: first -> second
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .


class: no relation, base class: first -> second
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
Finally , a <m> linear fully connected layer </m> is used to evaluate the saliency of a queried region .


class: no relation, base class: first -> second
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
Finally , a <m> fully - connected linear layer </m> projects to the output of the network , i.e. , the Q - values .


class: no relation, base class: second -> first
first:
The basic architecture of our neural network is similar to the fully convolutional segmentation network of , namely , we repeat the operation of convolving a vertex function by kernels and applying a <m> non - linear transformation </m> .
second:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .


class: no relation, base class: first -> second
first:
Also in this group , for those vertices associated with large residual errors under <m> affine mapping </m> , we encode their motion effectively using Newtonian motion estimates .
second:
Data Sharing Estimator creates application-specific data dependency signatures used by <m> Affine Mapping Finder </m> to determine the appropriate thread mapping of application for a given architecture .


class: no relation, base class: first -> second
first:
Also in this group , for those vertices associated with large residual errors under <m> affine mapping </m> , we encode their motion effectively using Newtonian motion estimates .
second:
Some explicit bounds of the interpolation error are obtained based on some sharp estimates of the integral over $ \frac{1}{|J|^{p-1}}$ for $ 1\leq p\leq\infty$ on the reference element , where $ J$ is the Jacobian of the <m> nonaffine mapping </m> .


class: no relation, base class: second -> first
first:
Data Sharing Estimator creates application-specific data dependency signatures used by <m> Affine Mapping Finder </m> to determine the appropriate thread mapping of application for a given architecture .
second:
GCPs derived from each source were used independently for the GCP-based geometric correction of IRS PAN sensor images by using <m> affine mapping function </m> .


class: no relation, base class: second -> first
first:
Data Sharing Estimator creates application-specific data dependency signatures used by <m> Affine Mapping Finder </m> to determine the appropriate thread mapping of application for a given architecture .
second:
In this paper we introduce a class of constraint logic programs such that their termination can be proved by using <m> affine level mappings </m> .


class: no relation, base class: second -> first
first:
Data Sharing Estimator creates application-specific data dependency signatures used by <m> Affine Mapping Finder </m> to determine the appropriate thread mapping of application for a given architecture .
second:
To be specific , we propose <m> affine mapping based schemes </m> for the problem transformation and outsourcing so that the cloud is unable to learn any key information from the transformed problem .


class: no relation, base class: first -> second
first:
GCPs derived from each source were used independently for the GCP-based geometric correction of IRS PAN sensor images by using <m> affine mapping function </m> .
second:
Some explicit bounds of the interpolation error are obtained based on some sharp estimates of the integral over $ \frac{1}{|J|^{p-1}}$ for $ 1\leq p\leq\infty$ on the reference element , where $ J$ is the Jacobian of the <m> nonaffine mapping </m> .


class: no relation, base class: first -> second
first:
In this paper we introduce a class of constraint logic programs such that their termination can be proved by using <m> affine level mappings </m> .
second:
Some explicit bounds of the interpolation error are obtained based on some sharp estimates of the integral over $ \frac{1}{|J|^{p-1}}$ for $ 1\leq p\leq\infty$ on the reference element , where $ J$ is the Jacobian of the <m> nonaffine mapping </m> .


class: no relation, base class: second -> first
first:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .
second:
We use two additional <m> linear layers </m> and principal component analysis to reduce the dimension of both inputs and internal representations and to transmit the essential information .


class: no relation, base class: second -> first
first:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .
second:
The three methods , named Reconstructing DDT and LAT According to Weight , Executing <m> Linear Layer Operations </m> in Minimal Cost and Merging Two 4-bit S-boxes into One 8-bit S-box respectively , can efficiently speed up the search process by reducing the search space as much as possible and reducing the cost of executing linear layer operations .


class: no relation, base class: second -> first
first:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .
second:
This method is fundamentally different from existing methods because we use the domain features to calculate a <m> linear combination of linear layers </m> .


class: no relation, base class: second -> first
first:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .
second:
<m> Linear layer ( design ) model </m> gave best outcomes ( MSE : 0.000293366 , RMSE : 0.017127919 , R : 0.996479613 ) .


class: no relation, base class: second -> first
first:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .
second:
The process model described is a <m> linear layered model </m> .


class: no relation, base class: second -> first
first:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .
second:
However , being a <m> linear model </m> , LR can only ascribe a uniform policy - in this case , “ treat everyone ” .


class: no relation, base class: second -> first
first:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .
second:
Finally , we pass this fixed vector through a <m> linear layer </m> to obtain the prediction whether the question is answerable .


class: no relation, base class: first -> second
first:
To be specific , we propose <m> affine mapping based schemes </m> for the problem transformation and outsourcing so that the cloud is unable to learn any key information from the transformed problem .
second:
Some explicit bounds of the interpolation error are obtained based on some sharp estimates of the integral over $ \frac{1}{|J|^{p-1}}$ for $ 1\leq p\leq\infty$ on the reference element , where $ J$ is the Jacobian of the <m> nonaffine mapping </m> .


class: no relation, base class: first -> second
first:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .
second:
Rainbow uses <m> noisy linear layers </m> and ReLU activations throughout the network , whereas Reactor uses standard linear layers and concatenated ReLU activations throughout .


class: no relation, base class: second -> first
first:
<m> Anti mutual-information subsampling </m> does not require to remove from the brain signals the shared variance between aging and fluid intelligence , and hence does not display this pessimistic behavior .
second:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .


class: no relation, base class: second -> first
first:
<m> Anti mutual-information subsampling </m> does not require to remove from the brain signals the shared variance between aging and fluid intelligence , and hence does not display this pessimistic behavior .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: no relation, base class: second -> first
first:
<m> Anti mutual-information subsampling </m> does not require to remove from the brain signals the shared variance between aging and fluid intelligence , and hence does not display this pessimistic behavior .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: no relation, base class: second -> first
first:
<m> Anti mutual-information subsampling </m> does not require to remove from the brain signals the shared variance between aging and fluid intelligence , and hence does not display this pessimistic behavior .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: no relation, base class: second -> first
first:
<m> Anti mutual-information subsampling </m> does not require to remove from the brain signals the shared variance between aging and fluid intelligence , and hence does not display this pessimistic behavior .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: no relation, base class: second -> first
first:
A <m> subsampling stochastic gradient descent algorithm </m> has been proposed to accelerate the training of recurrent neural network language models .
second:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .


class: no relation, base class: same cluster
first:
A <m> subsampling stochastic gradient descent algorithm </m> has been proposed to accelerate the training of recurrent neural network language models .
second:
In this contribution , we present and conduct an investigation aimed at locating a robust threshold for <m> downsampling BTF images </m> without loosing perceptual quality .


class: no relation, base class: second -> first
first:
A <m> subsampling stochastic gradient descent algorithm </m> has been proposed to accelerate the training of recurrent neural network language models .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: no relation, base class: first -> second
first:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .
second:
The spatial frequency decomposition is accomplished by an efficient encoding algorithm incorporating a hierarchical cascading Gaussian pyramid algorithm which is an alternating sequence of image output passing through Nyquist low pass spatial filter and <m> subsampling local operators </m> for image encoding .


class: no relation, base class: second -> first
first:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .
second:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .


class: no relation, base class: first -> second
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .


class: no relation, base class: first -> second
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
The third one is the <m> volumetric downsampling block </m> that is identical to a volumetric max pooling layer .


class: no relation, base class: second -> first
first:
We change the ( user , item ) entry subsampling strategy in SGD training in the original paper to the <m> user - level subsampling </m> as we did with Mult - vae pr and Mult - dae .
second:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .


class: no relation, base class: second -> first
first:
We change the ( user , item ) entry subsampling strategy in SGD training in the original paper to the <m> user - level subsampling </m> as we did with Mult - vae pr and Mult - dae .
second:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .


class: no relation, base class: second -> first
first:
We change the ( user , item ) entry subsampling strategy in SGD training in the original paper to the <m> user - level subsampling </m> as we did with Mult - vae pr and Mult - dae .
second:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .


class: no relation, base class: second -> first
first:
We change the ( user , item ) entry subsampling strategy in SGD training in the original paper to the <m> user - level subsampling </m> as we did with Mult - vae pr and Mult - dae .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: no relation, base class: second -> first
first:
We change the ( user , item ) entry subsampling strategy in SGD training in the original paper to the <m> user - level subsampling </m> as we did with Mult - vae pr and Mult - dae .
second:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .


class: no relation, base class: second -> first
first:
We change the ( user , item ) entry subsampling strategy in SGD training in the original paper to the <m> user - level subsampling </m> as we did with Mult - vae pr and Mult - dae .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: no relation, base class: second -> first
first:
We change the ( user , item ) entry subsampling strategy in SGD training in the original paper to the <m> user - level subsampling </m> as we did with Mult - vae pr and Mult - dae .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: no relation, base class: second -> first
first:
We change the ( user , item ) entry subsampling strategy in SGD training in the original paper to the <m> user - level subsampling </m> as we did with Mult - vae pr and Mult - dae .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: no relation, base class: second -> first
first:
We change the ( user , item ) entry subsampling strategy in SGD training in the original paper to the <m> user - level subsampling </m> as we did with Mult - vae pr and Mult - dae .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: no relation, base class: second -> first
first:
We change the ( user , item ) entry subsampling strategy in SGD training in the original paper to the <m> user - level subsampling </m> as we did with Mult - vae pr and Mult - dae .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: no relation, base class: second -> first
first:
We change the ( user , item ) entry subsampling strategy in SGD training in the original paper to the <m> user - level subsampling </m> as we did with Mult - vae pr and Mult - dae .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: no relation, base class: second -> first
first:
We change the ( user , item ) entry subsampling strategy in SGD training in the original paper to the <m> user - level subsampling </m> as we did with Mult - vae pr and Mult - dae .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: no relation, base class: second -> first
first:
We change the ( user , item ) entry subsampling strategy in SGD training in the original paper to the <m> user - level subsampling </m> as we did with Mult - vae pr and Mult - dae .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: no relation, base class: second -> first
first:
We change the ( user , item ) entry subsampling strategy in SGD training in the original paper to the <m> user - level subsampling </m> as we did with Mult - vae pr and Mult - dae .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: no relation, base class: first -> second
first:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .
second:
Multi-resolution analysis is based on <m> downsampling expansion </m> with maximum energy extraction followed by upsampling reconstruction .


class: no relation, base class: first -> second
first:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .
second:
Multi-resolution analysis is based on <m> downsampling expansion </m> with maximum energy extraction followed by upsampling reconstruction .


class: no relation, base class: first -> second
first:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .
second:
Multi-resolution analysis is based on <m> downsampling expansion </m> with maximum energy extraction followed by upsampling reconstruction .


class: no relation, base class: second -> first
first:
For that , we propose in this paper a new fusion schema ( coupled multiresolution decomposition model ( CMD ) ) allowing the reconstruction of a high-resolution MS given its approximation and details obtained by <m> MTF-tailored downsampling </m> and wavelet decomposition , respectively .
second:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .


class: no relation, base class: second -> first
first:
For that , we propose in this paper a new fusion schema ( coupled multiresolution decomposition model ( CMD ) ) allowing the reconstruction of a high-resolution MS given its approximation and details obtained by <m> MTF-tailored downsampling </m> and wavelet decomposition , respectively .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: no relation, base class: second -> first
first:
Multi-resolution analysis is based on <m> downsampling expansion </m> with maximum energy extraction followed by upsampling reconstruction .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: no relation, base class: second -> first
first:
Multi-resolution analysis is based on <m> downsampling expansion </m> with maximum energy extraction followed by upsampling reconstruction .
second:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .


class: no relation, base class: second -> first
first:
Multi-resolution analysis is based on <m> downsampling expansion </m> with maximum energy extraction followed by upsampling reconstruction .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: no relation, base class: second -> first
first:
Multi-resolution analysis is based on <m> downsampling expansion </m> with maximum energy extraction followed by upsampling reconstruction .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: no relation, base class: second -> first
first:
Multi-resolution analysis is based on <m> downsampling expansion </m> with maximum energy extraction followed by upsampling reconstruction .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: no relation, base class: second -> first
first:
Multi-resolution analysis is based on <m> downsampling expansion </m> with maximum energy extraction followed by upsampling reconstruction .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: no relation, base class: second -> first
first:
Multi-resolution analysis is based on <m> downsampling expansion </m> with maximum energy extraction followed by upsampling reconstruction .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: no relation, base class: second -> first
first:
Multi-resolution analysis is based on <m> downsampling expansion </m> with maximum energy extraction followed by upsampling reconstruction .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: no relation, base class: second -> first
first:
Multi-resolution analysis is based on <m> downsampling expansion </m> with maximum energy extraction followed by upsampling reconstruction .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: no relation, base class: second -> first
first:
Multi-resolution analysis is based on <m> downsampling expansion </m> with maximum energy extraction followed by upsampling reconstruction .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: no relation, base class: second -> first
first:
Multi-resolution analysis is based on <m> downsampling expansion </m> with maximum energy extraction followed by upsampling reconstruction .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: no relation, base class: first -> second
first:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .
second:
In this contribution , we present and conduct an investigation aimed at locating a robust threshold for <m> downsampling BTF images </m> without loosing perceptual quality .


class: no relation, base class: first -> second
first:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .
second:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .


class: no relation, base class: first -> second
first:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .
second:
Note that SNIPER also performs <m> adaptive downsampling </m> .


class: no relation, base class: second -> first
first:
In this contribution , we present and conduct an investigation aimed at locating a robust threshold for <m> downsampling BTF images </m> without loosing perceptual quality .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: no relation, base class: second -> first
first:
After segmentation , the HR cell image is downsampled by the <m> bicubic downsampling algorithm </m> to obtain a 20 × 20 low resolution ( LR ) cell image .
second:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .


class: no relation, base class: second -> first
first:
This paper explores the problem <m> oftask-oriented downsampling </m> , which aims to downsample a pointcloud and maintain the performance of subsequent applications asmuch as possible .
second:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .


class: no relation, base class: second -> first
first:
More precisely , the contributions achieved during this thesis are : * A feature-preserving filter for the <m> downsampling of scalar volume data </m> .
second:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .


class: no relation, base class: second -> first
first:
More precisely , the contributions achieved during this thesis are : * A feature-preserving filter for the <m> downsampling of scalar volume data </m> .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: no relation, base class: second -> first
first:
More precisely , the contributions achieved during this thesis are : * A feature-preserving filter for the <m> downsampling of scalar volume data </m> .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: no relation, base class: second -> first
first:
More precisely , the contributions achieved during this thesis are : * A feature-preserving filter for the <m> downsampling of scalar volume data </m> .
second:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .


class: no relation, base class: second -> first
first:
More precisely , the contributions achieved during this thesis are : * A feature-preserving filter for the <m> downsampling of scalar volume data </m> .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: no relation, base class: second -> first
first:
We further evaluate three potential defense methods to mitigate such attacks , including adversarial training , <m> audio downsampling </m> , and moving average filtering , which leads to promising directions for further research .
second:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .


class: no relation, base class: second -> first
first:
Performance analysis demonstrates that the proposed <m> downsampling nodes </m> and full nodes have similar broadcast accuracy with appropriate $ M$. The simulation results show that the proposed algorithm can provide a better cost-effective choice for nodes between broadcast accuracy and storage .
second:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .


class: no relation, base class: first -> second
first:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .
second:
Note that SNIPER also performs <m> adaptive downsampling </m> .


class: no relation, base class: second -> first
first:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .
second:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .


class: no relation, base class: first -> second
first:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .
second:
Certified metrics are available with <m> Alexa Pro plans </m> , which gives traffic data accurately by the code installed in the server of the website .


class: no relation, base class: second -> first
first:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .
second:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .


class: no relation, base class: second -> first
first:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .
second:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .


class: no relation, base class: first -> second
first:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .
second:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .


class: no relation, base class: first -> second
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
Therefore , website ’s ranking could be figured out via the rates of computers with the <m> Alexa tool bar </m> and Internet traffic data collected .


class: no relation, base class: first -> second
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
Design/methodology/approach – The study applies web analytical tools , such as <m> Alexa.com </m> , in the collection of data about Canadian libraries ' visibility performance in the ranking of search engine results .


class: no relation, base class: second -> first
first:
While the combination of VoiceOver and <m> Siri </m> gives the user some degree of accessibility , the most notorious access problems were the difficulty to do manual gestures involving more than one finger , and editing text with the keyboard .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: no relation, base class: second -> first
first:
Technologies like “ <m> Apple Siri </m> ” or “ Amazon Alexa ” serve as digital assistants to enhance both accessibility and productivity .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: no relation, base class: second -> first
first:
This paper formulates two <m> stochastic nonautonomous SIRI epidemic systems </m> with nonlinear perturbations .
second:
The benchmarking software is made available as an open-source application so that others can perform their own experiments , and so that app developers can use this library as a foundation for building new applications based on the <m> SIRI format </m> .


class: no relation, base class: second -> first
first:
The main aim of this study is to investigate stochastic dynamics of the two <m> SIRI epidemic systems </m> and obtain their thresholds .
second:
The benchmarking software is made available as an open-source application so that others can perform their own experiments , and so that app developers can use this library as a foundation for building new applications based on the <m> SIRI format </m> .


class: no relation, base class: second -> first
first:
For the <m> nonautonomous stochastic SIRI epidemic system </m> with white noise , the authors provide analytic results regarding the stochastic boundedness , stochastic permanence and persistence in mean .
second:
The benchmarking software is made available as an open-source application so that others can perform their own experiments , and so that app developers can use this library as a foundation for building new applications based on the <m> SIRI format </m> .


class: no relation, base class: second -> first
first:
Age was found not to be the only factor influencing the usability of the <m> Siri virtual assistant </m> .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: no relation, base class: second -> first
first:
Amazon has a <m> Siri-like device </m> [ 4 ] which does far field listening .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: no relation, base class: second -> first
first:
The <m> intelligent personal assistant Siri </m> was chosen to perform an experiment with repeated measures design consisting of three levels : Siri 's speech , natural speech mimicking Siri 's prosody and natural speech .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: no relation, base class: second -> first
first:
In this paper , we address challenges arising from the use of the <m> conversational interface Siri </m> during a literacy activity in a learning context .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: no relation, base class: second -> first
first:
Moreover , when combined with automatic speech recognition systems , task - oriented dialogue systems provide the foundation of intelligent assistants such as Amazon Alexa , <m> Apple Siri </m> , and Google Assistant .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: no relation, base class: first -> second
first:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .
second:
For a C-UI we define a taskoid for the creation of state machines for <m> Alexa Controls </m> using the Alexa Gadget API and CLI tools .


class: no relation, base class: second -> first
first:
Popular conversational agents frameworks such as <m> Alexa Skills Kit </m> ( ASK ) and Google Actions ( gActions ) offer unprecedented opportunities for facilitating the development and deployment of voice-enabled AI solutions in various verticals .
second:
For a C-UI we define a taskoid for the creation of state machines for <m> Alexa Controls </m> using the Alexa Gadget API and CLI tools .


class: no relation, base class: second -> first
first:
Popular conversational agents frameworks such as <m> Alexa Skills Kit </m> ( ASK ) and Google Actions ( gActions ) offer unprecedented opportunities for facilitating the development and deployment of voice-enabled AI solutions in various verticals .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: no relation, base class: first -> second
first:
Therefore , website ’s ranking could be figured out via the rates of computers with the <m> Alexa tool bar </m> and Internet traffic data collected .
second:
For a C-UI we define a taskoid for the creation of state machines for <m> Alexa Controls </m> using the Alexa Gadget API and CLI tools .


class: no relation, base class: first -> second
first:
Design/methodology/approach – The study applies web analytical tools , such as <m> Alexa.com </m> , in the collection of data about Canadian libraries ' visibility performance in the ranking of search engine results .
second:
For a C-UI we define a taskoid for the creation of state machines for <m> Alexa Controls </m> using the Alexa Gadget API and CLI tools .


class: no relation, base class: first -> second
first:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .
second:
For a C-UI we define a taskoid for the creation of state machines for <m> Alexa Controls </m> using the Alexa Gadget API and CLI tools .


class: no relation, base class: same cluster
first:
For a C-UI we define a taskoid for the creation of state machines for <m> Alexa Controls </m> using the Alexa Gadget API and CLI tools .
second:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .


class: no relation, base class: second -> first
first:
For a C-UI we define a taskoid for the creation of state machines for <m> Alexa Controls </m> using the Alexa Gadget API and CLI tools .
second:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .


class: no relation, base class: second -> first
first:
For a C-UI we define a taskoid for the creation of state machines for <m> Alexa Controls </m> using the Alexa Gadget API and CLI tools .
second:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .


class: no relation, base class: second -> first
first:
For a C-UI we define a taskoid for the creation of state machines for <m> Alexa Controls </m> using the Alexa Gadget API and CLI tools .
second:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .


class: no relation, base class: second -> first
first:
The <m> Virtual Assistant Designer </m> .
second:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .


class: no relation, base class: same cluster
first:
The parallelism of link partitioning relative to <m> node partitioning </m> is examined in terms of an idealized execution using the well-known YAWNS synchronization algorithm .
second:
Through a numerical experiment using benchmark datasets from the UCI machine-learning repository , we confirm that ( i ) the suggested <m> node partition method </m> is efficient compared to a random partition method , and ( ii ) the classification performance of HMC-LAD is superior to existing multi-class LAD algorithms and other supervised learning approaches .


class: no relation, base class: same cluster
first:
The parallelism of link partitioning relative to <m> node partitioning </m> is examined in terms of an idealized execution using the well-known YAWNS synchronization algorithm .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: no relation, base class: same cluster
first:
The parallelism of link partitioning relative to <m> node partitioning </m> is examined in terms of an idealized execution using the well-known YAWNS synchronization algorithm .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: no relation, base class: same cluster
first:
The parallelism of link partitioning relative to <m> node partitioning </m> is examined in terms of an idealized execution using the well-known YAWNS synchronization algorithm .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: no relation, base class: same cluster
first:
The parallelism of link partitioning relative to <m> node partitioning </m> is examined in terms of an idealized execution using the well-known YAWNS synchronization algorithm .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: no relation, base class: same cluster
first:
The parallelism of link partitioning relative to <m> node partitioning </m> is examined in terms of an idealized execution using the well-known YAWNS synchronization algorithm .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: no relation, base class: same cluster
first:
The parallelism of link partitioning relative to <m> node partitioning </m> is examined in terms of an idealized execution using the well-known YAWNS synchronization algorithm .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: no relation, base class: same cluster
first:
Through a numerical experiment using benchmark datasets from the UCI machine-learning repository , we confirm that ( i ) the suggested <m> node partition method </m> is efficient compared to a random partition method , and ( ii ) the classification performance of HMC-LAD is superior to existing multi-class LAD algorithms and other supervised learning approaches .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: no relation, base class: first -> second
first:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .
second:
In this article , Based on the idea of coverage compensation a <m> distributed node partition algorithm </m> for random deployments is presented to generate a minimum cover set by using the optimal node distributions created by the centralized algorithms such as GA .


class: no relation, base class: first -> second
first:
Moreover , the scheme incorporates several source coding techniques , such as Wyner-Ziv coding , <m> binning </m> and superposition coding .
second:
<m> Quantile binning </m> is popular in several fields .


class: no relation, base class: first -> second
first:
Moreover , the scheme incorporates several source coding techniques , such as Wyner-Ziv coding , <m> binning </m> and superposition coding .
second:
A fast HOTLink ( 400Mbit/s , Cypress Semiconductor inc . ) serial bus is used to transfer the coincidence data to networked data acquisition and image processing computers , and the <m> sinogram binning </m> and image reconstruction can be processed in parallel .


class: no relation, base class: first -> second
first:
Moreover , the scheme incorporates several source coding techniques , such as Wyner-Ziv coding , <m> binning </m> and superposition coding .
second:
A feasible distributed data indexing algorithm is proposed for Hadoop data mining , based on <m> ZSCORE binning </m> and inverted indexing and on the Hadoop SequenceFile format .


class: no relation, base class: first -> second
first:
We describe a new <m> binning technic </m> for informed data hiding problem .
second:
<m> Sub-pixel binning </m> : Chandra coordinates contain positional accuracy finer than one ACIS pixel ( 0.492 arcsec ) through dither and aspect correction .


class: no relation, base class: first -> second
first:
We describe a new <m> binning technic </m> for informed data hiding problem .
second:
A fast HOTLink ( 400Mbit/s , Cypress Semiconductor inc . ) serial bus is used to transfer the coincidence data to networked data acquisition and image processing computers , and the <m> sinogram binning </m> and image reconstruction can be processed in parallel .


class: no relation, base class: first -> second
first:
We describe a new <m> binning technic </m> for informed data hiding problem .
second:
Although conceptually simple , the calculation of radial distribution functions via <m> distance binning </m> requires the evaluation of O ( N 2 ) particle-pair distances where N is the number of particles under consideration .


class: no relation, base class: first -> second
first:
We describe a new <m> binning technic </m> for informed data hiding problem .
second:
A feasible distributed data indexing algorithm is proposed for Hadoop data mining , based on <m> ZSCORE binning </m> and inverted indexing and on the Hadoop SequenceFile format .


class: no relation, base class: second -> first
first:
To improve its efficiency this paper proposes GbDBSCAN(Grid based DBSCAN).GbDBSCAN adopts <m> gird and data binning technique </m> to query density reachable objects for all objects in dataset .
second:
The method shows significant improvement over the <m> binning and stacking method </m> when applied to both synthetic as well as real data .


class: no relation, base class: first -> second
first:
The results show the potential of the <m> word embeddings approach </m> for sentiment analysis in the social sciences .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: no relation, base class: first -> second
first:
We study the effectiveness of <m> word embeddings </m> to overcome this disadvantage of ROUGE .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: no relation, base class: first -> second
first:
We combine convolutional neural networks with subword-level information vectors , which are <m> word embedding representations </m> learned from Wikipedia that take advantage of the words morphology ; so each word is represented as a bag of their character n-grams .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: no relation, base class: first -> second
first:
Then , for testing sentiment similarity , we use : Similarity Measures methods between words and cosine similarity measure between the <m> word embedding representations </m> ( e.g. word2vec , GloVE ) .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: no relation, base class: first -> second
first:
This candidate list was then compared with an expert-generated list of marijuana terms to assess the accuracy and efficacy of using <m> word-vector embeddings </m> to search for novel drug terminology .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: no relation, base class: first -> second
first:
This candidate list was then compared with an expert-generated list of marijuana terms to assess the accuracy and efficacy of using <m> word-vector embeddings </m> to search for novel drug terminology .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: no relation, base class: first -> second
first:
In this paper , we utilize <m> word vector embeddings </m> along with fastText sentence classification algorithm to perform the task of classification of tweets posted during natural disasters .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: no relation, base class: first -> second
first:
Then , the actual input to the Transformer is the element - wise addition of the <m> word embeddings </m> and the positional encodings .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: no relation, base class: same cluster
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: same cluster
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: same cluster
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: same cluster
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: first -> second
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: same cluster
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: same cluster
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: same cluster
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: same cluster
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: first -> second
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: first -> second
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: same cluster
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: same cluster
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: same cluster
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: same cluster
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: first -> second
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: first -> second
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: first -> second
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: first -> second
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: first -> second
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: first -> second
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: second -> first
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: second -> first
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: second -> first
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: second -> first
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: first -> second
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: first -> second
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: first -> second
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: first -> second
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: no relation, base class: first -> second
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
In our experiments , each paragraph vector is a combination of two vectors : one learned by the standard paragraph vector with distributed memory ( PV - DM ) and one learned by the paragraph vector with <m> distributed bag of words </m> ( PV - DBOW ) .


class: no relation, base class: second -> first
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: second -> first
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: second -> first
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: second -> first
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: first -> second
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
In our experiments , each paragraph vector is a combination of two vectors : one learned by the standard paragraph vector with distributed memory ( PV - DM ) and one learned by the paragraph vector with <m> distributed bag of words </m> ( PV - DBOW ) .


class: no relation, base class: second -> first
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: second -> first
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: second -> first
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: second -> first
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: first -> second
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
In our experiments , each paragraph vector is a combination of two vectors : one learned by the standard paragraph vector with distributed memory ( PV - DM ) and one learned by the paragraph vector with <m> distributed bag of words </m> ( PV - DBOW ) .


class: no relation, base class: second -> first
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: second -> first
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: second -> first
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: second -> first
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: first -> second
first:
Word vectors are <m> distributed representations of word features </m> .
second:
In our experiments , each paragraph vector is a combination of two vectors : one learned by the standard paragraph vector with distributed memory ( PV - DM ) and one learned by the paragraph vector with <m> distributed bag of words </m> ( PV - DBOW ) .


class: no relation, base class: second -> first
first:
Word vectors are <m> distributed representations of word features </m> .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: second -> first
first:
Word vectors are <m> distributed representations of word features </m> .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: second -> first
first:
Word vectors are <m> distributed representations of word features </m> .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: second -> first
first:
Word vectors are <m> distributed representations of word features </m> .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: first -> second
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
In our experiments , each paragraph vector is a combination of two vectors : one learned by the standard paragraph vector with distributed memory ( PV - DM ) and one learned by the paragraph vector with <m> distributed bag of words </m> ( PV - DBOW ) .


class: no relation, base class: second -> first
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: second -> first
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: second -> first
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: second -> first
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: first -> second
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
In our experiments , each paragraph vector is a combination of two vectors : one learned by the standard paragraph vector with distributed memory ( PV - DM ) and one learned by the paragraph vector with <m> distributed bag of words </m> ( PV - DBOW ) .


class: no relation, base class: second -> first
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: second -> first
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: second -> first
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: second -> first
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: first -> second
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
In our experiments , each paragraph vector is a combination of two vectors : one learned by the standard paragraph vector with distributed memory ( PV - DM ) and one learned by the paragraph vector with <m> distributed bag of words </m> ( PV - DBOW ) .


class: no relation, base class: second -> first
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: second -> first
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: second -> first
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: second -> first
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: second -> first
first:
In our experiments , each paragraph vector is a combination of two vectors : one learned by the standard paragraph vector with distributed memory ( PV - DM ) and one learned by the paragraph vector with <m> distributed bag of words </m> ( PV - DBOW ) .
second:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .


class: no relation, base class: second -> first
first:
In our experiments , each paragraph vector is a combination of two vectors : one learned by the standard paragraph vector with distributed memory ( PV - DM ) and one learned by the paragraph vector with <m> distributed bag of words </m> ( PV - DBOW ) .
second:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .


class: no relation, base class: second -> first
first:
In our experiments , each paragraph vector is a combination of two vectors : one learned by the standard paragraph vector with distributed memory ( PV - DM ) and one learned by the paragraph vector with <m> distributed bag of words </m> ( PV - DBOW ) .
second:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .


class: no relation, base class: second -> first
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: second -> first
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: second -> first
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: second -> first
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: second -> first
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: second -> first
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: second -> first
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: second -> first
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: second -> first
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: second -> first
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: second -> first
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: second -> first
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: second -> first
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .


class: no relation, base class: second -> first
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: no relation, base class: second -> first
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .


class: no relation, base class: second -> first
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: first -> second
first:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: no relation, base class: first -> second
first:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: no relation, base class: first -> second
first:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: no relation, base class: first -> second
first:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: no relation, base class: first -> second
first:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: no relation, base class: first -> second
first:
The <m> embedding vector space techniques </m> which are frequently used in complex objects matching area are proposed , by selecting random parameters to generate new coordinate system , embedding the original data into the new space and processing the similarity linking there .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: no relation, base class: second -> first
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: second -> first
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
In summary , the hypernetwork projects a <m> vector embedding </m> of each relation via a fully connected layer , the result of which is reshaped to give a set of convolutional filter weight vectors for each relation .


class: no relation, base class: first -> second
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: no relation, base class: first -> second
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: no relation, base class: first -> second
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: no relation, base class: first -> second
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: no relation, base class: first -> second
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: no relation, base class: first -> second
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: no relation, base class: first -> second
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: no relation, base class: first -> second
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: no relation, base class: first -> second
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: no relation, base class: first -> second
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: no relation, base class: first -> second
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: no relation, base class: first -> second
first:
The research results have showed that learners can basically complete shallow level in multiple dimensions and expand some <m> deep level learning </m> , but the degree of deep learning needs to be improved .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: no relation, base class: first -> second
first:
The research results have showed that learners can basically complete shallow level in multiple dimensions and expand some <m> deep level learning </m> , but the degree of deep learning needs to be improved .
second:
This <m> branched architecture </m> enables the network to achieve comparable performance at a significantly lower computational cost .


class: no relation, base class: first -> second
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: no relation, base class: first -> second
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: no relation, base class: first -> second
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: no relation, base class: first -> second
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: no relation, base class: first -> second
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: no relation, base class: first -> second
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: no relation, base class: first -> second
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: no relation, base class: first -> second
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: no relation, base class: first -> second
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: no relation, base class: first -> second
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: no relation, base class: first -> second
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: no relation, base class: first -> second
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: no relation, base class: second -> first
first:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .
second:
The English Arts and Crafts architect and theorist W. Lethaby wrote in 1891 : ' ... so is building but the vehicle of <m> architecture </m> which is the thought behind form , embodied and realized for the purpose of its manifestation and transmission ' . Or we call that architecture is the built environment with aesthetically and spiritually stimulating space and form .


class: no relation, base class: second -> first
first:
A recurrent network with long term-short memory ( LSTM ) units encodes the time-varying changes in the vector-momentum sequence , and a convolutional neural network ( <m> CNN </m> ) encodes the baseline image of the vector momenta .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: no relation, base class: second -> first
first:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .
second:
The English Arts and Crafts architect and theorist W. Lethaby wrote in 1891 : ' ... so is building but the vehicle of <m> architecture </m> which is the thought behind form , embodied and realized for the purpose of its manifestation and transmission ' . Or we call that architecture is the built environment with aesthetically and spiritually stimulating space and form .


class: no relation, base class: second -> first
first:
document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( <m> CNN </m> ) in person re - identification ( re - ID ) , i.e. , verification and identification models .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: no relation, base class: first -> second
first:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: no relation, base class: first -> second
first:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: no relation, base class: first -> second
first:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: no relation, base class: first -> second
first:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: no relation, base class: first -> second
first:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: no relation, base class: first -> second
first:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: no relation, base class: first -> second
first:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: no relation, base class: first -> second
first:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .
second:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .


class: no relation, base class: second -> first
first:
Character - level language modelling is a representative example of discrete [ reference ] 1.67 MRNN [ reference ] 1.60 <m> GF - LSTM </m> [ reference ] 1.58 Grid - LSTM [ reference ] 1.47 MI - LSTM 1.44 Recurrent Memory Array Structures ( Rocki , 2016a ) 1.40 SF - LSTM
second:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .


class: no relation, base class: first -> second
first:
The English Arts and Crafts architect and theorist W. Lethaby wrote in 1891 : ' ... so is building but the vehicle of <m> architecture </m> which is the thought behind form , embodied and realized for the purpose of its manifestation and transmission ' . Or we call that architecture is the built environment with aesthetically and spiritually stimulating space and form .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: no relation, base class: first -> second
first:
The English Arts and Crafts architect and theorist W. Lethaby wrote in 1891 : ' ... so is building but the vehicle of <m> architecture </m> which is the thought behind form , embodied and realized for the purpose of its manifestation and transmission ' . Or we call that architecture is the built environment with aesthetically and spiritually stimulating space and form .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: no relation, base class: first -> second
first:
The English Arts and Crafts architect and theorist W. Lethaby wrote in 1891 : ' ... so is building but the vehicle of <m> architecture </m> which is the thought behind form , embodied and realized for the purpose of its manifestation and transmission ' . Or we call that architecture is the built environment with aesthetically and spiritually stimulating space and form .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: no relation, base class: first -> second
first:
The English Arts and Crafts architect and theorist W. Lethaby wrote in 1891 : ' ... so is building but the vehicle of <m> architecture </m> which is the thought behind form , embodied and realized for the purpose of its manifestation and transmission ' . Or we call that architecture is the built environment with aesthetically and spiritually stimulating space and form .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: no relation, base class: first -> second
first:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: no relation, base class: first -> second
first:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: no relation, base class: first -> second
first:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: no relation, base class: first -> second
first:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: no relation, base class: second -> first
first:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: no relation, base class: second -> first
first:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: no relation, base class: second -> first
first:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: no relation, base class: second -> first
first:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: no relation, base class: second -> first
first:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: no relation, base class: second -> first
first:
This produced lower perplexity and word error than the standard , <m> single - hidden - layer RNNLM architecture </m> .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: no relation, base class: first -> second
first:
In both approaches , the actual <m> classifier / predictor </m> is learned in a separate step using the feature representation learned by autoencoder ( s ) .
second:
Fang et al . won the 2015 COCO Captioning Challenge with an approach that is similar to ours in as much as it applies a visual concept ( i.e. , <m> attribute ) detection process </m> before generating sentences .


class: no relation, base class: first -> second
first:
In both approaches , the actual <m> classifier / predictor </m> is learned in a separate step using the feature representation learned by autoencoder ( s ) .
second:
Since we formulate the <m> attribute prediction </m> as a multi - label problem , our attributes prediction network can be replaced by any other multi - label classification framework and it also can be benefit from the development of the multi - label classification researches .


class: no relation, base class: first -> second
first:
In both approaches , the actual <m> classifier / predictor </m> is learned in a separate step using the feature representation learned by autoencoder ( s ) .
second:
There have been works on first learning <m> attribute classifiers </m> and using attribute predictions for face recognition .


class: no relation, base class: first -> second
first:
However , SVM has a weakness for parameter selection or suitable <m> feature </m> .
second:
In this paper , at first briefly discuss the existing approaches , and then present a <m> motion feature based key frame extracting method </m> using fuzzy reasoning theory for MPEG compressed video .


class: no relation, base class: first -> second
first:
However , SVM has a weakness for parameter selection or suitable <m> feature </m> .
second:
Different from the state of the art methods where features are extracted by a <m> sole feature extraction network </m> for both tasks , the proposed HybridNet improves the features extraction by separating the relevant features for one task from those which are relevant for both .


class: no relation, base class: first -> second
first:
However , SVM has a weakness for parameter selection or suitable <m> feature </m> .
second:
The feature extrapolating layer extrapolates 4 scales with equal intervals between every two input scales , so the final <m> conv feature pyramid </m> has 21 scales for KITTI and 16 scales for PASCAL .


class: no relation, base class: second -> first
first:
The <m> event-driven feature </m> improves the system performance in terms of resources utilization and power consumption compared to the counter classical ones .
second:
subsection : Locality Sensitive Hashing ( LSH ) An alternative approach to BASS and DISCO is to use well - established <m> feature generation methods </m> that are agnostic about the type of input they receive .


class: no relation, base class: second -> first
first:
The <m> event-driven feature </m> improves the system performance in terms of resources utilization and power consumption compared to the counter classical ones .
second:
subsection : Unimodal Feature Extraction For a fair comparison with the state - of - the - art method , conversational memory networks ( CMN ) , we follow identical <m> feature extraction procedures </m> .


class: no relation, base class: second -> first
first:
This paper presents a convolutional time-delay deep neural network structure ( CT-DNN ) for <m> speaker feature learning </m> .
second:
This would amount to an additional <m> feature level </m> GAN loss ( see Figure [ reference ] orange portion ) : Taken together , these loss functions form our complete objective : This ultimately corresponds to solving for a target model according to the optimization problem We have introduced a method for unsupervised adaptation which generalizes adversarial objectives to be viewed as operating at the pixel or feature level .


class: no relation, base class: second -> first
first:
In this paper , at first briefly discuss the existing approaches , and then present a <m> motion feature based key frame extracting method </m> using fuzzy reasoning theory for MPEG compressed video .
second:
subsection : Locality Sensitive Hashing ( LSH ) An alternative approach to BASS and DISCO is to use well - established <m> feature generation methods </m> that are agnostic about the type of input they receive .


class: no relation, base class: second -> first
first:
In this paper , at first briefly discuss the existing approaches , and then present a <m> motion feature based key frame extracting method </m> using fuzzy reasoning theory for MPEG compressed video .
second:
subsection : Unimodal Feature Extraction For a fair comparison with the state - of - the - art method , conversational memory networks ( CMN ) , we follow identical <m> feature extraction procedures </m> .


class: no relation, base class: second -> first
first:
In our Adversarial Spatial Transformer Network , we focus on <m> feature map rotations </m> .
second:
This would amount to an additional <m> feature level </m> GAN loss ( see Figure [ reference ] orange portion ) : Taken together , these loss functions form our complete objective : This ultimately corresponds to solving for a target model according to the optimization problem We have introduced a method for unsupervised adaptation which generalizes adversarial objectives to be viewed as operating at the pixel or feature level .


class: no relation, base class: second -> first
first:
For video face recognition , most of these methods either use <m> pairwise frame feature similarity computation </m> [ reference ][ reference ] or naive ( average / max ) frame feature pooling [ reference ][ reference ][ reference ] .
second:
subsection : Locality Sensitive Hashing ( LSH ) An alternative approach to BASS and DISCO is to use well - established <m> feature generation methods </m> that are agnostic about the type of input they receive .


class: no relation, base class: second -> first
first:
For video face recognition , most of these methods either use <m> pairwise frame feature similarity computation </m> [ reference ][ reference ] or naive ( average / max ) frame feature pooling [ reference ][ reference ][ reference ] .
second:
subsection : Unimodal Feature Extraction For a fair comparison with the state - of - the - art method , conversational memory networks ( CMN ) , we follow identical <m> feature extraction procedures </m> .


class: no relation, base class: first -> second
first:
Objectives : This article aims to produce a <m> covariate adjustment method </m> that allows for automatic variable selection , so that practitioners need not commit to any specific set of covariates prior to seeing the data .
second:
subsection : Batch Normalization Review Batch Normalization has been introduced in as an effective tool to reduce <m> internal covariate shift </m> in deep networks and accelerate the training process .


class: no relation, base class: first -> second
first:
Objectives : This article aims to produce a <m> covariate adjustment method </m> that allows for automatic variable selection , so that practitioners need not commit to any specific set of covariates prior to seeing the data .
second:
subsection : Batch Normalization Batch normalization has been recently proposed in the machine learning community and addresses the so - called <m> internal covariate shift problem </m> by normalizing the mean and the variance of each layer ’s pre - activations for each training mini - batch .


class: no relation, base class: first -> second
first:
Up to date in the area of feature extraction , image feature representation method based on the spatial gradient is still lacking in efficiency especially for the <m> covariate case </m> like carrying bag and wearing a coat .
second:
Seven methodological frameworks were identified : <m> time-dependent covariate modelling </m> , generalised estimating equations , landmark analysis , two-stage modelling , joint-modelling , trajectory classification and machine learning .


class: no relation, base class: first -> second
first:
Up to date in the area of feature extraction , image feature representation method based on the spatial gradient is still lacking in efficiency especially for the <m> covariate case </m> like carrying bag and wearing a coat .
second:
We study <m> covariate imputation approaches </m> using fully conditional specification .


class: no relation, base class: first -> second
first:
Up to date in the area of feature extraction , image feature representation method based on the spatial gradient is still lacking in efficiency especially for the <m> covariate case </m> like carrying bag and wearing a coat .
second:
Under the <m> covariate shift assumption </m> , this would make the label prediction accuracy on the target domain to be the same as on the source domain .


class: no relation, base class: second -> first
first:
subsection : Batch Normalization Review Batch Normalization has been introduced in as an effective tool to reduce <m> internal covariate shift </m> in deep networks and accelerate the training process .
second:
One of the most widely used approaches to estimating ATE is <m> covariate adjustment </m> , also known as back - door adjustment or the G - computation formula .


class: no relation, base class: second -> first
first:
subsection : Batch Normalization Review Batch Normalization has been introduced in as an effective tool to reduce <m> internal covariate shift </m> in deep networks and accelerate the training process .
second:
Therefore , <m> covariate adjustment methods </m> are the most natural candidates for estimating ITE as well as ATE , using the estimates of .


class: no relation, base class: first -> second
first:
One of the most widely used approaches to estimating ATE is <m> covariate adjustment </m> , also known as back - door adjustment or the G - computation formula .
second:
subsection : Batch Normalization Batch normalization has been recently proposed in the machine learning community and addresses the so - called <m> internal covariate shift problem </m> by normalizing the mean and the variance of each layer ’s pre - activations for each training mini - batch .


class: no relation, base class: first -> second
first:
Therefore , <m> covariate adjustment methods </m> are the most natural candidates for estimating ITE as well as ATE , using the estimates of .
second:
subsection : Batch Normalization Batch normalization has been recently proposed in the machine learning community and addresses the so - called <m> internal covariate shift problem </m> by normalizing the mean and the variance of each layer ’s pre - activations for each training mini - batch .


class: no relation, base class: first -> second
first:
I should like to express my gratitude to Keith Rapley ( now Innovation Manager ) at British Airways for arranging sponsorship of this work , to Mark Raskino and Neil Morrison for early stimulation of ideas concerning neural networks ; and to Dr Maurice Barr , Louis Busuttil , Paul Summerbell , Dr Himadri Chatterjee and Rupert Blackley for making possible the forecasting and <m> attribute </m> work .
second:
First , we propose a <m> fully trainable attribute - based neural network </m> founded upon the CNN + RNN architecture , that can be applied to multiple V2L problems .


class: no relation, base class: first -> second
first:
I should like to express my gratitude to Keith Rapley ( now Innovation Manager ) at British Airways for arranging sponsorship of this work , to Mark Raskino and Neil Morrison for early stimulation of ideas concerning neural networks ; and to Dr Maurice Barr , Louis Busuttil , Paul Summerbell , Dr Himadri Chatterjee and Rupert Blackley for making possible the forecasting and <m> attribute </m> work .
second:
Shen et al . learn the residual image in the GAN framework , and adopt dual learning to learn two <m> reverse attribute transfer models </m> simultaneously .


class: no relation, base class: first -> second
first:
Compared with conventional perceptual loss , ours is more effective in computation , tailored to our specific <m> attribute transfer task </m> , and can serve as a kind of hidden - layer supervision or regularization to ease the training of the DIAT model .
second:
<m> Adversarial attribute loss </m> .


class: no relation, base class: second -> first
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
Our evaluation results show that the use of Bi-NN indeed produces promising <m> algorithm classification </m> results both within one language and across languages , and the encoding of dependencies from code into the underlying neural networks helps improve algorithm classification accuracy further .


class: no relation, base class: second -> first
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
Acoustic feature and its statistical moments passed through the different types of the <m> algorithm-based classifier </m> .


class: no relation, base class: second -> first
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
Our evaluation results show that the use of Bi-NN indeed produces promising <m> algorithm classification </m> results both within one language and across languages , and the encoding of dependencies from code into the underlying neural networks helps improve algorithm classification accuracy further .


class: no relation, base class: second -> first
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
Acoustic feature and its statistical moments passed through the different types of the <m> algorithm-based classifier </m> .


class: no relation, base class: second -> first
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
Our evaluation results show that the use of Bi-NN indeed produces promising <m> algorithm classification </m> results both within one language and across languages , and the encoding of dependencies from code into the underlying neural networks helps improve algorithm classification accuracy further .


class: no relation, base class: second -> first
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
Acoustic feature and its statistical moments passed through the different types of the <m> algorithm-based classifier </m> .


class: no relation, base class: second -> first
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .


class: no relation, base class: second -> first
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
Our evaluation results show that the use of Bi-NN indeed produces promising <m> algorithm classification </m> results both within one language and across languages , and the encoding of dependencies from code into the underlying neural networks helps improve algorithm classification accuracy further .


class: no relation, base class: second -> first
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
Acoustic feature and its statistical moments passed through the different types of the <m> algorithm-based classifier </m> .


class: no relation, base class: second -> first
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
Our evaluation results show that the use of Bi-NN indeed produces promising <m> algorithm classification </m> results both within one language and across languages , and the encoding of dependencies from code into the underlying neural networks helps improve algorithm classification accuracy further .


class: no relation, base class: second -> first
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
Acoustic feature and its statistical moments passed through the different types of the <m> algorithm-based classifier </m> .


class: no relation, base class: second -> first
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .


class: no relation, base class: second -> first
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
Our evaluation results show that the use of Bi-NN indeed produces promising <m> algorithm classification </m> results both within one language and across languages , and the encoding of dependencies from code into the underlying neural networks helps improve algorithm classification accuracy further .


class: no relation, base class: second -> first
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
Acoustic feature and its statistical moments passed through the different types of the <m> algorithm-based classifier </m> .


class: no relation, base class: same cluster
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
These qualities make it suitable for a broad class of real world applications such as <m> network classification </m> , and anomaly detection .


class: no relation, base class: same cluster
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
Our evaluation results show that the use of Bi-NN indeed produces promising <m> algorithm classification </m> results both within one language and across languages , and the encoding of dependencies from code into the underlying neural networks helps improve algorithm classification accuracy further .


class: no relation, base class: same cluster
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
Acoustic feature and its statistical moments passed through the different types of the <m> algorithm-based classifier </m> .


class: no relation, base class: first -> second
first:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .
second:
These qualities make it suitable for a broad class of real world applications such as <m> network classification </m> , and anomaly detection .


class: no relation, base class: first -> second
first:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .
second:
Our evaluation results show that the use of Bi-NN indeed produces promising <m> algorithm classification </m> results both within one language and across languages , and the encoding of dependencies from code into the underlying neural networks helps improve algorithm classification accuracy further .


class: no relation, base class: first -> second
first:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .
second:
Acoustic feature and its statistical moments passed through the different types of the <m> algorithm-based classifier </m> .


class: no relation, base class: same cluster
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
Our evaluation results show that the use of Bi-NN indeed produces promising <m> algorithm classification </m> results both within one language and across languages , and the encoding of dependencies from code into the underlying neural networks helps improve algorithm classification accuracy further .


class: no relation, base class: same cluster
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
Acoustic feature and its statistical moments passed through the different types of the <m> algorithm-based classifier </m> .


class: no relation, base class: same cluster
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
Our evaluation results show that the use of Bi-NN indeed produces promising <m> algorithm classification </m> results both within one language and across languages , and the encoding of dependencies from code into the underlying neural networks helps improve algorithm classification accuracy further .


class: no relation, base class: same cluster
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
Acoustic feature and its statistical moments passed through the different types of the <m> algorithm-based classifier </m> .


class: no relation, base class: second -> first
first:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .
second:
Our evaluation results show that the use of Bi-NN indeed produces promising <m> algorithm classification </m> results both within one language and across languages , and the encoding of dependencies from code into the underlying neural networks helps improve algorithm classification accuracy further .


class: no relation, base class: second -> first
first:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .
second:
Acoustic feature and its statistical moments passed through the different types of the <m> algorithm-based classifier </m> .


class: no relation, base class: second -> first
first:
These qualities make it suitable for a broad class of real world applications such as <m> network classification </m> , and anomaly detection .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: no relation, base class: first -> second
first:
These qualities make it suitable for a broad class of real world applications such as <m> network classification </m> , and anomaly detection .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: no relation, base class: same cluster
first:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .
second:
For example , a <m> fully - connected convolutional neural network </m> ( FCN ) also provides state - of - the - art results for image segmentation tasks in computer vision [ reference ] .


class: no relation, base class: second -> first
first:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: no relation, base class: second -> first
first:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .
second:
In our work , activation function of <m> neuron </m> is implemented with simple CMOS inverter to save area overhead .


class: no relation, base class: second -> first
first:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: no relation, base class: same cluster
first:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .
second:
For example , a <m> fully - connected convolutional neural network </m> ( FCN ) also provides state - of - the - art results for image segmentation tasks in computer vision [ reference ] .


class: no relation, base class: second -> first
first:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: no relation, base class: same cluster
first:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .
second:
For example , a <m> fully - connected convolutional neural network </m> ( FCN ) also provides state - of - the - art results for image segmentation tasks in computer vision [ reference ] .


class: no relation, base class: second -> first
first:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: no relation, base class: same cluster
first:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .
second:
For example , a <m> fully - connected convolutional neural network </m> ( FCN ) also provides state - of - the - art results for image segmentation tasks in computer vision [ reference ] .


class: no relation, base class: second -> first
first:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: no relation, base class: same cluster
first:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .
second:
For example , a <m> fully - connected convolutional neural network </m> ( FCN ) also provides state - of - the - art results for image segmentation tasks in computer vision [ reference ] .


class: no relation, base class: second -> first
first:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: no relation, base class: same cluster
first:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .
second:
For example , a <m> fully - connected convolutional neural network </m> ( FCN ) also provides state - of - the - art results for image segmentation tasks in computer vision [ reference ] .


class: no relation, base class: second -> first
first:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: no relation, base class: same cluster
first:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .
second:
For example , a <m> fully - connected convolutional neural network </m> ( FCN ) also provides state - of - the - art results for image segmentation tasks in computer vision [ reference ] .


class: no relation, base class: second -> first
first:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: no relation, base class: first -> second
first:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .
second:
Our evaluation results show that the use of Bi-NN indeed produces promising <m> algorithm classification </m> results both within one language and across languages , and the encoding of dependencies from code into the underlying neural networks helps improve algorithm classification accuracy further .


class: no relation, base class: first -> second
first:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .
second:
Acoustic feature and its statistical moments passed through the different types of the <m> algorithm-based classifier </m> .


class: no relation, base class: same cluster
first:
For example , a <m> fully - connected convolutional neural network </m> ( FCN ) also provides state - of - the - art results for image segmentation tasks in computer vision [ reference ] .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: no relation, base class: same cluster
first:
For example , a <m> fully - connected convolutional neural network </m> ( FCN ) also provides state - of - the - art results for image segmentation tasks in computer vision [ reference ] .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: no relation, base class: second -> first
first:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: no relation, base class: second -> first
first:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .
second:
In our work , activation function of <m> neuron </m> is implemented with simple CMOS inverter to save area overhead .


class: no relation, base class: second -> first
first:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: no relation, base class: second -> first
first:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: no relation, base class: first -> second
first:
In our work , activation function of <m> neuron </m> is implemented with simple CMOS inverter to save area overhead .
second:
Results : Table [ reference ] shows that using more than one output neuron in the discriminator significantly improved the performance of repulsive loss over the <m> one - neuron case </m> on CIFAR - 10 dataset .


class: no relation, base class: first -> second
first:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .
second:
In this algorithm , the <m> aggregation computation of process neuron </m> can be simplified and the complex integration procedure can be avoided , and the training of process neuron networks can equate to the training of common networks , namely the arbitrary function approximating problems are converted to the function optimization problems .


class: no relation, base class: second -> first
first:
Results indicate that , the stochastic resonance response of the double layers FHN neuron network is better than the <m> single FHN neuron model </m> , and has better stability , and can be effectively detected for input signal at a wider range of noise intensity .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: no relation, base class: first -> second
first:
Here we introduce a simpler method for <m> neuron targeting </m> .
second:
In particular , “ transfer ” means that DNN weights trained on large - scale data can be used in other tasks by two <m> light - weight neuron operations </m> : Scaling and Shifting ( SS ) , i.e. .


class: no relation, base class: second -> first
first:
That is , their output is computed as where is the total input to the neuron ( equivalently , the output of the <m> neuron ’s linear filter </m> added to the bias ) .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: no relation, base class: second -> first
first:
This nonlinearity has several advantages over traditional <m> saturating neuron models </m> , including a significant reduction in the training time required to reach a given error rate .
second:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .


class: no relation, base class: same cluster
first:
Our evaluation results show that the use of Bi-NN indeed produces promising <m> algorithm classification </m> results both within one language and across languages , and the encoding of dependencies from code into the underlying neural networks helps improve algorithm classification accuracy further .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: no relation, base class: same cluster
first:
Our evaluation results show that the use of Bi-NN indeed produces promising <m> algorithm classification </m> results both within one language and across languages , and the encoding of dependencies from code into the underlying neural networks helps improve algorithm classification accuracy further .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: no relation, base class: same cluster
first:
Our evaluation results show that the use of Bi-NN indeed produces promising <m> algorithm classification </m> results both within one language and across languages , and the encoding of dependencies from code into the underlying neural networks helps improve algorithm classification accuracy further .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: no relation, base class: same cluster
first:
Acoustic feature and its statistical moments passed through the different types of the <m> algorithm-based classifier </m> .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: no relation, base class: same cluster
first:
Acoustic feature and its statistical moments passed through the different types of the <m> algorithm-based classifier </m> .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: no relation, base class: same cluster
first:
Acoustic feature and its statistical moments passed through the different types of the <m> algorithm-based classifier </m> .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: no relation, base class: first -> second
first:
<m> Human-based evaluation </m> shows that people are generally positive towards PASS in regards to its clarity and fluency , and that the tailoring is accurately recognized in most cases .
second:
Table 2 shows the various normalized scores for <m> human start evaluation </m> .


class: no relation, base class: first -> second
first:
Design/methodology/approach – After reviewing the existing literature , particularly focusing on QA frameworks , procedures , and methodology , a process‐oriented model structured around three sequential non‐linear phases is presented : before : planning and analysis ; during : design , <m> prototype </m> and production ; and after : post‐production and delivery .
second:
If the feature vectors are sparse , we can maintain a single <m> prototype estimator </m> for all the features that have not yet been observed .


class: no relation, base class: first -> second
first:
Design/methodology/approach – After reviewing the existing literature , particularly focusing on QA frameworks , procedures , and methodology , a process‐oriented model structured around three sequential non‐linear phases is presented : before : planning and analysis ; during : design , <m> prototype </m> and production ; and after : post‐production and delivery .
second:
In practice , we build the <m> prototype dictionary </m> using k - means , as it is designed to minimize the approximation error between input descriptors and resulting centroids ( i.e . prototypes ) .


class: no relation, base class: first -> second
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: no relation, base class: first -> second
first:
After the recent developments in Artificial Neural Networks and deep learning techniques , <m> representation learning </m> has become the focus of many research interests .
second:
Since <m> DNS representation </m> is not necessarily unique , new algorithms to generate the number or representations and all equivalent representations are presented in this paper .


class: no relation, base class: first -> second
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: no relation, base class: first -> second
first:
Such a <m> representation learning problem </m> is referred to as network embedding , and it has attracted significant attention in recent years .
second:
Since <m> DNS representation </m> is not necessarily unique , new algorithms to generate the number or representations and all equivalent representations are presented in this paper .


class: no relation, base class: first -> second
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: no relation, base class: first -> second
first:
The developed <m> representation learning process </m> is based on a cascade of cascades of decision tree forests , where the high memory requirement and the high time cost inhibit the training of large models .
second:
Since <m> DNS representation </m> is not necessarily unique , new algorithms to generate the number or representations and all equivalent representations are presented in this paper .


class: no relation, base class: first -> second
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: no relation, base class: first -> second
first:
In this work , we present a new <m> representation learning-based approach </m> called SEMAC that jointly exploits fine-grained node features as well as the overall graph topology .
second:
Since <m> DNS representation </m> is not necessarily unique , new algorithms to generate the number or representations and all equivalent representations are presented in this paper .


class: no relation, base class: first -> second
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: no relation, base class: first -> second
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
Since <m> DNS representation </m> is not necessarily unique , new algorithms to generate the number or representations and all equivalent representations are presented in this paper .


class: no relation, base class: first -> second
first:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: no relation, base class: first -> second
first:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .
second:
Since <m> DNS representation </m> is not necessarily unique , new algorithms to generate the number or representations and all equivalent representations are presented in this paper .


class: no relation, base class: first -> second
first:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: no relation, base class: first -> second
first:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .
second:
Since <m> DNS representation </m> is not necessarily unique , new algorithms to generate the number or representations and all equivalent representations are presented in this paper .


class: no relation, base class: first -> second
first:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: no relation, base class: first -> second
first:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .
second:
Since <m> DNS representation </m> is not necessarily unique , new algorithms to generate the number or representations and all equivalent representations are presented in this paper .


class: no relation, base class: first -> second
first:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: no relation, base class: first -> second
first:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .
second:
Since <m> DNS representation </m> is not necessarily unique , new algorithms to generate the number or representations and all equivalent representations are presented in this paper .


class: no relation, base class: first -> second
first:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: no relation, base class: first -> second
first:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .
second:
Since <m> DNS representation </m> is not necessarily unique , new algorithms to generate the number or representations and all equivalent representations are presented in this paper .


class: no relation, base class: first -> second
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: no relation, base class: first -> second
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
Since <m> DNS representation </m> is not necessarily unique , new algorithms to generate the number or representations and all equivalent representations are presented in this paper .


class: no relation, base class: first -> second
first:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: no relation, base class: first -> second
first:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .
second:
Since <m> DNS representation </m> is not necessarily unique , new algorithms to generate the number or representations and all equivalent representations are presented in this paper .


class: no relation, base class: first -> second
first:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: no relation, base class: first -> second
first:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .
second:
Since <m> DNS representation </m> is not necessarily unique , new algorithms to generate the number or representations and all equivalent representations are presented in this paper .


class: no relation, base class: first -> second
first:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: no relation, base class: first -> second
first:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .
second:
Since <m> DNS representation </m> is not necessarily unique , new algorithms to generate the number or representations and all equivalent representations are presented in this paper .


class: no relation, base class: first -> second
first:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .
second:
We have also examined the score model with a network with a single <m> linear hidden layer </m> , with the embedding vector representation , which is equivalent to a linear regression model with the ability of representation learning .


class: no relation, base class: first -> second
first:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .
second:
Since <m> DNS representation </m> is not necessarily unique , new algorithms to generate the number or representations and all equivalent representations are presented in this paper .


class: no relation, base class: first -> second
first:
It includes the designs of <m> intermediate layer </m> and mapping mechanism , and in detail describes the virtual station data structure which can indicate the data change of wireless terminal station , and the mapping relationship between this data structure and wireless stations .
second:
Our method uses a deep convolutional network trained to directly optimize the embedding itself , rather than an <m> intermediate bottleneck layer </m> as in previous deep learning approaches .


class: no relation, base class: second -> first
first:
The added advantage of <m> hidden layer </m> for lip reading is that it takes into account the nonlinear variation of lip features while speaking .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: second -> first
first:
The added advantage of <m> hidden layer </m> for lip reading is that it takes into account the nonlinear variation of lip features while speaking .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: second -> first
first:
The added advantage of <m> hidden layer </m> for lip reading is that it takes into account the nonlinear variation of lip features while speaking .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: second -> first
first:
The added advantage of <m> hidden layer </m> for lip reading is that it takes into account the nonlinear variation of lip features while speaking .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: second -> first
first:
The added advantage of <m> hidden layer </m> for lip reading is that it takes into account the nonlinear variation of lip features while speaking .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: second -> first
first:
The added advantage of <m> hidden layer </m> for lip reading is that it takes into account the nonlinear variation of lip features while speaking .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: second -> first
first:
The experiment showed that the best <m> hidden layer architecture </m> ( HLA ) is 5 - 10 - 11 - 12 - 13 - 1 with learning function ( LF ) of trainlm , activation function ( AF ) of logsig and purelin , and learning rate ( LR ) of 0.5 .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: second -> first
first:
The experiment showed that the best <m> hidden layer architecture </m> ( HLA ) is 5 - 10 - 11 - 12 - 13 - 1 with learning function ( LF ) of trainlm , activation function ( AF ) of logsig and purelin , and learning rate ( LR ) of 0.5 .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: second -> first
first:
The experiment showed that the best <m> hidden layer architecture </m> ( HLA ) is 5 - 10 - 11 - 12 - 13 - 1 with learning function ( LF ) of trainlm , activation function ( AF ) of logsig and purelin , and learning rate ( LR ) of 0.5 .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: second -> first
first:
The experiment showed that the best <m> hidden layer architecture </m> ( HLA ) is 5 - 10 - 11 - 12 - 13 - 1 with learning function ( LF ) of trainlm , activation function ( AF ) of logsig and purelin , and learning rate ( LR ) of 0.5 .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: second -> first
first:
The experiment showed that the best <m> hidden layer architecture </m> ( HLA ) is 5 - 10 - 11 - 12 - 13 - 1 with learning function ( LF ) of trainlm , activation function ( AF ) of logsig and purelin , and learning rate ( LR ) of 0.5 .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: second -> first
first:
The experiment showed that the best <m> hidden layer architecture </m> ( HLA ) is 5 - 10 - 11 - 12 - 13 - 1 with learning function ( LF ) of trainlm , activation function ( AF ) of logsig and purelin , and learning rate ( LR ) of 0.5 .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: same cluster
first:
<m> Hidden Data Layer </m> consists of three essential elements , i.e. a Head Resident which doubles the functions of a public SDN controller and is a superior unit concerning public one , a Hidden Layer Protocol realising communication between individual network elements .
second:
PNN : For the purpose of capturing high - order feature interactions , PNN imposes a product layer between the embedding layer and the <m> first hidden layer </m> .


class: no relation, base class: second -> first
first:
<m> Hidden Data Layer </m> consists of three essential elements , i.e. a Head Resident which doubles the functions of a public SDN controller and is a superior unit concerning public one , a Hidden Layer Protocol realising communication between individual network elements .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: second -> first
first:
<m> Hidden Data Layer </m> consists of three essential elements , i.e. a Head Resident which doubles the functions of a public SDN controller and is a superior unit concerning public one , a Hidden Layer Protocol realising communication between individual network elements .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: second -> first
first:
<m> Hidden Data Layer </m> consists of three essential elements , i.e. a Head Resident which doubles the functions of a public SDN controller and is a superior unit concerning public one , a Hidden Layer Protocol realising communication between individual network elements .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: second -> first
first:
<m> Hidden Data Layer </m> consists of three essential elements , i.e. a Head Resident which doubles the functions of a public SDN controller and is a superior unit concerning public one , a Hidden Layer Protocol realising communication between individual network elements .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: second -> first
first:
<m> Hidden Data Layer </m> consists of three essential elements , i.e. a Head Resident which doubles the functions of a public SDN controller and is a superior unit concerning public one , a Hidden Layer Protocol realising communication between individual network elements .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: second -> first
first:
<m> Hidden Data Layer </m> consists of three essential elements , i.e. a Head Resident which doubles the functions of a public SDN controller and is a superior unit concerning public one , a Hidden Layer Protocol realising communication between individual network elements .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: second -> first
first:
Further <m> Hidden layer 1 </m> helps to extract data to estimate the forecasting equation and also it helps for evaluating estimated Model using different measures .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: second -> first
first:
Further <m> Hidden layer 1 </m> helps to extract data to estimate the forecasting equation and also it helps for evaluating estimated Model using different measures .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: second -> first
first:
Further <m> Hidden layer 1 </m> helps to extract data to estimate the forecasting equation and also it helps for evaluating estimated Model using different measures .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: second -> first
first:
Further <m> Hidden layer 1 </m> helps to extract data to estimate the forecasting equation and also it helps for evaluating estimated Model using different measures .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: second -> first
first:
Further <m> Hidden layer 1 </m> helps to extract data to estimate the forecasting equation and also it helps for evaluating estimated Model using different measures .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: second -> first
first:
Further <m> Hidden layer 1 </m> helps to extract data to estimate the forecasting equation and also it helps for evaluating estimated Model using different measures .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: second -> first
first:
where and are the question and the answer encoded vectors and denotes the output of the <m> hidden layer </m> of the MLP .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: second -> first
first:
where and are the question and the answer encoded vectors and denotes the output of the <m> hidden layer </m> of the MLP .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: second -> first
first:
where and are the question and the answer encoded vectors and denotes the output of the <m> hidden layer </m> of the MLP .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: second -> first
first:
where and are the question and the answer encoded vectors and denotes the output of the <m> hidden layer </m> of the MLP .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: second -> first
first:
where and are the question and the answer encoded vectors and denotes the output of the <m> hidden layer </m> of the MLP .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: second -> first
first:
where and are the question and the answer encoded vectors and denotes the output of the <m> hidden layer </m> of the MLP .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: second -> first
first:
Similar to our LoNGAE model , the GCN model can learn <m> hidden layer representations </m> that encode both local graph structure and features of nodes .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: second -> first
first:
Similar to our LoNGAE model , the GCN model can learn <m> hidden layer representations </m> that encode both local graph structure and features of nodes .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: second -> first
first:
Similar to our LoNGAE model , the GCN model can learn <m> hidden layer representations </m> that encode both local graph structure and features of nodes .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: second -> first
first:
Similar to our LoNGAE model , the GCN model can learn <m> hidden layer representations </m> that encode both local graph structure and features of nodes .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: second -> first
first:
Similar to our LoNGAE model , the GCN model can learn <m> hidden layer representations </m> that encode both local graph structure and features of nodes .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: second -> first
first:
Similar to our LoNGAE model , the GCN model can learn <m> hidden layer representations </m> that encode both local graph structure and features of nodes .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: second -> first
first:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: second -> first
first:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: second -> first
first:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: second -> first
first:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: second -> first
first:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: second -> first
first:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: second -> first
first:
PNN : For the purpose of capturing high - order feature interactions , PNN imposes a product layer between the embedding layer and the <m> first hidden layer </m> .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: second -> first
first:
PNN : For the purpose of capturing high - order feature interactions , PNN imposes a product layer between the embedding layer and the <m> first hidden layer </m> .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: second -> first
first:
PNN : For the purpose of capturing high - order feature interactions , PNN imposes a product layer between the embedding layer and the <m> first hidden layer </m> .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: second -> first
first:
PNN : For the purpose of capturing high - order feature interactions , PNN imposes a product layer between the embedding layer and the <m> first hidden layer </m> .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: second -> first
first:
PNN : For the purpose of capturing high - order feature interactions , PNN imposes a product layer between the embedding layer and the <m> first hidden layer </m> .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: second -> first
first:
PNN : For the purpose of capturing high - order feature interactions , PNN imposes a product layer between the embedding layer and the <m> first hidden layer </m> .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: second -> first
first:
They demonstrate large improvements in reducing the number of parameters of the output softmax layer , but only modest improvements for the <m> hidden fully connected layers </m> .
second:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .


class: no relation, base class: second -> first
first:
They demonstrate large improvements in reducing the number of parameters of the output softmax layer , but only modest improvements for the <m> hidden fully connected layers </m> .
second:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .


class: no relation, base class: second -> first
first:
They demonstrate large improvements in reducing the number of parameters of the output softmax layer , but only modest improvements for the <m> hidden fully connected layers </m> .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: no relation, base class: second -> first
first:
They demonstrate large improvements in reducing the number of parameters of the output softmax layer , but only modest improvements for the <m> hidden fully connected layers </m> .
second:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .


class: no relation, base class: second -> first
first:
They demonstrate large improvements in reducing the number of parameters of the output softmax layer , but only modest improvements for the <m> hidden fully connected layers </m> .
second:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .


class: no relation, base class: second -> first
first:
They demonstrate large improvements in reducing the number of parameters of the output softmax layer , but only modest improvements for the <m> hidden fully connected layers </m> .
second:
Because of the potential of <m> neural-like representations </m> for addressing the symbol-grounding problem , this sort of model holds a good deal of promise as a new explanatory mechanism for both language evolution and acquisition .


class: no relation, base class: second -> first
first:
They demonstrate large improvements in reducing the number of parameters of the output softmax layer , but only modest improvements for the <m> hidden fully connected layers </m> .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: no relation, base class: second -> first
first:
Their method simultaneously minimizes classification error while improving the directness and transparency of the <m> hidden layer learning process </m> .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: second -> first
first:
Their method simultaneously minimizes classification error while improving the directness and transparency of the <m> hidden layer learning process </m> .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: second -> first
first:
Their method simultaneously minimizes classification error while improving the directness and transparency of the <m> hidden layer learning process </m> .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: second -> first
first:
Their method simultaneously minimizes classification error while improving the directness and transparency of the <m> hidden layer learning process </m> .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: second -> first
first:
Their method simultaneously minimizes classification error while improving the directness and transparency of the <m> hidden layer learning process </m> .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: second -> first
first:
Their method simultaneously minimizes classification error while improving the directness and transparency of the <m> hidden layer learning process </m> .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: second -> first
first:
This was found sufficient , with the following <m> hidden layers learning </m> to combine the multi - scale features .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: second -> first
first:
This was found sufficient , with the following <m> hidden layers learning </m> to combine the multi - scale features .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: second -> first
first:
This was found sufficient , with the following <m> hidden layers learning </m> to combine the multi - scale features .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: second -> first
first:
This was found sufficient , with the following <m> hidden layers learning </m> to combine the multi - scale features .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: second -> first
first:
This was found sufficient , with the following <m> hidden layers learning </m> to combine the multi - scale features .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: second -> first
first:
This was found sufficient , with the following <m> hidden layers learning </m> to combine the multi - scale features .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: second -> first
first:
The resulting feature vectors are then scored using a non - linear function , namely a multi - layer perceptron with <m> one hidden layer </m> ( MLP ) :
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: second -> first
first:
The resulting feature vectors are then scored using a non - linear function , namely a multi - layer perceptron with <m> one hidden layer </m> ( MLP ) :
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: second -> first
first:
The resulting feature vectors are then scored using a non - linear function , namely a multi - layer perceptron with <m> one hidden layer </m> ( MLP ) :
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: second -> first
first:
The resulting feature vectors are then scored using a non - linear function , namely a multi - layer perceptron with <m> one hidden layer </m> ( MLP ) :
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: no relation, base class: second -> first
first:
The resulting feature vectors are then scored using a non - linear function , namely a multi - layer perceptron with <m> one hidden layer </m> ( MLP ) :
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: no relation, base class: second -> first
first:
The resulting feature vectors are then scored using a non - linear function , namely a multi - layer perceptron with <m> one hidden layer </m> ( MLP ) :
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: no relation, base class: first -> second
first:
Since <m> DNS representation </m> is not necessarily unique , new algorithms to generate the number or representations and all equivalent representations are presented in this paper .
second:
Our work is also related to the recent efforts in visualizing the <m> internal representations of DNNs </m> .


class: no relation, base class: second -> first
first:
The <m> firstlevel DNNs </m> are bottleneck feature extractors and the secondlevel DNNs serve as not only acoustic models but also feature combination modules .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: second -> first
first:
The <m> firstlevel DNNs </m> are bottleneck feature extractors and the secondlevel DNNs serve as not only acoustic models but also feature combination modules .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: second -> first
first:
The <m> firstlevel DNNs </m> are bottleneck feature extractors and the secondlevel DNNs serve as not only acoustic models but also feature combination modules .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: second -> first
first:
This dissertation explores model compression and acceleration methods for deep neural networks to save both memory and computation resources for the <m> hardware implementation of DNNs </m> .
second:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .


class: no relation, base class: second -> first
first:
This dissertation explores model compression and acceleration methods for deep neural networks to save both memory and computation resources for the <m> hardware implementation of DNNs </m> .
second:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .


class: no relation, base class: second -> first
first:
This dissertation explores model compression and acceleration methods for deep neural networks to save both memory and computation resources for the <m> hardware implementation of DNNs </m> .
second:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .


class: no relation, base class: second -> first
first:
Our experimental results show that DAN/K-DAN outperform the present <m> S-DNNs </m> and also the BP-trained DNNs , including multiplayer perceptron , deep belief network , etc . , without data augmentation applied .
second:
Our work is also related to the recent efforts in visualizing the <m> internal representations of DNNs </m> .


class: no relation, base class: first -> second
first:
Tuple-versioning of relations is the adopted method of <m> temporal data representation </m> .
second:
The results demonstrate that the algorithm can also learn <m> temporal sequence detection </m> .


class: no relation, base class: first -> second
first:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .
second:
The results demonstrate that the algorithm can also learn <m> temporal sequence detection </m> .


class: no relation, base class: first -> second
first:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .
second:
This user modeling approach comprises the following new novel aspects ( 1 ) Modeling microblog users behavior evolution by considering the different event phases ( 2 ) Characterizing users activity over time through a <m> temporal sequence representation </m> ( 3 ) Time-series-based selection of the most discriminative features characterizing users at each event phase .


class: no relation, base class: first -> second
first:
We claim that these refinement iterations should have a more direct and shared <m> temporal representation </m> .
second:
The results demonstrate that the algorithm can also learn <m> temporal sequence detection </m> .


class: no relation, base class: first -> second
first:
Furthermore , the optimal organization of <m> numerical data processing </m> in enterprises is analyzed on the example of predicting demand in the firm .
second:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .


class: no relation, base class: first -> second
first:
Furthermore , the optimal organization of <m> numerical data processing </m> in enterprises is analyzed on the example of predicting demand in the firm .
second:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .


class: no relation, base class: first -> second
first:
It is designed to support distributed computing and data sharing on the Internet through the use of distributed objects and a very general <m> numerical data model </m> .
second:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .


class: no relation, base class: first -> second
first:
It is designed to support distributed computing and data sharing on the Internet through the use of distributed objects and a very general <m> numerical data model </m> .
second:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .


class: no relation, base class: first -> second
first:
The fourth and last module , entitled Backtesting and <m> Numerical Results </m> , evaluates the accuracy of the trend predictive numerical models over a " significant " test set via two generic backtesting plans .
second:
Aiming at this problem , a novel <m> numerical feature extraction approach </m> EPLS is proposed .


class: no relation, base class: first -> second
first:
Formation features of the <m> numerical data source model </m> were examined .
second:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .


class: no relation, base class: first -> second
first:
Formation features of the <m> numerical data source model </m> were examined .
second:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .


class: no relation, base class: first -> second
first:
The selected papers are grouped into sessions on Process Algebras , <m> Categorical Approaches </m> , The pi-Calculus , Decidability & Complexity , Probability , Functional & Constraint Programming , Petri Nets , Verification , Automata & Causality , Practical Models , Shared Memory Systems .
second:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .


class: no relation, base class: first -> second
first:
The selected papers are grouped into sessions on Process Algebras , <m> Categorical Approaches </m> , The pi-Calculus , Decidability & Complexity , Probability , Functional & Constraint Programming , Petri Nets , Verification , Automata & Causality , Practical Models , Shared Memory Systems .
second:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .


class: no relation, base class: first -> second
first:
The selected papers are grouped into sessions on Process Algebras , <m> Categorical Approaches </m> , The pi-Calculus , Decidability & Complexity , Probability , Functional & Constraint Programming , Petri Nets , Verification , Automata & Causality , Practical Models , Shared Memory Systems .
second:
Features can be either signal-based , like spectrograms , or model-based , like <m> categorical classifiers </m> .


class: no relation, base class: first -> second
first:
The selected papers are grouped into sessions on Process Algebras , <m> Categorical Approaches </m> , The pi-Calculus , Decidability & Complexity , Probability , Functional & Constraint Programming , Petri Nets , Verification , Automata & Causality , Practical Models , Shared Memory Systems .
second:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .


class: no relation, base class: first -> second
first:
The selected papers are grouped into sessions on Process Algebras , <m> Categorical Approaches </m> , The pi-Calculus , Decidability & Complexity , Probability , Functional & Constraint Programming , Petri Nets , Verification , Automata & Causality , Practical Models , Shared Memory Systems .
second:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .


class: no relation, base class: first -> second
first:
The selected papers are grouped into sessions on Process Algebras , <m> Categorical Approaches </m> , The pi-Calculus , Decidability & Complexity , Probability , Functional & Constraint Programming , Petri Nets , Verification , Automata & Causality , Practical Models , Shared Memory Systems .
second:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .


class: no relation, base class: first -> second
first:
The selected papers are grouped into sessions on Process Algebras , <m> Categorical Approaches </m> , The pi-Calculus , Decidability & Complexity , Probability , Functional & Constraint Programming , Petri Nets , Verification , Automata & Causality , Practical Models , Shared Memory Systems .
second:
In this paper , two centroid-based classifiers are proposed for <m> categorical data classification </m> .


class: no relation, base class: first -> second
first:
The selected papers are grouped into sessions on Process Algebras , <m> Categorical Approaches </m> , The pi-Calculus , Decidability & Complexity , Probability , Functional & Constraint Programming , Petri Nets , Verification , Automata & Causality , Practical Models , Shared Memory Systems .
second:
Analysis of quantitative ( Bias , Relative RMSE ) and <m> categorical statistics </m> ( POD , FAR ) for the whole period show a more accurate spatial distribution of mean daily rainfall estimations in the lowlands than in the Andean regions .


class: no relation, base class: first -> second
first:
The selected papers are grouped into sessions on Process Algebras , <m> Categorical Approaches </m> , The pi-Calculus , Decidability & Complexity , Probability , Functional & Constraint Programming , Petri Nets , Verification , Automata & Causality , Practical Models , Shared Memory Systems .
second:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .


class: no relation, base class: first -> second
first:
The selected papers are grouped into sessions on Process Algebras , <m> Categorical Approaches </m> , The pi-Calculus , Decidability & Complexity , Probability , Functional & Constraint Programming , Petri Nets , Verification , Automata & Causality , Practical Models , Shared Memory Systems .
second:
The <m> categorical parameterization </m> , using the projected KL loss , has also been used in recent work to improve the critic of a policy gradient algorithm , D4PG , achieving significantly improved robustness and state - of - the - art performance across a variety of continuous control tasks .


class: no relation, base class: same cluster
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
Features can be either signal-based , like spectrograms , or model-based , like <m> categorical classifiers </m> .


class: no relation, base class: same cluster
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
In this paper , two centroid-based classifiers are proposed for <m> categorical data classification </m> .


class: no relation, base class: same cluster
first:
Furthermore , existing models ( in particular <m> categorical ones </m> ) are often too “ coarse-grained ” insofar as they impose various equalities on proofs which are , at the least , debatable .
second:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .


class: no relation, base class: same cluster
first:
Furthermore , existing models ( in particular <m> categorical ones </m> ) are often too “ coarse-grained ” insofar as they impose various equalities on proofs which are , at the least , debatable .
second:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .


class: no relation, base class: same cluster
first:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .
second:
Basic methods of the research : <m> categorical analysis </m> , determining of statistic parameters of word-forming models .


class: no relation, base class: same cluster
first:
Basic methods of the research : <m> categorical analysis </m> , determining of statistic parameters of word-forming models .
second:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .


class: no relation, base class: first -> second
first:
Analysis of quantitative ( Bias , Relative RMSE ) and <m> categorical statistics </m> ( POD , FAR ) for the whole period show a more accurate spatial distribution of mean daily rainfall estimations in the lowlands than in the Andean regions .
second:
section : Inference Across all of the presented experiments , we use <m> categorical sampling </m> during decoding with a tempered PixelRecursiveSuperResolution .


class: no relation, base class: first -> second
first:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: no relation, base class: first -> second
first:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .
second:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .


class: no relation, base class: first -> second
first:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .
second:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .


class: no relation, base class: first -> second
first:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: no relation, base class: first -> second
first:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: no relation, base class: first -> second
first:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: no relation, base class: first -> second
first:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .
second:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .


class: no relation, base class: first -> second
first:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .
second:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .


class: no relation, base class: first -> second
first:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: no relation, base class: first -> second
first:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: no relation, base class: first -> second
first:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .
second:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .


class: no relation, base class: first -> second
first:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .
second:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .


class: no relation, base class: first -> second
first:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: no relation, base class: first -> second
first:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: no relation, base class: first -> second
first:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: no relation, base class: first -> second
first:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .
second:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .


class: no relation, base class: first -> second
first:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .
second:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .


class: no relation, base class: first -> second
first:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: no relation, base class: same cluster
first:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .
second:
<m> Continuous feature set </m> and content monitoring of real-time monitoring traffic are designed .


class: no relation, base class: first -> second
first:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: no relation, base class: first -> second
first:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .
second:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .


class: no relation, base class: first -> second
first:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .
second:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .


class: no relation, base class: first -> second
first:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: no relation, base class: first -> second
first:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: no relation, base class: first -> second
first:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: no relation, base class: first -> second
first:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .
second:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .


class: no relation, base class: first -> second
first:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .
second:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .


class: no relation, base class: first -> second
first:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: no relation, base class: first -> second
first:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: no relation, base class: first -> second
first:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .
second:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .


class: no relation, base class: first -> second
first:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .
second:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .


class: no relation, base class: first -> second
first:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: no relation, base class: first -> second
first:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: no relation, base class: first -> second
first:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: no relation, base class: first -> second
first:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .
second:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .


class: no relation, base class: first -> second
first:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .
second:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .


class: no relation, base class: first -> second
first:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: no relation, base class: same cluster
first:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .
second:
<m> Continuous feature set </m> and content monitoring of real-time monitoring traffic are designed .


class: no relation, base class: first -> second
first:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: no relation, base class: first -> second
first:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .
second:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .


class: no relation, base class: first -> second
first:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .
second:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .


class: no relation, base class: first -> second
first:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: no relation, base class: first -> second
first:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: no relation, base class: first -> second
first:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: no relation, base class: first -> second
first:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .
second:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .


class: no relation, base class: first -> second
first:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .
second:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .


class: no relation, base class: first -> second
first:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: no relation, base class: first -> second
first:
<m> Continuous feature set </m> and content monitoring of real-time monitoring traffic are designed .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: no relation, base class: first -> second
first:
<m> Continuous feature set </m> and content monitoring of real-time monitoring traffic are designed .
second:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .


class: no relation, base class: first -> second
first:
<m> Continuous feature set </m> and content monitoring of real-time monitoring traffic are designed .
second:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .


class: no relation, base class: first -> second
first:
<m> Continuous feature set </m> and content monitoring of real-time monitoring traffic are designed .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: no relation, base class: first -> second
first:
<m> Continuous feature set </m> and content monitoring of real-time monitoring traffic are designed .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: no relation, base class: first -> second
first:
<m> Continuous feature set </m> and content monitoring of real-time monitoring traffic are designed .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: no relation, base class: first -> second
first:
<m> Continuous feature set </m> and content monitoring of real-time monitoring traffic are designed .
second:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .


class: no relation, base class: first -> second
first:
<m> Continuous feature set </m> and content monitoring of real-time monitoring traffic are designed .
second:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .


class: no relation, base class: first -> second
first:
<m> Continuous feature set </m> and content monitoring of real-time monitoring traffic are designed .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: no relation, base class: first -> second
first:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: no relation, base class: first -> second
first:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .


class: no relation, base class: first -> second
first:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .


class: no relation, base class: first -> second
first:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: no relation, base class: first -> second
first:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: no relation, base class: first -> second
first:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: no relation, base class: first -> second
first:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .


class: no relation, base class: first -> second
first:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .


class: no relation, base class: first -> second
first:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: no relation, base class: first -> second
first:
When uniform noise is added ( with values in the interval [ 0 , 1 ] ) , then the log - likelihoods of <m> continuous </m> and discrete models are directly comparable .
second:
This is due to the generator network in GAN is designed to be able to adjust the output continuously , which does not work on <m> discrete data generation </m> .


class: no relation, base class: same cluster
first:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .
second:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .


class: no relation, base class: same cluster
first:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .
second:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .


class: no relation, base class: same cluster
first:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: no relation, base class: same cluster
first:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .
second:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .


class: no relation, base class: same cluster
first:
When uniform noise is added ( with values in the interval [ 0 , 1 ] ) , then the log - likelihoods of continuous and <m> discrete models </m> are directly comparable .
second:
This is due to the generator network in GAN is designed to be able to adjust the output continuously , which does not work on <m> discrete data generation </m> .


class: no relation, base class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
The remainder of the book is devoted to a review of some of the existing AI software products for microcomputers , such as natural language query systems , decision support systems , expert system development shells , and <m> AI programming languages </m> .


class: no relation, base class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
subsection : Architecture - I ( Arc - I ) Architecture - I ( Arc - I ) , as illustrated in Figure [ reference ] , takes a conventional approach : It first finds the representation of each sentence , and then compares the representation for the two sentences with a multi - layer perceptron ( <m> MLP </m> ) .


class: no relation, base class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
Adding <m> MLP in </m> does not seem to help , yielding slightly worse result than without MLP .


class: no relation, base class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
subsection : Architecture - I ( Arc - I ) Architecture - I ( Arc - I ) , as illustrated in Figure [ reference ] , takes a conventional approach : It first finds the representation of each sentence , and then compares the representation for the two sentences with a multi - layer perceptron ( <m> MLP </m> ) .


class: no relation, base class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
Adding <m> MLP in </m> does not seem to help , yielding slightly worse result than without MLP .


class: no relation, base class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
subsection : Architecture - I ( Arc - I ) Architecture - I ( Arc - I ) , as illustrated in Figure [ reference ] , takes a conventional approach : It first finds the representation of each sentence , and then compares the representation for the two sentences with a multi - layer perceptron ( <m> MLP </m> ) .


class: no relation, base class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
Adding <m> MLP in </m> does not seem to help , yielding slightly worse result than without MLP .


class: no relation, base class: first -> second
first:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: no relation, base class: first -> second
first:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: no relation, base class: first -> second
first:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: no relation, base class: first -> second
first:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: no relation, base class: first -> second
first:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: no relation, base class: first -> second
first:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: no relation, base class: first -> second
first:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: no relation, base class: first -> second
first:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: no relation, base class: first -> second
first:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: no relation, base class: first -> second
first:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: no relation, base class: first -> second
first:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: no relation, base class: first -> second
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: no relation, base class: first -> second
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
The remainder of the book is devoted to a review of some of the existing AI software products for microcomputers , such as natural language query systems , decision support systems , expert system development shells , and <m> AI programming languages </m> .


class: no relation, base class: second -> first
first:
A classic problem , the spatial movement to goal problem , is used to illustrate the power of causal learning in vastly reducing the problem solving search space involved , and this is contrasted with the traditional <m> AI A * algorithm </m> which requires a huge search space .
second:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .


class: no relation, base class: first -> second
first:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: no relation, base class: first -> second
first:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: no relation, base class: first -> second
first:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: no relation, base class: first -> second
first:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: no relation, base class: first -> second
first:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: no relation, base class: first -> second
first:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: no relation, base class: first -> second
first:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: same cluster
first:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .
second:
The proposed Multi-level Authentication and Key Agreement Protocol ( <m> ML-AKA </m> ) focus on threats generated by all the layer of the IoT starting from application to the core network layer .


class: no relation, base class: first -> second
first:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .
second:
subsubsection : Comparison of Metric Learning Algorithms We evaluated the proposed XQDA algorithm and several metric learning algorithms , including Euclidean distance , Mahalanobis distance trained with genuine pairs , LMNN v2.5 , <m> ITML </m> , KISSME , and RLDA , with the same LOMO feature .


class: no relation, base class: second -> first
first:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .
second:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .


class: no relation, base class: same cluster
first:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .
second:
To reduce the complexity of the joint grid search in ML algorithm , a Modified <m> ML ( MML ) algorithm </m> with multiple one-dimensional searches is also proposed .


class: no relation, base class: second -> first
first:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
In this paper we define the concept of the Machine Learning Morphism ( <m> MLM </m> ) as a fundamental building block to express operations performed in machine learning such as data preprocessing , feature extraction , and model training .
second:
We share our experience and our reluctances , being aware of the fact that more advanced colleagues ( using SIARDS or <m> ADDML </m> , for instance ) may regard all as these primitive solutions . But , in the same time , we are convinced we are not the only ones lacking proper skills and expertise and our examples may be an example for other colleagues .


class: no relation, base class: first -> second
first:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .
second:
The proposed Multi-level Authentication and Key Agreement Protocol ( <m> ML-AKA </m> ) focus on threats generated by all the layer of the IoT starting from application to the core network layer .


class: no relation, base class: first -> second
first:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .
second:
We choose 7 popular conferences including AAAI , CIKM , <m> ICML </m> , KDD , NIPS , SIGIR , and WWW as the classification categories .


class: no relation, base class: second -> first
first:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: no relation, base class: second -> first
first:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: no relation, base class: second -> first
first:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: no relation, base class: second -> first
first:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: no relation, base class: second -> first
first:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: no relation, base class: second -> first
first:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: no relation, base class: second -> first
first:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: no relation, base class: second -> first
first:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: no relation, base class: second -> first
first:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: no relation, base class: second -> first
first:
To reduce the complexity of the joint grid search in ML algorithm , a Modified <m> ML ( MML ) algorithm </m> with multiple one-dimensional searches is also proposed .
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
To reduce the complexity of the joint grid search in ML algorithm , a Modified <m> ML ( MML ) algorithm </m> with multiple one-dimensional searches is also proposed .
second:
We share our experience and our reluctances , being aware of the fact that more advanced colleagues ( using SIARDS or <m> ADDML </m> , for instance ) may regard all as these primitive solutions . But , in the same time , we are convinced we are not the only ones lacking proper skills and expertise and our examples may be an example for other colleagues .


class: no relation, base class: second -> first
first:
To reduce the complexity of the joint grid search in ML algorithm , a Modified <m> ML ( MML ) algorithm </m> with multiple one-dimensional searches is also proposed .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: no relation, base class: same cluster
first:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?
second:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .


class: no relation, base class: first -> second
first:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?
second:
The proposed Multi-level Authentication and Key Agreement Protocol ( <m> ML-AKA </m> ) focus on threats generated by all the layer of the IoT starting from application to the core network layer .


class: no relation, base class: first -> second
first:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .
second:
The proposed Multi-level Authentication and Key Agreement Protocol ( <m> ML-AKA </m> ) focus on threats generated by all the layer of the IoT starting from application to the core network layer .


class: no relation, base class: same cluster
first:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: no relation, base class: first -> second
first:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .
second:
We choose 7 popular conferences including AAAI , CIKM , <m> ICML </m> , KDD , NIPS , SIGIR , and WWW as the classification categories .


class: no relation, base class: first -> second
first:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .
second:
subsubsection : Comparison of Metric Learning Algorithms We evaluated the proposed XQDA algorithm and several metric learning algorithms , including Euclidean distance , Mahalanobis distance trained with genuine pairs , LMNN v2.5 , <m> ITML </m> , KISSME , and RLDA , with the same LOMO feature .


class: no relation, base class: second -> first
first:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: no relation, base class: second -> first
first:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: no relation, base class: second -> first
first:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: no relation, base class: second -> first
first:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: no relation, base class: second -> first
first:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: no relation, base class: second -> first
first:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: no relation, base class: second -> first
first:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: no relation, base class: second -> first
first:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: no relation, base class: second -> first
first:
Standard <m> ML with modules </m> is extended by allowing axioms in module interface specifications and in place of code .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: no relation, base class: second -> first
first:
The proposed Multi-level Authentication and Key Agreement Protocol ( <m> ML-AKA </m> ) focus on threats generated by all the layer of the IoT starting from application to the core network layer .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: no relation, base class: second -> first
first:
The proposed Multi-level Authentication and Key Agreement Protocol ( <m> ML-AKA </m> ) focus on threats generated by all the layer of the IoT starting from application to the core network layer .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: no relation, base class: first -> second
first:
In other words , can one design a <m> filter </m> which , when given a query to any item I in the dataset , returns a sound item J that , although not necessarily in the dataset , differs from I as infrequently as possible .
second:
It first depicts the performance of the proposed models on different length of sentences , and then conducts a sensitivity analysis of <m> 2D filter </m> and max pooling size .


class: no relation, base class: first -> second
first:
In other words , can one design a <m> filter </m> which , when given a query to any item I in the dataset , returns a sound item J that , although not necessarily in the dataset , differs from I as infrequently as possible .
second:
. , ŷ ll , n ) is a diagonal matrix of spectral multipliers representing a <m> learnable filter </m> in the spectral domain , and ξ is a nonlinearity ( e.g. ReLU ) applied on the vertex - wise function values .


class: no relation, base class: second -> first
first:
The presented approach is expected to yield high echo suppression and fast convergence for the <m> filter weight adaptation </m> by means of multirate parallel processing of the filter bank .
second:
In addition , not using an explicit <m> interpolation filter </m> means that the network implicitly learns the processing necessary for SR .


class: no relation, base class: second -> first
first:
The <m> digital ND filter </m> can be used with regular image sensors and does not require hardware modifications .
second:
The proposal step also serves as a <m> filter </m> whose goal is to preserve the object boxes with high recall rate , while removing the easy negatives .


class: no relation, base class: second -> first
first:
The proposed algorithm follows the classical <m> particle filter stages </m> when a confidence measurement can be obtained from the system .
second:
In addition , not using an explicit <m> interpolation filter </m> means that the network implicitly learns the processing necessary for SR .


class: no relation, base class: second -> first
first:
<m> hat rhe hesr filter </m> for image segmentation can be found using a least squares fit between rhe input image and a user-dejned ideal image .
second:
In addition , not using an explicit <m> interpolation filter </m> means that the network implicitly learns the processing necessary for SR .


class: no relation, base class: first -> second
first:
The proposal step also serves as a <m> filter </m> whose goal is to preserve the object boxes with high recall rate , while removing the easy negatives .
second:
Before the DU - Net , a <m> Conv ( ) filter </m> with stride 2 and a max pooling would produce 128 features with resolution 64 64 .


class: no relation, base class: second -> first
first:
Sometimes methods adopt a more generic <m> filter constancy assumption </m> .
second:
In addition , not using an explicit <m> interpolation filter </m> means that the network implicitly learns the processing necessary for SR .


class: no relation, base class: second -> first
first:
Unlike activations of intermediate layers of a CNN , <m> linear filter weights </m> have a well defined semantics that can be visualized and analyzed using well developed tools of linear signal processing .
second:
In addition , not using an explicit <m> interpolation filter </m> means that the network implicitly learns the processing necessary for SR .


class: no relation, base class: second -> first
first:
The next step of the mean - field iteration is taking a weighted sum of the M filter outputs from the previous step , for each class label l. When each class label is considered individually , this can be viewed as usual convolution with a <m> 1 × 1 filter </m> with M input channels , and one output channel .
second:
In addition , not using an explicit <m> interpolation filter </m> means that the network implicitly learns the processing necessary for SR .


class: no relation, base class: first -> second
first:
In addition , not using an explicit <m> interpolation filter </m> means that the network implicitly learns the processing necessary for SR .
second:
Abstract Let U n be U -statistics based on a <m> symmetric kernel h </m> ( x , y ) and i.i.d . samples { X n ; n ≥ 1 } .


class: no relation, base class: first -> second
first:
In addition , not using an explicit <m> interpolation filter </m> means that the network implicitly learns the processing necessary for SR .
second:
Using tools from representation theory , we explain the limited expressive power of the Kendall kernel by characterizing its degenerate spectrum , and in sharp contrast , we prove that <m> Mallows ' kernel </m> is universal and characteristic .


class: no relation, base class: same cluster
first:
Behind the API it consists of a <m> kernel </m> implementing a reference monitor which controls access to security-relevant objects and attributes based on a configurable security policy .
second:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .


class: no relation, base class: first -> second
first:
Behind the API it consists of a <m> kernel </m> implementing a reference monitor which controls access to security-relevant objects and attributes based on a configurable security policy .
second:
Conventional methods include asymmetric metric learning , subspace interpolation , <m> geodesic flow kernel </m> , subspace alignment , covariance matrix alignment , .


class: no relation, base class: same cluster
first:
Behind the API it consists of a <m> kernel </m> implementing a reference monitor which controls access to security-relevant objects and attributes based on a configurable security policy .
second:
For example , defines a <m> kernel with dilation </m> .


class: no relation, base class: first -> second
first:
Behind the API it consists of a <m> kernel </m> implementing a reference monitor which controls access to security-relevant objects and attributes based on a configurable security policy .
second:
Assume an <m> isotropic stationary kernel </m> ( ) is used in MMD .


class: no relation, base class: same cluster
first:
Behind the API it consists of a <m> kernel </m> implementing a reference monitor which controls access to security-relevant objects and attributes based on a configurable security policy .
second:
a <m> kernel size </m> c × f 1 ×


class: no relation, base class: same cluster
first:
Behind the API it consists of a <m> kernel </m> implementing a reference monitor which controls access to security-relevant objects and attributes based on a configurable security policy .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: no relation, base class: same cluster
first:
Behind the API it consists of a <m> kernel </m> implementing a reference monitor which controls access to security-relevant objects and attributes based on a configurable security policy .
second:
It has been demonstrated that many distance metric learning or discriminative subspace based methods for person re - i d benefit from <m> kernelisation </m> because of the non - linearity in person ’s appearance .


class: no relation, base class: second -> first
first:
The <m> reproducing kernel particle method </m> is an efficient mesh free technique for the numerical solution of partial differential equations .
second:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .


class: no relation, base class: second -> first
first:
The <m> reproducing kernel particle method </m> is an efficient mesh free technique for the numerical solution of partial differential equations .
second:
However , the approach used fixed convolutional features , and can not jointly learn the <m> kernel classifier </m> and convolutional filters .


class: no relation, base class: second -> first
first:
The <m> reproducing kernel particle method </m> is an efficient mesh free technique for the numerical solution of partial differential equations .
second:
For example , defines a <m> kernel with dilation </m> .


class: no relation, base class: second -> first
first:
The <m> reproducing kernel particle method </m> is an efficient mesh free technique for the numerical solution of partial differential equations .
second:
a <m> kernel size </m> c × f 1 ×


class: no relation, base class: second -> first
first:
The <m> reproducing kernel particle method </m> is an efficient mesh free technique for the numerical solution of partial differential equations .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: no relation, base class: second -> first
first:
The <m> reproducing kernel particle method </m> is an efficient mesh free technique for the numerical solution of partial differential equations .
second:
It has been demonstrated that many distance metric learning or discriminative subspace based methods for person re - i d benefit from <m> kernelisation </m> because of the non - linearity in person ’s appearance .


class: no relation, base class: second -> first
first:
The candidate models belong to the Neyman-Scott Process ( NSP ) and results show that the NSP with <m> variance gamma kernel </m> is the most accurate for matching the microcells distribution for different scenarios .
second:
However , the approach used fixed convolutional features , and can not jointly learn the <m> kernel classifier </m> and convolutional filters .


class: no relation, base class: second -> first
first:
The candidate models belong to the Neyman-Scott Process ( NSP ) and results show that the NSP with <m> variance gamma kernel </m> is the most accurate for matching the microcells distribution for different scenarios .
second:
For example , defines a <m> kernel with dilation </m> .


class: no relation, base class: second -> first
first:
The candidate models belong to the Neyman-Scott Process ( NSP ) and results show that the NSP with <m> variance gamma kernel </m> is the most accurate for matching the microcells distribution for different scenarios .
second:
a <m> kernel size </m> c × f 1 ×


class: no relation, base class: second -> first
first:
The candidate models belong to the Neyman-Scott Process ( NSP ) and results show that the NSP with <m> variance gamma kernel </m> is the most accurate for matching the microcells distribution for different scenarios .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: no relation, base class: second -> first
first:
The candidate models belong to the Neyman-Scott Process ( NSP ) and results show that the NSP with <m> variance gamma kernel </m> is the most accurate for matching the microcells distribution for different scenarios .
second:
It has been demonstrated that many distance metric learning or discriminative subspace based methods for person re - i d benefit from <m> kernelisation </m> because of the non - linearity in person ’s appearance .


class: no relation, base class: second -> first
first:
The proposed <m> online kernel method </m> is evaluated by handling the Cascades Tanks system .
second:
Realistic software research environments will be provided by connecting PSIs to encourage <m> kernel language implementation </m> and parallel operating system development .


class: no relation, base class: second -> first
first:
In the constant bit rate encoder , the <m> kernel MPEG encoder circuit </m> is followed by a buffer having a certain actual capacity .
second:
Such kernels are built with a parameterized “ neural response ” function , which consists in computing the maximal response of a <m> base kernel </m> over a local neighborhood .


class: no relation, base class: second -> first
first:
Kernel online convex optimization ( KOCO ) is a framework combining the expressiveness of <m> non-parametric kernel models </m> with the regret guarantees of online learning .
second:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .


class: no relation, base class: second -> first
first:
Kernel online convex optimization ( KOCO ) is a framework combining the expressiveness of <m> non-parametric kernel models </m> with the regret guarantees of online learning .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: no relation, base class: second -> first
first:
Iris location is a <m> kernel procession </m> in an iris recognition system .
second:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .


class: no relation, base class: second -> first
first:
Iris location is a <m> kernel procession </m> in an iris recognition system .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: no relation, base class: first -> second
first:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .
second:
Conventional methods include asymmetric metric learning , subspace interpolation , <m> geodesic flow kernel </m> , subspace alignment , covariance matrix alignment , .


class: no relation, base class: same cluster
first:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .
second:
This thesis addresses TCP/IP 's application to wireless and secure area to discuss , it analyzes several problems about a few of critical parts within TCP/IP in details , simulaneously delves into slow start algorithm , initial window size , increasing policy of ini- tial window , RTO estimator , delay acknowledgement , deficiency of <m> kernel implementation </m> and security leakage and gives some constructive solutions in order to further research to pave a basis .


class: no relation, base class: same cluster
first:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .
second:
The <m> kernel-level implementation framework </m> of VMDFS is illustrated based on Linux 2.4.22 .


class: no relation, base class: same cluster
first:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .
second:
From the implementation perspective , we show that using application level measurements is highly CPU intensive , while a <m> kernel based implementation </m> has comparably a very low CPU usage .


class: no relation, base class: same cluster
first:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .
second:
This paper describes a <m> kernel-mode implementation </m> and compares two different design alternatives .


class: no relation, base class: same cluster
first:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .
second:
The principle and <m> kernel implementing technology </m> of Sybase web solution are introduced in detail .


class: no relation, base class: same cluster
first:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .
second:
a <m> kernel size </m> c × f 1 ×


class: no relation, base class: same cluster
first:
However , the approach used fixed convolutional features , and can not jointly learn the <m> kernel classifier </m> and convolutional filters .
second:
For example , defines a <m> kernel with dilation </m> .


class: no relation, base class: first -> second
first:
However , the approach used fixed convolutional features , and can not jointly learn the <m> kernel classifier </m> and convolutional filters .
second:
Assume an <m> isotropic stationary kernel </m> ( ) is used in MMD .


class: no relation, base class: same cluster
first:
However , the approach used fixed convolutional features , and can not jointly learn the <m> kernel classifier </m> and convolutional filters .
second:
a <m> kernel size </m> c × f 1 ×


class: no relation, base class: same cluster
first:
However , the approach used fixed convolutional features , and can not jointly learn the <m> kernel classifier </m> and convolutional filters .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: no relation, base class: same cluster
first:
However , the approach used fixed convolutional features , and can not jointly learn the <m> kernel classifier </m> and convolutional filters .
second:
It has been demonstrated that many distance metric learning or discriminative subspace based methods for person re - i d benefit from <m> kernelisation </m> because of the non - linearity in person ’s appearance .


class: no relation, base class: first -> second
first:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .
second:
Conventional methods include asymmetric metric learning , subspace interpolation , <m> geodesic flow kernel </m> , subspace alignment , covariance matrix alignment , .


class: no relation, base class: same cluster
first:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .
second:
For example , defines a <m> kernel with dilation </m> .


class: no relation, base class: first -> second
first:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .
second:
Assume an <m> isotropic stationary kernel </m> ( ) is used in MMD .


class: no relation, base class: same cluster
first:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .
second:
a <m> kernel size </m> c × f 1 ×


class: no relation, base class: same cluster
first:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: no relation, base class: second -> first
first:
Conventional methods include asymmetric metric learning , subspace interpolation , <m> geodesic flow kernel </m> , subspace alignment , covariance matrix alignment , .
second:
For example , defines a <m> kernel with dilation </m> .


class: no relation, base class: second -> first
first:
Conventional methods include asymmetric metric learning , subspace interpolation , <m> geodesic flow kernel </m> , subspace alignment , covariance matrix alignment , .
second:
This thesis addresses TCP/IP 's application to wireless and secure area to discuss , it analyzes several problems about a few of critical parts within TCP/IP in details , simulaneously delves into slow start algorithm , initial window size , increasing policy of ini- tial window , RTO estimator , delay acknowledgement , deficiency of <m> kernel implementation </m> and security leakage and gives some constructive solutions in order to further research to pave a basis .


class: no relation, base class: second -> first
first:
Conventional methods include asymmetric metric learning , subspace interpolation , <m> geodesic flow kernel </m> , subspace alignment , covariance matrix alignment , .
second:
The <m> kernel-level implementation framework </m> of VMDFS is illustrated based on Linux 2.4.22 .


class: no relation, base class: second -> first
first:
Conventional methods include asymmetric metric learning , subspace interpolation , <m> geodesic flow kernel </m> , subspace alignment , covariance matrix alignment , .
second:
From the implementation perspective , we show that using application level measurements is highly CPU intensive , while a <m> kernel based implementation </m> has comparably a very low CPU usage .


class: no relation, base class: second -> first
first:
Conventional methods include asymmetric metric learning , subspace interpolation , <m> geodesic flow kernel </m> , subspace alignment , covariance matrix alignment , .
second:
This paper describes a <m> kernel-mode implementation </m> and compares two different design alternatives .


class: no relation, base class: second -> first
first:
Conventional methods include asymmetric metric learning , subspace interpolation , <m> geodesic flow kernel </m> , subspace alignment , covariance matrix alignment , .
second:
The principle and <m> kernel implementing technology </m> of Sybase web solution are introduced in detail .


class: no relation, base class: second -> first
first:
Conventional methods include asymmetric metric learning , subspace interpolation , <m> geodesic flow kernel </m> , subspace alignment , covariance matrix alignment , .
second:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .


class: no relation, base class: second -> first
first:
Conventional methods include asymmetric metric learning , subspace interpolation , <m> geodesic flow kernel </m> , subspace alignment , covariance matrix alignment , .
second:
a <m> kernel size </m> c × f 1 ×


class: no relation, base class: second -> first
first:
Conventional methods include asymmetric metric learning , subspace interpolation , <m> geodesic flow kernel </m> , subspace alignment , covariance matrix alignment , .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: no relation, base class: second -> first
first:
Conventional methods include asymmetric metric learning , subspace interpolation , <m> geodesic flow kernel </m> , subspace alignment , covariance matrix alignment , .
second:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .


class: no relation, base class: second -> first
first:
For <m> graphlet kernel </m> ( GK ) , we chose graphlets size and for deep graph kernels ( DGK ) , we report the best classification accuracy obtained among : deep graphlet kernel , deep shortest path kernel and deep Weisfeiler - Lehman kernel .
second:
Realistic software research environments will be provided by connecting PSIs to encourage <m> kernel language implementation </m> and parallel operating system development .


class: no relation, base class: first -> second
first:
For example , defines a <m> kernel with dilation </m> .
second:
Assume an <m> isotropic stationary kernel </m> ( ) is used in MMD .


class: no relation, base class: same cluster
first:
For example , defines a <m> kernel with dilation </m> .
second:
This thesis addresses TCP/IP 's application to wireless and secure area to discuss , it analyzes several problems about a few of critical parts within TCP/IP in details , simulaneously delves into slow start algorithm , initial window size , increasing policy of ini- tial window , RTO estimator , delay acknowledgement , deficiency of <m> kernel implementation </m> and security leakage and gives some constructive solutions in order to further research to pave a basis .


class: no relation, base class: same cluster
first:
For example , defines a <m> kernel with dilation </m> .
second:
The <m> kernel-level implementation framework </m> of VMDFS is illustrated based on Linux 2.4.22 .


class: no relation, base class: same cluster
first:
For example , defines a <m> kernel with dilation </m> .
second:
From the implementation perspective , we show that using application level measurements is highly CPU intensive , while a <m> kernel based implementation </m> has comparably a very low CPU usage .


class: no relation, base class: same cluster
first:
For example , defines a <m> kernel with dilation </m> .
second:
This paper describes a <m> kernel-mode implementation </m> and compares two different design alternatives .


class: no relation, base class: same cluster
first:
For example , defines a <m> kernel with dilation </m> .
second:
The principle and <m> kernel implementing technology </m> of Sybase web solution are introduced in detail .


class: no relation, base class: same cluster
first:
For example , defines a <m> kernel with dilation </m> .
second:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .


class: no relation, base class: same cluster
first:
For example , defines a <m> kernel with dilation </m> .
second:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .


class: no relation, base class: same cluster
first:
For example , defines a <m> kernel with dilation </m> .
second:
It has been demonstrated that many distance metric learning or discriminative subspace based methods for person re - i d benefit from <m> kernelisation </m> because of the non - linearity in person ’s appearance .


class: no relation, base class: second -> first
first:
Assume an <m> isotropic stationary kernel </m> ( ) is used in MMD .
second:
This thesis addresses TCP/IP 's application to wireless and secure area to discuss , it analyzes several problems about a few of critical parts within TCP/IP in details , simulaneously delves into slow start algorithm , initial window size , increasing policy of ini- tial window , RTO estimator , delay acknowledgement , deficiency of <m> kernel implementation </m> and security leakage and gives some constructive solutions in order to further research to pave a basis .


class: no relation, base class: second -> first
first:
Assume an <m> isotropic stationary kernel </m> ( ) is used in MMD .
second:
The <m> kernel-level implementation framework </m> of VMDFS is illustrated based on Linux 2.4.22 .


class: no relation, base class: second -> first
first:
Assume an <m> isotropic stationary kernel </m> ( ) is used in MMD .
second:
From the implementation perspective , we show that using application level measurements is highly CPU intensive , while a <m> kernel based implementation </m> has comparably a very low CPU usage .


class: no relation, base class: second -> first
first:
Assume an <m> isotropic stationary kernel </m> ( ) is used in MMD .
second:
This paper describes a <m> kernel-mode implementation </m> and compares two different design alternatives .


class: no relation, base class: second -> first
first:
Assume an <m> isotropic stationary kernel </m> ( ) is used in MMD .
second:
The principle and <m> kernel implementing technology </m> of Sybase web solution are introduced in detail .


class: no relation, base class: second -> first
first:
Assume an <m> isotropic stationary kernel </m> ( ) is used in MMD .
second:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .


class: no relation, base class: second -> first
first:
Assume an <m> isotropic stationary kernel </m> ( ) is used in MMD .
second:
a <m> kernel size </m> c × f 1 ×


class: no relation, base class: second -> first
first:
Assume an <m> isotropic stationary kernel </m> ( ) is used in MMD .
second:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .


class: no relation, base class: second -> first
first:
Assume an <m> isotropic stationary kernel </m> ( ) is used in MMD .
second:
It has been demonstrated that many distance metric learning or discriminative subspace based methods for person re - i d benefit from <m> kernelisation </m> because of the non - linearity in person ’s appearance .


class: no relation, base class: same cluster
first:
This thesis addresses TCP/IP 's application to wireless and secure area to discuss , it analyzes several problems about a few of critical parts within TCP/IP in details , simulaneously delves into slow start algorithm , initial window size , increasing policy of ini- tial window , RTO estimator , delay acknowledgement , deficiency of <m> kernel implementation </m> and security leakage and gives some constructive solutions in order to further research to pave a basis .
second:
a <m> kernel size </m> c × f 1 ×


class: no relation, base class: same cluster
first:
This thesis addresses TCP/IP 's application to wireless and secure area to discuss , it analyzes several problems about a few of critical parts within TCP/IP in details , simulaneously delves into slow start algorithm , initial window size , increasing policy of ini- tial window , RTO estimator , delay acknowledgement , deficiency of <m> kernel implementation </m> and security leakage and gives some constructive solutions in order to further research to pave a basis .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: no relation, base class: same cluster
first:
This thesis addresses TCP/IP 's application to wireless and secure area to discuss , it analyzes several problems about a few of critical parts within TCP/IP in details , simulaneously delves into slow start algorithm , initial window size , increasing policy of ini- tial window , RTO estimator , delay acknowledgement , deficiency of <m> kernel implementation </m> and security leakage and gives some constructive solutions in order to further research to pave a basis .
second:
It has been demonstrated that many distance metric learning or discriminative subspace based methods for person re - i d benefit from <m> kernelisation </m> because of the non - linearity in person ’s appearance .


class: no relation, base class: same cluster
first:
The <m> kernel-level implementation framework </m> of VMDFS is illustrated based on Linux 2.4.22 .
second:
a <m> kernel size </m> c × f 1 ×


class: no relation, base class: same cluster
first:
The <m> kernel-level implementation framework </m> of VMDFS is illustrated based on Linux 2.4.22 .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: no relation, base class: same cluster
first:
The <m> kernel-level implementation framework </m> of VMDFS is illustrated based on Linux 2.4.22 .
second:
It has been demonstrated that many distance metric learning or discriminative subspace based methods for person re - i d benefit from <m> kernelisation </m> because of the non - linearity in person ’s appearance .


class: no relation, base class: same cluster
first:
From the implementation perspective , we show that using application level measurements is highly CPU intensive , while a <m> kernel based implementation </m> has comparably a very low CPU usage .
second:
a <m> kernel size </m> c × f 1 ×


class: no relation, base class: same cluster
first:
From the implementation perspective , we show that using application level measurements is highly CPU intensive , while a <m> kernel based implementation </m> has comparably a very low CPU usage .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: no relation, base class: same cluster
first:
From the implementation perspective , we show that using application level measurements is highly CPU intensive , while a <m> kernel based implementation </m> has comparably a very low CPU usage .
second:
It has been demonstrated that many distance metric learning or discriminative subspace based methods for person re - i d benefit from <m> kernelisation </m> because of the non - linearity in person ’s appearance .


class: no relation, base class: same cluster
first:
This paper describes a <m> kernel-mode implementation </m> and compares two different design alternatives .
second:
a <m> kernel size </m> c × f 1 ×


class: no relation, base class: same cluster
first:
This paper describes a <m> kernel-mode implementation </m> and compares two different design alternatives .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: no relation, base class: same cluster
first:
This paper describes a <m> kernel-mode implementation </m> and compares two different design alternatives .
second:
It has been demonstrated that many distance metric learning or discriminative subspace based methods for person re - i d benefit from <m> kernelisation </m> because of the non - linearity in person ’s appearance .


class: no relation, base class: same cluster
first:
The principle and <m> kernel implementing technology </m> of Sybase web solution are introduced in detail .
second:
a <m> kernel size </m> c × f 1 ×


class: no relation, base class: same cluster
first:
The principle and <m> kernel implementing technology </m> of Sybase web solution are introduced in detail .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: no relation, base class: same cluster
first:
The principle and <m> kernel implementing technology </m> of Sybase web solution are introduced in detail .
second:
It has been demonstrated that many distance metric learning or discriminative subspace based methods for person re - i d benefit from <m> kernelisation </m> because of the non - linearity in person ’s appearance .


class: no relation, base class: first -> second
first:
Realistic software research environments will be provided by connecting PSIs to encourage <m> kernel language implementation </m> and parallel operating system development .
second:
Graphical abstractDisplay Omitted HighlightsDiscretization and spectrum-free computation of the <m> volumetric heat kernel </m> .


class: no relation, base class: second -> first
first:
Realistic software research environments will be provided by connecting PSIs to encourage <m> kernel language implementation </m> and parallel operating system development .
second:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .


class: no relation, base class: first -> second
first:
Realistic software research environments will be provided by connecting PSIs to encourage <m> kernel language implementation </m> and parallel operating system development .
second:
They can be used for <m> highest order Volterra kernel realization </m> as well .


class: no relation, base class: second -> first
first:
Realistic software research environments will be provided by connecting PSIs to encourage <m> kernel language implementation </m> and parallel operating system development .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: no relation, base class: same cluster
first:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .
second:
a <m> kernel size </m> c × f 1 ×


class: no relation, base class: same cluster
first:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: no relation, base class: same cluster
first:
a <m> kernel size </m> c × f 1 ×
second:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .


class: no relation, base class: same cluster
first:
a <m> kernel size </m> c × f 1 ×
second:
It has been demonstrated that many distance metric learning or discriminative subspace based methods for person re - i d benefit from <m> kernelisation </m> because of the non - linearity in person ’s appearance .


class: no relation, base class: same cluster
first:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .
second:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .


class: no relation, base class: same cluster
first:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .
second:
It has been demonstrated that many distance metric learning or discriminative subspace based methods for person re - i d benefit from <m> kernelisation </m> because of the non - linearity in person ’s appearance .


class: no relation, base class: same cluster
first:
The <m> receptive field concept </m> is described , along with how this concept has been applied to retinal ganglion cells .
second:
Learning pooling functions is analogous to <m> receptive field learning </m> .


class: no relation, base class: same cluster
first:
The <m> receptive field concept </m> is described , along with how this concept has been applied to retinal ganglion cells .
second:
[ t ! ] <m> ReceptiveField </m> : Create Receptive Field { algorithmic} [ 1 ] vertex v , graph labeling ℓ , receptive field size k Gnorm We have the following result concerning the complexity of the optimal normalization problem .


class: no relation, base class: second -> first
first:
Subsequently , we improved the RTLSR model into a four-parameter version ( RTLSRV4p ) with a new <m> volumetric scattering kernel </m> derived from the assumption of vertical leaf inclination .
second:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .


class: no relation, base class: second -> first
first:
Subsequently , we improved the RTLSR model into a four-parameter version ( RTLSRV4p ) with a new <m> volumetric scattering kernel </m> derived from the assumption of vertical leaf inclination .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: no relation, base class: second -> first
first:
The convolutions performed in each stage use <m> volumetric kernels </m> having size 5 × 5 × 5 voxels .
second:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .


class: no relation, base class: same cluster
first:
The convolutions performed in each stage use <m> volumetric kernels </m> having size 5 × 5 × 5 voxels .
second:
The learned kernel features include : Spatio_dim kernel feature , Grad_nor kernel feature , <m> Volum_shape kernel feature </m> , and KPCA kernel feature .


class: no relation, base class: second -> first
first:
The convolutions performed in each stage use <m> volumetric kernels </m> having size 5 × 5 × 5 voxels .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: no relation, base class: first -> second
first:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .
second:
In this paper , we present an algorithm that allows for different memory lengths of the <m> linear and quadratic Volterra kernel </m> while preserving the advantages of fast convolution techniques in the frequency domain for a second-order Volterra filter .


class: no relation, base class: first -> second
first:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .
second:
Subsequently , a general model that computes the ratio of two non-linear functionals , each comprising <m> linear ( first order Volterra kernel </m> ) and quadratic ( second-order Volterra kernel ) filters , is proposed .


class: no relation, base class: first -> second
first:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .
second:
It provides a set of diagnostic features formed on base of <m> multidimensional Volterra kernels </m> : discrete values of Volterra kernels , heuristic features , moments and wavelet transform coefficients .


class: no relation, base class: first -> second
first:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .
second:
The learned kernel features include : Spatio_dim kernel feature , Grad_nor kernel feature , <m> Volum_shape kernel feature </m> , and KPCA kernel feature .


class: no relation, base class: first -> second
first:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .
second:
The convolutions performed in each stage use <m> volumetric kernels </m> having size voxels .


class: no relation, base class: first -> second
first:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .
second:
This is performed through convolution with <m> voxels wide kernels </m> applied with stride ( Figure [ reference ] ) .


class: no relation, base class: same cluster
first:
In this paper , we present an algorithm that allows for different memory lengths of the <m> linear and quadratic Volterra kernel </m> while preserving the advantages of fast convolution techniques in the frequency domain for a second-order Volterra filter .
second:
It provides a set of diagnostic features formed on base of <m> multidimensional Volterra kernels </m> : discrete values of Volterra kernels , heuristic features , moments and wavelet transform coefficients .


class: no relation, base class: second -> first
first:
In this paper , we present an algorithm that allows for different memory lengths of the <m> linear and quadratic Volterra kernel </m> while preserving the advantages of fast convolution techniques in the frequency domain for a second-order Volterra filter .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: no relation, base class: same cluster
first:
Subsequently , a general model that computes the ratio of two non-linear functionals , each comprising <m> linear ( first order Volterra kernel </m> ) and quadratic ( second-order Volterra kernel ) filters , is proposed .
second:
It provides a set of diagnostic features formed on base of <m> multidimensional Volterra kernels </m> : discrete values of Volterra kernels , heuristic features , moments and wavelet transform coefficients .


class: no relation, base class: second -> first
first:
Subsequently , a general model that computes the ratio of two non-linear functionals , each comprising <m> linear ( first order Volterra kernel </m> ) and quadratic ( second-order Volterra kernel ) filters , is proposed .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: no relation, base class: second -> first
first:
It provides a set of diagnostic features formed on base of <m> multidimensional Volterra kernels </m> : discrete values of Volterra kernels , heuristic features , moments and wavelet transform coefficients .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: no relation, base class: second -> first
first:
The learned kernel features include : Spatio_dim kernel feature , Grad_nor kernel feature , <m> Volum_shape kernel feature </m> , and KPCA kernel feature .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: no relation, base class: same cluster
first:
The learned kernel features include : Spatio_dim kernel feature , Grad_nor kernel feature , <m> Volum_shape kernel feature </m> , and KPCA kernel feature .
second:
The convolutions performed in each stage use <m> volumetric kernels </m> having size voxels .


class: no relation, base class: first -> second
first:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .
second:
The convolutions performed in each stage use <m> volumetric kernels </m> having size voxels .


class: no relation, base class: first -> second
first:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .
second:
This is performed through convolution with <m> voxels wide kernels </m> applied with stride ( Figure [ reference ] ) .


class: no relation, base class: first -> second
first:
An acoustically-derived lexicon is used to construct word models based upon <m> subword segment models </m> .
second:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .


class: no relation, base class: first -> second
first:
Inspired by three characteristics , we develop three steps to <m> word and subword segmentation approach </m> : in the word or sub-words segmentation step , a peaks detection function is adopted to model the maximum and minimum peaks .
second:
The improved Chinese word segmentation is made of a bigram segmentation and a segmentation correction , new words recognition and disambiguation through the <m> bigram segmentation </m> , check the accuracy of segmentation results using the segmentation correction from the perspective of syntax .


class: no relation, base class: first -> second
first:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .
second:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .


class: no relation, base class: first -> second
first:
Among the different techniques , the <m> word/sub-word segmentation </m> is simple and produces very good results .
second:
The improved Chinese word segmentation is made of a bigram segmentation and a segmentation correction , new words recognition and disambiguation through the <m> bigram segmentation </m> , check the accuracy of segmentation results using the segmentation correction from the perspective of syntax .


class: no relation, base class: first -> second
first:
The parallelism of the convolutional , pooling , and highway layers allows training speed comparable to <m> subword - level models </m> without hard - coded text segmentation .
second:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .


class: no relation, base class: first -> second
first:
The beam widths are and respectively for the <m> subword - level </m> and character - level decoders .
second:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .


class: no relation, base class: first -> second
first:
Neural Machine Translation of Rare Words with <m> Subword Units </m> section :
second:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .


class: no relation, base class: same cluster
first:
An extended comparison between CA and Back Propagation Error ( <m> BPE </m> ) are presented and the potential of information induction and reduction from the training set is revised for both methods .
second:
There are many <m> BPE methods </m> used to collect building data and inform designs .


class: no relation, base class: first -> second
first:
An extended comparison between CA and Back Propagation Error ( <m> BPE </m> ) are presented and the potential of information induction and reduction from the training set is revised for both methods .
second:
For English→German , we observe the best BLEU score of 25.3 with C2 - 50k , but the best CHRF3 score of 54.1 with <m> BPE - J90k </m> .


class: no relation, base class: same cluster
first:
An extended comparison between CA and Back Propagation Error ( <m> BPE </m> ) are presented and the potential of information induction and reduction from the training set is revised for both methods .
second:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .


class: no relation, base class: same cluster
first:
There are many <m> BPE methods </m> used to collect building data and inform designs .
second:
The resultant signal-to-noise ratio ( SNR ) often varies across the reconstructed image when using the <m> BPE technique </m> , and the image SNR depends on the reconstruction method .


class: no relation, base class: first -> second
first:
There are many <m> BPE methods </m> used to collect building data and inform designs .
second:
For English→German , we observe the best BLEU score of 25.3 with C2 - 50k , but the best CHRF3 score of 54.1 with <m> BPE - J90k </m> .


class: no relation, base class: same cluster
first:
There are many <m> BPE methods </m> used to collect building data and inform designs .
second:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .


class: no relation, base class: first -> second
first:
( 1 ) Process models from the old <m> BPE platform </m> have to be converted to the target process definition language on the target BPE platform .
second:
Additionally , we present <m> AST BPE </m> , which is a version of BPE that uses the Abstract Syntax Tree ( AST ) of the SQL statement to guide BPE merges and therefore produce BPE encodings that generalize better .


class: no relation, base class: first -> second
first:
The complex negotiation mechanism consists of : 1 ) a <m> bargaining-position-estimation ( BPE ) strategy </m> for the multilateral negotiations between consumer and broker agents in a service market and 2 ) a regression-based coordination ( RBC ) strategy for concurrent negotiations between broker and provider agents in multiple resource markets .
second:
For Switchboard , our <m> phone-based BPE system </m> achieves 6.8%/14.4 % word error rate ( WER ) on the Switchboard/CallHome portion of the test set while the ensemble system achieves 6.3%/13.3 % WER .


class: no relation, base class: first -> second
first:
Empirical results show that agents adopting the <m> BPE strategy </m> can better respond to different market conditions than agents adopting the time-dependent strategy because they do not make excessive ( respectively , inadequate ) amounts of concessions in favorable ( respectively , unfavorable ) markets .
second:
For Switchboard , our <m> phone-based BPE system </m> achieves 6.8%/14.4 % word error rate ( WER ) on the Switchboard/CallHome portion of the test set while the ensemble system achieves 6.3%/13.3 % WER .


class: no relation, base class: first -> second
first:
The resultant signal-to-noise ratio ( SNR ) often varies across the reconstructed image when using the <m> BPE technique </m> , and the image SNR depends on the reconstruction method .
second:
For English→German , we observe the best BLEU score of 25.3 with C2 - 50k , but the best CHRF3 score of 54.1 with <m> BPE - J90k </m> .


class: no relation, base class: same cluster
first:
The resultant signal-to-noise ratio ( SNR ) often varies across the reconstructed image when using the <m> BPE technique </m> , and the image SNR depends on the reconstruction method .
second:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .


class: no relation, base class: same cluster
first:
We evaluate the effect of reordering using three simple methods : unigram count ( UC ) , <m> unigram ratio </m> ( UR ) , and first four characters match ( FFCM ) .
second:
The model is based on ( Goldwater et al. , 2006 ) <m> unigram word segmentation model </m> and assumes a simple prior distribution over morph length .


class: no relation, base class: first -> second
first:
The model is based on ( Goldwater et al. , 2006 ) <m> unigram word segmentation model </m> and assumes a simple prior distribution over morph length .
second:
One may also use a <m> unigram distribution </m> which has been shown to work better on some tasks pereyra2017regularize .


class: no relation, base class: first -> second
first:
In the first part , the scope and features of <m> digital advertising </m> as well as the tools deployed are presented and defined .
second:
We hypothesize that digital fabrication ( DF ) , e.g. 3D printing , may be an opportunity to involve patients in the process of custom design and <m> creation of personalized ADs </m> .


class: no relation, base class: first -> second
first:
While predictive modeling has been thoroughly researched in recent years in the <m> digital advertising domain </m> , the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification .
second:
We hypothesize that digital fabrication ( DF ) , e.g. 3D printing , may be an opportunity to involve patients in the process of custom design and <m> creation of personalized ADs </m> .


class: no relation, base class: first -> second
first:
This paper reports a multi-phase study to address the following questions in the <m> digital advertising context </m> : ( 1 ) to what extent did consumers use ads in the past ? ( 2 ) What are consumers ’ perceived values toward ads ?
second:
We hypothesize that digital fabrication ( DF ) , e.g. 3D printing , may be an opportunity to involve patients in the process of custom design and <m> creation of personalized ADs </m> .


class: no relation, base class: first -> second
first:
What is the simplicity and complexity of the digital advertising design structure?The purpose of the research is to focus on the discovery of the rug and the complexity of the structure of the design of digital advertising , while the importance of research by contributing to enrich the knowledge and application of the specialists and specialists to study the simplicity and complexity in the structure of <m> digital advertising design </m> and the introduction of specialized designers working on the subject of simplicity and complexity in the design of digital advertising And the terminology in it , and the researcher has developed spatial boundaries represented by a number of Iraqi digital ad designs , and time limits ( limited in the period of 2017 ) As for the substantive boundaries represented in ( study of simplicity and complexity in the structure of design Digital advertising.while the second section chapter which consist the theoretical frame which interest in three researcher , the first research turning to ( Simplicity in the digital advertising design)while the the second research interest in ( complexity in the digital advertising design)and the third research turning into about the(structure digital advertising design ) .
second:
We hypothesize that digital fabrication ( DF ) , e.g. 3D printing , may be an opportunity to involve patients in the process of custom design and <m> creation of personalized ADs </m> .


class: no relation, base class: same cluster
first:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .
second:
A method for <m> delivery of advertisements or ads </m> , which comprises the step of providing drinks container lid having on its upper surface a printed label , wherein the label comprises a printed advertising or information of news , wherein the printed label is attached to the device for attaching the cover at a predetermined orientation labels advertising of news or information on the cover . ! 2 .


class: no relation, base class: second -> first
first:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .
second:
A method for <m> delivery of advertisements or ads </m> , which comprises the step of providing drinks container lid having on its upper surface a printed label , wherein the label comprises a printed advertising or information of news , wherein the printed label is attached to the device for attaching the cover at a predetermined orientation labels advertising of news or information on the cover . ! 2 .


class: no relation, base class: second -> first
first:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .
second:
A method for <m> delivery of advertisements or ads </m> , which comprises the step of providing drinks container lid having on its upper surface a printed label , wherein the label comprises a printed advertising or information of news , wherein the printed label is attached to the device for attaching the cover at a predetermined orientation labels advertising of news or information on the cover . ! 2 .


class: no relation, base class: second -> first
first:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .
second:
A method for <m> delivery of advertisements or ads </m> , which comprises the step of providing drinks container lid having on its upper surface a printed label , wherein the label comprises a printed advertising or information of news , wherein the printed label is attached to the device for attaching the cover at a predetermined orientation labels advertising of news or information on the cover . ! 2 .


class: no relation, base class: first -> second
first:
A method for <m> delivery of advertisements or ads </m> , which comprises the step of providing drinks container lid having on its upper surface a printed label , wherein the label comprises a printed advertising or information of news , wherein the printed label is attached to the device for attaching the cover at a predetermined orientation labels advertising of news or information on the cover . ! 2 .
second:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .


class: no relation, base class: second -> first
first:
Text information extraction comprises of <m> text image classification </m> , text detection , localization , segmentation , enhancement and recognition .
second:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .


class: no relation, base class: second -> first
first:
KeywoRDS Confidence Computation , Document Image , Fuzzy Matching , Handwritten Documents , Performance Analysis , Printed Documents , SVM , <m> Text Image Classification </m> , Word Association
second:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .


class: no relation, base class: first -> second
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
These traditional methods adopt a bottom - up strategy and often needs several steps to detect texts ( e.g. , character detection , text line construction and <m> text line classification </m> ) .


class: no relation, base class: first -> second
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
We design a fast Class-Feature-Centroid ( CFC ) classifier for <m> multi-class , single-label text categorization </m> .


class: no relation, base class: first -> second
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
A number of language-independent text pre-processing techniques , to support <m> multi-class single-label text classification </m> , are described and compared .


class: no relation, base class: first -> second
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
We justified the proposed approach on several benchmark datasets including both multi - label and <m> multi - class text classification tasks </m> .


class: no relation, base class: first -> second
first:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .
second:
The <m> supervised text classification problem </m> used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .


