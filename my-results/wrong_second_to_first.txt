class: second -> first, base class: no relation, new model class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
We take the final hidden state as the segment embedding vector , then can be represented as where corresponds to one layer or multiple layers of <m> linear or non - linear transformation </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
We take the final hidden state as the segment embedding vector , then can be represented as where corresponds to one layer or multiple layers of <m> linear or non - linear transformation </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
We take the final hidden state as the segment embedding vector , then can be represented as where corresponds to one layer or multiple layers of <m> linear or non - linear transformation </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
We take the final hidden state as the segment embedding vector , then can be represented as where corresponds to one layer or multiple layers of <m> linear or non - linear transformation </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
We take the final hidden state as the segment embedding vector , then can be represented as where corresponds to one layer or multiple layers of <m> linear or non - linear transformation </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
We take the final hidden state as the segment embedding vector , then can be represented as where corresponds to one layer or multiple layers of <m> linear or non - linear transformation </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
We take the final hidden state as the segment embedding vector , then can be represented as where corresponds to one layer or multiple layers of <m> linear or non - linear transformation </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
We take the final hidden state as the segment embedding vector , then can be represented as where corresponds to one layer or multiple layers of <m> linear or non - linear transformation </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .
second:
We take the final hidden state as the segment embedding vector , then can be represented as where corresponds to one layer or multiple layers of <m> linear or non - linear transformation </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In this paper , we investigate the effects of three DA approaches ; <m> affine transformation-based approach </m> , synthetic image-based approach , and generative adversarial network (GAN)-based approach , and evaluate the effectiveness of DA for pupil center point detection task using an off-the-shelf CNN model .
second:
After that , the pattern elements are extracted with the <m> affine transformation theory </m> and bilinear interpolation algorithm .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In this paper , we investigate the effects of three DA approaches ; <m> affine transformation-based approach </m> , synthetic image-based approach , and generative adversarial network (GAN)-based approach , and evaluate the effectiveness of DA for pupil center point detection task using an off-the-shelf CNN model .
second:
The content includes : dividing an image frame including a target object , determining a target object region , using a feature information classifier to scan the determined target object region , determining central point position information of a region that feature information to be positioned occupies in the target object region , performing <m> affine transformation operation </m> on the determined central point position information , and obtaining initial position information corresponding to the region that the feature information to be positioned occupies in the target object region , thereby improving precision of initial positioning ; performing iteration processing on the obtained initial position information , obtaining position information corresponding to the region that the feature information to be positioned occupies in the target object region , integrating the obtained position information corresponding to the region that the feature information to be positioned occupies in the target object region , and obtaining the feature information of the target object , thereby improving precision of positioning the feature information of the target object .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In this paper , we investigate the effects of three DA approaches ; <m> affine transformation-based approach </m> , synthetic image-based approach , and generative adversarial network (GAN)-based approach , and evaluate the effectiveness of DA for pupil center point detection task using an off-the-shelf CNN model .
second:
Specific topics include : Loading images from JAR files Describing shapes and the affine transformation in Java 2D Designing a class library of shapes Designing a component class for interactive drawing Designing a component class for displaying images Printing in Java 2D Implementing pan and zoom Implementing interactive rotate , shear , and flip Manipulating images at the pixel level using the Java 2D API Working with tiles in JAI to render large images Image manipulation in JAI <m> usingaffine transformation </m> , projections , and warping Image analysis in JAI , including edge detection , statistics , and region of interest computation ( ROI ) Remote imaging using RMI and JAI Internet imaging , including the use of Java 2D and JAI with servlets and JSP , the design of Web-based imaging applications , and the Internet Imaging Protocol ( IIP ) Using the new Image I/O framework to read and write images " Putting It All Together " sections help you put vital concepts and techniques into practice with interactive examples using actual applications .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In this paper , we investigate the effects of three DA approaches ; <m> affine transformation-based approach </m> , synthetic image-based approach , and generative adversarial network (GAN)-based approach , and evaluate the effectiveness of DA for pupil center point detection task using an off-the-shelf CNN model .
second:
Since the textual and visual vectors have different dimensionality and belong to different spaces , we first map them to a mutual space using an <m> affine transformation </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In this paper , we investigate the effects of three DA approaches ; <m> affine transformation-based approach </m> , synthetic image-based approach , and generative adversarial network (GAN)-based approach , and evaluate the effectiveness of DA for pupil center point detection task using an off-the-shelf CNN model .
second:
Furthermore , the learned <m> affine transform </m> applied to these normalized activations allows the BN transform to represent the identity transformation and preserves the network capacity .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper , we investigate the effects of three DA approaches ; <m> affine transformation-based approach </m> , synthetic image-based approach , and generative adversarial network (GAN)-based approach , and evaluate the effectiveness of DA for pupil center point detection task using an off-the-shelf CNN model .
second:
The question nodes and edges use vector embeddings implemented as look - up tables , and the scene nodes and edges use <m> affine projections </m> : with the word embedding ( usually pretrained , see supplementary material ) , the embedding of dependencies , and weight matrices , and and biases .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , we investigate the effects of three DA approaches ; <m> affine transformation-based approach </m> , synthetic image-based approach , and generative adversarial network (GAN)-based approach , and evaluate the effectiveness of DA for pupil center point detection task using an off-the-shelf CNN model .
second:
Also in this group , for those vertices associated with large residual errors under <m> affine mapping </m> , we encode their motion effectively using Newtonian motion estimates .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , we investigate the effects of three DA approaches ; <m> affine transformation-based approach </m> , synthetic image-based approach , and generative adversarial network (GAN)-based approach , and evaluate the effectiveness of DA for pupil center point detection task using an off-the-shelf CNN model .
second:
GCPs derived from each source were used independently for the GCP-based geometric correction of IRS PAN sensor images by using <m> affine mapping function </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , we investigate the effects of three DA approaches ; <m> affine transformation-based approach </m> , synthetic image-based approach , and generative adversarial network (GAN)-based approach , and evaluate the effectiveness of DA for pupil center point detection task using an off-the-shelf CNN model .
second:
In this paper we introduce a class of constraint logic programs such that their termination can be proved by using <m> affine level mappings </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , we investigate the effects of three DA approaches ; <m> affine transformation-based approach </m> , synthetic image-based approach , and generative adversarial network (GAN)-based approach , and evaluate the effectiveness of DA for pupil center point detection task using an off-the-shelf CNN model .
second:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , we investigate the effects of three DA approaches ; <m> affine transformation-based approach </m> , synthetic image-based approach , and generative adversarial network (GAN)-based approach , and evaluate the effectiveness of DA for pupil center point detection task using an off-the-shelf CNN model .
second:
To be specific , we propose <m> affine mapping based schemes </m> for the problem transformation and outsourcing so that the cloud is unable to learn any key information from the transformed problem .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper , we investigate the effects of three DA approaches ; <m> affine transformation-based approach </m> , synthetic image-based approach , and generative adversarial network (GAN)-based approach , and evaluate the effectiveness of DA for pupil center point detection task using an off-the-shelf CNN model .
second:
The same results can be obtained when we precompose $ \La$ with an <m> affine matrix-valued mapping </m> $ A$ , provided that this mapping satisfies a regularity condition ( { \em transversality condition } ) .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In this paper , we investigate the effects of three DA approaches ; <m> affine transformation-based approach </m> , synthetic image-based approach , and generative adversarial network (GAN)-based approach , and evaluate the effectiveness of DA for pupil center point detection task using an off-the-shelf CNN model .
second:
The algorithm estimates the 12 calibration parameters , which are elements of an <m> affine transformation mapping </m> measured , and thus , erroneous magnetometer measurements into corrected field measurements .


class: second -> first, base class: same cluster, new model class: same cluster
first:
While <m> affine image transformation </m> in conventional image processing is a relatively simple task , learning these transformations is an important part of having neural networks ( NNs ) function as generalized image processors .
second:
It creates Shearlet functions , which have different characteristics through zooming , shearing translating and other <m> affine transforming methods </m> and enables its capable of optimally sparse representation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
While <m> affine image transformation </m> in conventional image processing is a relatively simple task , learning these transformations is an important part of having neural networks ( NNs ) function as generalized image processors .
second:
First , we can see that LSTM - based leaf transformation has a clear advantage over the <m> affine - transformation - based one </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
This visualisation , combined with an <m> affine transformation process </m> provided a simple , cost-effective way to accurately co-register photographs and MR images of subcutaneous hematomas located on the thigh .
second:
It creates Shearlet functions , which have different characteristics through zooming , shearing translating and other <m> affine transforming methods </m> and enables its capable of optimally sparse representation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
This visualisation , combined with an <m> affine transformation process </m> provided a simple , cost-effective way to accurately co-register photographs and MR images of subcutaneous hematomas located on the thigh .
second:
First , we can see that LSTM - based leaf transformation has a clear advantage over the <m> affine - transformation - based one </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
It creates Shearlet functions , which have different characteristics through zooming , shearing translating and other <m> affine transforming methods </m> and enables its capable of optimally sparse representation .
second:
Specific topics include : Loading images from JAR files Describing shapes and the affine transformation in Java 2D Designing a class library of shapes Designing a component class for interactive drawing Designing a component class for displaying images Printing in Java 2D Implementing pan and zoom Implementing interactive rotate , shear , and flip Manipulating images at the pixel level using the Java 2D API Working with tiles in JAI to render large images Image manipulation in JAI <m> usingaffine transformation </m> , projections , and warping Image analysis in JAI , including edge detection , statistics , and region of interest computation ( ROI ) Remote imaging using RMI and JAI Internet imaging , including the use of Java 2D and JAI with servlets and JSP , the design of Web-based imaging applications , and the Internet Imaging Protocol ( IIP ) Using the new Image I/O framework to read and write images " Putting It All Together " sections help you put vital concepts and techniques into practice with interactive examples using actual applications .


class: second -> first, base class: same cluster, new model class: same cluster
first:
It creates Shearlet functions , which have different characteristics through zooming , shearing translating and other <m> affine transforming methods </m> and enables its capable of optimally sparse representation .
second:
Since the textual and visual vectors have different dimensionality and belong to different spaces , we first map them to a mutual space using an <m> affine transformation </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
It creates Shearlet functions , which have different characteristics through zooming , shearing translating and other <m> affine transforming methods </m> and enables its capable of optimally sparse representation .
second:
Furthermore , the learned <m> affine transform </m> applied to these normalized activations allows the BN transform to represent the identity transformation and preserves the network capacity .


class: second -> first, base class: no relation, new model class: no relation
first:
It creates Shearlet functions , which have different characteristics through zooming , shearing translating and other <m> affine transforming methods </m> and enables its capable of optimally sparse representation .
second:
The question nodes and edges use vector embeddings implemented as look - up tables , and the scene nodes and edges use <m> affine projections </m> : with the word embedding ( usually pretrained , see supplementary material ) , the embedding of dependencies , and weight matrices , and and biases .


class: second -> first, base class: no relation, new model class: same cluster
first:
It creates Shearlet functions , which have different characteristics through zooming , shearing translating and other <m> affine transforming methods </m> and enables its capable of optimally sparse representation .
second:
Also in this group , for those vertices associated with large residual errors under <m> affine mapping </m> , we encode their motion effectively using Newtonian motion estimates .


class: second -> first, base class: no relation, new model class: same cluster
first:
It creates Shearlet functions , which have different characteristics through zooming , shearing translating and other <m> affine transforming methods </m> and enables its capable of optimally sparse representation .
second:
GCPs derived from each source were used independently for the GCP-based geometric correction of IRS PAN sensor images by using <m> affine mapping function </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
It creates Shearlet functions , which have different characteristics through zooming , shearing translating and other <m> affine transforming methods </m> and enables its capable of optimally sparse representation .
second:
In this paper we introduce a class of constraint logic programs such that their termination can be proved by using <m> affine level mappings </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
It creates Shearlet functions , which have different characteristics through zooming , shearing translating and other <m> affine transforming methods </m> and enables its capable of optimally sparse representation .
second:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .


class: second -> first, base class: no relation, new model class: same cluster
first:
It creates Shearlet functions , which have different characteristics through zooming , shearing translating and other <m> affine transforming methods </m> and enables its capable of optimally sparse representation .
second:
To be specific , we propose <m> affine mapping based schemes </m> for the problem transformation and outsourcing so that the cloud is unable to learn any key information from the transformed problem .


class: second -> first, base class: no relation, new model class: no relation
first:
It creates Shearlet functions , which have different characteristics through zooming , shearing translating and other <m> affine transforming methods </m> and enables its capable of optimally sparse representation .
second:
The same results can be obtained when we precompose $ \La$ with an <m> affine matrix-valued mapping </m> $ A$ , provided that this mapping satisfies a regularity condition ( { \em transversality condition } ) .


class: second -> first, base class: same cluster, new model class: same cluster
first:
It creates Shearlet functions , which have different characteristics through zooming , shearing translating and other <m> affine transforming methods </m> and enables its capable of optimally sparse representation .
second:
The algorithm estimates the 12 calibration parameters , which are elements of an <m> affine transformation mapping </m> measured , and thus , erroneous magnetometer measurements into corrected field measurements .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We have also experimented with more sophisticated data augmentation techniques , including random histogram matching , <m> affine image transforms </m> , and random image filtering , which did not demonstrate any additional improvements .
second:
First , we can see that LSTM - based leaf transformation has a clear advantage over the <m> affine - transformation - based one </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
First , we can see that LSTM - based leaf transformation has a clear advantage over the <m> affine - transformation - based one </m> .
second:
Also in this group , for those vertices associated with large residual errors under <m> affine mapping </m> , we encode their motion effectively using Newtonian motion estimates .


class: second -> first, base class: no relation, new model class: same cluster
first:
First , we can see that LSTM - based leaf transformation has a clear advantage over the <m> affine - transformation - based one </m> .
second:
GCPs derived from each source were used independently for the GCP-based geometric correction of IRS PAN sensor images by using <m> affine mapping function </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
First , we can see that LSTM - based leaf transformation has a clear advantage over the <m> affine - transformation - based one </m> .
second:
In this paper we introduce a class of constraint logic programs such that their termination can be proved by using <m> affine level mappings </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
First , we can see that LSTM - based leaf transformation has a clear advantage over the <m> affine - transformation - based one </m> .
second:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .


class: second -> first, base class: no relation, new model class: same cluster
first:
First , we can see that LSTM - based leaf transformation has a clear advantage over the <m> affine - transformation - based one </m> .
second:
To be specific , we propose <m> affine mapping based schemes </m> for the problem transformation and outsourcing so that the cloud is unable to learn any key information from the transformed problem .


class: second -> first, base class: no relation, new model class: no relation
first:
First , we can see that LSTM - based leaf transformation has a clear advantage over the <m> affine - transformation - based one </m> .
second:
The same results can be obtained when we precompose $ \La$ with an <m> affine matrix-valued mapping </m> $ A$ , provided that this mapping satisfies a regularity condition ( { \em transversality condition } ) .


class: second -> first, base class: same cluster, new model class: same cluster
first:
First , we can see that LSTM - based leaf transformation has a clear advantage over the <m> affine - transformation - based one </m> .
second:
The algorithm estimates the 12 calibration parameters , which are elements of an <m> affine transformation mapping </m> measured , and thus , erroneous magnetometer measurements into corrected field measurements .


class: second -> first, base class: no relation, new model class: no relation
first:
Finally , a <m> linear fully connected layer </m> is used to evaluate the saliency of a queried region .
second:
Among them , the chaotic neuron layer realizes data diffusion , the <m> linear neuron layer </m> realizes data confusion , and the two layers are repeated for several times to strengthen the cipher .


class: second -> first, base class: no relation, new model class: no relation
first:
Finally , a <m> linear fully connected layer </m> is used to evaluate the saliency of a queried region .
second:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .


class: second -> first, base class: no relation, new model class: no relation
first:
To obtain accurate classification results , this paper proposes the Ensemble-Margin Based Random Forests ( EMRFs ) method , which combines RFs and a new <m> subsampling iterative technique </m> making use of computed ensemble margin values .
second:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .


class: second -> first, base class: no relation, new model class: no relation
first:
To obtain accurate classification results , this paper proposes the Ensemble-Margin Based Random Forests ( EMRFs ) method , which combines RFs and a new <m> subsampling iterative technique </m> making use of computed ensemble margin values .
second:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
To obtain accurate classification results , this paper proposes the Ensemble-Margin Based Random Forests ( EMRFs ) method , which combines RFs and a new <m> subsampling iterative technique </m> making use of computed ensemble margin values .
second:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .


class: second -> first, base class: no relation, new model class: no relation
first:
To obtain accurate classification results , this paper proposes the Ensemble-Margin Based Random Forests ( EMRFs ) method , which combines RFs and a new <m> subsampling iterative technique </m> making use of computed ensemble margin values .
second:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .


class: second -> first, base class: no relation, new model class: no relation
first:
To obtain accurate classification results , this paper proposes the Ensemble-Margin Based Random Forests ( EMRFs ) method , which combines RFs and a new <m> subsampling iterative technique </m> making use of computed ensemble margin values .
second:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .


class: second -> first, base class: no relation, new model class: no relation
first:
To obtain accurate classification results , this paper proposes the Ensemble-Margin Based Random Forests ( EMRFs ) method , which combines RFs and a new <m> subsampling iterative technique </m> making use of computed ensemble margin values .
second:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .


class: second -> first, base class: no relation, new model class: no relation
first:
To obtain accurate classification results , this paper proposes the Ensemble-Margin Based Random Forests ( EMRFs ) method , which combines RFs and a new <m> subsampling iterative technique </m> making use of computed ensemble margin values .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: second -> first, base class: no relation, new model class: no relation
first:
To obtain accurate classification results , this paper proposes the Ensemble-Margin Based Random Forests ( EMRFs ) method , which combines RFs and a new <m> subsampling iterative technique </m> making use of computed ensemble margin values .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
To obtain accurate classification results , this paper proposes the Ensemble-Margin Based Random Forests ( EMRFs ) method , which combines RFs and a new <m> subsampling iterative technique </m> making use of computed ensemble margin values .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
To obtain accurate classification results , this paper proposes the Ensemble-Margin Based Random Forests ( EMRFs ) method , which combines RFs and a new <m> subsampling iterative technique </m> making use of computed ensemble margin values .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: second -> first, base class: no relation, new model class: no relation
first:
To obtain accurate classification results , this paper proposes the Ensemble-Margin Based Random Forests ( EMRFs ) method , which combines RFs and a new <m> subsampling iterative technique </m> making use of computed ensemble margin values .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: second -> first, base class: no relation, new model class: no relation
first:
To obtain accurate classification results , this paper proposes the Ensemble-Margin Based Random Forests ( EMRFs ) method , which combines RFs and a new <m> subsampling iterative technique </m> making use of computed ensemble margin values .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: second -> first, base class: no relation, new model class: no relation
first:
To obtain accurate classification results , this paper proposes the Ensemble-Margin Based Random Forests ( EMRFs ) method , which combines RFs and a new <m> subsampling iterative technique </m> making use of computed ensemble margin values .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Anti mutual-information subsampling </m> does not require to remove from the brain signals the shared variance between aging and fluid intelligence , and hence does not display this pessimistic behavior .
second:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Anti mutual-information subsampling </m> does not require to remove from the brain signals the shared variance between aging and fluid intelligence , and hence does not display this pessimistic behavior .
second:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Anti mutual-information subsampling </m> does not require to remove from the brain signals the shared variance between aging and fluid intelligence , and hence does not display this pessimistic behavior .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
A <m> subsampling stochastic gradient descent algorithm </m> has been proposed to accelerate the training of recurrent neural network language models .
second:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
A <m> subsampling stochastic gradient descent algorithm </m> has been proposed to accelerate the training of recurrent neural network language models .
second:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .


class: second -> first, base class: no relation, new model class: no relation
first:
A <m> subsampling stochastic gradient descent algorithm </m> has been proposed to accelerate the training of recurrent neural network language models .
second:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .


class: second -> first, base class: no relation, new model class: no relation
first:
A <m> subsampling stochastic gradient descent algorithm </m> has been proposed to accelerate the training of recurrent neural network language models .
second:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .


class: second -> first, base class: no relation, new model class: no relation
first:
A <m> subsampling stochastic gradient descent algorithm </m> has been proposed to accelerate the training of recurrent neural network language models .
second:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .


class: second -> first, base class: no relation, new model class: no relation
first:
A <m> subsampling stochastic gradient descent algorithm </m> has been proposed to accelerate the training of recurrent neural network language models .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: second -> first, base class: no relation, new model class: no relation
first:
A <m> subsampling stochastic gradient descent algorithm </m> has been proposed to accelerate the training of recurrent neural network language models .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
A <m> subsampling stochastic gradient descent algorithm </m> has been proposed to accelerate the training of recurrent neural network language models .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
A <m> subsampling stochastic gradient descent algorithm </m> has been proposed to accelerate the training of recurrent neural network language models .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: second -> first, base class: no relation, new model class: no relation
first:
A <m> subsampling stochastic gradient descent algorithm </m> has been proposed to accelerate the training of recurrent neural network language models .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: second -> first, base class: no relation, new model class: no relation
first:
A <m> subsampling stochastic gradient descent algorithm </m> has been proposed to accelerate the training of recurrent neural network language models .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: second -> first, base class: no relation, new model class: no relation
first:
A <m> subsampling stochastic gradient descent algorithm </m> has been proposed to accelerate the training of recurrent neural network language models .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: second -> first, base class: no relation, new model class: no relation
first:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .
second:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .
second:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .


class: second -> first, base class: no relation, new model class: no relation
first:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .
second:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .


class: second -> first, base class: no relation, new model class: no relation
first:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .
second:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .


class: second -> first, base class: no relation, new model class: no relation
first:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .
second:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .


class: second -> first, base class: no relation, new model class: no relation
first:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: second -> first, base class: no relation, new model class: no relation
first:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: second -> first, base class: no relation, new model class: no relation
first:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: second -> first, base class: no relation, new model class: no relation
first:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: second -> first, base class: no relation, new model class: no relation
first:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: second -> first, base class: no relation, new model class: no relation
first:
Subsampling in CSXI is accomplished by either <m> view angle spectral subsampling </m> , spatial subsampling enabled by block-unblock coded apertures placed at the source or detector side , or both .
second:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Subsampling in CSXI is accomplished by either <m> view angle spectral subsampling </m> , spatial subsampling enabled by block-unblock coded apertures placed at the source or detector side , or both .
second:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .


class: second -> first, base class: no relation, new model class: no relation
first:
Subsampling in CSXI is accomplished by either <m> view angle spectral subsampling </m> , spatial subsampling enabled by block-unblock coded apertures placed at the source or detector side , or both .
second:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .


class: second -> first, base class: no relation, new model class: no relation
first:
Subsampling in CSXI is accomplished by either <m> view angle spectral subsampling </m> , spatial subsampling enabled by block-unblock coded apertures placed at the source or detector side , or both .
second:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .


class: second -> first, base class: no relation, new model class: no relation
first:
Subsampling in CSXI is accomplished by either <m> view angle spectral subsampling </m> , spatial subsampling enabled by block-unblock coded apertures placed at the source or detector side , or both .
second:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .


class: second -> first, base class: no relation, new model class: no relation
first:
Subsampling in CSXI is accomplished by either <m> view angle spectral subsampling </m> , spatial subsampling enabled by block-unblock coded apertures placed at the source or detector side , or both .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: second -> first, base class: no relation, new model class: no relation
first:
Subsampling in CSXI is accomplished by either <m> view angle spectral subsampling </m> , spatial subsampling enabled by block-unblock coded apertures placed at the source or detector side , or both .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Subsampling in CSXI is accomplished by either <m> view angle spectral subsampling </m> , spatial subsampling enabled by block-unblock coded apertures placed at the source or detector side , or both .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Subsampling in CSXI is accomplished by either <m> view angle spectral subsampling </m> , spatial subsampling enabled by block-unblock coded apertures placed at the source or detector side , or both .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: second -> first, base class: no relation, new model class: no relation
first:
Subsampling in CSXI is accomplished by either <m> view angle spectral subsampling </m> , spatial subsampling enabled by block-unblock coded apertures placed at the source or detector side , or both .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: second -> first, base class: no relation, new model class: no relation
first:
Subsampling in CSXI is accomplished by either <m> view angle spectral subsampling </m> , spatial subsampling enabled by block-unblock coded apertures placed at the source or detector side , or both .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: second -> first, base class: no relation, new model class: no relation
first:
Subsampling in CSXI is accomplished by either <m> view angle spectral subsampling </m> , spatial subsampling enabled by block-unblock coded apertures placed at the source or detector side , or both .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: second -> first, base class: no relation, new model class: no relation
first:
The spatial frequency decomposition is accomplished by an efficient encoding algorithm incorporating a hierarchical cascading Gaussian pyramid algorithm which is an alternating sequence of image output passing through Nyquist low pass spatial filter and <m> subsampling local operators </m> for image encoding .
second:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The spatial frequency decomposition is accomplished by an efficient encoding algorithm incorporating a hierarchical cascading Gaussian pyramid algorithm which is an alternating sequence of image output passing through Nyquist low pass spatial filter and <m> subsampling local operators </m> for image encoding .
second:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .


class: second -> first, base class: no relation, new model class: no relation
first:
The spatial frequency decomposition is accomplished by an efficient encoding algorithm incorporating a hierarchical cascading Gaussian pyramid algorithm which is an alternating sequence of image output passing through Nyquist low pass spatial filter and <m> subsampling local operators </m> for image encoding .
second:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .


class: second -> first, base class: no relation, new model class: no relation
first:
The spatial frequency decomposition is accomplished by an efficient encoding algorithm incorporating a hierarchical cascading Gaussian pyramid algorithm which is an alternating sequence of image output passing through Nyquist low pass spatial filter and <m> subsampling local operators </m> for image encoding .
second:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .


class: second -> first, base class: no relation, new model class: no relation
first:
The spatial frequency decomposition is accomplished by an efficient encoding algorithm incorporating a hierarchical cascading Gaussian pyramid algorithm which is an alternating sequence of image output passing through Nyquist low pass spatial filter and <m> subsampling local operators </m> for image encoding .
second:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .


class: second -> first, base class: no relation, new model class: no relation
first:
The spatial frequency decomposition is accomplished by an efficient encoding algorithm incorporating a hierarchical cascading Gaussian pyramid algorithm which is an alternating sequence of image output passing through Nyquist low pass spatial filter and <m> subsampling local operators </m> for image encoding .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: second -> first, base class: no relation, new model class: no relation
first:
The spatial frequency decomposition is accomplished by an efficient encoding algorithm incorporating a hierarchical cascading Gaussian pyramid algorithm which is an alternating sequence of image output passing through Nyquist low pass spatial filter and <m> subsampling local operators </m> for image encoding .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The spatial frequency decomposition is accomplished by an efficient encoding algorithm incorporating a hierarchical cascading Gaussian pyramid algorithm which is an alternating sequence of image output passing through Nyquist low pass spatial filter and <m> subsampling local operators </m> for image encoding .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The spatial frequency decomposition is accomplished by an efficient encoding algorithm incorporating a hierarchical cascading Gaussian pyramid algorithm which is an alternating sequence of image output passing through Nyquist low pass spatial filter and <m> subsampling local operators </m> for image encoding .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: second -> first, base class: no relation, new model class: no relation
first:
The spatial frequency decomposition is accomplished by an efficient encoding algorithm incorporating a hierarchical cascading Gaussian pyramid algorithm which is an alternating sequence of image output passing through Nyquist low pass spatial filter and <m> subsampling local operators </m> for image encoding .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: second -> first, base class: no relation, new model class: no relation
first:
The spatial frequency decomposition is accomplished by an efficient encoding algorithm incorporating a hierarchical cascading Gaussian pyramid algorithm which is an alternating sequence of image output passing through Nyquist low pass spatial filter and <m> subsampling local operators </m> for image encoding .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: second -> first, base class: no relation, new model class: no relation
first:
The spatial frequency decomposition is accomplished by an efficient encoding algorithm incorporating a hierarchical cascading Gaussian pyramid algorithm which is an alternating sequence of image output passing through Nyquist low pass spatial filter and <m> subsampling local operators </m> for image encoding .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: second -> first, base class: same cluster, new model class: same cluster
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .


class: second -> first, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .


class: second -> first, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .


class: second -> first, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .


class: second -> first, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: second -> first, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: second -> first, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: second -> first, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: second -> first, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: second -> first, base class: same cluster, new model class: same cluster
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .


class: second -> first, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .


class: second -> first, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .


class: second -> first, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .


class: second -> first, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: second -> first, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: second -> first, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: second -> first, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: second -> first, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Results of Hierarchical Subsampling We first demonstrate the results of the <m> hierarchical subsampling recurrent network </m> , which is the key to speed up our experiments .
second:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Results of Hierarchical Subsampling We first demonstrate the results of the <m> hierarchical subsampling recurrent network </m> , which is the key to speed up our experiments .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Results of Hierarchical Subsampling We first demonstrate the results of the <m> hierarchical subsampling recurrent network </m> , which is the key to speed up our experiments .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Results of Hierarchical Subsampling We first demonstrate the results of the <m> hierarchical subsampling recurrent network </m> , which is the key to speed up our experiments .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Results of Hierarchical Subsampling We first demonstrate the results of the <m> hierarchical subsampling recurrent network </m> , which is the key to speed up our experiments .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Results of Hierarchical Subsampling We first demonstrate the results of the <m> hierarchical subsampling recurrent network </m> , which is the key to speed up our experiments .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .


class: second -> first, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .


class: second -> first, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .


class: second -> first, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .


class: second -> first, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: second -> first, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: second -> first, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: second -> first, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: second -> first, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: second -> first, base class: no relation, new model class: no relation
first:
Then <m> p - norm subsampling </m> ( or pooling ) with pooling size ( or half - length ) and stride applied to the feature map is a 3 - dimensional array with the following entries : where is the function mapping from positions in to positions in respecting the stride , is the order of the p - norm ( for , it becomes the commonly used max pooling ) .
second:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Then <m> p - norm subsampling </m> ( or pooling ) with pooling size ( or half - length ) and stride applied to the feature map is a 3 - dimensional array with the following entries : where is the function mapping from positions in to positions in respecting the stride , is the order of the p - norm ( for , it becomes the commonly used max pooling ) .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Then <m> p - norm subsampling </m> ( or pooling ) with pooling size ( or half - length ) and stride applied to the feature map is a 3 - dimensional array with the following entries : where is the function mapping from positions in to positions in respecting the stride , is the order of the p - norm ( for , it becomes the commonly used max pooling ) .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: second -> first, base class: no relation, new model class: no relation
first:
Then <m> p - norm subsampling </m> ( or pooling ) with pooling size ( or half - length ) and stride applied to the feature map is a 3 - dimensional array with the following entries : where is the function mapping from positions in to positions in respecting the stride , is the order of the p - norm ( for , it becomes the commonly used max pooling ) .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: second -> first, base class: no relation, new model class: no relation
first:
Then <m> p - norm subsampling </m> ( or pooling ) with pooling size ( or half - length ) and stride applied to the feature map is a 3 - dimensional array with the following entries : where is the function mapping from positions in to positions in respecting the stride , is the order of the p - norm ( for , it becomes the commonly used max pooling ) .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: second -> first, base class: no relation, new model class: no relation
first:
Then <m> p - norm subsampling </m> ( or pooling ) with pooling size ( or half - length ) and stride applied to the feature map is a 3 - dimensional array with the following entries : where is the function mapping from positions in to positions in respecting the stride , is the order of the p - norm ( for , it becomes the commonly used max pooling ) .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: second -> first, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .


class: second -> first, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .


class: second -> first, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .


class: second -> first, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .


class: second -> first, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: second -> first, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: second -> first, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: second -> first, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: second -> first, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: second -> first, base class: no relation, new model class: no relation
first:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .
second:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .
second:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .


class: second -> first, base class: no relation, new model class: no relation
first:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .
second:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .


class: second -> first, base class: no relation, new model class: no relation
first:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .
second:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .


class: second -> first, base class: no relation, new model class: no relation
first:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .
second:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .


class: second -> first, base class: no relation, new model class: no relation
first:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: second -> first, base class: no relation, new model class: no relation
first:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: second -> first, base class: no relation, new model class: no relation
first:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: second -> first, base class: no relation, new model class: no relation
first:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: second -> first, base class: no relation, new model class: no relation
first:
We have addressed this practical problem by <m> spatially subsampling </m> ( by simple decimation ) the first FC layer to ( or ) spatial size .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: second -> first, base class: no relation, new model class: first -> second
first:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .
second:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .


class: second -> first, base class: no relation, new model class: same cluster
first:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: second -> first, base class: no relation, new model class: same cluster
first:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: second -> first, base class: no relation, new model class: same cluster
first:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: second -> first, base class: no relation, new model class: same cluster
first:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: second -> first, base class: no relation, new model class: same cluster
first:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: second -> first, base class: first -> second, new model class: first -> second
first:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: second -> first, base class: first -> second, new model class: no relation
first:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: second -> first, base class: first -> second, new model class: no relation
first:
The PLS-Net is based on an asymmetric encoder-decoder architecture with three novel components : ( i ) 3D depthwise separable convolutions to improve the network efficiency by factorising each regular 3D convolution into two simpler operations ; ( ii ) dilated residual dense blocks to efficiently expand the receptive field of the network and aggregate multi-scale contextual information for segmentation ; and ( iii ) input reinforcement at each downsampled resolution to compensate for the loss of spatial information due to <m> convolutional and downsampling operations </m> .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper explores the problem <m> oftask-oriented downsampling </m> , which aims to downsample a pointcloud and maintain the performance of subsequent applications asmuch as possible .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper explores the problem <m> oftask-oriented downsampling </m> , which aims to downsample a pointcloud and maintain the performance of subsequent applications asmuch as possible .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: second -> first, base class: no relation, new model class: no relation
first:
More precisely , the contributions achieved during this thesis are : * A feature-preserving filter for the <m> downsampling of scalar volume data </m> .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In an MPEG ( Motion Pictures Experts Group ) video coding system , a <m> color component downsampling </m> from 1050 2:1 60 Hz to 525 1:1 30 Hz line scan formats must be carried out on the U and V color component source signals .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In an MPEG ( Motion Pictures Experts Group ) video coding system , a <m> color component downsampling </m> from 1050 2:1 60 Hz to 525 1:1 30 Hz line scan formats must be carried out on the U and V color component source signals .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In an MPEG ( Motion Pictures Experts Group ) video coding system , a <m> color component downsampling </m> from 1050 2:1 60 Hz to 525 1:1 30 Hz line scan formats must be carried out on the U and V color component source signals .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: second -> first, base class: no relation, new model class: no relation
first:
In an MPEG ( Motion Pictures Experts Group ) video coding system , a <m> color component downsampling </m> from 1050 2:1 60 Hz to 525 1:1 30 Hz line scan formats must be carried out on the U and V color component source signals .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: second -> first, base class: no relation, new model class: no relation
first:
In an MPEG ( Motion Pictures Experts Group ) video coding system , a <m> color component downsampling </m> from 1050 2:1 60 Hz to 525 1:1 30 Hz line scan formats must be carried out on the U and V color component source signals .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: second -> first, base class: no relation, new model class: no relation
first:
In an MPEG ( Motion Pictures Experts Group ) video coding system , a <m> color component downsampling </m> from 1050 2:1 60 Hz to 525 1:1 30 Hz line scan formats must be carried out on the U and V color component source signals .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: second -> first, base class: same cluster, new model class: same cluster
first:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: second -> first, base class: same cluster, new model class: same cluster
first:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: second -> first, base class: same cluster, new model class: same cluster
first:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: second -> first, base class: same cluster, new model class: same cluster
first:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: second -> first, base class: same cluster, new model class: same cluster
first:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: second -> first, base class: same cluster, new model class: same cluster
first:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In these cases , neither the ground - truth images nor the <m> downsampling kernels </m> are available .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: second -> first, base class: same cluster, new model class: same cluster
first:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: second -> first, base class: first -> second, new model class: no relation
first:
Each hourglass module in the network applies several <m> convolution and downsampling layers </m> to downsize the input feature maps .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: second -> first, base class: no relation, new model class: first -> second
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .


class: second -> first, base class: no relation, new model class: first -> second
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .


class: second -> first, base class: no relation, new model class: first -> second
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .


class: second -> first, base class: no relation, new model class: first -> second
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .


class: second -> first, base class: no relation, new model class: first -> second
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .


class: second -> first, base class: no relation, new model class: first -> second
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .


class: second -> first, base class: no relation, new model class: first -> second
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .


class: second -> first, base class: no relation, new model class: no relation
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: second -> first, base class: first -> second, new model class: no relation
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: second -> first, base class: no relation, new model class: first -> second
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
These features do not yet exist in the <m> chatbot application system </m> in other studies .


class: second -> first, base class: no relation, new model class: first -> second
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .


class: second -> first, base class: no relation, new model class: no relation
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: second -> first, base class: no relation, new model class: no relation
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: second -> first, base class: no relation, new model class: no relation
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: second -> first, base class: no relation, new model class: no relation
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: second -> first, base class: no relation, new model class: no relation
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: second -> first, base class: no relation, new model class: no relation
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: second -> first, base class: no relation, new model class: no relation
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: second -> first, base class: no relation, new model class: no relation
first:
Xu2016 created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a <m> conversation model </m> .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: second -> first, base class: no relation, new model class: no relation
first:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: second -> first, base class: no relation, new model class: no relation
first:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: second -> first, base class: no relation, new model class: no relation
first:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: second -> first, base class: no relation, new model class: no relation
first:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: second -> first, base class: no relation, new model class: no relation
first:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: second -> first, base class: no relation, new model class: no relation
first:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: second -> first, base class: no relation, new model class: no relation
first:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: second -> first, base class: no relation, new model class: no relation
first:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: second -> first, base class: no relation, new model class: no relation
first:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: second -> first, base class: no relation, new model class: no relation
first:
Finally , we launch our new system on the <m> chatbot platform Eva </m> in our E-commerce site AliExpress .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: second -> first, base class: no relation, new model class: no relation
first:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: second -> first, base class: no relation, new model class: no relation
first:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: second -> first, base class: no relation, new model class: no relation
first:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: second -> first, base class: no relation, new model class: no relation
first:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: second -> first, base class: no relation, new model class: no relation
first:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: second -> first, base class: no relation, new model class: no relation
first:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: second -> first, base class: no relation, new model class: no relation
first:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: second -> first, base class: no relation, new model class: no relation
first:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: second -> first, base class: no relation, new model class: no relation
first:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: second -> first, base class: no relation, new model class: no relation
first:
However , it is still in infancy and has not been applied widely in <m> educational chatbot development </m> .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: second -> first, base class: no relation, new model class: no relation
first:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: second -> first, base class: no relation, new model class: no relation
first:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: second -> first, base class: no relation, new model class: no relation
first:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: second -> first, base class: no relation, new model class: no relation
first:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: second -> first, base class: no relation, new model class: no relation
first:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: second -> first, base class: no relation, new model class: no relation
first:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: second -> first, base class: no relation, new model class: no relation
first:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: second -> first, base class: no relation, new model class: no relation
first:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: second -> first, base class: no relation, new model class: no relation
first:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: second -> first, base class: no relation, new model class: no relation
first:
Raiffeisen bank implemented the first <m> enterprise chatbot solution </m> in Serbia .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: second -> first, base class: first -> second, new model class: no relation
first:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: second -> first, base class: no relation, new model class: no relation
first:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: second -> first, base class: no relation, new model class: no relation
first:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .
second:
These features do not yet exist in the <m> chatbot application system </m> in other studies .


class: second -> first, base class: no relation, new model class: no relation
first:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .
second:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .


class: second -> first, base class: no relation, new model class: no relation
first:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: second -> first, base class: no relation, new model class: no relation
first:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: second -> first, base class: no relation, new model class: no relation
first:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: second -> first, base class: no relation, new model class: no relation
first:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: second -> first, base class: no relation, new model class: no relation
first:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: second -> first, base class: no relation, new model class: no relation
first:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: second -> first, base class: no relation, new model class: no relation
first:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: second -> first, base class: no relation, new model class: no relation
first:
There are many <m> chatbot builder platforms </m> and frameworks available in the market that can be used to build chatbots .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: second -> first, base class: no relation, new model class: no relation
first:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: second -> first, base class: no relation, new model class: no relation
first:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: second -> first, base class: no relation, new model class: no relation
first:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: second -> first, base class: no relation, new model class: no relation
first:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: second -> first, base class: no relation, new model class: no relation
first:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: second -> first, base class: no relation, new model class: no relation
first:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: second -> first, base class: no relation, new model class: no relation
first:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: second -> first, base class: no relation, new model class: no relation
first:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: second -> first, base class: no relation, new model class: no relation
first:
The BOTTA database files are publicly available for researchers working on <m> Arabic chatbot technologies </m> .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: second -> first, base class: no relation, new model class: no relation
first:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: second -> first, base class: no relation, new model class: no relation
first:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: second -> first, base class: no relation, new model class: no relation
first:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: second -> first, base class: no relation, new model class: no relation
first:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: second -> first, base class: no relation, new model class: no relation
first:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: second -> first, base class: no relation, new model class: no relation
first:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: second -> first, base class: no relation, new model class: no relation
first:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: second -> first, base class: no relation, new model class: no relation
first:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: second -> first, base class: no relation, new model class: no relation
first:
It turned out that a high percentage of the functionalities , which are decisive for a valuable <m> entrepreneurial chatbot application </m> , are covered by the providers , however , with nuanced differences being observed regarding particular features .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .
second:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .


class: second -> first, base class: no relation, new model class: no relation
first:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: second -> first, base class: no relation, new model class: no relation
first:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: second -> first, base class: no relation, new model class: no relation
first:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: second -> first, base class: no relation, new model class: no relation
first:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: second -> first, base class: no relation, new model class: no relation
first:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: second -> first, base class: no relation, new model class: no relation
first:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: second -> first, base class: no relation, new model class: no relation
first:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: second -> first, base class: no relation, new model class: no relation
first:
The PA4C model optimizes conventional RL models with action parameterization and auxiliary tasks for <m> chatbot training </m> , which address the problems of a large action space and zero-reward states .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: second -> first, base class: no relation, new model class: no relation
first:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: second -> first, base class: no relation, new model class: no relation
first:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: second -> first, base class: no relation, new model class: no relation
first:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: second -> first, base class: no relation, new model class: no relation
first:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: second -> first, base class: no relation, new model class: no relation
first:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: second -> first, base class: no relation, new model class: no relation
first:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: second -> first, base class: no relation, new model class: no relation
first:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: second -> first, base class: no relation, new model class: no relation
first:
Moreover , the survey is conducted with classification of <m> chatbot evaluation methods </m> and their analysis according to chatbot types and three main evaluation schemes ; content evaluation , user satisfaction , and chat function .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .
second:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed chatbot is a <m> cancer chatbot </m> designed only for people dealing with cancer .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Interaction Layer Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference and <m> retrieve - based chatbot </m> .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: second -> first, base class: no relation, new model class: no relation
first:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: second -> first, base class: no relation, new model class: no relation
first:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: second -> first, base class: no relation, new model class: no relation
first:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: second -> first, base class: no relation, new model class: no relation
first:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: second -> first, base class: no relation, new model class: no relation
first:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: second -> first, base class: no relation, new model class: no relation
first:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: second -> first, base class: no relation, new model class: no relation
first:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: second -> first, base class: no relation, new model class: no relation
first:
This leads to the prediction of a universal power law in the distribution of the number of pages per site which we confirm experimentally by analyzing data from large crawls made by the search engines <m> Alexa </m> and Infoseek .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: second -> first, base class: no relation, new model class: no relation
first:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: second -> first, base class: no relation, new model class: no relation
first:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: second -> first, base class: no relation, new model class: no relation
first:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: second -> first, base class: no relation, new model class: no relation
first:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: second -> first, base class: no relation, new model class: no relation
first:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: second -> first, base class: no relation, new model class: no relation
first:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: second -> first, base class: no relation, new model class: no relation
first:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: second -> first, base class: no relation, new model class: no relation
first:
We conducted a study of <m> Amazon Alexa usage </m> and explored the manifestations and possible correlates of users ' personification of Alexa .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: second -> first, base class: no relation, new model class: no relation
first:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: second -> first, base class: no relation, new model class: no relation
first:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: second -> first, base class: no relation, new model class: no relation
first:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: second -> first, base class: no relation, new model class: no relation
first:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: second -> first, base class: no relation, new model class: no relation
first:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: second -> first, base class: no relation, new model class: no relation
first:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: second -> first, base class: no relation, new model class: no relation
first:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: second -> first, base class: no relation, new model class: no relation
first:
Recently I came across this new revolutionary set of <m> voice controlled Alexa Devices </m> and love the freedom it gives me .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: second -> first, base class: no relation, new model class: no relation
first:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: second -> first, base class: no relation, new model class: no relation
first:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: second -> first, base class: no relation, new model class: no relation
first:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: second -> first, base class: no relation, new model class: no relation
first:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: second -> first, base class: no relation, new model class: no relation
first:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: second -> first, base class: no relation, new model class: no relation
first:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: second -> first, base class: no relation, new model class: no relation
first:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: second -> first, base class: no relation, new model class: no relation
first:
The data about <m> Alexa usage </m> were collected from 19 participants via the online questionnaire and diary methods over the course of several days .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: second -> first, base class: no relation, new model class: no relation
first:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: second -> first, base class: no relation, new model class: no relation
first:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: second -> first, base class: no relation, new model class: no relation
first:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: second -> first, base class: no relation, new model class: no relation
first:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: second -> first, base class: no relation, new model class: no relation
first:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: second -> first, base class: no relation, new model class: no relation
first:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: second -> first, base class: no relation, new model class: no relation
first:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: second -> first, base class: no relation, new model class: no relation
first:
The convenience provided by an Alexaenabled device comes at the cost of <m> Alexa service </m> 's voice recording and storage behavior , raising privacy concerns .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: second -> first, base class: no relation, new model class: no relation
first:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: second -> first, base class: no relation, new model class: no relation
first:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: second -> first, base class: no relation, new model class: no relation
first:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: second -> first, base class: no relation, new model class: no relation
first:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: second -> first, base class: no relation, new model class: no relation
first:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: second -> first, base class: no relation, new model class: no relation
first:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: second -> first, base class: no relation, new model class: no relation
first:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: second -> first, base class: no relation, new model class: no relation
first:
Five different cloud - based providers are compared to Snips NLU ( Microsoft ’s Luis , API.AI now Google ’s Dialogflow , Facebook ’s Wit.ai , and <m> Amazon Alexa </m> ) .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The <m> Virtual Operative Assistant </m> successfully classified skilled and novice participants using 4 metrics with an accuracy , specificity and sensitivity of 92 , 82 and 100 % , respectively .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: second -> first, base class: no relation, new model class: no relation
first:
Simply adding <m> time binning </m> works well when the source is strong and there are many counts reconstructed in each time bin .
second:
The central task of automatic data visualization is , given a dataset , to visualize its compelling stories by transforming the data ( e.g. , selecting attributes , <m> grouping and binning values </m> ) and deciding the right type of visualization ( e.g. , bar or line charts ) .


class: second -> first, base class: no relation, new model class: no relation
first:
Simply adding <m> time binning </m> works well when the source is strong and there are many counts reconstructed in each time bin .
second:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .


class: second -> first, base class: no relation, new model class: no relation
first:
Simply adding <m> time binning </m> works well when the source is strong and there are many counts reconstructed in each time bin .
second:
To overcome this problem , <m> bucketing </m> or binning techniques have been used .


class: second -> first, base class: no relation, new model class: no relation
first:
Simply adding <m> time binning </m> works well when the source is strong and there are many counts reconstructed in each time bin .
second:
Köpf et al. adopted this approach to evaluate the contribution of a <m> bucketing technique </m> that was proposed by themselves .


class: second -> first, base class: no relation, new model class: no relation
first:
Simply adding <m> time binning </m> works well when the source is strong and there are many counts reconstructed in each time bin .
second:
The algorithm is based on the <m> bucketing approach </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Simply adding <m> time binning </m> works well when the source is strong and there are many counts reconstructed in each time bin .
second:
We also try to answer several questions , which are generally -and perhaps voluntarily-bypassed : " does the <m> bucketing strategy </m> influence the regression process ? " ; " how should the data be split into buckets to get the best fits both numerically and physically ? " ...


class: second -> first, base class: no relation, new model class: no relation
first:
Simply adding <m> time binning </m> works well when the source is strong and there are many counts reconstructed in each time bin .
second:
In this paper , we propose a variant of the classical <m> bucketing algorithm </m> that ( 1 ) solves the convex-hull problem for any multiset of points , ( 2 ) uses O ( √ n ) words of extra space , ( 3 ) runs in O(n ) expected time on points drawn independently and uniformly from a rectangle , and ( 4 ) requires O(n lgn ) time in the worst case .


class: second -> first, base class: no relation, new model class: no relation
first:
Simply adding <m> time binning </m> works well when the source is strong and there are many counts reconstructed in each time bin .
second:
This technique is extremely fast , polynomial in the number of days and stocks , and does not add any errors to those already incurred in the <m> companion bucketing scheme </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Simply adding <m> time binning </m> works well when the source is strong and there are many counts reconstructed in each time bin .
second:
The pipeline , entirely written in CUDA , supports both fully conservative and thin voxelizations , multiple boolean , floating point , vector-typed render targets , user-defined vertex and fragment shaders , and a <m> bucketing mode </m> which can be used to generate 3D A-buffers containing the entire list of fragments belonging to each voxel .


class: second -> first, base class: no relation, new model class: no relation
first:
In this study , we propose a method com-bining K-means clustering algorithm and <m> unsupervised binning approaches </m> to improve the performance in metagenome-based disease prediction .
second:
The central task of automatic data visualization is , given a dataset , to visualize its compelling stories by transforming the data ( e.g. , selecting attributes , <m> grouping and binning values </m> ) and deciding the right type of visualization ( e.g. , bar or line charts ) .


class: second -> first, base class: no relation, new model class: no relation
first:
In this study , we propose a method com-bining K-means clustering algorithm and <m> unsupervised binning approaches </m> to improve the performance in metagenome-based disease prediction .
second:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .


class: second -> first, base class: no relation, new model class: no relation
first:
In this study , we propose a method com-bining K-means clustering algorithm and <m> unsupervised binning approaches </m> to improve the performance in metagenome-based disease prediction .
second:
To overcome this problem , <m> bucketing </m> or binning techniques have been used .


class: second -> first, base class: no relation, new model class: no relation
first:
In this study , we propose a method com-bining K-means clustering algorithm and <m> unsupervised binning approaches </m> to improve the performance in metagenome-based disease prediction .
second:
Köpf et al. adopted this approach to evaluate the contribution of a <m> bucketing technique </m> that was proposed by themselves .


class: second -> first, base class: no relation, new model class: no relation
first:
In this study , we propose a method com-bining K-means clustering algorithm and <m> unsupervised binning approaches </m> to improve the performance in metagenome-based disease prediction .
second:
The algorithm is based on the <m> bucketing approach </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In this study , we propose a method com-bining K-means clustering algorithm and <m> unsupervised binning approaches </m> to improve the performance in metagenome-based disease prediction .
second:
We also try to answer several questions , which are generally -and perhaps voluntarily-bypassed : " does the <m> bucketing strategy </m> influence the regression process ? " ; " how should the data be split into buckets to get the best fits both numerically and physically ? " ...


class: second -> first, base class: no relation, new model class: no relation
first:
In this study , we propose a method com-bining K-means clustering algorithm and <m> unsupervised binning approaches </m> to improve the performance in metagenome-based disease prediction .
second:
In this paper , we propose a variant of the classical <m> bucketing algorithm </m> that ( 1 ) solves the convex-hull problem for any multiset of points , ( 2 ) uses O ( √ n ) words of extra space , ( 3 ) runs in O(n ) expected time on points drawn independently and uniformly from a rectangle , and ( 4 ) requires O(n lgn ) time in the worst case .


class: second -> first, base class: no relation, new model class: no relation
first:
In this study , we propose a method com-bining K-means clustering algorithm and <m> unsupervised binning approaches </m> to improve the performance in metagenome-based disease prediction .
second:
This technique is extremely fast , polynomial in the number of days and stocks , and does not add any errors to those already incurred in the <m> companion bucketing scheme </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In this study , we propose a method com-bining K-means clustering algorithm and <m> unsupervised binning approaches </m> to improve the performance in metagenome-based disease prediction .
second:
The pipeline , entirely written in CUDA , supports both fully conservative and thin voxelizations , multiple boolean , floating point , vector-typed render targets , user-defined vertex and fragment shaders , and a <m> bucketing mode </m> which can be used to generate 3D A-buffers containing the entire list of fragments belonging to each voxel .


class: second -> first, base class: no relation, new model class: no relation
first:
We then perform <m> NILS binning </m> to assign different magnitude of retargeting to different NILS bins .
second:
The central task of automatic data visualization is , given a dataset , to visualize its compelling stories by transforming the data ( e.g. , selecting attributes , <m> grouping and binning values </m> ) and deciding the right type of visualization ( e.g. , bar or line charts ) .


class: second -> first, base class: no relation, new model class: no relation
first:
We then perform <m> NILS binning </m> to assign different magnitude of retargeting to different NILS bins .
second:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .


class: second -> first, base class: no relation, new model class: no relation
first:
We then perform <m> NILS binning </m> to assign different magnitude of retargeting to different NILS bins .
second:
To overcome this problem , <m> bucketing </m> or binning techniques have been used .


class: second -> first, base class: no relation, new model class: no relation
first:
We then perform <m> NILS binning </m> to assign different magnitude of retargeting to different NILS bins .
second:
Köpf et al. adopted this approach to evaluate the contribution of a <m> bucketing technique </m> that was proposed by themselves .


class: second -> first, base class: no relation, new model class: no relation
first:
We then perform <m> NILS binning </m> to assign different magnitude of retargeting to different NILS bins .
second:
The algorithm is based on the <m> bucketing approach </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We then perform <m> NILS binning </m> to assign different magnitude of retargeting to different NILS bins .
second:
We also try to answer several questions , which are generally -and perhaps voluntarily-bypassed : " does the <m> bucketing strategy </m> influence the regression process ? " ; " how should the data be split into buckets to get the best fits both numerically and physically ? " ...


class: second -> first, base class: no relation, new model class: no relation
first:
We then perform <m> NILS binning </m> to assign different magnitude of retargeting to different NILS bins .
second:
In this paper , we propose a variant of the classical <m> bucketing algorithm </m> that ( 1 ) solves the convex-hull problem for any multiset of points , ( 2 ) uses O ( √ n ) words of extra space , ( 3 ) runs in O(n ) expected time on points drawn independently and uniformly from a rectangle , and ( 4 ) requires O(n lgn ) time in the worst case .


class: second -> first, base class: no relation, new model class: no relation
first:
We then perform <m> NILS binning </m> to assign different magnitude of retargeting to different NILS bins .
second:
This technique is extremely fast , polynomial in the number of days and stocks , and does not add any errors to those already incurred in the <m> companion bucketing scheme </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We then perform <m> NILS binning </m> to assign different magnitude of retargeting to different NILS bins .
second:
The pipeline , entirely written in CUDA , supports both fully conservative and thin voxelizations , multiple boolean , floating point , vector-typed render targets , user-defined vertex and fragment shaders , and a <m> bucketing mode </m> which can be used to generate 3D A-buffers containing the entire list of fragments belonging to each voxel .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper demonstrates and compares different approaches for incorporating deterministic models of physical parameters into probabilistic models ; <m> parameter range binning </m> , response curves , and integral deterministic models .
second:
We describe a new <m> binning technic </m> for informed data hiding problem .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper demonstrates and compares different approaches for incorporating deterministic models of physical parameters into probabilistic models ; <m> parameter range binning </m> , response curves , and integral deterministic models .
second:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper demonstrates and compares different approaches for incorporating deterministic models of physical parameters into probabilistic models ; <m> parameter range binning </m> , response curves , and integral deterministic models .
second:
To overcome this problem , <m> bucketing </m> or binning techniques have been used .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper demonstrates and compares different approaches for incorporating deterministic models of physical parameters into probabilistic models ; <m> parameter range binning </m> , response curves , and integral deterministic models .
second:
Köpf et al. adopted this approach to evaluate the contribution of a <m> bucketing technique </m> that was proposed by themselves .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper demonstrates and compares different approaches for incorporating deterministic models of physical parameters into probabilistic models ; <m> parameter range binning </m> , response curves , and integral deterministic models .
second:
The algorithm is based on the <m> bucketing approach </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper demonstrates and compares different approaches for incorporating deterministic models of physical parameters into probabilistic models ; <m> parameter range binning </m> , response curves , and integral deterministic models .
second:
We also try to answer several questions , which are generally -and perhaps voluntarily-bypassed : " does the <m> bucketing strategy </m> influence the regression process ? " ; " how should the data be split into buckets to get the best fits both numerically and physically ? " ...


class: second -> first, base class: no relation, new model class: no relation
first:
This paper demonstrates and compares different approaches for incorporating deterministic models of physical parameters into probabilistic models ; <m> parameter range binning </m> , response curves , and integral deterministic models .
second:
In this paper , we propose a variant of the classical <m> bucketing algorithm </m> that ( 1 ) solves the convex-hull problem for any multiset of points , ( 2 ) uses O ( √ n ) words of extra space , ( 3 ) runs in O(n ) expected time on points drawn independently and uniformly from a rectangle , and ( 4 ) requires O(n lgn ) time in the worst case .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper demonstrates and compares different approaches for incorporating deterministic models of physical parameters into probabilistic models ; <m> parameter range binning </m> , response curves , and integral deterministic models .
second:
This technique is extremely fast , polynomial in the number of days and stocks , and does not add any errors to those already incurred in the <m> companion bucketing scheme </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper demonstrates and compares different approaches for incorporating deterministic models of physical parameters into probabilistic models ; <m> parameter range binning </m> , response curves , and integral deterministic models .
second:
The pipeline , entirely written in CUDA , supports both fully conservative and thin voxelizations , multiple boolean , floating point , vector-typed render targets , user-defined vertex and fragment shaders , and a <m> bucketing mode </m> which can be used to generate 3D A-buffers containing the entire list of fragments belonging to each voxel .


class: second -> first, base class: no relation, new model class: no relation
first:
Results We present here a <m> scalable pre-assembly binning scheme </m> ( i.e. , operating on unassembled short reads ) enabling latent genome recovery by leveraging sparse dictionary learning and elastic-net regularization , and its use to recover hundreds of metagenome-assembled genomes , including very low-abundance genomes , from a joint analysis of microbiomes from the LifeLines DEEP population cohort ( n = 1,135 , > 1010 reads ) .
second:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .


class: second -> first, base class: no relation, new model class: no relation
first:
Results We present here a <m> scalable pre-assembly binning scheme </m> ( i.e. , operating on unassembled short reads ) enabling latent genome recovery by leveraging sparse dictionary learning and elastic-net regularization , and its use to recover hundreds of metagenome-assembled genomes , including very low-abundance genomes , from a joint analysis of microbiomes from the LifeLines DEEP population cohort ( n = 1,135 , > 1010 reads ) .
second:
To overcome this problem , <m> bucketing </m> or binning techniques have been used .


class: second -> first, base class: no relation, new model class: no relation
first:
Results We present here a <m> scalable pre-assembly binning scheme </m> ( i.e. , operating on unassembled short reads ) enabling latent genome recovery by leveraging sparse dictionary learning and elastic-net regularization , and its use to recover hundreds of metagenome-assembled genomes , including very low-abundance genomes , from a joint analysis of microbiomes from the LifeLines DEEP population cohort ( n = 1,135 , > 1010 reads ) .
second:
Köpf et al. adopted this approach to evaluate the contribution of a <m> bucketing technique </m> that was proposed by themselves .


class: second -> first, base class: no relation, new model class: no relation
first:
Results We present here a <m> scalable pre-assembly binning scheme </m> ( i.e. , operating on unassembled short reads ) enabling latent genome recovery by leveraging sparse dictionary learning and elastic-net regularization , and its use to recover hundreds of metagenome-assembled genomes , including very low-abundance genomes , from a joint analysis of microbiomes from the LifeLines DEEP population cohort ( n = 1,135 , > 1010 reads ) .
second:
The algorithm is based on the <m> bucketing approach </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Results We present here a <m> scalable pre-assembly binning scheme </m> ( i.e. , operating on unassembled short reads ) enabling latent genome recovery by leveraging sparse dictionary learning and elastic-net regularization , and its use to recover hundreds of metagenome-assembled genomes , including very low-abundance genomes , from a joint analysis of microbiomes from the LifeLines DEEP population cohort ( n = 1,135 , > 1010 reads ) .
second:
We also try to answer several questions , which are generally -and perhaps voluntarily-bypassed : " does the <m> bucketing strategy </m> influence the regression process ? " ; " how should the data be split into buckets to get the best fits both numerically and physically ? " ...


class: second -> first, base class: no relation, new model class: no relation
first:
Results We present here a <m> scalable pre-assembly binning scheme </m> ( i.e. , operating on unassembled short reads ) enabling latent genome recovery by leveraging sparse dictionary learning and elastic-net regularization , and its use to recover hundreds of metagenome-assembled genomes , including very low-abundance genomes , from a joint analysis of microbiomes from the LifeLines DEEP population cohort ( n = 1,135 , > 1010 reads ) .
second:
In this paper , we propose a variant of the classical <m> bucketing algorithm </m> that ( 1 ) solves the convex-hull problem for any multiset of points , ( 2 ) uses O ( √ n ) words of extra space , ( 3 ) runs in O(n ) expected time on points drawn independently and uniformly from a rectangle , and ( 4 ) requires O(n lgn ) time in the worst case .


class: second -> first, base class: no relation, new model class: no relation
first:
Results We present here a <m> scalable pre-assembly binning scheme </m> ( i.e. , operating on unassembled short reads ) enabling latent genome recovery by leveraging sparse dictionary learning and elastic-net regularization , and its use to recover hundreds of metagenome-assembled genomes , including very low-abundance genomes , from a joint analysis of microbiomes from the LifeLines DEEP population cohort ( n = 1,135 , > 1010 reads ) .
second:
This technique is extremely fast , polynomial in the number of days and stocks , and does not add any errors to those already incurred in the <m> companion bucketing scheme </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Results We present here a <m> scalable pre-assembly binning scheme </m> ( i.e. , operating on unassembled short reads ) enabling latent genome recovery by leveraging sparse dictionary learning and elastic-net regularization , and its use to recover hundreds of metagenome-assembled genomes , including very low-abundance genomes , from a joint analysis of microbiomes from the LifeLines DEEP population cohort ( n = 1,135 , > 1010 reads ) .
second:
The pipeline , entirely written in CUDA , supports both fully conservative and thin voxelizations , multiple boolean , floating point , vector-typed render targets , user-defined vertex and fragment shaders , and a <m> bucketing mode </m> which can be used to generate 3D A-buffers containing the entire list of fragments belonging to each voxel .


class: second -> first, base class: no relation, new model class: no relation
first:
As in previous versions , a filtering and a <m> weighted binning </m> based on the uncertainty of the SM retrievals by means of the Data Quality Index ( DQX ) was applied for the L3 production .
second:
We describe a new <m> binning technic </m> for informed data hiding problem .


class: second -> first, base class: no relation, new model class: no relation
first:
As in previous versions , a filtering and a <m> weighted binning </m> based on the uncertainty of the SM retrievals by means of the Data Quality Index ( DQX ) was applied for the L3 production .
second:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .


class: second -> first, base class: no relation, new model class: no relation
first:
As in previous versions , a filtering and a <m> weighted binning </m> based on the uncertainty of the SM retrievals by means of the Data Quality Index ( DQX ) was applied for the L3 production .
second:
To overcome this problem , <m> bucketing </m> or binning techniques have been used .


class: second -> first, base class: no relation, new model class: no relation
first:
As in previous versions , a filtering and a <m> weighted binning </m> based on the uncertainty of the SM retrievals by means of the Data Quality Index ( DQX ) was applied for the L3 production .
second:
Köpf et al. adopted this approach to evaluate the contribution of a <m> bucketing technique </m> that was proposed by themselves .


class: second -> first, base class: no relation, new model class: no relation
first:
As in previous versions , a filtering and a <m> weighted binning </m> based on the uncertainty of the SM retrievals by means of the Data Quality Index ( DQX ) was applied for the L3 production .
second:
The algorithm is based on the <m> bucketing approach </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
As in previous versions , a filtering and a <m> weighted binning </m> based on the uncertainty of the SM retrievals by means of the Data Quality Index ( DQX ) was applied for the L3 production .
second:
We also try to answer several questions , which are generally -and perhaps voluntarily-bypassed : " does the <m> bucketing strategy </m> influence the regression process ? " ; " how should the data be split into buckets to get the best fits both numerically and physically ? " ...


class: second -> first, base class: no relation, new model class: no relation
first:
As in previous versions , a filtering and a <m> weighted binning </m> based on the uncertainty of the SM retrievals by means of the Data Quality Index ( DQX ) was applied for the L3 production .
second:
In this paper , we propose a variant of the classical <m> bucketing algorithm </m> that ( 1 ) solves the convex-hull problem for any multiset of points , ( 2 ) uses O ( √ n ) words of extra space , ( 3 ) runs in O(n ) expected time on points drawn independently and uniformly from a rectangle , and ( 4 ) requires O(n lgn ) time in the worst case .


class: second -> first, base class: no relation, new model class: no relation
first:
As in previous versions , a filtering and a <m> weighted binning </m> based on the uncertainty of the SM retrievals by means of the Data Quality Index ( DQX ) was applied for the L3 production .
second:
This technique is extremely fast , polynomial in the number of days and stocks , and does not add any errors to those already incurred in the <m> companion bucketing scheme </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
As in previous versions , a filtering and a <m> weighted binning </m> based on the uncertainty of the SM retrievals by means of the Data Quality Index ( DQX ) was applied for the L3 production .
second:
The pipeline , entirely written in CUDA , supports both fully conservative and thin voxelizations , multiple boolean , floating point , vector-typed render targets , user-defined vertex and fragment shaders , and a <m> bucketing mode </m> which can be used to generate 3D A-buffers containing the entire list of fragments belonging to each voxel .


class: second -> first, base class: no relation, new model class: no relation
first:
Obtaining a coherent binned representation in higher dimensions is challenging because <m> multidimensional binning </m> can suffer from the curse of dimensionality .
second:
To overcome this problem , <m> bucketing </m> or binning techniques have been used .


class: second -> first, base class: no relation, new model class: no relation
first:
Obtaining a coherent binned representation in higher dimensions is challenging because <m> multidimensional binning </m> can suffer from the curse of dimensionality .
second:
Köpf et al. adopted this approach to evaluate the contribution of a <m> bucketing technique </m> that was proposed by themselves .


class: second -> first, base class: no relation, new model class: no relation
first:
Obtaining a coherent binned representation in higher dimensions is challenging because <m> multidimensional binning </m> can suffer from the curse of dimensionality .
second:
The algorithm is based on the <m> bucketing approach </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Obtaining a coherent binned representation in higher dimensions is challenging because <m> multidimensional binning </m> can suffer from the curse of dimensionality .
second:
We also try to answer several questions , which are generally -and perhaps voluntarily-bypassed : " does the <m> bucketing strategy </m> influence the regression process ? " ; " how should the data be split into buckets to get the best fits both numerically and physically ? " ...


class: second -> first, base class: no relation, new model class: no relation
first:
Obtaining a coherent binned representation in higher dimensions is challenging because <m> multidimensional binning </m> can suffer from the curse of dimensionality .
second:
In this paper , we propose a variant of the classical <m> bucketing algorithm </m> that ( 1 ) solves the convex-hull problem for any multiset of points , ( 2 ) uses O ( √ n ) words of extra space , ( 3 ) runs in O(n ) expected time on points drawn independently and uniformly from a rectangle , and ( 4 ) requires O(n lgn ) time in the worst case .


class: second -> first, base class: no relation, new model class: no relation
first:
Obtaining a coherent binned representation in higher dimensions is challenging because <m> multidimensional binning </m> can suffer from the curse of dimensionality .
second:
This technique is extremely fast , polynomial in the number of days and stocks , and does not add any errors to those already incurred in the <m> companion bucketing scheme </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Obtaining a coherent binned representation in higher dimensions is challenging because <m> multidimensional binning </m> can suffer from the curse of dimensionality .
second:
The pipeline , entirely written in CUDA , supports both fully conservative and thin voxelizations , multiple boolean , floating point , vector-typed render targets , user-defined vertex and fragment shaders , and a <m> bucketing mode </m> which can be used to generate 3D A-buffers containing the entire list of fragments belonging to each voxel .


class: second -> first, base class: no relation, new model class: no relation
first:
Peak picking and separation are evaluated against the conventional methods of <m> mass binning </m> and manually selecting regions of a peak to image on a model data set .
second:
To overcome this problem , <m> bucketing </m> or binning techniques have been used .


class: second -> first, base class: no relation, new model class: no relation
first:
Peak picking and separation are evaluated against the conventional methods of <m> mass binning </m> and manually selecting regions of a peak to image on a model data set .
second:
Köpf et al. adopted this approach to evaluate the contribution of a <m> bucketing technique </m> that was proposed by themselves .


class: second -> first, base class: no relation, new model class: no relation
first:
Peak picking and separation are evaluated against the conventional methods of <m> mass binning </m> and manually selecting regions of a peak to image on a model data set .
second:
The algorithm is based on the <m> bucketing approach </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Peak picking and separation are evaluated against the conventional methods of <m> mass binning </m> and manually selecting regions of a peak to image on a model data set .
second:
We also try to answer several questions , which are generally -and perhaps voluntarily-bypassed : " does the <m> bucketing strategy </m> influence the regression process ? " ; " how should the data be split into buckets to get the best fits both numerically and physically ? " ...


class: second -> first, base class: no relation, new model class: no relation
first:
Peak picking and separation are evaluated against the conventional methods of <m> mass binning </m> and manually selecting regions of a peak to image on a model data set .
second:
In this paper , we propose a variant of the classical <m> bucketing algorithm </m> that ( 1 ) solves the convex-hull problem for any multiset of points , ( 2 ) uses O ( √ n ) words of extra space , ( 3 ) runs in O(n ) expected time on points drawn independently and uniformly from a rectangle , and ( 4 ) requires O(n lgn ) time in the worst case .


class: second -> first, base class: no relation, new model class: no relation
first:
Peak picking and separation are evaluated against the conventional methods of <m> mass binning </m> and manually selecting regions of a peak to image on a model data set .
second:
This technique is extremely fast , polynomial in the number of days and stocks , and does not add any errors to those already incurred in the <m> companion bucketing scheme </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Peak picking and separation are evaluated against the conventional methods of <m> mass binning </m> and manually selecting regions of a peak to image on a model data set .
second:
The pipeline , entirely written in CUDA , supports both fully conservative and thin voxelizations , multiple boolean , floating point , vector-typed render targets , user-defined vertex and fragment shaders , and a <m> bucketing mode </m> which can be used to generate 3D A-buffers containing the entire list of fragments belonging to each voxel .


class: second -> first, base class: no relation, new model class: no relation
first:
On the basis of this principle , this study aims to verify whether the performance of <m> GloVe </m> can be improved after processing certain calibrations on the GloVe model .
second:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .


class: second -> first, base class: no relation, new model class: no relation
first:
On the basis of this principle , this study aims to verify whether the performance of <m> GloVe </m> can be improved after processing certain calibrations on the GloVe model .
second:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .


class: second -> first, base class: no relation, new model class: no relation
first:
We propose a new word embedding model , inspired by <m> GloVe </m> , which is formulated as a feasible least squares optimization problem .
second:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .


class: second -> first, base class: no relation, new model class: no relation
first:
We propose a new word embedding model , inspired by <m> GloVe </m> , which is formulated as a feasible least squares optimization problem .
second:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .


class: second -> first, base class: no relation, new model class: no relation
first:
We also show that distributed representations produced by the <m> GloVe model </m> are better than those produced by the Skip-gram model when being used in dependency parsing .
second:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .


class: second -> first, base class: no relation, new model class: no relation
first:
We also show that distributed representations produced by the <m> GloVe model </m> are better than those produced by the Skip-gram model when being used in dependency parsing .
second:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .


class: second -> first, base class: no relation, new model class: no relation
first:
For topic detection , we design a novel real-time topic model dubbed as a Cost-Effective And Scalable Embedding model ( CEASE ) based on improved <m> GloVe Models </m> and keyword frequency clustering algorithm .
second:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .


class: second -> first, base class: no relation, new model class: no relation
first:
For topic detection , we design a novel real-time topic model dubbed as a Cost-Effective And Scalable Embedding model ( CEASE ) based on improved <m> GloVe Models </m> and keyword frequency clustering algorithm .
second:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .


class: second -> first, base class: no relation, new model class: no relation
first:
The latent features consist of representations for a person learned by using a word2vec model and representations for profession/nationality values extracted from a <m> pre-trained GloVe embedding model </m> .
second:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .


class: second -> first, base class: no relation, new model class: no relation
first:
The latent features consist of representations for a person learned by using a word2vec model and representations for profession/nationality values extracted from a <m> pre-trained GloVe embedding model </m> .
second:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .


class: second -> first, base class: no relation, new model class: no relation
first:
Recent approaches to word vector representations , e.g. , ‘ w2vec ’ and ‘ <m> GloVe ’ </m> , have been shown to be powerful methods for capturing the semantics and syntax of words in a text .
second:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .


class: second -> first, base class: no relation, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .


class: second -> first, base class: no relation, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
Through large scale human evaluation , we report that our resulting word embedddings are much more interpretable than the original <m> GloVe and word2vec embeddings </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
First , we propose an online topic clustering algorithm based on keyword frequency to combine the extended TextRank with weighted titles and the improved <m> GloVe methods </m> .
second:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .


class: second -> first, base class: no relation, new model class: no relation
first:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .
second:
Through large scale human evaluation , we report that our resulting word embedddings are much more interpretable than the original <m> GloVe and word2vec embeddings </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .
second:
Through large scale human evaluation , we report that our resulting word embedddings are much more interpretable than the original <m> GloVe and word2vec embeddings </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .
second:
Through large scale human evaluation , we report that our resulting word embedddings are much more interpretable than the original <m> GloVe and word2vec embeddings </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .
second:
Through large scale human evaluation , we report that our resulting word embedddings are much more interpretable than the original <m> GloVe and word2vec embeddings </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .
second:
Through large scale human evaluation , we report that our resulting word embedddings are much more interpretable than the original <m> GloVe and word2vec embeddings </m> .


class: second -> first, base class: same cluster, new model class: no relation
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .


class: second -> first, base class: same cluster, new model class: no relation
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .


class: second -> first, base class: same cluster, new model class: no relation
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .


class: second -> first, base class: same cluster, new model class: no relation
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .


class: second -> first, base class: same cluster, new model class: no relation
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .


class: second -> first, base class: same cluster, new model class: no relation
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .


class: second -> first, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .


class: second -> first, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .


class: second -> first, base class: no relation, new model class: no relation
first:
In our experiments , each paragraph vector is a combination of two vectors : one learned by the standard paragraph vector with distributed memory ( PV - DM ) and one learned by the paragraph vector with <m> distributed bag of words </m> ( PV - DBOW ) .
second:
Natural language processing research has made major advances with the concept of representing words , sentences , paragraphs , and even documents by <m> embedded vector representations </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In our experiments , each paragraph vector is a combination of two vectors : one learned by the standard paragraph vector with distributed memory ( PV - DM ) and one learned by the paragraph vector with <m> distributed bag of words </m> ( PV - DBOW ) .
second:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .
second:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
After that , we exploit pseudo word sequence as the input of <m> embedding vector model </m> and finally learn the text feature which could reflect the text semantic with social network characteristics .


class: second -> first, base class: no relation, new model class: no relation
first:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: second -> first, base class: no relation, new model class: no relation
first:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: second -> first, base class: no relation, new model class: no relation
first:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: second -> first, base class: no relation, new model class: no relation
first:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: second -> first, base class: no relation, new model class: no relation
first:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: second -> first, base class: no relation, new model class: no relation
first:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: second -> first, base class: no relation, new model class: no relation
first:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: second -> first, base class: no relation, new model class: no relation
first:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: second -> first, base class: no relation, new model class: no relation
first:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: second -> first, base class: no relation, new model class: no relation
first:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: second -> first, base class: no relation, new model class: no relation
first:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: second -> first, base class: no relation, new model class: no relation
first:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: second -> first, base class: no relation, new model class: no relation
first:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: second -> first, base class: no relation, new model class: no relation
first:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: second -> first, base class: no relation, new model class: no relation
first:
Word2Vec has two main formulations for the target prediction : skip - gram ( SG ) and <m> continuous bag - of - words </m> ( CBOW ) .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .


class: second -> first, base class: no relation, new model class: same cluster
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .


class: second -> first, base class: no relation, new model class: same cluster
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .


class: second -> first, base class: no relation, new model class: same cluster
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .


class: second -> first, base class: no relation, new model class: same cluster
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .


class: second -> first, base class: no relation, new model class: same cluster
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .


class: second -> first, base class: no relation, new model class: same cluster
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: second -> first, base class: no relation, new model class: same cluster
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: second -> first, base class: no relation, new model class: same cluster
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: second -> first, base class: no relation, new model class: same cluster
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: second -> first, base class: no relation, new model class: same cluster
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: second -> first, base class: no relation, new model class: same cluster
first:
Representation learning is the basis for modern approaches to natural language processing and artificial neural networks , in particular <m> deep learning </m> , which includes popular models such as convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .


class: second -> first, base class: no relation, new model class: same cluster
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .


class: second -> first, base class: no relation, new model class: same cluster
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .


class: second -> first, base class: no relation, new model class: same cluster
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .


class: second -> first, base class: no relation, new model class: same cluster
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .


class: second -> first, base class: no relation, new model class: same cluster
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .


class: second -> first, base class: no relation, new model class: same cluster
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: second -> first, base class: no relation, new model class: same cluster
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: second -> first, base class: no relation, new model class: same cluster
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: second -> first, base class: no relation, new model class: same cluster
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: second -> first, base class: no relation, new model class: same cluster
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: second -> first, base class: no relation, new model class: same cluster
first:
To tackle this issue , we present a <m> deep learningbased method </m> for detecting wildfires at an early stage by identifying flames and smokes at once .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .


class: second -> first, base class: no relation, new model class: same cluster
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .


class: second -> first, base class: no relation, new model class: same cluster
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .


class: second -> first, base class: no relation, new model class: same cluster
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .


class: second -> first, base class: no relation, new model class: same cluster
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .


class: second -> first, base class: no relation, new model class: same cluster
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .


class: second -> first, base class: no relation, new model class: same cluster
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: second -> first, base class: no relation, new model class: same cluster
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: second -> first, base class: no relation, new model class: same cluster
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: second -> first, base class: no relation, new model class: same cluster
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: second -> first, base class: no relation, new model class: same cluster
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: second -> first, base class: no relation, new model class: same cluster
first:
The recent rapid progress on <m> deep learn- </m> ing within the machine learning community , and the growing number of deep learning-based models in the cognitive sci- The goal of the workshop is to explore the relevance of re- cent deep learning advances to cognition , to bring together cognitive science-oriented deep learning researchers , and to facilitate exchanges between the machine learning and cog- nitive science communities .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , we focus on deep learning method to achieve high precision in stock market forecast . And a deep belief networks(DBNs ) , which is a kind of <m> deep learning algorithm model </m> , coupled with stock technical indicators(STIs ) and two-dimensional principal component analysis((2D)2PCA ) is introduced as a novel approach to predict the closing price of stock market .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .


class: second -> first, base class: no relation, new model class: same cluster
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .


class: second -> first, base class: no relation, new model class: same cluster
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .


class: second -> first, base class: no relation, new model class: same cluster
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .


class: second -> first, base class: no relation, new model class: same cluster
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .


class: second -> first, base class: no relation, new model class: same cluster
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .


class: second -> first, base class: no relation, new model class: same cluster
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: second -> first, base class: no relation, new model class: same cluster
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: second -> first, base class: no relation, new model class: same cluster
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: second -> first, base class: no relation, new model class: same cluster
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: second -> first, base class: no relation, new model class: same cluster
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: second -> first, base class: no relation, new model class: same cluster
first:
Specifically , by feeding the DBM-based deep network with prior-described features such as Log-Gabor , HoG and Gist , we get a significant improvement on performance in comparison with the state-of-art results and the best performance is achieved by using a fusion form of the three descriptors as source material for <m> learning deep model </m> .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Deep Learning Deep Learning </m> [ 2 - 3 ] originates from the research of artificial neural networks and is a new field in machine learning research .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .


class: second -> first, base class: no relation, new model class: same cluster
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .


class: second -> first, base class: no relation, new model class: same cluster
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .


class: second -> first, base class: no relation, new model class: same cluster
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .


class: second -> first, base class: no relation, new model class: same cluster
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .


class: second -> first, base class: no relation, new model class: same cluster
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .


class: second -> first, base class: no relation, new model class: same cluster
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: second -> first, base class: no relation, new model class: same cluster
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: second -> first, base class: no relation, new model class: same cluster
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: second -> first, base class: no relation, new model class: same cluster
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: second -> first, base class: no relation, new model class: same cluster
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: second -> first, base class: no relation, new model class: same cluster
first:
This study implements <m> deep learning based learning systems </m> using the convolutional neural network ( CNN ) method with YOLOv3 .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Learning in deep models </m> using Bayesian methods has generated significant attention recently .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .


class: second -> first, base class: no relation, new model class: same cluster
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .


class: second -> first, base class: no relation, new model class: same cluster
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .


class: second -> first, base class: no relation, new model class: same cluster
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .


class: second -> first, base class: no relation, new model class: same cluster
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .


class: second -> first, base class: no relation, new model class: same cluster
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .


class: second -> first, base class: no relation, new model class: same cluster
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: second -> first, base class: no relation, new model class: same cluster
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: second -> first, base class: no relation, new model class: same cluster
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: second -> first, base class: no relation, new model class: same cluster
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: second -> first, base class: no relation, new model class: same cluster
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: second -> first, base class: no relation, new model class: same cluster
first:
section : Introduction <m> Deep learning models </m> have achieved great success in visual recognition tasks .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .


class: second -> first, base class: no relation, new model class: same cluster
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .


class: second -> first, base class: no relation, new model class: same cluster
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .


class: second -> first, base class: no relation, new model class: same cluster
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .


class: second -> first, base class: no relation, new model class: same cluster
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .


class: second -> first, base class: no relation, new model class: same cluster
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .


class: second -> first, base class: no relation, new model class: same cluster
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: second -> first, base class: no relation, new model class: same cluster
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: second -> first, base class: no relation, new model class: same cluster
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: second -> first, base class: no relation, new model class: same cluster
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: second -> first, base class: no relation, new model class: same cluster
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: second -> first, base class: no relation, new model class: same cluster
first:
Recently , due to the availability of much larger training dataset and the powerful GPU implementation , <m> deep learning based methods </m> achieve great success in many fields , including both high level and low level computer vision problems .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .


class: second -> first, base class: no relation, new model class: same cluster
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .


class: second -> first, base class: no relation, new model class: same cluster
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .


class: second -> first, base class: no relation, new model class: same cluster
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .


class: second -> first, base class: no relation, new model class: same cluster
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .


class: second -> first, base class: no relation, new model class: same cluster
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .


class: second -> first, base class: no relation, new model class: same cluster
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: second -> first, base class: no relation, new model class: same cluster
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: second -> first, base class: no relation, new model class: same cluster
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: second -> first, base class: no relation, new model class: same cluster
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: second -> first, base class: no relation, new model class: same cluster
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: second -> first, base class: no relation, new model class: same cluster
first:
Continuous feature representations are the backbone of many <m> deep learning algorithms </m> , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .


class: second -> first, base class: no relation, new model class: same cluster
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .


class: second -> first, base class: no relation, new model class: same cluster
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .


class: second -> first, base class: no relation, new model class: same cluster
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .


class: second -> first, base class: no relation, new model class: same cluster
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .


class: second -> first, base class: no relation, new model class: same cluster
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .


class: second -> first, base class: no relation, new model class: same cluster
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: second -> first, base class: no relation, new model class: same cluster
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: second -> first, base class: no relation, new model class: same cluster
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: second -> first, base class: no relation, new model class: same cluster
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: second -> first, base class: no relation, new model class: same cluster
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: second -> first, base class: no relation, new model class: same cluster
first:
document : DR - BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference We present a novel <m> deep learning architecture </m> to address the natural language inference ( NLI ) task .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .


class: second -> first, base class: no relation, new model class: same cluster
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .


class: second -> first, base class: no relation, new model class: same cluster
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .


class: second -> first, base class: no relation, new model class: same cluster
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .


class: second -> first, base class: no relation, new model class: same cluster
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .


class: second -> first, base class: no relation, new model class: same cluster
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .


class: second -> first, base class: no relation, new model class: same cluster
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: second -> first, base class: no relation, new model class: same cluster
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: second -> first, base class: no relation, new model class: same cluster
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: second -> first, base class: no relation, new model class: same cluster
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: second -> first, base class: no relation, new model class: same cluster
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: second -> first, base class: no relation, new model class: same cluster
first:
We use the rectifier linear unit as the activation function , which is a common choice in the <m> deep learning literature </m> Lecun:2015 .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .


class: second -> first, base class: no relation, new model class: same cluster
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .


class: second -> first, base class: no relation, new model class: same cluster
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .


class: second -> first, base class: no relation, new model class: same cluster
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .


class: second -> first, base class: no relation, new model class: same cluster
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .


class: second -> first, base class: no relation, new model class: same cluster
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .


class: second -> first, base class: no relation, new model class: same cluster
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: second -> first, base class: no relation, new model class: same cluster
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: second -> first, base class: no relation, new model class: same cluster
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: second -> first, base class: no relation, new model class: same cluster
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: second -> first, base class: no relation, new model class: same cluster
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: second -> first, base class: no relation, new model class: same cluster
first:
The softmax function has been extensively used in CNN architectures before and is therefore well known in the <m> deep learning community </m> .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .


class: second -> first, base class: no relation, new model class: same cluster
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .


class: second -> first, base class: no relation, new model class: same cluster
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .


class: second -> first, base class: no relation, new model class: same cluster
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .


class: second -> first, base class: no relation, new model class: same cluster
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .


class: second -> first, base class: no relation, new model class: same cluster
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .


class: second -> first, base class: no relation, new model class: same cluster
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: second -> first, base class: no relation, new model class: same cluster
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: second -> first, base class: no relation, new model class: same cluster
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: second -> first, base class: no relation, new model class: same cluster
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: second -> first, base class: no relation, new model class: same cluster
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: second -> first, base class: no relation, new model class: same cluster
first:
Graph Convolutional Neural Networks ( GCNNs ) are the most recent exciting advancement in <m> deep learning field </m> and their applications are quickly spreading in multi - cross - domains including bioinformatics , chemoinformatics , social networks , natural language processing and computer vision .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .


class: second -> first, base class: no relation, new model class: same cluster
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .


class: second -> first, base class: no relation, new model class: same cluster
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .


class: second -> first, base class: no relation, new model class: same cluster
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .


class: second -> first, base class: no relation, new model class: same cluster
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .


class: second -> first, base class: no relation, new model class: same cluster
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .


class: second -> first, base class: no relation, new model class: same cluster
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: second -> first, base class: no relation, new model class: same cluster
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: second -> first, base class: no relation, new model class: same cluster
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: second -> first, base class: no relation, new model class: same cluster
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: second -> first, base class: no relation, new model class: same cluster
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: second -> first, base class: no relation, new model class: same cluster
first:
In particular , time - delay networks used in the early days of <m> deep learning research </m> are essentially convolutional networks that model sequential data .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: second -> first, base class: no relation, new model class: no relation
first:
We will compare the following models : Faster R - CNN [ reference ] , R 2 CNN - 1 , R 2 CNN - 2 , <m> R 2 CNN - 3 </m> , R 2 CNN - 4 , and R 2 CNN - 5 .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: second -> first, base class: no relation, new model class: no relation
first:
We will compare the following models : Faster R - CNN [ reference ] , R 2 CNN - 1 , R 2 CNN - 2 , <m> R 2 CNN - 3 </m> , R 2 CNN - 4 , and R 2 CNN - 5 .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We will compare the following models : Faster R - CNN [ reference ] , R 2 CNN - 1 , R 2 CNN - 2 , <m> R 2 CNN - 3 </m> , R 2 CNN - 4 , and R 2 CNN - 5 .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: second -> first, base class: no relation, new model class: no relation
first:
We will compare the following models : Faster R - CNN [ reference ] , R 2 CNN - 1 , R 2 CNN - 2 , <m> R 2 CNN - 3 </m> , R 2 CNN - 4 , and R 2 CNN - 5 .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: second -> first, base class: no relation, new model class: no relation
first:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: second -> first, base class: no relation, new model class: no relation
first:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: second -> first, base class: no relation, new model class: no relation
first:
We use ResNet - 50 [ reference ] to implement the <m> CNN regressor </m> .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: second -> first, base class: no relation, new model class: first -> second
first:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: second -> first, base class: no relation, new model class: first -> second
first:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: second -> first, base class: no relation, new model class: first -> second
first:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: second -> first, base class: no relation, new model class: first -> second
first:
Despite DC Flow ( a hybrid method consists of <m> CNN and post - processing </m> ) performs better than LiteFlowNet , its GPU runtime requires several seconds that makes it formidable in many applications .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: second -> first, base class: no relation, new model class: no relation
first:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .
second:
The <m> Deep CNN </m> applied in this paper is more advantageous when compared to regular MLP .


class: second -> first, base class: no relation, new model class: no relation
first:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .
second:
We compare their performance and find that the best results were achieved using a <m> deep CNN architecture </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .
second:
In addition , the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods ( including <m> deep CNN based method </m> ) under the FERET and FRGC 2.0 evaluation protocols .


class: second -> first, base class: no relation, new model class: no relation
first:
In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the <m> CNN Softmax sub - network </m> .
second:
[ reference ][ reference ] formulated <m> CNN - like deep neural architectures </m> on graphs in the spectral domain , employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator [ reference ] .


class: second -> first, base class: no relation, new model class: same cluster
first:
After that , <m> Conv - LSTM layers </m> with 256 channels are inserted into the single - shot TLL network .
second:
We use the <m> LSTM variant </m> implemented in PyCNN , and optimize using the Adam optimizer .


class: second -> first, base class: no relation, new model class: no relation
first:
Using our techniques , the <m> hardware choice predictor </m> of a hybrid predictor can be completely eliminated from the processor and replaced with our off-line profiling schemes .
second:
In both approaches , the actual <m> classifier / predictor </m> is learned in a separate step using the feature representation learned by autoencoder ( s ) .


class: second -> first, base class: no relation, new model class: no relation
first:
We apply domainadversarial learning , as we consider a <m> descriptor predictor </m> trained with a Siamese - like loss instead of the label predictor trained with a classification loss .
second:
subsection : Locality Sensitive Hashing ( LSH ) An alternative approach to BASS and DISCO is to use well - established <m> feature generation methods </m> that are agnostic about the type of input they receive .


class: second -> first, base class: no relation, new model class: no relation
first:
We apply domainadversarial learning , as we consider a <m> descriptor predictor </m> trained with a Siamese - like loss instead of the label predictor trained with a classification loss .
second:
subsection : Unimodal Feature Extraction For a fair comparison with the state - of - the - art method , conversational memory networks ( CMN ) , we follow identical <m> feature extraction procedures </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We apply domainadversarial learning , as we consider a <m> descriptor predictor </m> trained with a Siamese - like loss instead of the label predictor trained with a classification loss .
second:
When used as <m> generic feature extractors </m> , feature aggregation techniques designed for hand - crafted features can also work with CNN embeddings and achieve competitive performance .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper presents a convolutional time-delay deep neural network structure ( CT-DNN ) for <m> speaker feature learning </m> .
second:
subsection : Locality Sensitive Hashing ( LSH ) An alternative approach to BASS and DISCO is to use well - established <m> feature generation methods </m> that are agnostic about the type of input they receive .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper presents a convolutional time-delay deep neural network structure ( CT-DNN ) for <m> speaker feature learning </m> .
second:
subsection : Unimodal Feature Extraction For a fair comparison with the state - of - the - art method , conversational memory networks ( CMN ) , we follow identical <m> feature extraction procedures </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper presents a convolutional time-delay deep neural network structure ( CT-DNN ) for <m> speaker feature learning </m> .
second:
When used as <m> generic feature extractors </m> , feature aggregation techniques designed for hand - crafted features can also work with CNN embeddings and achieve competitive performance .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Different from the state of the art methods where features are extracted by a <m> sole feature extraction network </m> for both tasks , the proposed HybridNet improves the features extraction by separating the relevant features for one task from those which are relevant for both .
second:
When used as <m> generic feature extractors </m> , feature aggregation techniques designed for hand - crafted features can also work with CNN embeddings and achieve competitive performance .


class: second -> first, base class: no relation, new model class: no relation
first:
In our Adversarial Spatial Transformer Network , we focus on <m> feature map rotations </m> .
second:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .


class: second -> first, base class: no relation, new model class: no relation
first:
For video face recognition , most of these methods either use <m> pairwise frame feature similarity computation </m> [ reference ][ reference ] or naive ( average / max ) frame feature pooling [ reference ][ reference ][ reference ] .
second:
We pursue two separate analyses : an <m> analysis ofindependent variables </m> , in which we tie each study to a particular guiding learning theory in an attempt to determine which guiding theory has had the most predictive success ; and an analysis of dependent variables , which enables us to determine which measurement techniques have been most sensitive to the learning benefits of AV technology .


class: second -> first, base class: no relation, new model class: no relation
first:
The feature extrapolating layer extrapolates 4 scales with equal intervals between every two input scales , so the final <m> conv feature pyramid </m> has 21 scales for KITTI and 16 scales for PASCAL .
second:
When used as <m> generic feature extractors </m> , feature aggregation techniques designed for hand - crafted features can also work with CNN embeddings and achieve competitive performance .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Locality Sensitive Hashing ( LSH ) An alternative approach to BASS and DISCO is to use well - established <m> feature generation methods </m> that are agnostic about the type of input they receive .
second:
The authors show that if the trail arrival process is modeled by three <m> independent random variables </m> and the decay factor is modeled as a random process , the observed distribution functions for the amplitude and duration of underdense meteor trails can be more accurately synthesized .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Locality Sensitive Hashing ( LSH ) An alternative approach to BASS and DISCO is to use well - established <m> feature generation methods </m> that are agnostic about the type of input they receive .
second:
The empirical results show that both the confirmation data of the <m> independent variable system </m> and the confirmation data of the dependent variable have the satisfying reliability .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Locality Sensitive Hashing ( LSH ) An alternative approach to BASS and DISCO is to use well - established <m> feature generation methods </m> that are agnostic about the type of input they receive .
second:
Up to date in the area of feature extraction , image feature representation method based on the spatial gradient is still lacking in efficiency especially for the <m> covariate case </m> like carrying bag and wearing a coat .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Locality Sensitive Hashing ( LSH ) An alternative approach to BASS and DISCO is to use well - established <m> feature generation methods </m> that are agnostic about the type of input they receive .
second:
I should like to express my gratitude to Keith Rapley ( now Innovation Manager ) at British Airways for arranging sponsorship of this work , to Mark Raskino and Neil Morrison for early stimulation of ideas concerning neural networks ; and to Dr Maurice Barr , Louis Busuttil , Paul Summerbell , Dr Himadri Chatterjee and Rupert Blackley for making possible the forecasting and <m> attribute </m> work .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Multi - scale feature maps The main goal of constructing <m> multi - scale feature maps </m> is to add more context without increasing the computational cost .
second:
subsection : Unimodal Feature Extraction For a fair comparison with the state - of - the - art method , conversational memory networks ( CMN ) , we follow identical <m> feature extraction procedures </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Multi - scale feature maps The main goal of constructing <m> multi - scale feature maps </m> is to add more context without increasing the computational cost .
second:
When used as <m> generic feature extractors </m> , feature aggregation techniques designed for hand - crafted features can also work with CNN embeddings and achieve competitive performance .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Unimodal Feature Extraction For a fair comparison with the state - of - the - art method , conversational memory networks ( CMN ) , we follow identical <m> feature extraction procedures </m> .
second:
The authors show that if the trail arrival process is modeled by three <m> independent random variables </m> and the decay factor is modeled as a random process , the observed distribution functions for the amplitude and duration of underdense meteor trails can be more accurately synthesized .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Unimodal Feature Extraction For a fair comparison with the state - of - the - art method , conversational memory networks ( CMN ) , we follow identical <m> feature extraction procedures </m> .
second:
The empirical results show that both the confirmation data of the <m> independent variable system </m> and the confirmation data of the dependent variable have the satisfying reliability .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Unimodal Feature Extraction For a fair comparison with the state - of - the - art method , conversational memory networks ( CMN ) , we follow identical <m> feature extraction procedures </m> .
second:
Up to date in the area of feature extraction , image feature representation method based on the spatial gradient is still lacking in efficiency especially for the <m> covariate case </m> like carrying bag and wearing a coat .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Unimodal Feature Extraction For a fair comparison with the state - of - the - art method , conversational memory networks ( CMN ) , we follow identical <m> feature extraction procedures </m> .
second:
I should like to express my gratitude to Keith Rapley ( now Innovation Manager ) at British Airways for arranging sponsorship of this work , to Mark Raskino and Neil Morrison for early stimulation of ideas concerning neural networks ; and to Dr Maurice Barr , Louis Busuttil , Paul Summerbell , Dr Himadri Chatterjee and Rupert Blackley for making possible the forecasting and <m> attribute </m> work .


class: second -> first, base class: no relation, new model class: no relation
first:
When used as <m> generic feature extractors </m> , feature aggregation techniques designed for hand - crafted features can also work with CNN embeddings and achieve competitive performance .
second:
This would amount to an additional <m> feature level </m> GAN loss ( see Figure [ reference ] orange portion ) : Taken together , these loss functions form our complete objective : This ultimately corresponds to solving for a target model according to the optimization problem We have introduced a method for unsupervised adaptation which generalizes adversarial objectives to be viewed as operating at the pixel or feature level .


class: second -> first, base class: no relation, new model class: no relation
first:
When used as <m> generic feature extractors </m> , feature aggregation techniques designed for hand - crafted features can also work with CNN embeddings and achieve competitive performance .
second:
The authors show that if the trail arrival process is modeled by three <m> independent random variables </m> and the decay factor is modeled as a random process , the observed distribution functions for the amplitude and duration of underdense meteor trails can be more accurately synthesized .


class: second -> first, base class: no relation, new model class: no relation
first:
When used as <m> generic feature extractors </m> , feature aggregation techniques designed for hand - crafted features can also work with CNN embeddings and achieve competitive performance .
second:
The empirical results show that both the confirmation data of the <m> independent variable system </m> and the confirmation data of the dependent variable have the satisfying reliability .


class: second -> first, base class: no relation, new model class: no relation
first:
When used as <m> generic feature extractors </m> , feature aggregation techniques designed for hand - crafted features can also work with CNN embeddings and achieve competitive performance .
second:
Up to date in the area of feature extraction , image feature representation method based on the spatial gradient is still lacking in efficiency especially for the <m> covariate case </m> like carrying bag and wearing a coat .


class: second -> first, base class: no relation, new model class: no relation
first:
When used as <m> generic feature extractors </m> , feature aggregation techniques designed for hand - crafted features can also work with CNN embeddings and achieve competitive performance .
second:
I should like to express my gratitude to Keith Rapley ( now Innovation Manager ) at British Airways for arranging sponsorship of this work , to Mark Raskino and Neil Morrison for early stimulation of ideas concerning neural networks ; and to Dr Maurice Barr , Louis Busuttil , Paul Summerbell , Dr Himadri Chatterjee and Rupert Blackley for making possible the forecasting and <m> attribute </m> work .


class: second -> first, base class: no relation, new model class: no relation
first:
Domestic learners establish financial risk forecasting system taking ST ( special treatment for abnormal situation including finance and others ) companies in domestic stock market as studying objects , adopting <m> single variable analysis </m> , Multiple Linear Regression Discriminant Model and multielement logical aggressive model [ 7 - 13 ] . But the current research has the following problems :  A serial of mathematical statistics is taken before model establishing , but this affects the effectiveness .
second:
We pursue two separate analyses : an <m> analysis ofindependent variables </m> , in which we tie each study to a particular guiding learning theory in an attempt to determine which guiding theory has had the most predictive success ; and an analysis of dependent variables , which enables us to determine which measurement techniques have been most sensitive to the learning benefits of AV technology .


class: second -> first, base class: same cluster, new model class: same cluster
first:
While we ensure the causal variables of the previous study are considered for inclusion in the models developed , this paper focuses on the ability to accurately predict conflict over individual <m> independent variable analysis </m> .
second:
We pursue two separate analyses : an <m> analysis ofindependent variables </m> , in which we tie each study to a particular guiding learning theory in an attempt to determine which guiding theory has had the most predictive success ; and an analysis of dependent variables , which enables us to determine which measurement techniques have been most sensitive to the learning benefits of AV technology .


class: second -> first, base class: no relation, new model class: no relation
first:
We pursue two separate analyses : an <m> analysis ofindependent variables </m> , in which we tie each study to a particular guiding learning theory in an attempt to determine which guiding theory has had the most predictive success ; and an analysis of dependent variables , which enables us to determine which measurement techniques have been most sensitive to the learning benefits of AV technology .
second:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Independent variable selection </m> Generally in a classification problem , the variable that is to be predicted is known as the dependent variable ( transportation mode in our case ) because its value depends upon , or is decided by , the values of all the other attributes .
second:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .


class: second -> first, base class: no relation, new model class: no relation
first:
Objectives : This article aims to produce a <m> covariate adjustment method </m> that allows for automatic variable selection , so that practitioners need not commit to any specific set of covariates prior to seeing the data .
second:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .


class: second -> first, base class: no relation, new model class: no relation
first:
We also describe the benefits of combining the above methods , present methodology for applying the embarrassingly parallel procedures when the number of machines is dynamic or unknown at inference time , illustrate how these methods can be applied for spatiotemporal analysis and in <m> covariate dependent models </m> , show ways tooptimize these methods by incorporating test-functions of interest , and demonstrate how these methods can be implemented in probabilistic programming frameworksfor automatic deployment .
second:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .


class: second -> first, base class: no relation, new model class: no relation
first:
The authors conceptualized <m> covariate procedures </m> as finite mixture models that researchers may use in either single or multiple imputation .
second:
I should like to express my gratitude to Keith Rapley ( now Innovation Manager ) at British Airways for arranging sponsorship of this work , to Mark Raskino and Neil Morrison for early stimulation of ideas concerning neural networks ; and to Dr Maurice Barr , Louis Busuttil , Paul Summerbell , Dr Himadri Chatterjee and Rupert Blackley for making possible the forecasting and <m> attribute </m> work .


class: second -> first, base class: no relation, new model class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .


class: second -> first, base class: no relation, new model class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .


class: second -> first, base class: no relation, new model class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .


class: second -> first, base class: no relation, new model class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .


class: second -> first, base class: no relation, new model class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .


class: second -> first, base class: no relation, new model class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .


class: second -> first, base class: no relation, new model class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: second -> first, base class: no relation, new model class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .


class: second -> first, base class: no relation, new model class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .


class: second -> first, base class: no relation, new model class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .


class: second -> first, base class: no relation, new model class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .


class: second -> first, base class: no relation, new model class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .


class: second -> first, base class: no relation, new model class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .


class: second -> first, base class: no relation, new model class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: second -> first, base class: no relation, new model class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .


class: second -> first, base class: no relation, new model class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .


class: second -> first, base class: no relation, new model class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .


class: second -> first, base class: no relation, new model class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .


class: second -> first, base class: no relation, new model class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .


class: second -> first, base class: no relation, new model class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .


class: second -> first, base class: no relation, new model class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: second -> first, base class: no relation, new model class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .


class: second -> first, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .


class: second -> first, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .


class: second -> first, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .


class: second -> first, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .


class: second -> first, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .


class: second -> first, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .


class: second -> first, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .


class: second -> first, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .


class: second -> first, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: second -> first, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: second -> first, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .


class: second -> first, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .


class: second -> first, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .


class: second -> first, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .


class: second -> first, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .


class: second -> first, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .


class: second -> first, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .


class: second -> first, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .


class: second -> first, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: second -> first, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: second -> first, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: no relation
first:
We evaluate the proposed learning rule by training <m> deep spiking multi-layer perceptron ( MLP ) </m> and spiking convolutional neural network ( CNN ) on the UCI machine learning and MNIST handwritten digit datasets .
second:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .


class: second -> first, base class: no relation, new model class: no relation
first:
We evaluate the proposed learning rule by training <m> deep spiking multi-layer perceptron ( MLP ) </m> and spiking convolutional neural network ( CNN ) on the UCI machine learning and MNIST handwritten digit datasets .
second:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .


class: second -> first, base class: no relation, new model class: no relation
first:
We evaluate the proposed learning rule by training <m> deep spiking multi-layer perceptron ( MLP ) </m> and spiking convolutional neural network ( CNN ) on the UCI machine learning and MNIST handwritten digit datasets .
second:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .


class: second -> first, base class: no relation, new model class: no relation
first:
We evaluate the proposed learning rule by training <m> deep spiking multi-layer perceptron ( MLP ) </m> and spiking convolutional neural network ( CNN ) on the UCI machine learning and MNIST handwritten digit datasets .
second:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .


class: second -> first, base class: no relation, new model class: no relation
first:
We evaluate the proposed learning rule by training <m> deep spiking multi-layer perceptron ( MLP ) </m> and spiking convolutional neural network ( CNN ) on the UCI machine learning and MNIST handwritten digit datasets .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: second -> first, base class: no relation, new model class: no relation
first:
We evaluate the proposed learning rule by training <m> deep spiking multi-layer perceptron ( MLP ) </m> and spiking convolutional neural network ( CNN ) on the UCI machine learning and MNIST handwritten digit datasets .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: second -> first, base class: no relation, new model class: no relation
first:
We evaluate the proposed learning rule by training <m> deep spiking multi-layer perceptron ( MLP ) </m> and spiking convolutional neural network ( CNN ) on the UCI machine learning and MNIST handwritten digit datasets .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: second -> first, base class: no relation, new model class: no relation
first:
We evaluate the proposed learning rule by training <m> deep spiking multi-layer perceptron ( MLP ) </m> and spiking convolutional neural network ( CNN ) on the UCI machine learning and MNIST handwritten digit datasets .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: second -> first, base class: no relation, new model class: no relation
first:
We evaluate the proposed learning rule by training <m> deep spiking multi-layer perceptron ( MLP ) </m> and spiking convolutional neural network ( CNN ) on the UCI machine learning and MNIST handwritten digit datasets .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: second -> first, base class: no relation, new model class: no relation
first:
We evaluate the proposed learning rule by training <m> deep spiking multi-layer perceptron ( MLP ) </m> and spiking convolutional neural network ( CNN ) on the UCI machine learning and MNIST handwritten digit datasets .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: second -> first, base class: no relation, new model class: no relation
first:
We evaluate the proposed learning rule by training <m> deep spiking multi-layer perceptron ( MLP ) </m> and spiking convolutional neural network ( CNN ) on the UCI machine learning and MNIST handwritten digit datasets .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We evaluate the proposed learning rule by training <m> deep spiking multi-layer perceptron ( MLP ) </m> and spiking convolutional neural network ( CNN ) on the UCI machine learning and MNIST handwritten digit datasets .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: second -> first, base class: no relation, new model class: no relation
first:
We evaluate the proposed learning rule by training <m> deep spiking multi-layer perceptron ( MLP ) </m> and spiking convolutional neural network ( CNN ) on the UCI machine learning and MNIST handwritten digit datasets .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: second -> first, base class: no relation, new model class: no relation
first:
We evaluate the proposed learning rule by training <m> deep spiking multi-layer perceptron ( MLP ) </m> and spiking convolutional neural network ( CNN ) on the UCI machine learning and MNIST handwritten digit datasets .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: second -> first, base class: no relation, new model class: no relation
first:
We evaluate the proposed learning rule by training <m> deep spiking multi-layer perceptron ( MLP ) </m> and spiking convolutional neural network ( CNN ) on the UCI machine learning and MNIST handwritten digit datasets .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: second -> first, base class: no relation, new model class: no relation
first:
We evaluate the proposed learning rule by training <m> deep spiking multi-layer perceptron ( MLP ) </m> and spiking convolutional neural network ( CNN ) on the UCI machine learning and MNIST handwritten digit datasets .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: second -> first, base class: no relation, new model class: no relation
first:
We evaluate the proposed learning rule by training <m> deep spiking multi-layer perceptron ( MLP ) </m> and spiking convolutional neural network ( CNN ) on the UCI machine learning and MNIST handwritten digit datasets .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: second -> first, base class: no relation, new model class: same cluster
first:
They are averaging , Karmarker 's linear programming algorithm , <m> MLP combiner </m> and fuzzy neural network .
second:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .


class: second -> first, base class: no relation, new model class: same cluster
first:
They are averaging , Karmarker 's linear programming algorithm , <m> MLP combiner </m> and fuzzy neural network .
second:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .


class: second -> first, base class: no relation, new model class: same cluster
first:
They are averaging , Karmarker 's linear programming algorithm , <m> MLP combiner </m> and fuzzy neural network .
second:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .


class: second -> first, base class: no relation, new model class: first -> second
first:
They are averaging , Karmarker 's linear programming algorithm , <m> MLP combiner </m> and fuzzy neural network .
second:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .


class: second -> first, base class: no relation, new model class: same cluster
first:
They are averaging , Karmarker 's linear programming algorithm , <m> MLP combiner </m> and fuzzy neural network .
second:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .


class: second -> first, base class: first -> second, new model class: first -> second
first:
They are averaging , Karmarker 's linear programming algorithm , <m> MLP combiner </m> and fuzzy neural network .
second:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .


class: second -> first, base class: no relation, new model class: first -> second
first:
They are averaging , Karmarker 's linear programming algorithm , <m> MLP combiner </m> and fuzzy neural network .
second:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .


class: second -> first, base class: no relation, new model class: no relation
first:
They are averaging , Karmarker 's linear programming algorithm , <m> MLP combiner </m> and fuzzy neural network .
second:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .


class: second -> first, base class: no relation, new model class: no relation
first:
They are averaging , Karmarker 's linear programming algorithm , <m> MLP combiner </m> and fuzzy neural network .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: second -> first, base class: no relation, new model class: no relation
first:
They are averaging , Karmarker 's linear programming algorithm , <m> MLP combiner </m> and fuzzy neural network .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: second -> first, base class: no relation, new model class: no relation
first:
They are averaging , Karmarker 's linear programming algorithm , <m> MLP combiner </m> and fuzzy neural network .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: second -> first, base class: no relation, new model class: no relation
first:
They are averaging , Karmarker 's linear programming algorithm , <m> MLP combiner </m> and fuzzy neural network .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: second -> first, base class: no relation, new model class: no relation
first:
They are averaging , Karmarker 's linear programming algorithm , <m> MLP combiner </m> and fuzzy neural network .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: second -> first, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .


class: second -> first, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .


class: second -> first, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .


class: second -> first, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .


class: second -> first, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .


class: second -> first, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .


class: second -> first, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .


class: second -> first, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .


class: second -> first, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .


class: second -> first, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: second -> first, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: first -> second
first:
Further experiment shows that using <m> MLP merging </m> and decision , the problem of variation of thresholds can be solved to some extent , and the robustness of the system can be improved effectively .
second:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .


class: second -> first, base class: no relation, new model class: first -> second
first:
Further experiment shows that using <m> MLP merging </m> and decision , the problem of variation of thresholds can be solved to some extent , and the robustness of the system can be improved effectively .
second:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .


class: second -> first, base class: no relation, new model class: first -> second
first:
Further experiment shows that using <m> MLP merging </m> and decision , the problem of variation of thresholds can be solved to some extent , and the robustness of the system can be improved effectively .
second:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .


class: second -> first, base class: no relation, new model class: no relation
first:
Further experiment shows that using <m> MLP merging </m> and decision , the problem of variation of thresholds can be solved to some extent , and the robustness of the system can be improved effectively .
second:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .


class: second -> first, base class: no relation, new model class: no relation
first:
Further experiment shows that using <m> MLP merging </m> and decision , the problem of variation of thresholds can be solved to some extent , and the robustness of the system can be improved effectively .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: second -> first, base class: no relation, new model class: no relation
first:
Further experiment shows that using <m> MLP merging </m> and decision , the problem of variation of thresholds can be solved to some extent , and the robustness of the system can be improved effectively .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: second -> first, base class: no relation, new model class: no relation
first:
Further experiment shows that using <m> MLP merging </m> and decision , the problem of variation of thresholds can be solved to some extent , and the robustness of the system can be improved effectively .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: second -> first, base class: no relation, new model class: no relation
first:
Further experiment shows that using <m> MLP merging </m> and decision , the problem of variation of thresholds can be solved to some extent , and the robustness of the system can be improved effectively .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: second -> first, base class: no relation, new model class: no relation
first:
Further experiment shows that using <m> MLP merging </m> and decision , the problem of variation of thresholds can be solved to some extent , and the robustness of the system can be improved effectively .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: second -> first, base class: no relation, new model class: no relation
first:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .
second:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .


class: second -> first, base class: no relation, new model class: no relation
first:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .
second:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .


class: second -> first, base class: no relation, new model class: no relation
first:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .
second:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .


class: second -> first, base class: no relation, new model class: no relation
first:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .
second:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .


class: second -> first, base class: no relation, new model class: no relation
first:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: second -> first, base class: no relation, new model class: no relation
first:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: second -> first, base class: no relation, new model class: no relation
first:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: second -> first, base class: no relation, new model class: no relation
first:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: second -> first, base class: no relation, new model class: no relation
first:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: second -> first, base class: no relation, new model class: no relation
first:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: second -> first, base class: no relation, new model class: no relation
first:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: second -> first, base class: no relation, new model class: no relation
first:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: second -> first, base class: no relation, new model class: no relation
first:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: second -> first, base class: no relation, new model class: no relation
first:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: second -> first, base class: no relation, new model class: no relation
first:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: second -> first, base class: no relation, new model class: no relation
first:
A second project has involved the installation of 3 Cyclops Coastal Imaging Stations in Durban and Cape Town which use an <m> RGB based MLP neural network </m> to extract waterlines on a short timescale .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .


class: second -> first, base class: no relation, new model class: no relation
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .


class: second -> first, base class: no relation, new model class: no relation
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .


class: second -> first, base class: no relation, new model class: no relation
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .


class: second -> first, base class: no relation, new model class: no relation
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .


class: second -> first, base class: no relation, new model class: no relation
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .


class: second -> first, base class: no relation, new model class: no relation
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: second -> first, base class: no relation, new model class: no relation
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .


class: second -> first, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .


class: second -> first, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .


class: second -> first, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .


class: second -> first, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .


class: second -> first, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .


class: second -> first, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .


class: second -> first, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .


class: second -> first, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .


class: second -> first, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: second -> first, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: second -> first, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: no relation
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .


class: second -> first, base class: no relation, new model class: no relation
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .


class: second -> first, base class: no relation, new model class: no relation
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .


class: second -> first, base class: no relation, new model class: no relation
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .


class: second -> first, base class: no relation, new model class: no relation
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .


class: second -> first, base class: no relation, new model class: no relation
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .


class: second -> first, base class: no relation, new model class: no relation
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: second -> first, base class: no relation, new model class: no relation
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :
second:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .


class: second -> first, base class: no relation, new model class: no relation
first:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :
second:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .


class: second -> first, base class: no relation, new model class: no relation
first:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :
second:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .


class: second -> first, base class: no relation, new model class: no relation
first:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: second -> first, base class: no relation, new model class: no relation
first:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: second -> first, base class: no relation, new model class: no relation
first:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: second -> first, base class: no relation, new model class: no relation
first:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: second -> first, base class: no relation, new model class: no relation
first:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: second -> first, base class: no relation, new model class: no relation
first:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: second -> first, base class: no relation, new model class: no relation
first:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: second -> first, base class: no relation, new model class: no relation
first:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: second -> first, base class: no relation, new model class: no relation
first:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: second -> first, base class: no relation, new model class: no relation
first:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: second -> first, base class: no relation, new model class: no relation
first:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: second -> first, base class: no relation, new model class: no relation
first:
In EXAM , we use a <m> non - linear MLP </m> to be the aggregation layer , and it will generalize FastText to a non - linear setting which might be more expressive than the original one . section :
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: second -> first, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .


class: second -> first, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .


class: second -> first, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .


class: second -> first, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .


class: second -> first, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .


class: second -> first, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .


class: second -> first, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .


class: second -> first, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .


class: second -> first, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .


class: second -> first, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: second -> first, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: second -> first, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .


class: second -> first, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .


class: second -> first, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .


class: second -> first, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .


class: second -> first, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .


class: second -> first, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .


class: second -> first, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .


class: second -> first, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .


class: second -> first, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .


class: second -> first, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: second -> first, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: second -> first, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .


class: second -> first, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .


class: second -> first, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .


class: second -> first, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .


class: second -> first, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .


class: second -> first, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .


class: second -> first, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .


class: second -> first, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .


class: second -> first, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .


class: second -> first, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: second -> first, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: second -> first, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: no relation
first:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: second -> first, base class: no relation, new model class: no relation
first:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: second -> first, base class: no relation, new model class: no relation
first:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: second -> first, base class: no relation, new model class: no relation
first:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: second -> first, base class: no relation, new model class: no relation
first:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: second -> first, base class: no relation, new model class: no relation
first:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: second -> first, base class: no relation, new model class: no relation
first:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: second -> first, base class: no relation, new model class: no relation
first:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: second -> first, base class: no relation, new model class: no relation
first:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: second -> first, base class: no relation, new model class: no relation
first:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: second -> first, base class: no relation, new model class: no relation
first:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: second -> first, base class: no relation, new model class: no relation
first:
paragraph : Thresholding Supervised Results For both the <m> multihead MLP </m> and the single linear layer instantiating of , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: second -> first, base class: no relation, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
We empirically showed that DLDL produces robust and competitive performances than traditional <m> classification or regression deep models </m> on several popular visual recognition tasks .


class: second -> first, base class: no relation, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
We empirically showed that DLDL produces robust and competitive performances than traditional <m> classification or regression deep models </m> on several popular visual recognition tasks .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
We empirically showed that DLDL produces robust and competitive performances than traditional <m> classification or regression deep models </m> on several popular visual recognition tasks .


class: second -> first, base class: no relation, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
We empirically showed that DLDL produces robust and competitive performances than traditional <m> classification or regression deep models </m> on several popular visual recognition tasks .


class: second -> first, base class: no relation, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
We empirically showed that DLDL produces robust and competitive performances than traditional <m> classification or regression deep models </m> on several popular visual recognition tasks .


class: second -> first, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
We empirically showed that DLDL produces robust and competitive performances than traditional <m> classification or regression deep models </m> on several popular visual recognition tasks .


class: second -> first, base class: no relation, new model class: first -> second
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
We empirically showed that DLDL produces robust and competitive performances than traditional <m> classification or regression deep models </m> on several popular visual recognition tasks .


class: second -> first, base class: no relation, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
We empirically showed that DLDL produces robust and competitive performances than traditional <m> classification or regression deep models </m> on several popular visual recognition tasks .


class: second -> first, base class: first -> second, new model class: no relation
first:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .
second:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .


class: second -> first, base class: first -> second, new model class: no relation
first:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .
second:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .


class: second -> first, base class: first -> second, new model class: no relation
first:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: second -> first, base class: same cluster, new model class: same cluster
first:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: second -> first, base class: no relation, new model class: no relation
first:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: first -> second, new model class: no relation
first:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: first -> second, new model class: no relation
first:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: first -> second, new model class: no relation
first:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: first -> second, new model class: no relation
first:
First , a speaker - adaptive Gaussian Mixture Model Hidden Markov Model ( GMM - HMM ) is trained on the speech corpus to obtain a context - dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the <m> neural network training </m> .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
We empirically showed that DLDL produces robust and competitive performances than traditional <m> classification or regression deep models </m> on several popular visual recognition tasks .


class: second -> first, base class: no relation, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
We empirically showed that DLDL produces robust and competitive performances than traditional <m> classification or regression deep models </m> on several popular visual recognition tasks .


class: second -> first, base class: no relation, new model class: no relation
first:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: second -> first, base class: no relation, new model class: no relation
first:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: second -> first, base class: no relation, new model class: no relation
first:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: first -> second, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
We empirically showed that DLDL produces robust and competitive performances than traditional <m> classification or regression deep models </m> on several popular visual recognition tasks .


class: second -> first, base class: no relation, new model class: same cluster
first:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: second -> first, base class: no relation, new model class: no relation
first:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: no relation
first:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: second -> first, base class: no relation, new model class: no relation
first:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: same cluster
first:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: same cluster
first:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: second -> first, base class: no relation, new model class: no relation
first:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: same cluster
first:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: second -> first, base class: no relation, new model class: no relation
first:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: same cluster
first:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: second -> first, base class: no relation, new model class: no relation
first:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
A selection of supernet models iii are reviewed and compared with a standard <m> fully connected neural network structure </m> .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: same cluster
first:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: second -> first, base class: no relation, new model class: no relation
first:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: same cluster
first:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: second -> first, base class: no relation, new model class: no relation
first:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: no relation
first:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .
second:
We empirically showed that DLDL produces robust and competitive performances than traditional <m> classification or regression deep models </m> on several popular visual recognition tasks .


class: second -> first, base class: no relation, new model class: no relation
first:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: same cluster
first:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: no relation
first:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: second -> first, base class: no relation, new model class: no relation
first:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: same cluster
first:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: second -> first, base class: no relation, new model class: no relation
first:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: second -> first, base class: no relation, new model class: no relation
first:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: no relation
first:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .
second:
We empirically showed that DLDL produces robust and competitive performances than traditional <m> classification or regression deep models </m> on several popular visual recognition tasks .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .
second:
We empirically showed that DLDL produces robust and competitive performances than traditional <m> classification or regression deep models </m> on several popular visual recognition tasks .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: no relation
first:
That is , their output is computed as where is the total input to the neuron ( equivalently , the output of the <m> neuron ’s linear filter </m> added to the bias ) .
second:
In particular , “ transfer ” means that DNN weights trained on large - scale data can be used in other tasks by two <m> light - weight neuron operations </m> : Scaling and Shifting ( SS ) , i.e. .


class: second -> first, base class: no relation, new model class: no relation
first:
This nonlinearity has several advantages over traditional <m> saturating neuron models </m> , including a significant reduction in the training time required to reach a given error rate .
second:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .


class: second -> first, base class: no relation, new model class: first -> second
first:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: second -> first, base class: no relation, new model class: no relation
first:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
This operation ( usually including some output nonlinearity ) is referred to as a " <m> neuron </m> " .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: no relation
first:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .
second:
We empirically showed that DLDL produces robust and competitive performances than traditional <m> classification or regression deep models </m> on several popular visual recognition tasks .


class: second -> first, base class: no relation, new model class: no relation
first:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: second -> first, base class: no relation, new model class: no relation
first:
We view each weight vector within a fully connected ( FC ) layer in a <m> convolutional neuron network </m> ( CNN ) as a projection basis .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: second -> first, base class: no relation, new model class: no relation
first:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .
second:
We empirically showed that DLDL produces robust and competitive performances than traditional <m> classification or regression deep models </m> on several popular visual recognition tasks .


class: second -> first, base class: no relation, new model class: no relation
first:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .
second:
We empirically showed that DLDL produces robust and competitive performances than traditional <m> classification or regression deep models </m> on several popular visual recognition tasks .


class: second -> first, base class: same cluster, new model class: no relation
first:
The evaluation is conducted through a <m> user empirical study </m> based on user observation and in-depth interviews .
second:
In the core of our paper , we survey colormap generation techniques , including the latest advances in the field by grouping these techniques into four classes : procedural methods , <m> user-study based methods </m> , rule-based methods , and data-driven methods ; we also include a section on methods that are beyond pure data comprehension purposes .


class: second -> first, base class: no relation, new model class: no relation
first:
The evaluation is conducted through a <m> user empirical study </m> based on user observation and in-depth interviews .
second:
The laboratory setting allows evaluators to create not only the environment , but also the scenario in which a <m> user study of system </m> is conducted .


class: second -> first, base class: no relation, new model class: no relation
first:
The evaluation is conducted through a <m> user empirical study </m> based on user observation and in-depth interviews .
second:
In this paper , we present the results of a <m> user study of a technique </m> used to pair two devices ( such as two cell phones ) which have good quality output interfaces in the form of a display , speaker , and/or vibration .


class: second -> first, base class: no relation, new model class: no relation
first:
The evaluation is conducted through a <m> user empirical study </m> based on user observation and in-depth interviews .
second:
System evaluation and <m> user study research </m> thus can play a significant role in infectious disease information research .


class: second -> first, base class: no relation, new model class: no relation
first:
The evaluation is conducted through a <m> user empirical study </m> based on user observation and in-depth interviews .
second:
To evaluate the green navigation service efficiency , we conducted a <m> user subject study </m> consisting of 22 users driving different vehicles over the course of several months in Urbana-Champaign , IL .


class: second -> first, base class: no relation, new model class: no relation
first:
The evaluation is conducted through a <m> user empirical study </m> based on user observation and in-depth interviews .
second:
For realism , we conduct a <m> user study </m> using pairwise comparison .


class: second -> first, base class: no relation, new model class: no relation
first:
Case studies include a survey of an Iron Age fort , a <m> rapid survey of exposed segments </m> of an intertidal wreck , both commissioned for heritage management purposes and a community survey of a 17th century gravestone undertaken by children under the age of 16 .
second:
In the <m> survey and exploration phase </m> , a number of studies have been proposed to carry out mineral potential mapping by integrating geochemical and geophysical explorations , geological maps , fault lines , and topographic information .


class: second -> first, base class: no relation, new model class: no relation
first:
It also puts forward the construction of <m> regular readers survey system </m> , multi-level journals recommended reading models and diversity journals recommended reading channels .
second:
In the <m> survey and exploration phase </m> , a number of studies have been proposed to carry out mineral potential mapping by integrating geochemical and geophysical explorations , geological maps , fault lines , and topographic information .


class: second -> first, base class: no relation, new model class: no relation
first:
Results from a <m> social survey of reaction </m> to aircraft noise are used to investigate the effectiveness of various measures of noise exposure .
second:
In the <m> survey and exploration phase </m> , a number of studies have been proposed to carry out mineral potential mapping by integrating geochemical and geophysical explorations , geological maps , fault lines , and topographic information .


class: second -> first, base class: no relation, new model class: no relation
first:
The thirteen possible interviewees were sorted into different types of users according to a ) their opinions of the automated scorer ( taken from <m> post-assignment survey </m> ) , b ) actual essay scores assigned by the instructor , and c ) average number of drafts they completed per response section for both cases .
second:
In the <m> survey and exploration phase </m> , a number of studies have been proposed to carry out mineral potential mapping by integrating geochemical and geophysical explorations , geological maps , fault lines , and topographic information .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> baseline behavioural survey </m> identified an increasing general awareness of open access , but a lower awareness of institutional and subject repositories .
second:
In the <m> survey and exploration phase </m> , a number of studies have been proposed to carry out mineral potential mapping by integrating geochemical and geophysical explorations , geological maps , fault lines , and topographic information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The experiments show promising results over other <m> feature based learning approaches </m> .
second:
Specifically , it employs an adversarial learning guided multi-label attention module to enhance the <m> feature learning part </m> which can learn discriminative feature representations and keep the cross-modal invariability .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The experiments show promising results over other <m> feature based learning approaches </m> .
second:
<m> Feature learning methods </m> based on the Skip - gram architecture have been originally developed in the context of natural language .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The experiments show promising results over other <m> feature based learning approaches </m> .
second:
We find that this simple <m> feature learning algorithm </m> is surprisingly successful when applied to visual object recognition .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The experiments show promising results over other <m> feature based learning approaches </m> .
second:
We formulate such a dual path architecture as follows : where and denote the extracted information at - th step from individual path , is a <m> feature learning function </m> as .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The experiments show promising results over other <m> feature based learning approaches </m> .
second:
In this paper , we address the <m> feature learning problem </m> in DL by presenting a new algorithm , deeply - supervised nets ( DSN ) , which enforces direct and early supervision for both the hidden layers and the output layer .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The experiments show promising results over other <m> feature based learning approaches </m> .
second:
The process of training a model 's parameters such that a few gradient steps , or even a single gradient step , can produce good results on a new task can be viewed from a <m> feature learning standpoint </m> as building an internal representation that is broadly suitable for many tasks .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The experiments show promising results over other <m> feature based learning approaches </m> .
second:
In recent years , deep neural networks ( DNNs ) have become successful in computer vision , speech recognition , and natural language processing with their great power of <m> feature representation learning </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The experiments show promising results over other <m> feature based learning approaches </m> .
second:
With the increasing sample size of ReID dataset , the <m> learning of features </m> from multi - class person identification tasks , denoted as ID - discriminative Embedding ( IDE ) , has shown great potentials on current large - scale person ReID datasets , such as MARS and PRW , where the IDE features are taken from the last hidden layer of Deep Convolutional Neural Networks ( DCNN ) .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The experiments show promising results over other <m> feature based learning approaches </m> .
second:
Thus , it is significant to explicitly involve the semantic boundary to guide the <m> learning of the features </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The big data trend has inspired <m> feature-driven learning tasks </m> , which can not be handled by conventional machine learning models .
second:
<m> Feature learning methods </m> based on the Skip - gram architecture have been originally developed in the context of natural language .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The big data trend has inspired <m> feature-driven learning tasks </m> , which can not be handled by conventional machine learning models .
second:
We find that this simple <m> feature learning algorithm </m> is surprisingly successful when applied to visual object recognition .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The big data trend has inspired <m> feature-driven learning tasks </m> , which can not be handled by conventional machine learning models .
second:
We formulate such a dual path architecture as follows : where and denote the extracted information at - th step from individual path , is a <m> feature learning function </m> as .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The big data trend has inspired <m> feature-driven learning tasks </m> , which can not be handled by conventional machine learning models .
second:
In this paper , we address the <m> feature learning problem </m> in DL by presenting a new algorithm , deeply - supervised nets ( DSN ) , which enforces direct and early supervision for both the hidden layers and the output layer .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The big data trend has inspired <m> feature-driven learning tasks </m> , which can not be handled by conventional machine learning models .
second:
The process of training a model 's parameters such that a few gradient steps , or even a single gradient step , can produce good results on a new task can be viewed from a <m> feature learning standpoint </m> as building an internal representation that is broadly suitable for many tasks .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The big data trend has inspired <m> feature-driven learning tasks </m> , which can not be handled by conventional machine learning models .
second:
In recent years , deep neural networks ( DNNs ) have become successful in computer vision , speech recognition , and natural language processing with their great power of <m> feature representation learning </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The big data trend has inspired <m> feature-driven learning tasks </m> , which can not be handled by conventional machine learning models .
second:
With the increasing sample size of ReID dataset , the <m> learning of features </m> from multi - class person identification tasks , denoted as ID - discriminative Embedding ( IDE ) , has shown great potentials on current large - scale person ReID datasets , such as MARS and PRW , where the IDE features are taken from the last hidden layer of Deep Convolutional Neural Networks ( DCNN ) .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The big data trend has inspired <m> feature-driven learning tasks </m> , which can not be handled by conventional machine learning models .
second:
Thus , it is significant to explicitly involve the semantic boundary to guide the <m> learning of the features </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
More specifically , the <m> representation learning paradigm </m> is applied to activities , traces , logs , and models in order to learn highly informative but low-dimensional vectors , often referred to as embeddings , based on a neural network architecture .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
Although these approaches successfullyimprove the predictive performance , ( 1 ) designing a good feature requiresdomain experts to make a great effort and ( 2 ) features obtained <m> fromrepresentation learning </m> are hard to interpret .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
We show how to solve this new <m> representation learning formulation </m> and illustrate it ’s favorable properties over the original IB when trained from a small sample .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
From the perspective of <m> representation learning </m> , this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
As a result , the model induces undirected relations among tokens as an intermediate step of <m> learning representations </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the field of machine learning , <m> representation-based learning </m> in deep learning context is gaining popularity in the recent years .
second:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
In contrast , our goal is to automate the whole process by casting feature extraction as a <m> representation learning problem </m> in which case we do not require any hand - engineered features .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
section : Conclusion This paper introduces a <m> representation learning algorithm </m> called Information Maximizing Generative Adversarial Networks ( InfoGAN ) .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These relations are soft and differentiable , and components of a larger <m> representation learning network </m> .
second:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The bound leads naturally to a new family of <m> representation - learning based algorithms </m> , which we show to match or outperform state - of - the - art methods on several causal effect inference tasks .
second:
A key disadvantage of pre - training is that the first <m> representation learning phase </m> does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .


class: second -> first, base class: no relation, new model class: no relation
first:
Our method uses a deep convolutional network trained to directly optimize the embedding itself , rather than an <m> intermediate bottleneck layer </m> as in previous deep learning approaches .
second:
The added advantage of <m> hidden layer </m> for lip reading is that it takes into account the nonlinear variation of lip features while speaking .


class: second -> first, base class: no relation, new model class: no relation
first:
Our method uses a deep convolutional network trained to directly optimize the embedding itself , rather than an <m> intermediate bottleneck layer </m> as in previous deep learning approaches .
second:
The experiment showed that the best <m> hidden layer architecture </m> ( HLA ) is 5 - 10 - 11 - 12 - 13 - 1 with learning function ( LF ) of trainlm , activation function ( AF ) of logsig and purelin , and learning rate ( LR ) of 0.5 .


class: second -> first, base class: no relation, new model class: no relation
first:
Our method uses a deep convolutional network trained to directly optimize the embedding itself , rather than an <m> intermediate bottleneck layer </m> as in previous deep learning approaches .
second:
Further <m> Hidden layer 1 </m> helps to extract data to estimate the forecasting equation and also it helps for evaluating estimated Model using different measures .


class: second -> first, base class: no relation, new model class: no relation
first:
Our method uses a deep convolutional network trained to directly optimize the embedding itself , rather than an <m> intermediate bottleneck layer </m> as in previous deep learning approaches .
second:
where and are the question and the answer encoded vectors and denotes the output of the <m> hidden layer </m> of the MLP .


class: second -> first, base class: no relation, new model class: no relation
first:
Our method uses a deep convolutional network trained to directly optimize the embedding itself , rather than an <m> intermediate bottleneck layer </m> as in previous deep learning approaches .
second:
Similar to our LoNGAE model , the GCN model can learn <m> hidden layer representations </m> that encode both local graph structure and features of nodes .


class: second -> first, base class: no relation, new model class: no relation
first:
Our method uses a deep convolutional network trained to directly optimize the embedding itself , rather than an <m> intermediate bottleneck layer </m> as in previous deep learning approaches .
second:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .


class: second -> first, base class: no relation, new model class: no relation
first:
Our method uses a deep convolutional network trained to directly optimize the embedding itself , rather than an <m> intermediate bottleneck layer </m> as in previous deep learning approaches .
second:
PNN : For the purpose of capturing high - order feature interactions , PNN imposes a product layer between the embedding layer and the <m> first hidden layer </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Our method uses a deep convolutional network trained to directly optimize the embedding itself , rather than an <m> intermediate bottleneck layer </m> as in previous deep learning approaches .
second:
Their method simultaneously minimizes classification error while improving the directness and transparency of the <m> hidden layer learning process </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Our method uses a deep convolutional network trained to directly optimize the embedding itself , rather than an <m> intermediate bottleneck layer </m> as in previous deep learning approaches .
second:
This was found sufficient , with the following <m> hidden layers learning </m> to combine the multi - scale features .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We suggest that the segmentation reflects the existence of <m> neural-image representations </m> with discrete levels of spatial accuracy .
second:
This <m> neural-level representation </m> is considered to support a “ direct activation ” of action by perception/imagination of movement at the behavioral level .


class: second -> first, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .


class: second -> first, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .


class: second -> first, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .


class: second -> first, base class: no relation, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: second -> first, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: second -> first, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
PytorX could perform <m> end-to-end training </m> , mapping , and evaluation for crossbar-based neural network accelerator , considering all above discussed non-ideal effects of ReRAM crossbar together .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .


class: second -> first, base class: no relation, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In general , however , these approaches are not fully differentiable and interrupt an <m> end-to-end training process </m> with the stochastic gradient descent in that they require either a parameter selection or a soft-thresholding step .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .


class: second -> first, base class: no relation, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We also show that such cascade training has significant computational and memory advantages over <m> end – end training </m> , and can be used as a pretraining algorithm to obtain a better performance .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .


class: second -> first, base class: no relation, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In recent years , deconvolution layers are widely used as key components in the state-of-the-art CNNs for <m> end-to-end training and models </m> to support tasks such as image segmentation .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .


class: second -> first, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .


class: second -> first, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .


class: second -> first, base class: no relation, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: second -> first, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: second -> first, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
By taking advantage of the two-stream deep structures and two types of image inputs , a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the <m> end-to-end training phase </m> , in order to realize adaptive two-stream feature subsets selection and optimization .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .


class: second -> first, base class: no relation, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Our purpose in this study is to investigate whether a recently introduced image transform , denoted as the Radon cumulative distribution transform ( R-CDT ) , can be used as a viable preprocessing step for augmenting the robustness of <m> training end-to-end systems </m> with fewer training samples .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .


class: second -> first, base class: no relation, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
These techniques permit <m> end-to-end training of models </m> with 10 + layers of latent variables .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .


class: second -> first, base class: no relation, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the conventional end-to-end training is inapplicable , a novel two-stage training approach is proposed to indirectly solve the <m> end-to-end training problem </m> .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .


class: second -> first, base class: no relation, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Secondly , in the <m> end-to-end training stage </m> , the auxiliary contour supervision is utilized to guide the model to learn the boundary awareness , so that the contour shape of segmentation mask is more precise .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .


class: second -> first, base class: no relation, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The end- <m> to-end training </m> jointly learns the background and speaker models by creating the representation space .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .


class: second -> first, base class: no relation, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : <m> End - to - end training </m> It ’s straightforward to compute the derivatives of the recurrent mean shift grouping module w.r.t based on the the chain rule so our whole system is end - to - end trainable through back - propagation .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .


class: second -> first, base class: no relation, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In the next section we introduce an <m> end - to - end training algorithm </m> to address the causal dependency .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .


class: second -> first, base class: no relation, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Gradients of both terms need to be considered in a theoretically sound end - to - <m> end training solution </m> .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .


class: second -> first, base class: no relation, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Table [ reference ] ( d ) shows the result of end - to - <m> end training </m> a 5 - stage cascade .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .


class: second -> first, base class: no relation, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We design an <m> end - to - end training strategy </m> and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .


class: second -> first, base class: no relation, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Inspired by recent advance on instance - aware semantic segmentation , we present an <m> end - end trainable framework </m> called Fused Text Segmentation Networks ( FTSN ) to handle arbitrary - shape text detection with no extra pipelines involved .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .


class: second -> first, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .


class: second -> first, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .


class: second -> first, base class: no relation, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: second -> first, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: second -> first, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
All of these results are achieved with a holistic , <m> end - to - end trained model </m> which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .


class: second -> first, base class: no relation, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Considering the non - linearity in Block 3 , it is non - trivial to tell if this is as good as the fully <m> end - to - end training case </m> , as illustrated by in the top right example .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .


class: second -> first, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .


class: second -> first, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .


class: second -> first, base class: no relation, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: second -> first, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: second -> first, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
On the basis of that analysis we calculated a new , more spatially efficient , and better performing architecture which actually achieves <m> fully end - to - end training </m> for large networks .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .


class: second -> first, base class: no relation, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Further , our novel architecture is capable of simultaneous multi - task learning of both link prediction and node classification in one efficient end - to - <m> end training stage </m> .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .
second:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .
second:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .
second:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .
second:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
subsection : QAN for image set embedding In this paper , feature generation and aggregation module is implemented through an <m> end - to - end convolutional neural network </m> named QAN as shown in Fig .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .
second:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .
second:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .
second:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In this work , we address this problem using a novel optimization procedure for the <m> end - to - end neural network training </m> on FGVC tasks .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Inspired by their work , Romera propose an <m> end - to - end recurrent network </m> with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance .
second:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: second -> first, base class: first -> second, new model class: same cluster
first:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Among various spectrums of dialogue generation approaches , <m> end-to-end neural generation models </m> have received an increase of attention .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We first investigate how <m> end-to-end neural sequence models </m> ( with pre-trained language model representations ) perform on document-level role filler extraction , as well as how the length of context captured affects the models ' performance .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: second -> first, base class: no relation, new model class: no relation
first:
What is Bootstrapping ? Estimation Confidence Sets and Hypothesis Testing Regression Analysis Forecasting and <m> Time Series Analysis </m> Which Resampling Method Should You Use ?
second:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
What is Bootstrapping ? Estimation Confidence Sets and Hypothesis Testing Regression Analysis Forecasting and <m> Time Series Analysis </m> Which Resampling Method Should You Use ?
second:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .


class: second -> first, base class: no relation, new model class: no relation
first:
What is Bootstrapping ? Estimation Confidence Sets and Hypothesis Testing Regression Analysis Forecasting and <m> Time Series Analysis </m> Which Resampling Method Should You Use ?
second:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .


class: second -> first, base class: no relation, new model class: no relation
first:
What is Bootstrapping ? Estimation Confidence Sets and Hypothesis Testing Regression Analysis Forecasting and <m> Time Series Analysis </m> Which Resampling Method Should You Use ?
second:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .


class: second -> first, base class: no relation, new model class: no relation
first:
What is Bootstrapping ? Estimation Confidence Sets and Hypothesis Testing Regression Analysis Forecasting and <m> Time Series Analysis </m> Which Resampling Method Should You Use ?
second:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .


class: second -> first, base class: no relation, new model class: no relation
first:
What is Bootstrapping ? Estimation Confidence Sets and Hypothesis Testing Regression Analysis Forecasting and <m> Time Series Analysis </m> Which Resampling Method Should You Use ?
second:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .


class: second -> first, base class: no relation, new model class: no relation
first:
What is Bootstrapping ? Estimation Confidence Sets and Hypothesis Testing Regression Analysis Forecasting and <m> Time Series Analysis </m> Which Resampling Method Should You Use ?
second:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .


class: second -> first, base class: no relation, new model class: no relation
first:
What is Bootstrapping ? Estimation Confidence Sets and Hypothesis Testing Regression Analysis Forecasting and <m> Time Series Analysis </m> Which Resampling Method Should You Use ?
second:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .


class: second -> first, base class: no relation, new model class: no relation
first:
What is Bootstrapping ? Estimation Confidence Sets and Hypothesis Testing Regression Analysis Forecasting and <m> Time Series Analysis </m> Which Resampling Method Should You Use ?
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
What is Bootstrapping ? Estimation Confidence Sets and Hypothesis Testing Regression Analysis Forecasting and <m> Time Series Analysis </m> Which Resampling Method Should You Use ?
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: second -> first, base class: no relation, new model class: no relation
first:
What is Bootstrapping ? Estimation Confidence Sets and Hypothesis Testing Regression Analysis Forecasting and <m> Time Series Analysis </m> Which Resampling Method Should You Use ?
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: second -> first, base class: no relation, new model class: no relation
first:
What is Bootstrapping ? Estimation Confidence Sets and Hypothesis Testing Regression Analysis Forecasting and <m> Time Series Analysis </m> Which Resampling Method Should You Use ?
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: second -> first, base class: no relation, new model class: no relation
first:
What is Bootstrapping ? Estimation Confidence Sets and Hypothesis Testing Regression Analysis Forecasting and <m> Time Series Analysis </m> Which Resampling Method Should You Use ?
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper we present the different approaches to modelling dynamic regression and structural equations• First , we give a survey of the traditional econometrie and the <m> time series approaches </m> to specification analysis of dynamic econometrie models .
second:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper we present the different approaches to modelling dynamic regression and structural equations• First , we give a survey of the traditional econometrie and the <m> time series approaches </m> to specification analysis of dynamic econometrie models .
second:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper we present the different approaches to modelling dynamic regression and structural equations• First , we give a survey of the traditional econometrie and the <m> time series approaches </m> to specification analysis of dynamic econometrie models .
second:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper we present the different approaches to modelling dynamic regression and structural equations• First , we give a survey of the traditional econometrie and the <m> time series approaches </m> to specification analysis of dynamic econometrie models .
second:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper we present the different approaches to modelling dynamic regression and structural equations• First , we give a survey of the traditional econometrie and the <m> time series approaches </m> to specification analysis of dynamic econometrie models .
second:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper we present the different approaches to modelling dynamic regression and structural equations• First , we give a survey of the traditional econometrie and the <m> time series approaches </m> to specification analysis of dynamic econometrie models .
second:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper we present the different approaches to modelling dynamic regression and structural equations• First , we give a survey of the traditional econometrie and the <m> time series approaches </m> to specification analysis of dynamic econometrie models .
second:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper we present the different approaches to modelling dynamic regression and structural equations• First , we give a survey of the traditional econometrie and the <m> time series approaches </m> to specification analysis of dynamic econometrie models .
second:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper we present the different approaches to modelling dynamic regression and structural equations• First , we give a survey of the traditional econometrie and the <m> time series approaches </m> to specification analysis of dynamic econometrie models .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper we present the different approaches to modelling dynamic regression and structural equations• First , we give a survey of the traditional econometrie and the <m> time series approaches </m> to specification analysis of dynamic econometrie models .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper we present the different approaches to modelling dynamic regression and structural equations• First , we give a survey of the traditional econometrie and the <m> time series approaches </m> to specification analysis of dynamic econometrie models .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper we present the different approaches to modelling dynamic regression and structural equations• First , we give a survey of the traditional econometrie and the <m> time series approaches </m> to specification analysis of dynamic econometrie models .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper we present the different approaches to modelling dynamic regression and structural equations• First , we give a survey of the traditional econometrie and the <m> time series approaches </m> to specification analysis of dynamic econometrie models .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: second -> first, base class: no relation, new model class: no relation
first:
As in the case of traditional data base systems , in <m> time series data systems </m> too the methods employed in capturing , indexing , representing and storing the data remains as the key issue .
second:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
As in the case of traditional data base systems , in <m> time series data systems </m> too the methods employed in capturing , indexing , representing and storing the data remains as the key issue .
second:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .


class: second -> first, base class: no relation, new model class: no relation
first:
As in the case of traditional data base systems , in <m> time series data systems </m> too the methods employed in capturing , indexing , representing and storing the data remains as the key issue .
second:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .


class: second -> first, base class: no relation, new model class: no relation
first:
As in the case of traditional data base systems , in <m> time series data systems </m> too the methods employed in capturing , indexing , representing and storing the data remains as the key issue .
second:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .


class: second -> first, base class: no relation, new model class: no relation
first:
As in the case of traditional data base systems , in <m> time series data systems </m> too the methods employed in capturing , indexing , representing and storing the data remains as the key issue .
second:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .


class: second -> first, base class: no relation, new model class: no relation
first:
As in the case of traditional data base systems , in <m> time series data systems </m> too the methods employed in capturing , indexing , representing and storing the data remains as the key issue .
second:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .


class: second -> first, base class: no relation, new model class: no relation
first:
As in the case of traditional data base systems , in <m> time series data systems </m> too the methods employed in capturing , indexing , representing and storing the data remains as the key issue .
second:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .


class: second -> first, base class: no relation, new model class: no relation
first:
As in the case of traditional data base systems , in <m> time series data systems </m> too the methods employed in capturing , indexing , representing and storing the data remains as the key issue .
second:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .


class: second -> first, base class: no relation, new model class: no relation
first:
As in the case of traditional data base systems , in <m> time series data systems </m> too the methods employed in capturing , indexing , representing and storing the data remains as the key issue .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
As in the case of traditional data base systems , in <m> time series data systems </m> too the methods employed in capturing , indexing , representing and storing the data remains as the key issue .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: second -> first, base class: no relation, new model class: no relation
first:
As in the case of traditional data base systems , in <m> time series data systems </m> too the methods employed in capturing , indexing , representing and storing the data remains as the key issue .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: second -> first, base class: no relation, new model class: no relation
first:
As in the case of traditional data base systems , in <m> time series data systems </m> too the methods employed in capturing , indexing , representing and storing the data remains as the key issue .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: second -> first, base class: no relation, new model class: no relation
first:
As in the case of traditional data base systems , in <m> time series data systems </m> too the methods employed in capturing , indexing , representing and storing the data remains as the key issue .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: second -> first, base class: no relation, new model class: no relation
first:
However , the disadvantages of Gaussian Process regression include its computation complexity and incapability to adapt to <m> time varying time-series systems </m> .
second:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In order to evaluate the ability of this proposed algorithm , the traditional statistical models are used as a benchmark vis-a-vis their <m> time series counterparts </m> .
second:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In order to evaluate the ability of this proposed algorithm , the traditional statistical models are used as a benchmark vis-a-vis their <m> time series counterparts </m> .
second:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .


class: second -> first, base class: no relation, new model class: no relation
first:
In order to evaluate the ability of this proposed algorithm , the traditional statistical models are used as a benchmark vis-a-vis their <m> time series counterparts </m> .
second:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .


class: second -> first, base class: no relation, new model class: no relation
first:
In order to evaluate the ability of this proposed algorithm , the traditional statistical models are used as a benchmark vis-a-vis their <m> time series counterparts </m> .
second:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .


class: second -> first, base class: no relation, new model class: no relation
first:
In order to evaluate the ability of this proposed algorithm , the traditional statistical models are used as a benchmark vis-a-vis their <m> time series counterparts </m> .
second:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .


class: second -> first, base class: no relation, new model class: no relation
first:
In order to evaluate the ability of this proposed algorithm , the traditional statistical models are used as a benchmark vis-a-vis their <m> time series counterparts </m> .
second:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .


class: second -> first, base class: no relation, new model class: no relation
first:
In order to evaluate the ability of this proposed algorithm , the traditional statistical models are used as a benchmark vis-a-vis their <m> time series counterparts </m> .
second:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .


class: second -> first, base class: no relation, new model class: no relation
first:
In order to evaluate the ability of this proposed algorithm , the traditional statistical models are used as a benchmark vis-a-vis their <m> time series counterparts </m> .
second:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .


class: second -> first, base class: no relation, new model class: no relation
first:
In order to evaluate the ability of this proposed algorithm , the traditional statistical models are used as a benchmark vis-a-vis their <m> time series counterparts </m> .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In order to evaluate the ability of this proposed algorithm , the traditional statistical models are used as a benchmark vis-a-vis their <m> time series counterparts </m> .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: second -> first, base class: no relation, new model class: no relation
first:
In order to evaluate the ability of this proposed algorithm , the traditional statistical models are used as a benchmark vis-a-vis their <m> time series counterparts </m> .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: second -> first, base class: no relation, new model class: no relation
first:
In order to evaluate the ability of this proposed algorithm , the traditional statistical models are used as a benchmark vis-a-vis their <m> time series counterparts </m> .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: second -> first, base class: no relation, new model class: no relation
first:
In order to evaluate the ability of this proposed algorithm , the traditional statistical models are used as a benchmark vis-a-vis their <m> time series counterparts </m> .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: second -> first, base class: no relation, new model class: no relation
first:
The judicature process is induced into the de-noising disposal of <m> time series in algorithm </m> , which builds a new de-nosing algorithm .
second:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The judicature process is induced into the de-noising disposal of <m> time series in algorithm </m> , which builds a new de-nosing algorithm .
second:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .


class: second -> first, base class: no relation, new model class: no relation
first:
The judicature process is induced into the de-noising disposal of <m> time series in algorithm </m> , which builds a new de-nosing algorithm .
second:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .


class: second -> first, base class: no relation, new model class: no relation
first:
The judicature process is induced into the de-noising disposal of <m> time series in algorithm </m> , which builds a new de-nosing algorithm .
second:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .


class: second -> first, base class: no relation, new model class: no relation
first:
The judicature process is induced into the de-noising disposal of <m> time series in algorithm </m> , which builds a new de-nosing algorithm .
second:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .


class: second -> first, base class: no relation, new model class: no relation
first:
The judicature process is induced into the de-noising disposal of <m> time series in algorithm </m> , which builds a new de-nosing algorithm .
second:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .


class: second -> first, base class: no relation, new model class: no relation
first:
The judicature process is induced into the de-noising disposal of <m> time series in algorithm </m> , which builds a new de-nosing algorithm .
second:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .


class: second -> first, base class: no relation, new model class: no relation
first:
The judicature process is induced into the de-noising disposal of <m> time series in algorithm </m> , which builds a new de-nosing algorithm .
second:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .


class: second -> first, base class: no relation, new model class: no relation
first:
The judicature process is induced into the de-noising disposal of <m> time series in algorithm </m> , which builds a new de-nosing algorithm .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The judicature process is induced into the de-noising disposal of <m> time series in algorithm </m> , which builds a new de-nosing algorithm .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: second -> first, base class: no relation, new model class: no relation
first:
The judicature process is induced into the de-noising disposal of <m> time series in algorithm </m> , which builds a new de-nosing algorithm .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: second -> first, base class: no relation, new model class: no relation
first:
The judicature process is induced into the de-noising disposal of <m> time series in algorithm </m> , which builds a new de-nosing algorithm .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: second -> first, base class: no relation, new model class: no relation
first:
The judicature process is induced into the de-noising disposal of <m> time series in algorithm </m> , which builds a new de-nosing algorithm .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: second -> first, base class: no relation, new model class: no relation
first:
Through the numerical experiments , we showed that the quality of the inferred genetic network is slightly improved by giving greater importance to static data than <m> time-series ones </m> .
second:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Through the numerical experiments , we showed that the quality of the inferred genetic network is slightly improved by giving greater importance to static data than <m> time-series ones </m> .
second:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .


class: second -> first, base class: no relation, new model class: no relation
first:
Through the numerical experiments , we showed that the quality of the inferred genetic network is slightly improved by giving greater importance to static data than <m> time-series ones </m> .
second:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .


class: second -> first, base class: no relation, new model class: no relation
first:
Through the numerical experiments , we showed that the quality of the inferred genetic network is slightly improved by giving greater importance to static data than <m> time-series ones </m> .
second:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .


class: second -> first, base class: no relation, new model class: no relation
first:
Through the numerical experiments , we showed that the quality of the inferred genetic network is slightly improved by giving greater importance to static data than <m> time-series ones </m> .
second:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .


class: second -> first, base class: no relation, new model class: no relation
first:
Through the numerical experiments , we showed that the quality of the inferred genetic network is slightly improved by giving greater importance to static data than <m> time-series ones </m> .
second:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .


class: second -> first, base class: no relation, new model class: no relation
first:
Through the numerical experiments , we showed that the quality of the inferred genetic network is slightly improved by giving greater importance to static data than <m> time-series ones </m> .
second:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .


class: second -> first, base class: no relation, new model class: no relation
first:
Through the numerical experiments , we showed that the quality of the inferred genetic network is slightly improved by giving greater importance to static data than <m> time-series ones </m> .
second:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .


class: second -> first, base class: no relation, new model class: no relation
first:
Through the numerical experiments , we showed that the quality of the inferred genetic network is slightly improved by giving greater importance to static data than <m> time-series ones </m> .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Through the numerical experiments , we showed that the quality of the inferred genetic network is slightly improved by giving greater importance to static data than <m> time-series ones </m> .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: second -> first, base class: no relation, new model class: no relation
first:
Through the numerical experiments , we showed that the quality of the inferred genetic network is slightly improved by giving greater importance to static data than <m> time-series ones </m> .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: second -> first, base class: no relation, new model class: no relation
first:
Through the numerical experiments , we showed that the quality of the inferred genetic network is slightly improved by giving greater importance to static data than <m> time-series ones </m> .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: second -> first, base class: no relation, new model class: no relation
first:
Through the numerical experiments , we showed that the quality of the inferred genetic network is slightly improved by giving greater importance to static data than <m> time-series ones </m> .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: second -> first, base class: no relation, new model class: no relation
first:
In addition , searching time series databases via examples , which may be either <m> time series or models </m> , is also part of this work .
second:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In addition , searching time series databases via examples , which may be either <m> time series or models </m> , is also part of this work .
second:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .


class: second -> first, base class: no relation, new model class: no relation
first:
In addition , searching time series databases via examples , which may be either <m> time series or models </m> , is also part of this work .
second:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .


class: second -> first, base class: no relation, new model class: no relation
first:
In addition , searching time series databases via examples , which may be either <m> time series or models </m> , is also part of this work .
second:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .


class: second -> first, base class: no relation, new model class: no relation
first:
In addition , searching time series databases via examples , which may be either <m> time series or models </m> , is also part of this work .
second:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .


class: second -> first, base class: no relation, new model class: no relation
first:
In addition , searching time series databases via examples , which may be either <m> time series or models </m> , is also part of this work .
second:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .


class: second -> first, base class: no relation, new model class: no relation
first:
In addition , searching time series databases via examples , which may be either <m> time series or models </m> , is also part of this work .
second:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .


class: second -> first, base class: no relation, new model class: no relation
first:
In addition , searching time series databases via examples , which may be either <m> time series or models </m> , is also part of this work .
second:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .


class: second -> first, base class: no relation, new model class: no relation
first:
In addition , searching time series databases via examples , which may be either <m> time series or models </m> , is also part of this work .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In addition , searching time series databases via examples , which may be either <m> time series or models </m> , is also part of this work .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: second -> first, base class: no relation, new model class: no relation
first:
In addition , searching time series databases via examples , which may be either <m> time series or models </m> , is also part of this work .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: second -> first, base class: no relation, new model class: no relation
first:
In addition , searching time series databases via examples , which may be either <m> time series or models </m> , is also part of this work .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: second -> first, base class: no relation, new model class: no relation
first:
In addition , searching time series databases via examples , which may be either <m> time series or models </m> , is also part of this work .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: second -> first, base class: no relation, new model class: no relation
first:
Several early-warning signals have been reported in <m> time series representing systems </m> near catastrophic shifts .
second:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Considering statistical methods , we propose the <m> Time Series ( TIME ) model </m> based on the analysis of users ' time behavior characteristics . And thinking from the theme of the users ' session , the improved Latent Dirichlet Allocation ( LDA ) malicious topic model is designed which has improved the sensitivity of the model to detect malicious topics .
second:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Considering statistical methods , we propose the <m> Time Series ( TIME ) model </m> based on the analysis of users ' time behavior characteristics . And thinking from the theme of the users ' session , the improved Latent Dirichlet Allocation ( LDA ) malicious topic model is designed which has improved the sensitivity of the model to detect malicious topics .
second:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .


class: second -> first, base class: no relation, new model class: no relation
first:
Considering statistical methods , we propose the <m> Time Series ( TIME ) model </m> based on the analysis of users ' time behavior characteristics . And thinking from the theme of the users ' session , the improved Latent Dirichlet Allocation ( LDA ) malicious topic model is designed which has improved the sensitivity of the model to detect malicious topics .
second:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .


class: second -> first, base class: no relation, new model class: no relation
first:
Considering statistical methods , we propose the <m> Time Series ( TIME ) model </m> based on the analysis of users ' time behavior characteristics . And thinking from the theme of the users ' session , the improved Latent Dirichlet Allocation ( LDA ) malicious topic model is designed which has improved the sensitivity of the model to detect malicious topics .
second:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .


class: second -> first, base class: no relation, new model class: no relation
first:
Considering statistical methods , we propose the <m> Time Series ( TIME ) model </m> based on the analysis of users ' time behavior characteristics . And thinking from the theme of the users ' session , the improved Latent Dirichlet Allocation ( LDA ) malicious topic model is designed which has improved the sensitivity of the model to detect malicious topics .
second:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .


class: second -> first, base class: no relation, new model class: no relation
first:
Considering statistical methods , we propose the <m> Time Series ( TIME ) model </m> based on the analysis of users ' time behavior characteristics . And thinking from the theme of the users ' session , the improved Latent Dirichlet Allocation ( LDA ) malicious topic model is designed which has improved the sensitivity of the model to detect malicious topics .
second:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .


class: second -> first, base class: no relation, new model class: no relation
first:
Considering statistical methods , we propose the <m> Time Series ( TIME ) model </m> based on the analysis of users ' time behavior characteristics . And thinking from the theme of the users ' session , the improved Latent Dirichlet Allocation ( LDA ) malicious topic model is designed which has improved the sensitivity of the model to detect malicious topics .
second:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .


class: second -> first, base class: no relation, new model class: no relation
first:
Considering statistical methods , we propose the <m> Time Series ( TIME ) model </m> based on the analysis of users ' time behavior characteristics . And thinking from the theme of the users ' session , the improved Latent Dirichlet Allocation ( LDA ) malicious topic model is designed which has improved the sensitivity of the model to detect malicious topics .
second:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .


class: second -> first, base class: no relation, new model class: no relation
first:
Considering statistical methods , we propose the <m> Time Series ( TIME ) model </m> based on the analysis of users ' time behavior characteristics . And thinking from the theme of the users ' session , the improved Latent Dirichlet Allocation ( LDA ) malicious topic model is designed which has improved the sensitivity of the model to detect malicious topics .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Considering statistical methods , we propose the <m> Time Series ( TIME ) model </m> based on the analysis of users ' time behavior characteristics . And thinking from the theme of the users ' session , the improved Latent Dirichlet Allocation ( LDA ) malicious topic model is designed which has improved the sensitivity of the model to detect malicious topics .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: second -> first, base class: no relation, new model class: no relation
first:
Considering statistical methods , we propose the <m> Time Series ( TIME ) model </m> based on the analysis of users ' time behavior characteristics . And thinking from the theme of the users ' session , the improved Latent Dirichlet Allocation ( LDA ) malicious topic model is designed which has improved the sensitivity of the model to detect malicious topics .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: second -> first, base class: no relation, new model class: no relation
first:
Considering statistical methods , we propose the <m> Time Series ( TIME ) model </m> based on the analysis of users ' time behavior characteristics . And thinking from the theme of the users ' session , the improved Latent Dirichlet Allocation ( LDA ) malicious topic model is designed which has improved the sensitivity of the model to detect malicious topics .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: second -> first, base class: no relation, new model class: no relation
first:
Considering statistical methods , we propose the <m> Time Series ( TIME ) model </m> based on the analysis of users ' time behavior characteristics . And thinking from the theme of the users ' session , the improved Latent Dirichlet Allocation ( LDA ) malicious topic model is designed which has improved the sensitivity of the model to detect malicious topics .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: second -> first, base class: no relation, new model class: no relation
first:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .
second:
Under DBNs , Bayesian inference provides an optimal framework for <m> sequential data analysis </m> and produces solutions potentially better than other non Bayesian-based methods .


class: second -> first, base class: no relation, new model class: no relation
first:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .
second:
Therefore , the introduced algorithm is especially suitable for the applications requiring <m> sequential data processing </m> at large scales/high rates .


class: second -> first, base class: no relation, new model class: no relation
first:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .
second:
However , complex enhancement algorithms also require the <m> sequential processing of data </m> and this can not be easily achieved in real-time on a GPU .


class: second -> first, base class: no relation, new model class: no relation
first:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .
second:
Furthermore , we develop a solution for generating differentially private sequential data , which will bring us one step closer to publicly available medical datasets via <m> sequential data </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .
second:
These <m> sequential data collection procedures </m> are known by different names , such as sequential experimental design , active learning , or adaptive sensing/sampling .


class: second -> first, base class: no relation, new model class: no relation
first:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .
second:
This paper gives a review of the most important methods for blood velocity vector flow imaging ( VFI ) for conventional <m> sequential data acquisition </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .
second:
T-patterns detection analysis provides a <m> sequential analysis of data </m> , illustrating the communicative style and flow of each coach .


class: second -> first, base class: no relation, new model class: no relation
first:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .
second:
Recurrent Neural Networks ( RNNs ) capture long dependencies and context , and 2 hence are the key component of typical <m> sequential data based tasks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .
second:
We present related techniques including data normalization , dimension reduction , classification , and spatial information integration and the way to accommodate these techniques to the context of <m> sequential data collecting and processing </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .
second:
subsection : Recurrent neural networks ( RNN ) Different from Artificial neural network ( ANN ) , RNN , a neural network with recurrent unit , has a better performance in solving many challenging problems involving <m> sequential data analysis </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .
second:
Under DBNs , Bayesian inference provides an optimal framework for <m> sequential data analysis </m> and produces solutions potentially better than other non Bayesian-based methods .


class: second -> first, base class: no relation, new model class: no relation
first:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .
second:
Therefore , the introduced algorithm is especially suitable for the applications requiring <m> sequential data processing </m> at large scales/high rates .


class: second -> first, base class: no relation, new model class: no relation
first:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .
second:
However , complex enhancement algorithms also require the <m> sequential processing of data </m> and this can not be easily achieved in real-time on a GPU .


class: second -> first, base class: no relation, new model class: no relation
first:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .
second:
Furthermore , we develop a solution for generating differentially private sequential data , which will bring us one step closer to publicly available medical datasets via <m> sequential data </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .
second:
These <m> sequential data collection procedures </m> are known by different names , such as sequential experimental design , active learning , or adaptive sensing/sampling .


class: second -> first, base class: no relation, new model class: no relation
first:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .
second:
This paper gives a review of the most important methods for blood velocity vector flow imaging ( VFI ) for conventional <m> sequential data acquisition </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .
second:
T-patterns detection analysis provides a <m> sequential analysis of data </m> , illustrating the communicative style and flow of each coach .


class: second -> first, base class: no relation, new model class: no relation
first:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .
second:
Recurrent Neural Networks ( RNNs ) capture long dependencies and context , and 2 hence are the key component of typical <m> sequential data based tasks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .
second:
We present related techniques including data normalization , dimension reduction , classification , and spatial information integration and the way to accommodate these techniques to the context of <m> sequential data collecting and processing </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .
second:
subsection : Recurrent neural networks ( RNN ) Different from Artificial neural network ( ANN ) , RNN , a neural network with recurrent unit , has a better performance in solving many challenging problems involving <m> sequential data analysis </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Tuple-versioning of relations is the adopted method of <m> temporal data representation </m> .
second:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Tuple-versioning of relations is the adopted method of <m> temporal data representation </m> .
second:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Tuple-versioning of relations is the adopted method of <m> temporal data representation </m> .
second:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Tuple-versioning of relations is the adopted method of <m> temporal data representation </m> .
second:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Tuple-versioning of relations is the adopted method of <m> temporal data representation </m> .
second:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Tuple-versioning of relations is the adopted method of <m> temporal data representation </m> .
second:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .


class: second -> first, base class: first -> second, new model class: no relation
first:
Tuple-versioning of relations is the adopted method of <m> temporal data representation </m> .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: second -> first, base class: first -> second, new model class: no relation
first:
Tuple-versioning of relations is the adopted method of <m> temporal data representation </m> .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: second -> first, base class: first -> second, new model class: no relation
first:
Tuple-versioning of relations is the adopted method of <m> temporal data representation </m> .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: second -> first, base class: first -> second, new model class: no relation
first:
Tuple-versioning of relations is the adopted method of <m> temporal data representation </m> .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: second -> first, base class: first -> second, new model class: no relation
first:
Tuple-versioning of relations is the adopted method of <m> temporal data representation </m> .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: second -> first, base class: no relation, new model class: no relation
first:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .
second:
Under DBNs , Bayesian inference provides an optimal framework for <m> sequential data analysis </m> and produces solutions potentially better than other non Bayesian-based methods .


class: second -> first, base class: no relation, new model class: no relation
first:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .
second:
Therefore , the introduced algorithm is especially suitable for the applications requiring <m> sequential data processing </m> at large scales/high rates .


class: second -> first, base class: no relation, new model class: no relation
first:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .
second:
However , complex enhancement algorithms also require the <m> sequential processing of data </m> and this can not be easily achieved in real-time on a GPU .


class: second -> first, base class: no relation, new model class: no relation
first:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .
second:
Furthermore , we develop a solution for generating differentially private sequential data , which will bring us one step closer to publicly available medical datasets via <m> sequential data </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .
second:
These <m> sequential data collection procedures </m> are known by different names , such as sequential experimental design , active learning , or adaptive sensing/sampling .


class: second -> first, base class: no relation, new model class: no relation
first:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .
second:
This paper gives a review of the most important methods for blood velocity vector flow imaging ( VFI ) for conventional <m> sequential data acquisition </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .
second:
T-patterns detection analysis provides a <m> sequential analysis of data </m> , illustrating the communicative style and flow of each coach .


class: second -> first, base class: no relation, new model class: no relation
first:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .
second:
Recurrent Neural Networks ( RNNs ) capture long dependencies and context , and 2 hence are the key component of typical <m> sequential data based tasks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .
second:
We present related techniques including data normalization , dimension reduction , classification , and spatial information integration and the way to accommodate these techniques to the context of <m> sequential data collecting and processing </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .
second:
subsection : Recurrent neural networks ( RNN ) Different from Artificial neural network ( ANN ) , RNN , a neural network with recurrent unit , has a better performance in solving many challenging problems involving <m> sequential data analysis </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .
second:
Under DBNs , Bayesian inference provides an optimal framework for <m> sequential data analysis </m> and produces solutions potentially better than other non Bayesian-based methods .


class: second -> first, base class: no relation, new model class: no relation
first:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .
second:
Therefore , the introduced algorithm is especially suitable for the applications requiring <m> sequential data processing </m> at large scales/high rates .


class: second -> first, base class: no relation, new model class: no relation
first:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .
second:
However , complex enhancement algorithms also require the <m> sequential processing of data </m> and this can not be easily achieved in real-time on a GPU .


class: second -> first, base class: no relation, new model class: no relation
first:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .
second:
Furthermore , we develop a solution for generating differentially private sequential data , which will bring us one step closer to publicly available medical datasets via <m> sequential data </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .
second:
These <m> sequential data collection procedures </m> are known by different names , such as sequential experimental design , active learning , or adaptive sensing/sampling .


class: second -> first, base class: no relation, new model class: no relation
first:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .
second:
This paper gives a review of the most important methods for blood velocity vector flow imaging ( VFI ) for conventional <m> sequential data acquisition </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .
second:
T-patterns detection analysis provides a <m> sequential analysis of data </m> , illustrating the communicative style and flow of each coach .


class: second -> first, base class: no relation, new model class: no relation
first:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .
second:
Recurrent Neural Networks ( RNNs ) capture long dependencies and context , and 2 hence are the key component of typical <m> sequential data based tasks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .
second:
We present related techniques including data normalization , dimension reduction , classification , and spatial information integration and the way to accommodate these techniques to the context of <m> sequential data collecting and processing </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .
second:
subsection : Recurrent neural networks ( RNN ) Different from Artificial neural network ( ANN ) , RNN , a neural network with recurrent unit , has a better performance in solving many challenging problems involving <m> sequential data analysis </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .
second:
Under DBNs , Bayesian inference provides an optimal framework for <m> sequential data analysis </m> and produces solutions potentially better than other non Bayesian-based methods .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .
second:
Therefore , the introduced algorithm is especially suitable for the applications requiring <m> sequential data processing </m> at large scales/high rates .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .
second:
However , complex enhancement algorithms also require the <m> sequential processing of data </m> and this can not be easily achieved in real-time on a GPU .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .
second:
Furthermore , we develop a solution for generating differentially private sequential data , which will bring us one step closer to publicly available medical datasets via <m> sequential data </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .
second:
These <m> sequential data collection procedures </m> are known by different names , such as sequential experimental design , active learning , or adaptive sensing/sampling .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .
second:
This paper gives a review of the most important methods for blood velocity vector flow imaging ( VFI ) for conventional <m> sequential data acquisition </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .
second:
T-patterns detection analysis provides a <m> sequential analysis of data </m> , illustrating the communicative style and flow of each coach .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .
second:
Recurrent Neural Networks ( RNNs ) capture long dependencies and context , and 2 hence are the key component of typical <m> sequential data based tasks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .
second:
We present related techniques including data normalization , dimension reduction , classification , and spatial information integration and the way to accommodate these techniques to the context of <m> sequential data collecting and processing </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .
second:
subsection : Recurrent neural networks ( RNN ) Different from Artificial neural network ( ANN ) , RNN , a neural network with recurrent unit , has a better performance in solving many challenging problems involving <m> sequential data analysis </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .
second:
Under DBNs , Bayesian inference provides an optimal framework for <m> sequential data analysis </m> and produces solutions potentially better than other non Bayesian-based methods .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .
second:
Therefore , the introduced algorithm is especially suitable for the applications requiring <m> sequential data processing </m> at large scales/high rates .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .
second:
However , complex enhancement algorithms also require the <m> sequential processing of data </m> and this can not be easily achieved in real-time on a GPU .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .
second:
Furthermore , we develop a solution for generating differentially private sequential data , which will bring us one step closer to publicly available medical datasets via <m> sequential data </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .
second:
These <m> sequential data collection procedures </m> are known by different names , such as sequential experimental design , active learning , or adaptive sensing/sampling .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .
second:
This paper gives a review of the most important methods for blood velocity vector flow imaging ( VFI ) for conventional <m> sequential data acquisition </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .
second:
T-patterns detection analysis provides a <m> sequential analysis of data </m> , illustrating the communicative style and flow of each coach .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .
second:
Recurrent Neural Networks ( RNNs ) capture long dependencies and context , and 2 hence are the key component of typical <m> sequential data based tasks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .
second:
We present related techniques including data normalization , dimension reduction , classification , and spatial information integration and the way to accommodate these techniques to the context of <m> sequential data collecting and processing </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .
second:
subsection : Recurrent neural networks ( RNN ) Different from Artificial neural network ( ANN ) , RNN , a neural network with recurrent unit , has a better performance in solving many challenging problems involving <m> sequential data analysis </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .
second:
Under DBNs , Bayesian inference provides an optimal framework for <m> sequential data analysis </m> and produces solutions potentially better than other non Bayesian-based methods .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .
second:
Therefore , the introduced algorithm is especially suitable for the applications requiring <m> sequential data processing </m> at large scales/high rates .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .
second:
However , complex enhancement algorithms also require the <m> sequential processing of data </m> and this can not be easily achieved in real-time on a GPU .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .
second:
Furthermore , we develop a solution for generating differentially private sequential data , which will bring us one step closer to publicly available medical datasets via <m> sequential data </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .
second:
These <m> sequential data collection procedures </m> are known by different names , such as sequential experimental design , active learning , or adaptive sensing/sampling .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .
second:
This paper gives a review of the most important methods for blood velocity vector flow imaging ( VFI ) for conventional <m> sequential data acquisition </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .
second:
T-patterns detection analysis provides a <m> sequential analysis of data </m> , illustrating the communicative style and flow of each coach .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .
second:
Recurrent Neural Networks ( RNNs ) capture long dependencies and context , and 2 hence are the key component of typical <m> sequential data based tasks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .
second:
We present related techniques including data normalization , dimension reduction , classification , and spatial information integration and the way to accommodate these techniques to the context of <m> sequential data collecting and processing </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .
second:
subsection : Recurrent neural networks ( RNN ) Different from Artificial neural network ( ANN ) , RNN , a neural network with recurrent unit , has a better performance in solving many challenging problems involving <m> sequential data analysis </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .
second:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .


class: second -> first, base class: first -> second, new model class: no relation
first:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: second -> first, base class: first -> second, new model class: no relation
first:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: second -> first, base class: first -> second, new model class: no relation
first:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: second -> first, base class: first -> second, new model class: no relation
first:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: second -> first, base class: first -> second, new model class: no relation
first:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .
second:
Under DBNs , Bayesian inference provides an optimal framework for <m> sequential data analysis </m> and produces solutions potentially better than other non Bayesian-based methods .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .
second:
Therefore , the introduced algorithm is especially suitable for the applications requiring <m> sequential data processing </m> at large scales/high rates .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .
second:
However , complex enhancement algorithms also require the <m> sequential processing of data </m> and this can not be easily achieved in real-time on a GPU .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .
second:
Furthermore , we develop a solution for generating differentially private sequential data , which will bring us one step closer to publicly available medical datasets via <m> sequential data </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .
second:
These <m> sequential data collection procedures </m> are known by different names , such as sequential experimental design , active learning , or adaptive sensing/sampling .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .
second:
This paper gives a review of the most important methods for blood velocity vector flow imaging ( VFI ) for conventional <m> sequential data acquisition </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .
second:
T-patterns detection analysis provides a <m> sequential analysis of data </m> , illustrating the communicative style and flow of each coach .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .
second:
Recurrent Neural Networks ( RNNs ) capture long dependencies and context , and 2 hence are the key component of typical <m> sequential data based tasks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .
second:
We present related techniques including data normalization , dimension reduction , classification , and spatial information integration and the way to accommodate these techniques to the context of <m> sequential data collecting and processing </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .
second:
subsection : Recurrent neural networks ( RNN ) Different from Artificial neural network ( ANN ) , RNN , a neural network with recurrent unit , has a better performance in solving many challenging problems involving <m> sequential data analysis </m> .


class: second -> first, base class: first -> second, new model class: no relation
first:
We claim that these refinement iterations should have a more direct and shared <m> temporal representation </m> .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: second -> first, base class: first -> second, new model class: no relation
first:
We claim that these refinement iterations should have a more direct and shared <m> temporal representation </m> .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: second -> first, base class: first -> second, new model class: no relation
first:
We claim that these refinement iterations should have a more direct and shared <m> temporal representation </m> .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: second -> first, base class: first -> second, new model class: no relation
first:
We claim that these refinement iterations should have a more direct and shared <m> temporal representation </m> .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: second -> first, base class: first -> second, new model class: no relation
first:
We claim that these refinement iterations should have a more direct and shared <m> temporal representation </m> .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: second -> first, base class: no relation, new model class: no relation
first:
Unlike <m> sequential models </m> , recursive neural networks compose word phrases over syntactic tree structure and have shown improved performance in sentiment analysis .
second:
subsection : Recurrent neural networks ( RNN ) Different from Artificial neural network ( ANN ) , RNN , a neural network with recurrent unit , has a better performance in solving many challenging problems involving <m> sequential data analysis </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The results demonstrate that the algorithm can also learn <m> temporal sequence detection </m> .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The results demonstrate that the algorithm can also learn <m> temporal sequence detection </m> .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The results demonstrate that the algorithm can also learn <m> temporal sequence detection </m> .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: second -> first, base class: no relation, new model class: no relation
first:
Proposed networks which perform <m> temporal sequence generation </m> are often in the form of a modification to an auto-associative memory by using heteroassociative or time-varying synaptic strengths , requiring some pre-chosen temporal functions .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: second -> first, base class: no relation, new model class: no relation
first:
Proposed networks which perform <m> temporal sequence generation </m> are often in the form of a modification to an auto-associative memory by using heteroassociative or time-varying synaptic strengths , requiring some pre-chosen temporal functions .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: second -> first, base class: no relation, new model class: no relation
first:
Proposed networks which perform <m> temporal sequence generation </m> are often in the form of a modification to an auto-associative memory by using heteroassociative or time-varying synaptic strengths , requiring some pre-chosen temporal functions .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: second -> first, base class: same cluster, new model class: same cluster
first:
This user modeling approach comprises the following new novel aspects ( 1 ) Modeling microblog users behavior evolution by considering the different event phases ( 2 ) Characterizing users activity over time through a <m> temporal sequence representation </m> ( 3 ) Time-series-based selection of the most discriminative features characterizing users at each event phase .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: second -> first, base class: same cluster, new model class: same cluster
first:
This user modeling approach comprises the following new novel aspects ( 1 ) Modeling microblog users behavior evolution by considering the different event phases ( 2 ) Characterizing users activity over time through a <m> temporal sequence representation </m> ( 3 ) Time-series-based selection of the most discriminative features characterizing users at each event phase .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: second -> first, base class: same cluster, new model class: same cluster
first:
This user modeling approach comprises the following new novel aspects ( 1 ) Modeling microblog users behavior evolution by considering the different event phases ( 2 ) Characterizing users activity over time through a <m> temporal sequence representation </m> ( 3 ) Time-series-based selection of the most discriminative features characterizing users at each event phase .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: second -> first, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .


class: second -> first, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .


class: second -> first, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .


class: second -> first, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: second -> first, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: second -> first, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .


class: second -> first, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .


class: second -> first, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .


class: second -> first, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: second -> first, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: second -> first, base class: no relation, new model class: same cluster
first:
Features can be either signal-based , like spectrograms , or model-based , like <m> categorical classifiers </m> .
second:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .


class: second -> first, base class: no relation, new model class: same cluster
first:
Features can be either signal-based , like spectrograms , or model-based , like <m> categorical classifiers </m> .
second:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
Features can be either signal-based , like spectrograms , or model-based , like <m> categorical classifiers </m> .
second:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .


class: second -> first, base class: no relation, new model class: same cluster
first:
Features can be either signal-based , like spectrograms , or model-based , like <m> categorical classifiers </m> .
second:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .


class: second -> first, base class: no relation, new model class: no relation
first:
Features can be either signal-based , like spectrograms , or model-based , like <m> categorical classifiers </m> .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: second -> first, base class: no relation, new model class: no relation
first:
Features can be either signal-based , like spectrograms , or model-based , like <m> categorical classifiers </m> .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: second -> first, base class: no relation, new model class: same cluster
first:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .
second:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .


class: second -> first, base class: no relation, new model class: same cluster
first:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .
second:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .
second:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .


class: second -> first, base class: no relation, new model class: same cluster
first:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .
second:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .


class: second -> first, base class: no relation, new model class: no relation
first:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: second -> first, base class: no relation, new model class: no relation
first:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .
second:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .
second:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .
second:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .
second:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: second -> first, base class: no relation, new model class: no relation
first:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .
second:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .


class: second -> first, base class: no relation, new model class: no relation
first:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .
second:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .
second:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .


class: second -> first, base class: no relation, new model class: no relation
first:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .
second:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .


class: second -> first, base class: no relation, new model class: no relation
first:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: second -> first, base class: no relation, new model class: no relation
first:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: second -> first, base class: same cluster, new model class: no relation
first:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .
second:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .


class: second -> first, base class: same cluster, new model class: no relation
first:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .
second:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .


class: second -> first, base class: same cluster, new model class: no relation
first:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .
second:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .


class: second -> first, base class: same cluster, new model class: no relation
first:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .
second:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .


class: second -> first, base class: no relation, new model class: no relation
first:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: second -> first, base class: no relation, new model class: no relation
first:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Basic methods of the research : <m> categorical analysis </m> , determining of statistic parameters of word-forming models .
second:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Basic methods of the research : <m> categorical analysis </m> , determining of statistic parameters of word-forming models .
second:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Basic methods of the research : <m> categorical analysis </m> , determining of statistic parameters of word-forming models .
second:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .


class: second -> first, base class: no relation, new model class: no relation
first:
Basic methods of the research : <m> categorical analysis </m> , determining of statistic parameters of word-forming models .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: second -> first, base class: no relation, new model class: no relation
first:
Basic methods of the research : <m> categorical analysis </m> , determining of statistic parameters of word-forming models .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , two centroid-based classifiers are proposed for <m> categorical data classification </m> .
second:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .


class: second -> first, base class: no relation, new model class: same cluster
first:
In this paper , two centroid-based classifiers are proposed for <m> categorical data classification </m> .
second:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper , two centroid-based classifiers are proposed for <m> categorical data classification </m> .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: second -> first, base class: no relation, new model class: no relation
first:
In this paper , two centroid-based classifiers are proposed for <m> categorical data classification </m> .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: second -> first, base class: no relation, new model class: same cluster
first:
Analysis of quantitative ( Bias , Relative RMSE ) and <m> categorical statistics </m> ( POD , FAR ) for the whole period show a more accurate spatial distribution of mean daily rainfall estimations in the lowlands than in the Andean regions .
second:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .


class: second -> first, base class: no relation, new model class: same cluster
first:
Analysis of quantitative ( Bias , Relative RMSE ) and <m> categorical statistics </m> ( POD , FAR ) for the whole period show a more accurate spatial distribution of mean daily rainfall estimations in the lowlands than in the Andean regions .
second:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .


class: second -> first, base class: no relation, new model class: no relation
first:
Analysis of quantitative ( Bias , Relative RMSE ) and <m> categorical statistics </m> ( POD , FAR ) for the whole period show a more accurate spatial distribution of mean daily rainfall estimations in the lowlands than in the Andean regions .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: second -> first, base class: no relation, new model class: no relation
first:
Analysis of quantitative ( Bias , Relative RMSE ) and <m> categorical statistics </m> ( POD , FAR ) for the whole period show a more accurate spatial distribution of mean daily rainfall estimations in the lowlands than in the Andean regions .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .
second:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .
second:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .


class: second -> first, base class: no relation, new model class: no relation
first:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: second -> first, base class: no relation, new model class: no relation
first:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: second -> first, base class: same cluster, new model class: no relation
first:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .
second:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .


class: second -> first, base class: no relation, new model class: no relation
first:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: second -> first, base class: no relation, new model class: no relation
first:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .
second:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: second -> first, base class: no relation, new model class: no relation
first:
section : Inference Across all of the presented experiments , we use <m> categorical sampling </m> during decoding with a tempered PixelRecursiveSuperResolution .
second:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .


class: second -> first, base class: no relation, new model class: no relation
first:
section : Inference Across all of the presented experiments , we use <m> categorical sampling </m> during decoding with a tempered PixelRecursiveSuperResolution .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: second -> first, base class: no relation, new model class: no relation
first:
section : Inference Across all of the presented experiments , we use <m> categorical sampling </m> during decoding with a tempered PixelRecursiveSuperResolution .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> categorical parameterization </m> , using the projected KL loss , has also been used in recent work to improve the critic of a policy gradient algorithm , D4PG , achieving significantly improved robustness and state - of - the - art performance across a variety of continuous control tasks .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> categorical parameterization </m> , using the projected KL loss , has also been used in recent work to improve the critic of a policy gradient algorithm , D4PG , achieving significantly improved robustness and state - of - the - art performance across a variety of continuous control tasks .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> categorical parameterization </m> , using the projected KL loss , has also been used in recent work to improve the critic of a policy gradient algorithm , D4PG , achieving significantly improved robustness and state - of - the - art performance across a variety of continuous control tasks .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: second -> first, base class: no relation, new model class: no relation
first:
The <m> categorical parameterization </m> , using the projected KL loss , has also been used in recent work to improve the critic of a policy gradient algorithm , D4PG , achieving significantly improved robustness and state - of - the - art performance across a variety of continuous control tasks .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : PixelGAN Autoencoders with <m> Categorical Priors </m> In this section , we present an architecture of the PixelGAN autoencoder that can separate the discrete information ( e.g. , class label ) from the continuous information ( e.g. , style information ) in the images .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : PixelGAN Autoencoders with <m> Categorical Priors </m> In this section , we present an architecture of the PixelGAN autoencoder that can separate the discrete information ( e.g. , class label ) from the continuous information ( e.g. , style information ) in the images .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : PixelGAN Autoencoders with <m> Categorical Priors </m> In this section , we present an architecture of the PixelGAN autoencoder that can separate the discrete information ( e.g. , class label ) from the continuous information ( e.g. , style information ) in the images .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : PixelGAN Autoencoders with <m> Categorical Priors </m> In this section , we present an architecture of the PixelGAN autoencoder that can separate the discrete information ( e.g. , class label ) from the continuous information ( e.g. , style information ) in the images .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: second -> first, base class: no relation, new model class: no relation
first:
Aiming at this problem , a novel <m> numerical feature extraction approach </m> EPLS is proposed .
second:
Conventional analyses address discriminative tasks of behaviors , e.g. , classification and clustering typically using the subsequences extracted from the trajectory of an object as a <m> numerical feature representation </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Aiming at this problem , a novel <m> numerical feature extraction approach </m> EPLS is proposed .
second:
Experimental results show that the proposed method presents an efficient and effective solution for <m> numerical feature analysis </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Aiming at this problem , a novel <m> numerical feature extraction approach </m> EPLS is proposed .
second:
Here , we present iFeature , a versatile Python‐based toolkit for generating various <m> numerical feature representation schemes </m> for both protein and peptide sequences .


class: second -> first, base class: no relation, new model class: no relation
first:
Aiming at this problem , a novel <m> numerical feature extraction approach </m> EPLS is proposed .
second:
Experimental comparisons also show the new method can significantly outperform standard <m> numerical feature-type methods </m> in terms of agreement with the clinical diagnosis gold standard .


class: second -> first, base class: no relation, new model class: no relation
first:
Aiming at this problem , a novel <m> numerical feature extraction approach </m> EPLS is proposed .
second:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .


class: second -> first, base class: no relation, new model class: no relation
first:
Aiming at this problem , a novel <m> numerical feature extraction approach </m> EPLS is proposed .
second:
MPEG-7 visual descriptors are <m> numerical representations of features </m> - such as : texture , shape and color - extracted from an image .


class: second -> first, base class: no relation, new model class: no relation
first:
Aiming at this problem , a novel <m> numerical feature extraction approach </m> EPLS is proposed .
second:
A text parser to match keywords against vocabularies and <m> numerical value descriptors </m> is introduced .


class: second -> first, base class: no relation, new model class: no relation
first:
Aiming at this problem , a novel <m> numerical feature extraction approach </m> EPLS is proposed .
second:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .


class: second -> first, base class: no relation, new model class: no relation
first:
Aiming at this problem , a novel <m> numerical feature extraction approach </m> EPLS is proposed .
second:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .


class: second -> first, base class: no relation, new model class: no relation
first:
Aiming at this problem , a novel <m> numerical feature extraction approach </m> EPLS is proposed .
second:
While previous studies have considered categorical emotions , expressive speech during human interaction conveys subtle behaviors that are better characterized with <m> continuous descriptors </m> ( e.g. , attributes such as arousal , valence , dominance ) .


class: second -> first, base class: no relation, new model class: no relation
first:
Aiming at this problem , a novel <m> numerical feature extraction approach </m> EPLS is proposed .
second:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Aiming at this problem , a novel <m> numerical feature extraction approach </m> EPLS is proposed .
second:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Aiming at this problem , a novel <m> numerical feature extraction approach </m> EPLS is proposed .
second:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .


class: second -> first, base class: no relation, new model class: no relation
first:
Aiming at this problem , a novel <m> numerical feature extraction approach </m> EPLS is proposed .
second:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .


class: second -> first, base class: no relation, new model class: no relation
first:
Aiming at this problem , a novel <m> numerical feature extraction approach </m> EPLS is proposed .
second:
The main goal of this dissertation work is to provide syntactically and semantically related words based on <m> continuous feature vector representation </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Aiming at this problem , a novel <m> numerical feature extraction approach </m> EPLS is proposed .
second:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: second -> first, base class: no relation, new model class: no relation
first:
Aiming at this problem , a novel <m> numerical feature extraction approach </m> EPLS is proposed .
second:
Here , the encoder maps an input sequence of symbol representations to a sequence of <m> continuous representations </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
For a <m> continuous feature extraction </m> , we use a template matching of the nose bridge in combination with selected features derived from the optical flow .
second:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
For a <m> continuous feature extraction </m> , we use a template matching of the nose bridge in combination with selected features derived from the optical flow .
second:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .


class: second -> first, base class: no relation, new model class: no relation
first:
For a <m> continuous feature extraction </m> , we use a template matching of the nose bridge in combination with selected features derived from the optical flow .
second:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .


class: second -> first, base class: no relation, new model class: no relation
first:
For a <m> continuous feature extraction </m> , we use a template matching of the nose bridge in combination with selected features derived from the optical flow .
second:
The main goal of this dissertation work is to provide syntactically and semantically related words based on <m> continuous feature vector representation </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
For a <m> continuous feature extraction </m> , we use a template matching of the nose bridge in combination with selected features derived from the optical flow .
second:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: second -> first, base class: no relation, new model class: no relation
first:
For a <m> continuous feature extraction </m> , we use a template matching of the nose bridge in combination with selected features derived from the optical flow .
second:
Here , the encoder maps an input sequence of symbol representations to a sequence of <m> continuous representations </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: second -> first, base class: no relation, new model class: no relation
first:
Most of the popular spam filters work depending on individual training corpus or <m> discrete feature extraction </m> , and these filters have n't considered spam 's bulk feature .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: second -> first, base class: same cluster, new model class: no relation
first:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: second -> first, base class: same cluster, new model class: no relation
first:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: second -> first, base class: same cluster, new model class: no relation
first:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: second -> first, base class: no relation, new model class: no relation
first:
By using steerable filters this search space may be scanned continouosly , though spanned by <m> discrete feature detectors </m> .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: second -> first, base class: same cluster, new model class: same cluster
first:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: second -> first, base class: same cluster, new model class: same cluster
first:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: second -> first, base class: same cluster, new model class: no relation
first:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: second -> first, base class: no relation, new model class: no relation
first:
Two popular contemporary algorithms for <m> discrete feature detection </m> : SIFT and SURF are regarded .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: second -> first, base class: no relation, new model class: no relation
first:
Quantitative attributes were extracted from protein sequences by applying three <m> discrete features extraction schemes </m> namely amino acid composition , pseudo amino acid composition and split amino acid composition ( SAAC ) .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: first -> second, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: same cluster, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: same cluster, new model class: same cluster
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .


class: second -> first, base class: same cluster, new model class: same cluster
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .


class: second -> first, base class: same cluster, new model class: same cluster
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .


class: second -> first, base class: same cluster, new model class: same cluster
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .


class: second -> first, base class: same cluster, new model class: same cluster
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .


class: second -> first, base class: same cluster, new model class: same cluster
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup


class: second -> first, base class: same cluster, new model class: same cluster
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .


class: second -> first, base class: same cluster, new model class: same cluster
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .


class: second -> first, base class: same cluster, new model class: same cluster
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .


class: second -> first, base class: same cluster, new model class: same cluster
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .


class: second -> first, base class: no relation, new model class: same cluster
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: second -> first, base class: no relation, new model class: first -> second
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: second -> first, base class: no relation, new model class: same cluster
first:
This chapter describes the problem of <m> machine learning of models </m> from aggregated data as compared to traditional learning from individual examples .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup


class: second -> first, base class: same cluster, new model class: same cluster
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .


class: second -> first, base class: no relation, new model class: same cluster
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: second -> first, base class: no relation, new model class: first -> second
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: second -> first, base class: no relation, new model class: same cluster
first:
The lists of recommended terms are ordered by a ranking model which is computed using the <m> machine learning approach Learning </m> To Rank ( L2R ) .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: same cluster, new model class: same cluster
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup


class: second -> first, base class: same cluster, new model class: same cluster
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .


class: second -> first, base class: same cluster, new model class: same cluster
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .


class: second -> first, base class: same cluster, new model class: same cluster
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .


class: second -> first, base class: same cluster, new model class: same cluster
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .


class: second -> first, base class: no relation, new model class: same cluster
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: second -> first, base class: no relation, new model class: first -> second
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: second -> first, base class: no relation, new model class: same cluster
first:
This is similar to a standard <m> machine learning problem of learning </m> from finite samples .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: first -> second, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: first -> second, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
<m> Artificial intelligence </m> is increasingly becoming a part of everyday life .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
Affective Computing is a new <m> Artificial Intelligence area </m> that deals with the possibility of making computers able to recognize human emotions in different ways .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
Swarm Intelligence is an <m> artificial intelligence discipline </m> which is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviours of social insects and other animal societies .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
Related Concepts of <m> Artificial Intelligence Artificial intelligence </m> mainly refers to the use of computing technology in order to achieve human thinking and behavior and achieve the intelligent computer technology through the simulation of computer technology .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
In the field of water resources and hydrology , <m> Artificial Intelligence system approaches </m> such as fuzzy logic ( FL ) , Artificial Neural Network ( ANN ) , etc . have been used successfully for nonlinear and non-stationary time series modeling .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
Artificial neural networks ( ANNs ) are a form of <m> artificial computer intelligence </m> which are the mathematical algorithms , generated by computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
Next , we included new <m> Artificial Intelligence ( IA ) techniques </m> with semantic interpretation and metrical testing to supply the user with a CASE ( Computer Aided Software Engineering ) tool with the ability of generating intelligent computerized information systems , which can learn from information from intelligent searches in ontology servers , case bases , specialist in the area , or even from fragmented data from the Internet , with the purpose of enhancing decision taking .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
This document briefly describes FrameLib , a C++ library to manipulate " frames " as they are used in the <m> Artificial Intelligence world </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
So , the visual question - answering tasks let the studies in <m> artificial intelligence </m> go beyond narrow tasks .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
Recent developments in Artificial Intelligence ( <m> AI </m> ) have resulted in breakthroughs in applications such as computer vision , natural language processing , robotics , and data mining .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: first -> second, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: same cluster, new model class: same cluster
first:
The ongoing developments in the area of <m> AI/AS </m> are critically evaluated and related advantages and serious concerns of the society are discussed .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: no relation, new model class: no relation
first:
A classic problem , the spatial movement to goal problem , is used to illustrate the power of causal learning in vastly reducing the problem solving search space involved , and this is contrasted with the traditional <m> AI A * algorithm </m> which requires a huge search space .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: second -> first, base class: no relation, new model class: no relation
first:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: second -> first, base class: no relation, new model class: no relation
first:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: second -> first, base class: no relation, new model class: no relation
first:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: second -> first, base class: no relation, new model class: no relation
first:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: second -> first, base class: no relation, new model class: no relation
first:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: second -> first, base class: no relation, new model class: no relation
first:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: second -> first, base class: no relation, new model class: no relation
first:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: no relation, new model class: no relation
first:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: no relation, new model class: no relation
first:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: no relation, new model class: no relation
first:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: no relation, new model class: no relation
first:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: no relation, new model class: no relation
first:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: no relation, new model class: no relation
first:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: no relation, new model class: no relation
first:
Combination of <m> AI/OR methods </m> is gaining a great deal of attention because many combinatorial problems especially in planning and scheduling areas can be solved by means of combined AI/OR techniques .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: no relation, new model class: same cluster
first:
An integration architecture tactic to guard <m> AI-first components </m> .
second:
<m> AI itself </m> is the senses cognitive and machine learning process in problem- solving .


class: second -> first, base class: no relation, new model class: same cluster
first:
An integration architecture tactic to guard <m> AI-first components </m> .
second:
Then the system architecture for the AI-enabled emotion communication is introduced in detail , and the no-tag learning model of dataset labeling and processing as well as the <m> AI algorithm model </m> for emotion recognition are elaborated in detail , and experiments are done to verify the interactive delay in the AI-enabled emotion communication system and accuracy of emotion recognition .


class: second -> first, base class: no relation, new model class: same cluster
first:
An integration architecture tactic to guard <m> AI-first components </m> .
second:
For robotics and <m> AI especially </m> this means that if the power behind the cloud could be harnessed , it would be possible to build smaller , more battery effective robots because there would be no need to have a powerful computer on board , but the brain of the robot can be in the cloud .


class: second -> first, base class: no relation, new model class: same cluster
first:
An integration architecture tactic to guard <m> AI-first components </m> .
second:
The <m> AI layer </m> accomplishes this by selecting and applying appropriate methods from the SID library and performing qualitative , symbolic , algebraic , and geometric reasoning on the user 's inputs .


class: second -> first, base class: no relation, new model class: same cluster
first:
An integration architecture tactic to guard <m> AI-first components </m> .
second:
They argued unlike physics , areas in <m> AI </m> are more likely to see an impact using more data - driven approaches .


class: second -> first, base class: no relation, new model class: same cluster
first:
An integration architecture tactic to guard <m> AI-first components </m> .
second:
What makes for a compelling " AI - complete " task ? We believe that in order to spawn the next generation of <m> AI algorithms </m> , an ideal task should ( i ) require multi - modal knowledge beyond a single sub - domain ( such as CV ) and ( ii ) have a well - defined quantitative evaluation metric to track progress .


class: second -> first, base class: no relation, new model class: same cluster
first:
An integration architecture tactic to guard <m> AI-first components </m> .
second:
On the other hand , it became immediately clear that everybody has strong feelings or at least a qualified opinion about the languages which he or she uses to create <m> AI programs </m> .


class: second -> first, base class: no relation, new model class: same cluster
first:
An integration architecture tactic to guard <m> AI-first components </m> .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: no relation, new model class: same cluster
first:
An integration architecture tactic to guard <m> AI-first components </m> .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: no relation, new model class: same cluster
first:
An integration architecture tactic to guard <m> AI-first components </m> .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: no relation, new model class: same cluster
first:
An integration architecture tactic to guard <m> AI-first components </m> .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: no relation, new model class: same cluster
first:
An integration architecture tactic to guard <m> AI-first components </m> .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: no relation, new model class: same cluster
first:
An integration architecture tactic to guard <m> AI-first components </m> .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: no relation, new model class: same cluster
first:
An integration architecture tactic to guard <m> AI-first components </m> .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: no relation, new model class: same cluster
first:
An integration architecture tactic to guard <m> AI-first components </m> .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .
second:
The question thus becomes the following : is the act of training a model for <m> ML purposes </m> a copyright relevant activity ?


class: second -> first, base class: same cluster, new model class: first -> second
first:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We do so by applying the maximum likelihood ( <m> ML </m> ) in order to obtain the best performance achievable .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Deploying a received signal model that takes care of flat fading , the proposed scheme first uses a <m> ML-based approach </m> to obtain a ML estimate of the ratio of jamming fading gains .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: second -> first, base class: no relation, new model class: no relation
first:
subsection : Architecture - I ( Arc - I ) Architecture - I ( Arc - I ) , as illustrated in Figure [ reference ] , takes a conventional approach : It first finds the representation of each sentence , and then compares the representation for the two sentences with a multi - layer perceptron ( <m> MLP </m> ) .
second:
Most researchers in <m> ML </m> use average overall accuracy .


class: second -> first, base class: no relation, new model class: same cluster
first:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .
second:
On the contrary , all <m> AI oriented programs </m> should fit in .


class: second -> first, base class: no relation, new model class: same cluster
first:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .
second:
This fact basically explains why expert systems and other <m> AI-application programs </m> implemented in Prolog run very slowly on traditional general-purpose computers .


class: second -> first, base class: no relation, new model class: same cluster
first:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .
second:
Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development : Future research is needed to evaluate all <m> AI based programs </m> before clinical implementation in non-research settings .


class: second -> first, base class: no relation, new model class: same cluster
first:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .
second:
Chess , Atari2600 and Go , are some of the most mediatic demonstrations where <m> AI computer programs </m> defeated human players .


class: second -> first, base class: no relation, new model class: same cluster
first:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .
second:
The level of awareness held by Google is explored and some recent developments in the uses of <m> AI programmes </m> for social media are covered .


class: second -> first, base class: no relation, new model class: same cluster
first:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .
second:
Abstract Prolog is one of the most important candidates to build expert systems and <m> AI-related programs </m> and has potential applications in embedded systems .


class: second -> first, base class: no relation, new model class: same cluster
first:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .
second:
The intent for developing such criteria is to have a logical rationale that may be used to guide the development of Ada tools and methodology to support KBS requirements , and to identify those <m> AI technology components </m> that may most readily and effectively be deployed in Ada versus those best left in a functional language .


class: second -> first, base class: no relation, new model class: same cluster
first:
We compared the performance of the modified <m> AI code </m> , and the results show that the new AI code outperforms the existing artificial intelligence of the game .
second:
Our hope is that by using the methodology advocated in this paper , we can work in a bottom - up fashion towards developing more sophisticated <m> AI technology </m> while still maintaining empirical rigor .


class: second -> first, base class: no relation, new model class: no relation
first:
Airlight vector was estimated based on statistics of dark channel prior at first , and then the atmospheric veil was inferred by the <m> estimation of fast bilateral filter </m> .
second:
Iris location is a <m> kernel procession </m> in an iris recognition system .


class: second -> first, base class: no relation, new model class: no relation
first:
Airlight vector was estimated based on statistics of dark channel prior at first , and then the atmospheric veil was inferred by the <m> estimation of fast bilateral filter </m> .
second:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .


class: second -> first, base class: no relation, new model class: no relation
first:
Airlight vector was estimated based on statistics of dark channel prior at first , and then the atmospheric veil was inferred by the <m> estimation of fast bilateral filter </m> .
second:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .


class: second -> first, base class: no relation, new model class: no relation
first:
Airlight vector was estimated based on statistics of dark channel prior at first , and then the atmospheric veil was inferred by the <m> estimation of fast bilateral filter </m> .
second:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .


class: second -> first, base class: no relation, new model class: no relation
first:
Airlight vector was estimated based on statistics of dark channel prior at first , and then the atmospheric veil was inferred by the <m> estimation of fast bilateral filter </m> .
second:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .


class: second -> first, base class: no relation, new model class: no relation
first:
In the feature extraction , iris feature information is extracted by the method of <m> 2D Log-Gabo filter </m> which can generate multi-channel filter though multi-scale and multiple directions , the method also can reduce the length of the iris code and the time of feature matching , improve the feature matching accuracy , can effectively improve the accuracy of iris recognition .
second:
Iris location is a <m> kernel procession </m> in an iris recognition system .


class: second -> first, base class: no relation, new model class: no relation
first:
In the feature extraction , iris feature information is extracted by the method of <m> 2D Log-Gabo filter </m> which can generate multi-channel filter though multi-scale and multiple directions , the method also can reduce the length of the iris code and the time of feature matching , improve the feature matching accuracy , can effectively improve the accuracy of iris recognition .
second:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .


class: second -> first, base class: no relation, new model class: no relation
first:
In the feature extraction , iris feature information is extracted by the method of <m> 2D Log-Gabo filter </m> which can generate multi-channel filter though multi-scale and multiple directions , the method also can reduce the length of the iris code and the time of feature matching , improve the feature matching accuracy , can effectively improve the accuracy of iris recognition .
second:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .


class: second -> first, base class: no relation, new model class: no relation
first:
In the feature extraction , iris feature information is extracted by the method of <m> 2D Log-Gabo filter </m> which can generate multi-channel filter though multi-scale and multiple directions , the method also can reduce the length of the iris code and the time of feature matching , improve the feature matching accuracy , can effectively improve the accuracy of iris recognition .
second:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .


class: second -> first, base class: no relation, new model class: no relation
first:
In the feature extraction , iris feature information is extracted by the method of <m> 2D Log-Gabo filter </m> which can generate multi-channel filter though multi-scale and multiple directions , the method also can reduce the length of the iris code and the time of feature matching , improve the feature matching accuracy , can effectively improve the accuracy of iris recognition .
second:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .


class: second -> first, base class: no relation, new model class: no relation
first:
It first depicts the performance of the proposed models on different length of sentences , and then conducts a sensitivity analysis of <m> 2D filter </m> and max pooling size .
second:
Before the DU - Net , a <m> Conv ( ) filter </m> with stride 2 and a max pooling would produce 128 features with resolution 64 64 .


class: second -> first, base class: no relation, new model class: no relation
first:
It first depicts the performance of the proposed models on different length of sentences , and then conducts a sensitivity analysis of <m> 2D filter </m> and max pooling size .
second:
Moreover , we give some model analysis on the <m> filter size configuration </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
It first depicts the performance of the proposed models on different length of sentences , and then conducts a sensitivity analysis of <m> 2D filter </m> and max pooling size .
second:
This was achieved by iterating over the differences using a <m> maximum filter </m> with decreasing filter size until 20 frames have been found .


class: second -> first, base class: no relation, new model class: no relation
first:
It first depicts the performance of the proposed models on different length of sentences , and then conducts a sensitivity analysis of <m> 2D filter </m> and max pooling size .
second:
. , ŷ ll , n ) is a diagonal matrix of spectral multipliers representing a <m> learnable filter </m> in the spectral domain , and ξ is a nonlinearity ( e.g. ReLU ) applied on the vertex - wise function values .


class: second -> first, base class: no relation, new model class: no relation
first:
Sometimes methods adopt a more generic <m> filter constancy assumption </m> .
second:
Before the DU - Net , a <m> Conv ( ) filter </m> with stride 2 and a max pooling would produce 128 features with resolution 64 64 .


class: second -> first, base class: no relation, new model class: no relation
first:
Sometimes methods adopt a more generic <m> filter constancy assumption </m> .
second:
Moreover , we give some model analysis on the <m> filter size configuration </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Sometimes methods adopt a more generic <m> filter constancy assumption </m> .
second:
This was achieved by iterating over the differences using a <m> maximum filter </m> with decreasing filter size until 20 frames have been found .


class: second -> first, base class: no relation, new model class: no relation
first:
Sometimes methods adopt a more generic <m> filter constancy assumption </m> .
second:
. , ŷ ll , n ) is a diagonal matrix of spectral multipliers representing a <m> learnable filter </m> in the spectral domain , and ξ is a nonlinearity ( e.g. ReLU ) applied on the vertex - wise function values .


class: second -> first, base class: no relation, new model class: no relation
first:
Unlike activations of intermediate layers of a CNN , <m> linear filter weights </m> have a well defined semantics that can be visualized and analyzed using well developed tools of linear signal processing .
second:
. , ŷ ll , n ) is a diagonal matrix of spectral multipliers representing a <m> learnable filter </m> in the spectral domain , and ξ is a nonlinearity ( e.g. ReLU ) applied on the vertex - wise function values .


class: second -> first, base class: no relation, new model class: no relation
first:
The next step of the mean - field iteration is taking a weighted sum of the M filter outputs from the previous step , for each class label l. When each class label is considered individually , this can be viewed as usual convolution with a <m> 1 × 1 filter </m> with M input channels , and one output channel .
second:
Such kernels are built with a parameterized “ neural response ” function , which consists in computing the maximal response of a <m> base kernel </m> over a local neighborhood .


class: second -> first, base class: no relation, new model class: no relation
first:
The next step of the mean - field iteration is taking a weighted sum of the M filter outputs from the previous step , for each class label l. When each class label is considered individually , this can be viewed as usual convolution with a <m> 1 × 1 filter </m> with M input channels , and one output channel .
second:
a <m> kernel size </m> c × f 1 ×


class: second -> first, base class: no relation, new model class: no relation
first:
The next step of the mean - field iteration is taking a weighted sum of the M filter outputs from the previous step , for each class label l. When each class label is considered individually , this can be viewed as usual convolution with a <m> 1 × 1 filter </m> with M input channels , and one output channel .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: second -> first, base class: no relation, new model class: no relation
first:
The next step of the mean - field iteration is taking a weighted sum of the M filter outputs from the previous step , for each class label l. When each class label is considered individually , this can be viewed as usual convolution with a <m> 1 × 1 filter </m> with M input channels , and one output channel .
second:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .


class: second -> first, base class: no relation, new model class: no relation
first:
The next step of the mean - field iteration is taking a weighted sum of the M filter outputs from the previous step , for each class label l. When each class label is considered individually , this can be viewed as usual convolution with a <m> 1 × 1 filter </m> with M input channels , and one output channel .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: second -> first, base class: no relation, new model class: no relation
first:
In addition , not using an explicit <m> interpolation filter </m> means that the network implicitly learns the processing necessary for SR .
second:
. , ŷ ll , n ) is a diagonal matrix of spectral multipliers representing a <m> learnable filter </m> in the spectral domain , and ξ is a nonlinearity ( e.g. ReLU ) applied on the vertex - wise function values .


class: second -> first, base class: no relation, new model class: no relation
first:
Abstract Let U n be U -statistics based on a <m> symmetric kernel h </m> ( x , y ) and i.i.d . samples { X n ; n ≥ 1 } .
second:
Iris location is a <m> kernel procession </m> in an iris recognition system .


class: second -> first, base class: no relation, new model class: no relation
first:
Abstract Let U n be U -statistics based on a <m> symmetric kernel h </m> ( x , y ) and i.i.d . samples { X n ; n ≥ 1 } .
second:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .


class: second -> first, base class: no relation, new model class: no relation
first:
The candidate models belong to the Neyman-Scott Process ( NSP ) and results show that the NSP with <m> variance gamma kernel </m> is the most accurate for matching the microcells distribution for different scenarios .
second:
Iris location is a <m> kernel procession </m> in an iris recognition system .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed <m> online kernel method </m> is evaluated by handling the Cascades Tanks system .
second:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed <m> online kernel method </m> is evaluated by handling the Cascades Tanks system .
second:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed <m> online kernel method </m> is evaluated by handling the Cascades Tanks system .
second:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .


class: second -> first, base class: no relation, new model class: no relation
first:
The proposed <m> online kernel method </m> is evaluated by handling the Cascades Tanks system .
second:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .


class: second -> first, base class: no relation, new model class: no relation
first:
In the constant bit rate encoder , the <m> kernel MPEG encoder circuit </m> is followed by a buffer having a certain actual capacity .
second:
Iris location is a <m> kernel procession </m> in an iris recognition system .


class: second -> first, base class: no relation, new model class: no relation
first:
In the constant bit rate encoder , the <m> kernel MPEG encoder circuit </m> is followed by a buffer having a certain actual capacity .
second:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .


class: second -> first, base class: no relation, new model class: no relation
first:
Kernel online convex optimization ( KOCO ) is a framework combining the expressiveness of <m> non-parametric kernel models </m> with the regret guarantees of online learning .
second:
Iris location is a <m> kernel procession </m> in an iris recognition system .


class: second -> first, base class: no relation, new model class: no relation
first:
Kernel online convex optimization ( KOCO ) is a framework combining the expressiveness of <m> non-parametric kernel models </m> with the regret guarantees of online learning .
second:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Linear-vertex kernel </m> .
second:
Iris location is a <m> kernel procession </m> in an iris recognition system .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Linear-vertex kernel </m> .
second:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Linear-vertex kernel </m> .
second:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Linear-vertex kernel </m> .
second:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Linear-vertex kernel </m> .
second:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .


class: second -> first, base class: no relation, new model class: no relation
first:
Using tools from representation theory , we explain the limited expressive power of the Kendall kernel by characterizing its degenerate spectrum , and in sharp contrast , we prove that <m> Mallows ' kernel </m> is universal and characteristic .
second:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .


class: second -> first, base class: no relation, new model class: no relation
first:
Using tools from representation theory , we explain the limited expressive power of the Kendall kernel by characterizing its degenerate spectrum , and in sharp contrast , we prove that <m> Mallows ' kernel </m> is universal and characteristic .
second:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .


class: second -> first, base class: no relation, new model class: no relation
first:
Using tools from representation theory , we explain the limited expressive power of the Kendall kernel by characterizing its degenerate spectrum , and in sharp contrast , we prove that <m> Mallows ' kernel </m> is universal and characteristic .
second:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .


class: second -> first, base class: no relation, new model class: no relation
first:
For <m> graphlet kernel </m> ( GK ) , we chose graphlets size and for deep graph kernels ( DGK ) , we report the best classification accuracy obtained among : deep graphlet kernel , deep shortest path kernel and deep Weisfeiler - Lehman kernel .
second:
a <m> kernel size </m> c × f 1 ×


class: second -> first, base class: no relation, new model class: no relation
first:
For <m> graphlet kernel </m> ( GK ) , we chose graphlets size and for deep graph kernels ( DGK ) , we report the best classification accuracy obtained among : deep graphlet kernel , deep shortest path kernel and deep Weisfeiler - Lehman kernel .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: second -> first, base class: no relation, new model class: no relation
first:
For <m> graphlet kernel </m> ( GK ) , we chose graphlets size and for deep graph kernels ( DGK ) , we report the best classification accuracy obtained among : deep graphlet kernel , deep shortest path kernel and deep Weisfeiler - Lehman kernel .
second:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .


class: second -> first, base class: no relation, new model class: no relation
first:
For <m> graphlet kernel </m> ( GK ) , we chose graphlets size and for deep graph kernels ( DGK ) , we report the best classification accuracy obtained among : deep graphlet kernel , deep shortest path kernel and deep Weisfeiler - Lehman kernel .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: second -> first, base class: no relation, new model class: no relation
first:
Different from image deblurring , the <m> blur kernel setting </m> of SISR is usually simple .
second:
a <m> kernel size </m> c × f 1 ×


class: second -> first, base class: same cluster, new model class: no relation
first:
For example , defines a <m> kernel with dilation </m> .
second:
a <m> kernel size </m> c × f 1 ×


class: second -> first, base class: no relation, new model class: no relation
first:
For example , defines a <m> kernel with dilation </m> .
second:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .


class: second -> first, base class: no relation, new model class: no relation
first:
For example , defines a <m> kernel with dilation </m> .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: second -> first, base class: no relation, new model class: no relation
first:
We apply a <m> 5×5 kernel </m> with 512 outputs , followed by sibling fully connected layers to predict a 14×14 mask ( 14 2 outputs ) and object score ( 1 output ) .
second:
a <m> kernel size </m> c × f 1 ×


class: second -> first, base class: no relation, new model class: no relation
first:
We apply a <m> 5×5 kernel </m> with 512 outputs , followed by sibling fully connected layers to predict a 14×14 mask ( 14 2 outputs ) and object score ( 1 output ) .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: second -> first, base class: no relation, new model class: no relation
first:
We apply a <m> 5×5 kernel </m> with 512 outputs , followed by sibling fully connected layers to predict a 14×14 mask ( 14 2 outputs ) and object score ( 1 output ) .
second:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .


class: second -> first, base class: no relation, new model class: no relation
first:
We apply a <m> 5×5 kernel </m> with 512 outputs , followed by sibling fully connected layers to predict a 14×14 mask ( 14 2 outputs ) and object score ( 1 output ) .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Moreover , since it does not need <m> word segmentation </m> , it can be applied to Eastern languages where they do not put clear spacing between words .
second:
Among the different techniques , the <m> word/sub-word segmentation </m> is simple and produces very good results .


class: second -> first, base class: no relation, new model class: no relation
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
Inspired by three characteristics , we develop three steps to <m> word and subword segmentation approach </m> : in the word or sub-words segmentation step , a peaks detection function is adopted to model the maximum and minimum peaks .


class: second -> first, base class: no relation, new model class: same cluster
first:
So far , the only way to overcome this BPE imperfection , its deterministic nature , was to create another <m> subword segmentation algorithm </m> ( Kudo , 2018 ) .
second:
Among the different techniques , the <m> word/sub-word segmentation </m> is simple and produces very good results .


class: second -> first, base class: no relation, new model class: no relation
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
Inspired by three characteristics , we develop three steps to <m> word and subword segmentation approach </m> : in the word or sub-words segmentation step , a peaks detection function is adopted to model the maximum and minimum peaks .


class: second -> first, base class: no relation, new model class: same cluster
first:
There are several approaches for improving neural machine translation for low-resource languages : Monolingual data can be exploited via pretraining or data augmentation ; Parallel corpora on related language pairs can be used via parameter sharing or transfer learning in multilingual models ; <m> Subword segmentation </m> and regularization techniques can be applied to ensure high coverage of the vocabulary .
second:
Among the different techniques , the <m> word/sub-word segmentation </m> is simple and produces very good results .


class: second -> first, base class: no relation, new model class: no relation
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
Inspired by three characteristics , we develop three steps to <m> word and subword segmentation approach </m> : in the word or sub-words segmentation step , a peaks detection function is adopted to model the maximum and minimum peaks .


class: second -> first, base class: no relation, new model class: same cluster
first:
The final experiments are conducted using <m> sub-word unit segmentation </m> , first on the source side and then later preliminary experimentation is conducted to facilitate character-level output .
second:
Among the different techniques , the <m> word/sub-word segmentation </m> is simple and produces very good results .


class: second -> first, base class: no relation, new model class: no relation
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
Inspired by three characteristics , we develop three steps to <m> word and subword segmentation approach </m> : in the word or sub-words segmentation step , a peaks detection function is adopted to model the maximum and minimum peaks .


class: second -> first, base class: no relation, new model class: same cluster
first:
While existing <m> subword segmentation tools </m> assume that the input is pre-tokenized into word sequences , SentencePiece can train subword models directly from raw sentences , which allows us to make a purely end-to-end and language independent system .
second:
Among the different techniques , the <m> word/sub-word segmentation </m> is simple and produces very good results .


class: second -> first, base class: no relation, new model class: no relation
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
Inspired by three characteristics , we develop three steps to <m> word and subword segmentation approach </m> : in the word or sub-words segmentation step , a peaks detection function is adopted to model the maximum and minimum peaks .


class: second -> first, base class: no relation, new model class: same cluster
first:
We applied a <m> subwordbased word segmenter </m> using CRFs and extended the segmenter with OOV words recognized by Accessor Variety .
second:
Among the different techniques , the <m> word/sub-word segmentation </m> is simple and produces very good results .


class: second -> first, base class: no relation, new model class: same cluster
first:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .
second:
Among the different techniques , the <m> word/sub-word segmentation </m> is simple and produces very good results .


class: second -> first, base class: no relation, new model class: no relation
first:
For Switchboard , our <m> phone-based BPE system </m> achieves 6.8%/14.4 % word error rate ( WER ) on the Switchboard/CallHome portion of the test set while the ensemble system achieves 6.3%/13.3 % WER .
second:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .


class: second -> first, base class: no relation, new model class: no relation
first:
For Switchboard , our <m> phone-based BPE system </m> achieves 6.8%/14.4 % word error rate ( WER ) on the Switchboard/CallHome portion of the test set while the ensemble system achieves 6.3%/13.3 % WER .
second:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .


class: second -> first, base class: no relation, new model class: no relation
first:
For Switchboard , our <m> phone-based BPE system </m> achieves 6.8%/14.4 % word error rate ( WER ) on the Switchboard/CallHome portion of the test set while the ensemble system achieves 6.3%/13.3 % WER .
second:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .


class: second -> first, base class: no relation, new model class: no relation
first:
For Switchboard , our <m> phone-based BPE system </m> achieves 6.8%/14.4 % word error rate ( WER ) on the Switchboard/CallHome portion of the test set while the ensemble system achieves 6.3%/13.3 % WER .
second:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .


class: second -> first, base class: no relation, new model class: no relation
first:
For Switchboard , our <m> phone-based BPE system </m> achieves 6.8%/14.4 % word error rate ( WER ) on the Switchboard/CallHome portion of the test set while the ensemble system achieves 6.3%/13.3 % WER .
second:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .


class: second -> first, base class: no relation, new model class: no relation
first:
For Switchboard , our <m> phone-based BPE system </m> achieves 6.8%/14.4 % word error rate ( WER ) on the Switchboard/CallHome portion of the test set while the ensemble system achieves 6.3%/13.3 % WER .
second:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .


class: second -> first, base class: no relation, new model class: no relation
first:
For Switchboard , our <m> phone-based BPE system </m> achieves 6.8%/14.4 % word error rate ( WER ) on the Switchboard/CallHome portion of the test set while the ensemble system achieves 6.3%/13.3 % WER .
second:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .


class: second -> first, base class: no relation, new model class: no relation
first:
For Switchboard , our <m> phone-based BPE system </m> achieves 6.8%/14.4 % word error rate ( WER ) on the Switchboard/CallHome portion of the test set while the ensemble system achieves 6.3%/13.3 % WER .
second:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .


class: second -> first, base class: no relation, new model class: no relation
first:
For Switchboard , our <m> phone-based BPE system </m> achieves 6.8%/14.4 % word error rate ( WER ) on the Switchboard/CallHome portion of the test set while the ensemble system achieves 6.3%/13.3 % WER .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: second -> first, base class: no relation, new model class: no relation
first:
For Switchboard , our <m> phone-based BPE system </m> achieves 6.8%/14.4 % word error rate ( WER ) on the Switchboard/CallHome portion of the test set while the ensemble system achieves 6.3%/13.3 % WER .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: second -> first, base class: no relation, new model class: no relation
first:
For Switchboard , our <m> phone-based BPE system </m> achieves 6.8%/14.4 % word error rate ( WER ) on the Switchboard/CallHome portion of the test set while the ensemble system achieves 6.3%/13.3 % WER .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: second -> first, base class: no relation, new model class: no relation
first:
For Switchboard , our <m> phone-based BPE system </m> achieves 6.8%/14.4 % word error rate ( WER ) on the Switchboard/CallHome portion of the test set while the ensemble system achieves 6.3%/13.3 % WER .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: second -> first, base class: no relation, new model class: no relation
first:
Additionally , we present <m> AST BPE </m> , which is a version of BPE that uses the Abstract Syntax Tree ( AST ) of the SQL statement to guide BPE merges and therefore produce BPE encodings that generalize better .
second:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .


class: second -> first, base class: no relation, new model class: no relation
first:
Additionally , we present <m> AST BPE </m> , which is a version of BPE that uses the Abstract Syntax Tree ( AST ) of the SQL statement to guide BPE merges and therefore produce BPE encodings that generalize better .
second:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .


class: second -> first, base class: no relation, new model class: no relation
first:
For English→German , we observe the best BLEU score of 25.3 with C2 - 50k , but the best CHRF3 score of 54.1 with <m> BPE - J90k </m> .
second:
We test both <m> word - based and BPE vocabularies </m> ( § 4 ) .


class: second -> first, base class: no relation, new model class: no relation
first:
We test both <m> word - based and BPE vocabularies </m> ( § 4 ) .
second:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .


class: second -> first, base class: no relation, new model class: no relation
first:
We test both <m> word - based and BPE vocabularies </m> ( § 4 ) .
second:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .


class: second -> first, base class: no relation, new model class: no relation
first:
We test both <m> word - based and BPE vocabularies </m> ( § 4 ) .
second:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .


class: second -> first, base class: no relation, new model class: no relation
first:
We test both <m> word - based and BPE vocabularies </m> ( § 4 ) .
second:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .


class: second -> first, base class: no relation, new model class: no relation
first:
We test both <m> word - based and BPE vocabularies </m> ( § 4 ) .
second:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .


class: second -> first, base class: no relation, new model class: no relation
first:
We test both <m> word - based and BPE vocabularies </m> ( § 4 ) .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: second -> first, base class: no relation, new model class: no relation
first:
We test both <m> word - based and BPE vocabularies </m> ( § 4 ) .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: second -> first, base class: no relation, new model class: no relation
first:
We test both <m> word - based and BPE vocabularies </m> ( § 4 ) .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: second -> first, base class: no relation, new model class: no relation
first:
We test both <m> word - based and BPE vocabularies </m> ( § 4 ) .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We used the <m> byte pair encoding compression algorithm </m> to build a new vocabulary for the inputs of the classifier .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We used the <m> byte pair encoding compression algorithm </m> to build a new vocabulary for the inputs of the classifier .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We used the <m> byte pair encoding compression algorithm </m> to build a new vocabulary for the inputs of the classifier .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We used the <m> byte pair encoding compression algorithm </m> to build a new vocabulary for the inputs of the classifier .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We discuss the suitability of different word segmentation techniques , including simple character n-gram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU , respectively .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We discuss the suitability of different word segmentation techniques , including simple character n-gram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU , respectively .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We discuss the suitability of different word segmentation techniques , including simple character n-gram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU , respectively .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We discuss the suitability of different word segmentation techniques , including simple character n-gram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU , respectively .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In this paper , a try has been made to develop few algorithms for random access text compression based on the <m> byte pair encoding scheme </m> ( Gage , 1997 ) .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In this paper , a try has been made to develop few algorithms for random access text compression based on the <m> byte pair encoding scheme </m> ( Gage , 1997 ) .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In this paper , a try has been made to develop few algorithms for random access text compression based on the <m> byte pair encoding scheme </m> ( Gage , 1997 ) .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In this paper , a try has been made to develop few algorithms for random access text compression based on the <m> byte pair encoding scheme </m> ( Gage , 1997 ) .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Five algorithms are developed based on this <m> byte pair encoding scheme </m> .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Five algorithms are developed based on this <m> byte pair encoding scheme </m> .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Five algorithms are developed based on this <m> byte pair encoding scheme </m> .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
Five algorithms are developed based on this <m> byte pair encoding scheme </m> .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: second -> first, base class: same cluster, new model class: same cluster
first:
A three-stage training based on three levels of architectural granularity namely , character encoder , <m> byte pair encoding ( BPE ) based encoder </m> , and attention decoder , is proposed .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
A three-stage training based on three levels of architectural granularity namely , character encoder , <m> byte pair encoding ( BPE ) based encoder </m> , and attention decoder , is proposed .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: second -> first, base class: same cluster, new model class: same cluster
first:
A three-stage training based on three levels of architectural granularity namely , character encoder , <m> byte pair encoding ( BPE ) based encoder </m> , and attention decoder , is proposed .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: second -> first, base class: same cluster, new model class: same cluster
first:
A three-stage training based on three levels of architectural granularity namely , character encoder , <m> byte pair encoding ( BPE ) based encoder </m> , and attention decoder , is proposed .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the <m> byte pair encoding compression algorithm </m> , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: second -> first, base class: first -> second, new model class: no relation
first:
We evaluate the effect of reordering using three simple methods : unigram count ( UC ) , <m> unigram ratio </m> ( UR ) , and first four characters match ( FFCM ) .
second:
One may also use a <m> unigram distribution </m> which has been shown to work better on some tasks pereyra2017regularize .


class: second -> first, base class: no relation, new model class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
A method for <m> delivery of advertisements or ads </m> , which comprises the step of providing drinks container lid having on its upper surface a printed label , wherein the label comprises a printed advertising or information of news , wherein the printed label is attached to the device for attaching the cover at a predetermined orientation labels advertising of news or information on the cover . ! 2 .


class: second -> first, base class: no relation, new model class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
A method for <m> delivery of advertisements or ads </m> , which comprises the step of providing drinks container lid having on its upper surface a printed label , wherein the label comprises a printed advertising or information of news , wherein the printed label is attached to the device for attaching the cover at a predetermined orientation labels advertising of news or information on the cover . ! 2 .


class: second -> first, base class: no relation, new model class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
A method for <m> delivery of advertisements or ads </m> , which comprises the step of providing drinks container lid having on its upper surface a printed label , wherein the label comprises a printed advertising or information of news , wherein the printed label is attached to the device for attaching the cover at a predetermined orientation labels advertising of news or information on the cover . ! 2 .


class: second -> first, base class: no relation, new model class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
A method for <m> delivery of advertisements or ads </m> , which comprises the step of providing drinks container lid having on its upper surface a printed label , wherein the label comprises a printed advertising or information of news , wherein the printed label is attached to the device for attaching the cover at a predetermined orientation labels advertising of news or information on the cover . ! 2 .


class: second -> first, base class: no relation, new model class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
A method for <m> delivery of advertisements or ads </m> , which comprises the step of providing drinks container lid having on its upper surface a printed label , wherein the label comprises a printed advertising or information of news , wherein the printed label is attached to the device for attaching the cover at a predetermined orientation labels advertising of news or information on the cover . ! 2 .


class: second -> first, base class: no relation, new model class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
A method for <m> delivery of advertisements or ads </m> , which comprises the step of providing drinks container lid having on its upper surface a printed label , wherein the label comprises a printed advertising or information of news , wherein the printed label is attached to the device for attaching the cover at a predetermined orientation labels advertising of news or information on the cover . ! 2 .


class: second -> first, base class: no relation, new model class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
A method for <m> delivery of advertisements or ads </m> , which comprises the step of providing drinks container lid having on its upper surface a printed label , wherein the label comprises a printed advertising or information of news , wherein the printed label is attached to the device for attaching the cover at a predetermined orientation labels advertising of news or information on the cover . ! 2 .


class: second -> first, base class: no relation, new model class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
A method for <m> delivery of advertisements or ads </m> , which comprises the step of providing drinks container lid having on its upper surface a printed label , wherein the label comprises a printed advertising or information of news , wherein the printed label is attached to the device for attaching the cover at a predetermined orientation labels advertising of news or information on the cover . ! 2 .


class: second -> first, base class: no relation, new model class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
A method for <m> delivery of advertisements or ads </m> , which comprises the step of providing drinks container lid having on its upper surface a printed label , wherein the label comprises a printed advertising or information of news , wherein the printed label is attached to the device for attaching the cover at a predetermined orientation labels advertising of news or information on the cover . ! 2 .


class: second -> first, base class: no relation, new model class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
A method for <m> delivery of advertisements or ads </m> , which comprises the step of providing drinks container lid having on its upper surface a printed label , wherein the label comprises a printed advertising or information of news , wherein the printed label is attached to the device for attaching the cover at a predetermined orientation labels advertising of news or information on the cover . ! 2 .


class: second -> first, base class: no relation, new model class: no relation
first:
In the first part , the scope and features of <m> digital advertising </m> as well as the tools deployed are presented and defined .
second:
A method for <m> delivery of advertisements or ads </m> , which comprises the step of providing drinks container lid having on its upper surface a printed label , wherein the label comprises a printed advertising or information of news , wherein the printed label is attached to the device for attaching the cover at a predetermined orientation labels advertising of news or information on the cover . ! 2 .


class: second -> first, base class: no relation, new model class: no relation
first:
While predictive modeling has been thoroughly researched in recent years in the <m> digital advertising domain </m> , the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification .
second:
A method for <m> delivery of advertisements or ads </m> , which comprises the step of providing drinks container lid having on its upper surface a printed label , wherein the label comprises a printed advertising or information of news , wherein the printed label is attached to the device for attaching the cover at a predetermined orientation labels advertising of news or information on the cover . ! 2 .


class: second -> first, base class: no relation, new model class: no relation
first:
This paper reports a multi-phase study to address the following questions in the <m> digital advertising context </m> : ( 1 ) to what extent did consumers use ads in the past ? ( 2 ) What are consumers ’ perceived values toward ads ?
second:
A method for <m> delivery of advertisements or ads </m> , which comprises the step of providing drinks container lid having on its upper surface a printed label , wherein the label comprises a printed advertising or information of news , wherein the printed label is attached to the device for attaching the cover at a predetermined orientation labels advertising of news or information on the cover . ! 2 .


class: second -> first, base class: no relation, new model class: no relation
first:
What is the simplicity and complexity of the digital advertising design structure?The purpose of the research is to focus on the discovery of the rug and the complexity of the structure of the design of digital advertising , while the importance of research by contributing to enrich the knowledge and application of the specialists and specialists to study the simplicity and complexity in the structure of <m> digital advertising design </m> and the introduction of specialized designers working on the subject of simplicity and complexity in the design of digital advertising And the terminology in it , and the researcher has developed spatial boundaries represented by a number of Iraqi digital ad designs , and time limits ( limited in the period of 2017 ) As for the substantive boundaries represented in ( study of simplicity and complexity in the structure of design Digital advertising.while the second section chapter which consist the theoretical frame which interest in three researcher , the first research turning to ( Simplicity in the digital advertising design)while the the second research interest in ( complexity in the digital advertising design)and the third research turning into about the(structure digital advertising design ) .
second:
A method for <m> delivery of advertisements or ads </m> , which comprises the step of providing drinks container lid having on its upper surface a printed label , wherein the label comprises a printed advertising or information of news , wherein the printed label is attached to the device for attaching the cover at a predetermined orientation labels advertising of news or information on the cover . ! 2 .


class: second -> first, base class: no relation, new model class: no relation
first:
A method for <m> delivery of advertisements or ads </m> , which comprises the step of providing drinks container lid having on its upper surface a printed label , wherein the label comprises a printed advertising or information of news , wherein the printed label is attached to the device for attaching the cover at a predetermined orientation labels advertising of news or information on the cover . ! 2 .
second:
The need to make sense of a mass of such touch-points makes process a prevalent and emerging concept in the Front- Office of enterprises , including organisational competences such as marketing operations , customer-relationship management , <m> campaign creation and monitoring </m> , brand management , sales and advisory services , multichannel management , service innovation and management life-cycle , among others .


class: second -> first, base class: first -> second, new model class: first -> second
first:
We discuss in detail the effects of roles on each NE and on semantic categories of noun and verb features in terms of accuracy , precision/recall and F-score measures for the <m> text classification task </m> .
second:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In this paper , we address the <m> text classification problem </m> that a period of time created test data is different from the training data , and present a method for text classification based on temporal adaptation .
second:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Given a small subset of relations between the individuals , the problem of learning social network is translated into a <m> text classification problem </m> .
second:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
We present a geo-enriched classifier joining established methods for <m> text-based classification </m> with location-based topic prediction .
second:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Using a matrix of closeness and a set of additional fusion rules , the proposed method improves the classification performance by only subjecting likely misclassified samples to a text-based classifier followed by additional fusion of both image-based classification and <m> text-based classification </m> results .
second:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
The model of <m> text classifying </m> using non-labeled training sample with more extensive application is realized .
second:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
For example , in <m> text data classification </m> , groups of documents of different types are categorized by different subsets of terms .
second:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This study contributes to the field of network troubleshooting , and the field of <m> text data classification </m> .
second:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Abstract The genetic selection of keyword sets , the text frequencies of which are considered as attributes in <m> text classification analysis </m> , has been analysed .
second:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
In <m> Text Classification of Methods </m> , there is a sentence “ Where P(B|A ) is the probability of B appearance when A is known ?
second:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
<m> TEXT CLASSIFICATION Text classification </m> is a commonly used technique as a basis for applications in document processing and Visualization , web mining , surveillance technology , patent Analysis , etc .
second:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
However , how well do these models perform on practical <m> text classification problems </m> , with real world data ?
second:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
It is difficult to fit a single model for <m> text classification </m> across domains , due to unknown words , specialized context , colloquial language , and other differences between domains .
second:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
section : Conclusion In this work we demonstrate that unsupervised pretraining and finetuning provides a flexible framework that is effective for difficult <m> text classification tasks </m> .
second:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
This paper studies CNN on <m> text categorization </m> to exploit the 1D structure ( namely , word order ) of text data for accurate prediction .
second:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Learning distributed sentence representation through neural network models requires little external domain knowledge and can reach satisfactory results in related tasks like sentiment classification , <m> text categorization </m> .
second:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .


class: second -> first, base class: first -> second, new model class: first -> second
first:
Meanwhile , linear classifiers are often considered as strong baselines for <m> text classification problems </m> .
second:
Finally , we achieve new state - of - the - art results on several <m> text classification and sentiment analysis tasks </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .
second:
In order to test the applicability of the obtained ontologies , a <m> text categorization </m> experiment has been proposed and the obtained results indicate that the approach can be applied with satisfactory results and warrants further research .


class: second -> first, base class: no relation, new model class: no relation
first:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .
second:
This paper describes a study on <m> text categorization </m> using a character n­grams approach for the morphological normalization .


class: second -> first, base class: no relation, new model class: no relation
first:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .
second:
In contrast to previous <m> text categorization research </m> in e-rulemaking [ 5 , 6 ] , and in an attempt to more closely duplicate the comment analysis process in federal agencies , we employ a set of rule-specific categories , each of which corresponds to a significant issue raised in the comments .


class: second -> first, base class: no relation, new model class: no relation
first:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .
second:
Therefore , <m> text categorization research </m> has become more important .


class: second -> first, base class: no relation, new model class: no relation
first:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .
second:
This method can improve the accuracy of <m> text categorizing </m> efficiently , which is proved by experiments .


class: second -> first, base class: no relation, new model class: no relation
first:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .
second:
Making authoritative corpus and standard of <m> text categorizing </m> are suggested for evaluation and application of text categorizing model .


class: second -> first, base class: no relation, new model class: no relation
first:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .
second:
In direct proportion to the heavy increase of online information data , the attention to <m> text categorization ( classification </m> ) has also increased .


class: second -> first, base class: no relation, new model class: no relation
first:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .
second:
While the former problem may be addressed using one of the well known techniques of <m> text categorization ( classification </m> ) , the latter seems to require some distinct approaches due to the fact that the set of cases is unknown in advance , as well as due to the assumed limited number of training documents , if a case should be interpreted as a classic category .


class: second -> first, base class: no relation, new model class: no relation
first:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .
second:
The use of word embedding models and deep learning algorithms are currently the most common and popular trends to enhance the overall performance of a <m> text classification/categorization system </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .
second:
For <m> text classification/categorization </m> , we investigate a steepest descent induction algorithm combined with a two-level preference relation on user ranking .


class: second -> first, base class: no relation, new model class: no relation
first:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .
second:
There are two approaches for this <m> text-based categorization </m> : manual and automatic .


class: second -> first, base class: no relation, new model class: no relation
first:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .
second:
<m> Text based categorization </m> is made use of for document classification with pattern recognition and machine learning .


class: second -> first, base class: no relation, new model class: no relation
first:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .
second:
The results and the statistical evaluation of this procedure showed that the proposed method may be characterized as highly accurate for <m> text categorization purposes </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .
second:
In this particular paper we describe the design and implementation of a distributed tree-based algorithm for <m> text categorization purposes </m> .


class: second -> first, base class: no relation, new model class: no relation
first:
Although this approach is the traditional architecture choice for <m> text classification CNNs </m> , it introduces a significant number of parameter in the network .
second:
To date , our focus has been on <m> text information categorization </m> using both existing text classification techniques ( e.g. Weighted Feature Vector ( WFV ) classification ) and machine learning algorithms based on inductive logic programming ( ILP ) and natural language processing techniques .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We examine supervised learning for <m> multi-class , multi-label text classification </m> .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We examine supervised learning for <m> multi-class , multi-label text classification </m> .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We examine supervised learning for <m> multi-class , multi-label text classification </m> .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We examine supervised learning for <m> multi-class , multi-label text classification </m> .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We explore supervised learning for <m> multi-class , multi-label text classification </m> , focusing on real-world settings , where the distribution of labels changes dynamically over time .
second:
They are efficient statistical models to solve <m> multi-label text categorization problem </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We explore supervised learning for <m> multi-class , multi-label text classification </m> , focusing on real-world settings , where the distribution of labels changes dynamically over time .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We explore supervised learning for <m> multi-class , multi-label text classification </m> , focusing on real-world settings , where the distribution of labels changes dynamically over time .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
We explore supervised learning for <m> multi-class , multi-label text classification </m> , focusing on real-world settings , where the distribution of labels changes dynamically over time .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: second -> first, base class: no relation, new model class: no relation
first:
We design a fast Class-Feature-Centroid ( CFC ) classifier for <m> multi-class , single-label text categorization </m> .
second:
As the input information is in the form of free textual entries , techniques used in <m> multi-class , multi-label text categorization </m> are well suited .


class: second -> first, base class: no relation, new model class: no relation
first:
We design a fast Class-Feature-Centroid ( CFC ) classifier for <m> multi-class , single-label text categorization </m> .
second:
In this paper , an artificial neural network with a quasi-Newton updating procedure is presented for <m> multi-label multi-class text classification </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the input information is in the form of free textual entries , techniques used in <m> multi-class , multi-label text categorization </m> are well suited .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the input information is in the form of free textual entries , techniques used in <m> multi-class , multi-label text categorization </m> are well suited .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
As the input information is in the form of free textual entries , techniques used in <m> multi-class , multi-label text categorization </m> are well suited .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


class: second -> first, base class: no relation, new model class: no relation
first:
A number of language-independent text pre-processing techniques , to support <m> multi-class single-label text classification </m> , are described and compared .
second:
In this paper , an artificial neural network with a quasi-Newton updating procedure is presented for <m> multi-label multi-class text classification </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In this paper , an artificial neural network with a quasi-Newton updating procedure is presented for <m> multi-label multi-class text classification </m> .
second:
Empirical evaluation on real world benchmark multi-class and multi-label text data sets and comparison with other state-of-the-art text classification as well as subspace clustering algorithms show that SISC provides superior performance for <m> multi-label text data classification </m> in a multi-class setting .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In this paper , an artificial neural network with a quasi-Newton updating procedure is presented for <m> multi-label multi-class text classification </m> .
second:
We apply the proposed EXAM on multi - class and <m> multi - label text classifications </m> .


class: second -> first, base class: same cluster, new model class: same cluster
first:
In this paper , an artificial neural network with a quasi-Newton updating procedure is presented for <m> multi-label multi-class text classification </m> .
second:
Performance on this task ( <m> multi - label categorization </m> ) is known to be sensitive to thresholding strategies , which are algorithms additional to the models we would like to test .


