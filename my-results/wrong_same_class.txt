class: same cluster, base class: no relation, new model class: no relation
first:
After <m> linear transformation </m> on speech subspace , speech recognizer outperforms by 7.57 % ( 62.14 % to 69.71 % ) under angry stress condition .
second:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .


class: same cluster, base class: no relation, new model class: no relation
first:
Specifically , we develop a <m> linear transformation function </m> which matches the marginal distributions of the source and target subspaces without a regularization term .
second:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .


class: same cluster, base class: no relation, new model class: no relation
first:
We consider the problem of developing a <m> linear transformation process </m> to compensate for range-dependent bistatic clutter spectral dispersion .
second:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .


class: same cluster, base class: no relation, new model class: no relation
first:
We conduct extensive experiments that demonstrate the proposed non-rigid alignment method is ( 1 ) effective , outperforming both the state-of-the-art <m> linear transformation-based methods </m> and node representation based methods , and ( 2 ) efficient , with a comparable computational time between the proposed multi-network representation learning component and its single-network counterpart .
second:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .


class: same cluster, base class: no relation, new model class: no relation
first:
After studying conventional geometrical methods , we decided to analyze both general slope curves and discontinuities of open pit mines used in excavations in rock mass to achieve meaningful 3D results by using <m> linear mathematical transformation </m> and isometric perspective methods .
second:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .


class: same cluster, base class: no relation, new model class: no relation
first:
Finally , the outputs of the bi-channel convolution operations are combined together and fed into a series of <m> linear transformation operations </m> to get the final relation classification result .
second:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .


class: same cluster, base class: no relation, new model class: no relation
first:
These sentence embeddings are used to initialize the decoder LSTM through a <m> linear transformation </m> , and are also concatenated to its input embeddings at every time step .
second:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .


class: same cluster, base class: no relation, new model class: no relation
first:
The bit - width of weights can be reduced to one or two bits through sign function or symmetrical threshold , whereas the layerwise gradients and inputs are quantized with <m> linear mapping </m> .
second:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .


class: same cluster, base class: no relation, new model class: no relation
first:
Both PCA and a Joint Bayesian model that effectively correspond to a <m> linear transform </m> in the embedding space are employed .
second:
The simple <m> linear projection </m> makes the method easy to interpret , while the visualization task is made well-defined by the novel information retrieval criterion .


class: same cluster, base class: no relation, new model class: no relation
first:
Two assumptions are made : 1 ) In a new pair of images the deformation is approximately the same size and has only been spatially relocated in the image , and that by a simple <m> affine transformation </m> one can identify the optimal configuration on this new pair of images ; and 2 ) The deformation is of similar size and shape on the original pair of images .
second:
The question nodes and edges use vector embeddings implemented as look - up tables , and the scene nodes and edges use <m> affine projections </m> : with the word embedding ( usually pretrained , see supplementary material ) , the embedding of dependencies , and weight matrices , and and biases .


class: same cluster, base class: no relation, new model class: no relation
first:
Two assumptions are made : 1 ) In a new pair of images the deformation is approximately the same size and has only been spatially relocated in the image , and that by a simple <m> affine transformation </m> one can identify the optimal configuration on this new pair of images ; and 2 ) The deformation is of similar size and shape on the original pair of images .
second:
The same results can be obtained when we precompose $ \La$ with an <m> affine matrix-valued mapping </m> $ A$ , provided that this mapping satisfies a regularity condition ( { \em transversality condition } ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Sum of Square Difference ( SSD ) and Bilinear interpolation models were used to establish the similarity measure between the images to be registered , resampling of the pixel-values and computation of non-integer coordinates respectively while Random Sampling Consensus ( RANSAC ) algorithm was used to exclude the outliers and to compute the transformation matrix using <m> affine transformation function </m> .
second:
The question nodes and edges use vector embeddings implemented as look - up tables , and the scene nodes and edges use <m> affine projections </m> : with the word embedding ( usually pretrained , see supplementary material ) , the embedding of dependencies , and weight matrices , and and biases .


class: same cluster, base class: no relation, new model class: no relation
first:
Sum of Square Difference ( SSD ) and Bilinear interpolation models were used to establish the similarity measure between the images to be registered , resampling of the pixel-values and computation of non-integer coordinates respectively while Random Sampling Consensus ( RANSAC ) algorithm was used to exclude the outliers and to compute the transformation matrix using <m> affine transformation function </m> .
second:
The same results can be obtained when we precompose $ \La$ with an <m> affine matrix-valued mapping </m> $ A$ , provided that this mapping satisfies a regularity condition ( { \em transversality condition } ) .


class: same cluster, base class: no relation, new model class: no relation
first:
After that , the pattern elements are extracted with the <m> affine transformation theory </m> and bilinear interpolation algorithm .
second:
The question nodes and edges use vector embeddings implemented as look - up tables , and the scene nodes and edges use <m> affine projections </m> : with the word embedding ( usually pretrained , see supplementary material ) , the embedding of dependencies , and weight matrices , and and biases .


class: same cluster, base class: no relation, new model class: no relation
first:
After that , the pattern elements are extracted with the <m> affine transformation theory </m> and bilinear interpolation algorithm .
second:
The same results can be obtained when we precompose $ \La$ with an <m> affine matrix-valued mapping </m> $ A$ , provided that this mapping satisfies a regularity condition ( { \em transversality condition } ) .


class: same cluster, base class: no relation, new model class: no relation
first:
The content includes : dividing an image frame including a target object , determining a target object region , using a feature information classifier to scan the determined target object region , determining central point position information of a region that feature information to be positioned occupies in the target object region , performing <m> affine transformation operation </m> on the determined central point position information , and obtaining initial position information corresponding to the region that the feature information to be positioned occupies in the target object region , thereby improving precision of initial positioning ; performing iteration processing on the obtained initial position information , obtaining position information corresponding to the region that the feature information to be positioned occupies in the target object region , integrating the obtained position information corresponding to the region that the feature information to be positioned occupies in the target object region , and obtaining the feature information of the target object , thereby improving precision of positioning the feature information of the target object .
second:
The question nodes and edges use vector embeddings implemented as look - up tables , and the scene nodes and edges use <m> affine projections </m> : with the word embedding ( usually pretrained , see supplementary material ) , the embedding of dependencies , and weight matrices , and and biases .


class: same cluster, base class: no relation, new model class: no relation
first:
The content includes : dividing an image frame including a target object , determining a target object region , using a feature information classifier to scan the determined target object region , determining central point position information of a region that feature information to be positioned occupies in the target object region , performing <m> affine transformation operation </m> on the determined central point position information , and obtaining initial position information corresponding to the region that the feature information to be positioned occupies in the target object region , thereby improving precision of initial positioning ; performing iteration processing on the obtained initial position information , obtaining position information corresponding to the region that the feature information to be positioned occupies in the target object region , integrating the obtained position information corresponding to the region that the feature information to be positioned occupies in the target object region , and obtaining the feature information of the target object , thereby improving precision of positioning the feature information of the target object .
second:
The same results can be obtained when we precompose $ \La$ with an <m> affine matrix-valued mapping </m> $ A$ , provided that this mapping satisfies a regularity condition ( { \em transversality condition } ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Specific topics include : Loading images from JAR files Describing shapes and the affine transformation in Java 2D Designing a class library of shapes Designing a component class for interactive drawing Designing a component class for displaying images Printing in Java 2D Implementing pan and zoom Implementing interactive rotate , shear , and flip Manipulating images at the pixel level using the Java 2D API Working with tiles in JAI to render large images Image manipulation in JAI <m> usingaffine transformation </m> , projections , and warping Image analysis in JAI , including edge detection , statistics , and region of interest computation ( ROI ) Remote imaging using RMI and JAI Internet imaging , including the use of Java 2D and JAI with servlets and JSP , the design of Web-based imaging applications , and the Internet Imaging Protocol ( IIP ) Using the new Image I/O framework to read and write images " Putting It All Together " sections help you put vital concepts and techniques into practice with interactive examples using actual applications .
second:
The question nodes and edges use vector embeddings implemented as look - up tables , and the scene nodes and edges use <m> affine projections </m> : with the word embedding ( usually pretrained , see supplementary material ) , the embedding of dependencies , and weight matrices , and and biases .


class: same cluster, base class: no relation, new model class: no relation
first:
Specific topics include : Loading images from JAR files Describing shapes and the affine transformation in Java 2D Designing a class library of shapes Designing a component class for interactive drawing Designing a component class for displaying images Printing in Java 2D Implementing pan and zoom Implementing interactive rotate , shear , and flip Manipulating images at the pixel level using the Java 2D API Working with tiles in JAI to render large images Image manipulation in JAI <m> usingaffine transformation </m> , projections , and warping Image analysis in JAI , including edge detection , statistics , and region of interest computation ( ROI ) Remote imaging using RMI and JAI Internet imaging , including the use of Java 2D and JAI with servlets and JSP , the design of Web-based imaging applications , and the Internet Imaging Protocol ( IIP ) Using the new Image I/O framework to read and write images " Putting It All Together " sections help you put vital concepts and techniques into practice with interactive examples using actual applications .
second:
The same results can be obtained when we precompose $ \La$ with an <m> affine matrix-valued mapping </m> $ A$ , provided that this mapping satisfies a regularity condition ( { \em transversality condition } ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Since the textual and visual vectors have different dimensionality and belong to different spaces , we first map them to a mutual space using an <m> affine transformation </m> .
second:
The question nodes and edges use vector embeddings implemented as look - up tables , and the scene nodes and edges use <m> affine projections </m> : with the word embedding ( usually pretrained , see supplementary material ) , the embedding of dependencies , and weight matrices , and and biases .


class: same cluster, base class: no relation, new model class: no relation
first:
Since the textual and visual vectors have different dimensionality and belong to different spaces , we first map them to a mutual space using an <m> affine transformation </m> .
second:
The same results can be obtained when we precompose $ \La$ with an <m> affine matrix-valued mapping </m> $ A$ , provided that this mapping satisfies a regularity condition ( { \em transversality condition } ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , the learned <m> affine transform </m> applied to these normalized activations allows the BN transform to represent the identity transformation and preserves the network capacity .
second:
The question nodes and edges use vector embeddings implemented as look - up tables , and the scene nodes and edges use <m> affine projections </m> : with the word embedding ( usually pretrained , see supplementary material ) , the embedding of dependencies , and weight matrices , and and biases .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , the learned <m> affine transform </m> applied to these normalized activations allows the BN transform to represent the identity transformation and preserves the network capacity .
second:
The same results can be obtained when we precompose $ \La$ with an <m> affine matrix-valued mapping </m> $ A$ , provided that this mapping satisfies a regularity condition ( { \em transversality condition } ) .


class: same cluster, base class: second -> first, new model class: no relation
first:
The question nodes and edges use vector embeddings implemented as look - up tables , and the scene nodes and edges use <m> affine projections </m> : with the word embedding ( usually pretrained , see supplementary material ) , the embedding of dependencies , and weight matrices , and and biases .
second:
Also in this group , for those vertices associated with large residual errors under <m> affine mapping </m> , we encode their motion effectively using Newtonian motion estimates .


class: same cluster, base class: second -> first, new model class: no relation
first:
The question nodes and edges use vector embeddings implemented as look - up tables , and the scene nodes and edges use <m> affine projections </m> : with the word embedding ( usually pretrained , see supplementary material ) , the embedding of dependencies , and weight matrices , and and biases .
second:
GCPs derived from each source were used independently for the GCP-based geometric correction of IRS PAN sensor images by using <m> affine mapping function </m> .


class: same cluster, base class: second -> first, new model class: no relation
first:
The question nodes and edges use vector embeddings implemented as look - up tables , and the scene nodes and edges use <m> affine projections </m> : with the word embedding ( usually pretrained , see supplementary material ) , the embedding of dependencies , and weight matrices , and and biases .
second:
In this paper we introduce a class of constraint logic programs such that their termination can be proved by using <m> affine level mappings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The question nodes and edges use vector embeddings implemented as look - up tables , and the scene nodes and edges use <m> affine projections </m> : with the word embedding ( usually pretrained , see supplementary material ) , the embedding of dependencies , and weight matrices , and and biases .
second:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .


class: same cluster, base class: second -> first, new model class: no relation
first:
The question nodes and edges use vector embeddings implemented as look - up tables , and the scene nodes and edges use <m> affine projections </m> : with the word embedding ( usually pretrained , see supplementary material ) , the embedding of dependencies , and weight matrices , and and biases .
second:
To be specific , we propose <m> affine mapping based schemes </m> for the problem transformation and outsourcing so that the cloud is unable to learn any key information from the transformed problem .


class: same cluster, base class: no relation, new model class: no relation
first:
The question nodes and edges use vector embeddings implemented as look - up tables , and the scene nodes and edges use <m> affine projections </m> : with the word embedding ( usually pretrained , see supplementary material ) , the embedding of dependencies , and weight matrices , and and biases .
second:
The same results can be obtained when we precompose $ \La$ with an <m> affine matrix-valued mapping </m> $ A$ , provided that this mapping satisfies a regularity condition ( { \em transversality condition } ) .


class: same cluster, base class: no relation, new model class: no relation
first:
The question nodes and edges use vector embeddings implemented as look - up tables , and the scene nodes and edges use <m> affine projections </m> : with the word embedding ( usually pretrained , see supplementary material ) , the embedding of dependencies , and weight matrices , and and biases .
second:
The algorithm estimates the 12 calibration parameters , which are elements of an <m> affine transformation mapping </m> measured , and thus , erroneous magnetometer measurements into corrected field measurements .


class: same cluster, base class: first -> second, new model class: no relation
first:
Also in this group , for those vertices associated with large residual errors under <m> affine mapping </m> , we encode their motion effectively using Newtonian motion estimates .
second:
The same results can be obtained when we precompose $ \La$ with an <m> affine matrix-valued mapping </m> $ A$ , provided that this mapping satisfies a regularity condition ( { \em transversality condition } ) .


class: same cluster, base class: first -> second, new model class: no relation
first:
GCPs derived from each source were used independently for the GCP-based geometric correction of IRS PAN sensor images by using <m> affine mapping function </m> .
second:
The same results can be obtained when we precompose $ \La$ with an <m> affine matrix-valued mapping </m> $ A$ , provided that this mapping satisfies a regularity condition ( { \em transversality condition } ) .


class: same cluster, base class: first -> second, new model class: no relation
first:
In this paper we introduce a class of constraint logic programs such that their termination can be proved by using <m> affine level mappings </m> .
second:
The same results can be obtained when we precompose $ \La$ with an <m> affine matrix-valued mapping </m> $ A$ , provided that this mapping satisfies a regularity condition ( { \em transversality condition } ) .


class: same cluster, base class: no relation, new model class: no relation
first:
First an <m> affine linear mapping </m> is determined , so that the so-called sum of squared differences , between the source images is as small as possible .
second:
The same results can be obtained when we precompose $ \La$ with an <m> affine matrix-valued mapping </m> $ A$ , provided that this mapping satisfies a regularity condition ( { \em transversality condition } ) .


class: same cluster, base class: first -> second, new model class: no relation
first:
To be specific , we propose <m> affine mapping based schemes </m> for the problem transformation and outsourcing so that the cloud is unable to learn any key information from the transformed problem .
second:
The same results can be obtained when we precompose $ \La$ with an <m> affine matrix-valued mapping </m> $ A$ , provided that this mapping satisfies a regularity condition ( { \em transversality condition } ) .


class: same cluster, base class: no relation, new model class: no relation
first:
The same results can be obtained when we precompose $ \La$ with an <m> affine matrix-valued mapping </m> $ A$ , provided that this mapping satisfies a regularity condition ( { \em transversality condition } ) .
second:
The algorithm estimates the 12 calibration parameters , which are elements of an <m> affine transformation mapping </m> measured , and thus , erroneous magnetometer measurements into corrected field measurements .


class: same cluster, base class: first -> second, new model class: no relation
first:
We use two additional <m> linear layers </m> and principal component analysis to reduce the dimension of both inputs and internal representations and to transmit the essential information .
second:
Among them , the chaotic neuron layer realizes data diffusion , the <m> linear neuron layer </m> realizes data confusion , and the two layers are repeated for several times to strengthen the cipher .


class: same cluster, base class: first -> second, new model class: no relation
first:
We use two additional <m> linear layers </m> and principal component analysis to reduce the dimension of both inputs and internal representations and to transmit the essential information .
second:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .


class: same cluster, base class: first -> second, new model class: no relation
first:
The three methods , named Reconstructing DDT and LAT According to Weight , Executing <m> Linear Layer Operations </m> in Minimal Cost and Merging Two 4-bit S-boxes into One 8-bit S-box respectively , can efficiently speed up the search process by reducing the search space as much as possible and reducing the cost of executing linear layer operations .
second:
Among them , the chaotic neuron layer realizes data diffusion , the <m> linear neuron layer </m> realizes data confusion , and the two layers are repeated for several times to strengthen the cipher .


class: same cluster, base class: first -> second, new model class: no relation
first:
The three methods , named Reconstructing DDT and LAT According to Weight , Executing <m> Linear Layer Operations </m> in Minimal Cost and Merging Two 4-bit S-boxes into One 8-bit S-box respectively , can efficiently speed up the search process by reducing the search space as much as possible and reducing the cost of executing linear layer operations .
second:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .


class: same cluster, base class: no relation, new model class: no relation
first:
The parameters of the <m> linear neural layer </m> are determined by solving a constrained optimization problem in a regularized framework .
second:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .


class: same cluster, base class: second -> first, new model class: no relation
first:
The parameters of the <m> linear neural layer </m> are determined by solving a constrained optimization problem in a regularized framework .
second:
However , being a <m> linear model </m> , LR can only ascribe a uniform policy - in this case , “ treat everyone ” .


class: same cluster, base class: first -> second, new model class: no relation
first:
This method is fundamentally different from existing methods because we use the domain features to calculate a <m> linear combination of linear layers </m> .
second:
Among them , the chaotic neuron layer realizes data diffusion , the <m> linear neuron layer </m> realizes data confusion , and the two layers are repeated for several times to strengthen the cipher .


class: same cluster, base class: first -> second, new model class: no relation
first:
This method is fundamentally different from existing methods because we use the domain features to calculate a <m> linear combination of linear layers </m> .
second:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .


class: same cluster, base class: first -> second, new model class: no relation
first:
<m> Linear layer ( design ) model </m> gave best outcomes ( MSE : 0.000293366 , RMSE : 0.017127919 , R : 0.996479613 ) .
second:
Among them , the chaotic neuron layer realizes data diffusion , the <m> linear neuron layer </m> realizes data confusion , and the two layers are repeated for several times to strengthen the cipher .


class: same cluster, base class: first -> second, new model class: no relation
first:
<m> Linear layer ( design ) model </m> gave best outcomes ( MSE : 0.000293366 , RMSE : 0.017127919 , R : 0.996479613 ) .
second:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .


class: same cluster, base class: first -> second, new model class: no relation
first:
The process model described is a <m> linear layered model </m> .
second:
Among them , the chaotic neuron layer realizes data diffusion , the <m> linear neuron layer </m> realizes data confusion , and the two layers are repeated for several times to strengthen the cipher .


class: same cluster, base class: first -> second, new model class: no relation
first:
The process model described is a <m> linear layered model </m> .
second:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .


class: same cluster, base class: no relation, new model class: no relation
first:
Among them , the chaotic neuron layer realizes data diffusion , the <m> linear neuron layer </m> realizes data confusion , and the two layers are repeated for several times to strengthen the cipher .
second:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .


class: same cluster, base class: second -> first, new model class: second -> first
first:
Among them , the chaotic neuron layer realizes data diffusion , the <m> linear neuron layer </m> realizes data confusion , and the two layers are repeated for several times to strengthen the cipher .
second:
However , being a <m> linear model </m> , LR can only ascribe a uniform policy - in this case , “ treat everyone ” .


class: same cluster, base class: second -> first, new model class: no relation
first:
Among them , the chaotic neuron layer realizes data diffusion , the <m> linear neuron layer </m> realizes data confusion , and the two layers are repeated for several times to strengthen the cipher .
second:
Finally , we pass this fixed vector through a <m> linear layer </m> to obtain the prediction whether the question is answerable .


class: same cluster, base class: second -> first, new model class: no relation
first:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .
second:
However , being a <m> linear model </m> , LR can only ascribe a uniform policy - in this case , “ treat everyone ” .


class: same cluster, base class: second -> first, new model class: no relation
first:
Logistic regression , on the other hand , gives ML estimates for <m> single layer linear models </m> only .
second:
Finally , we pass this fixed vector through a <m> linear layer </m> to obtain the prediction whether the question is answerable .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : <m> Layer - Wise Linear Model </m> A neural network model based on graph convolutions can therefore be built by stacking multiple convolutional layers of the form of Eq .
second:
Considering that a single <m> linear Weight Layer </m> might cause excessive response on some specific dimensions of the part vector , we add a nonlinear function to equalize the response of part feature vector , and the fused feature representation is where the and the are the global and part feature vectors .


class: same cluster, base class: no relation, new model class: first -> second
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .


class: same cluster, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .


class: same cluster, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .


class: same cluster, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .


class: same cluster, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .


class: same cluster, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: same cluster, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: same cluster, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: same cluster, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: same cluster, base class: no relation, new model class: no relation
first:
Also , by <m> subsampling </m> , the large low frequency ( 0 - 1 kHz ) codebook can be searched very rapidly resulting in a significant complexity advantage .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: same cluster, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .


class: same cluster, base class: no relation, new model class: second -> first
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .


class: same cluster, base class: no relation, new model class: second -> first
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .


class: same cluster, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .


class: same cluster, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .


class: same cluster, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .


class: same cluster, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: same cluster, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: same cluster, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: same cluster, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: same cluster, base class: no relation, new model class: no relation
first:
The novel modulator has significant improvements over previously published subsampling modulators as it provides jitter and image suppression and excess loop delay compensation to improve the dynamic range , thus enabling the modulator to be utilized in <m> subsampling receivers </m> when a relatively low sampling rate is desired .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: same cluster, base class: second -> first, new model class: no relation
first:
To obtain accurate classification results , this paper proposes the Ensemble-Margin Based Random Forests ( EMRFs ) method , which combines RFs and a new <m> subsampling iterative technique </m> making use of computed ensemble margin values .
second:
We explore a <m> subsampling bootstrap procedure </m> to serve as the basis for goodness of fit and model selection with a single observed network that circumvents the intractability of such likelihoods .


class: same cluster, base class: second -> first, new model class: second -> first
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .


class: same cluster, base class: second -> first, new model class: second -> first
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .


class: same cluster, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .


class: same cluster, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .


class: same cluster, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .


class: same cluster, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: same cluster, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: same cluster, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: same cluster, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: same cluster, base class: no relation, new model class: no relation
first:
Hardware overhead for particular algorithmic requirements , such as variable pixel resolution , <m> subsampling with offset </m> , and subpixel accuracy , is discussed in detail .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: same cluster, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .


class: same cluster, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .


class: same cluster, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .


class: same cluster, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .


class: same cluster, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: same cluster, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: same cluster, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: same cluster, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: same cluster, base class: no relation, new model class: no relation
first:
The paper proposes three modifications to the 3-step hierarchical search block-matching algorithm for video coding : a multiple-winner search that improves the estimation accuracy , a <m> method of subsampling </m> that reduces computation and input data amount , and an overlapping strategy that improves the accuracy of large-area search .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: same cluster, base class: second -> first, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .


class: same cluster, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: same cluster, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: same cluster, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: same cluster, base class: no relation, new model class: no relation
first:
In - network upsampling layers enable pixelwise prediction and learning in nets with <m> subsampling </m> .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: same cluster, base class: second -> first, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .


class: same cluster, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: same cluster, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: same cluster, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: same cluster, base class: no relation, new model class: no relation
first:
A CNN consists of two layers : a convolutional layer , followed by a <m> subsampling layer </m> .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: same cluster, base class: second -> first, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .


class: same cluster, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: same cluster, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: same cluster, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: same cluster, base class: no relation, new model class: no relation
first:
We discuss the practical training and decoding algorithms of this model for speech recognition , and the <m> subsampling network </m> to reduce the computational cost .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: same cluster, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .


class: same cluster, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .


class: same cluster, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .


class: same cluster, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .


class: same cluster, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: same cluster, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: same cluster, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: same cluster, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: same cluster, base class: no relation, new model class: no relation
first:
It is a highly ill - posed problem , due to the loss of high - frequency information that occurs during the non - invertible low - pass filtering and <m> subsampling operations </m> .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: same cluster, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .


class: same cluster, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .


class: same cluster, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: same cluster, base class: no relation, new model class: no relation
first:
Modern image classification networks integrate multi - scale contextual information via <m> successive pooling and subsampling layers </m> that reduce resolution until a global prediction is obtained .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: same cluster, base class: first -> second, new model class: first -> second
first:
A delta-sigma , or sigma-delta , analog-to-digital converter ( ADC ) comprises both a modulator , which implements oversampling and noise shaping , and a decimator , which implements low-pass filtering and <m> downsampling </m> .
second:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Through a study of a family of <m> downsampling schemes </m> , an optimal one is found and analyzed for EC .
second:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .


class: same cluster, base class: first -> second, new model class: first -> second
first:
It is performed by a three-level noise reduction algorithm : ( i ) <m> data downsampling </m> , ( ii ) feature synchronization , and ( iii ) a modified version of graph total variation .
second:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .


class: same cluster, base class: no relation, new model class: first -> second
first:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: same cluster, base class: no relation, new model class: first -> second
first:
First , for the temporal trend , period , closeness properties , we obtain low-dimensional features by <m> downsampling high-dimensional input features </m> .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: same cluster, base class: no relation, new model class: second -> first
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
In the first system , we consider a method for <m> reorganizing , downsampling </m> and interpolating the residual data .


class: same cluster, base class: second -> first, new model class: second -> first
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
For characteristics of the system , Grabcut algorithm is optimized with <m> downsampling technology </m> to accelerate the segmentation speed and concurrently accelerates trimap generating algorithm and Shared Matting to further improve the processing speed .


class: same cluster, base class: no relation, new model class: second -> first
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
Either of these two configurations can be selected in real time by changing clock signals and <m> output downsampling filters </m> .


class: same cluster, base class: second -> first, new model class: second -> first
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
These can be categorised as multi - image SR methods and exploit explicit redundancy by constraining the ill - posed problem with additional information and attempting to invert the <m> downsampling process </m> .


class: same cluster, base class: second -> first, new model class: second -> first
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
The <m> downsampling operation </m> is deterministic and known : to produce from , we first convolve using a Gaussian filter - thus simulating the camera ’s point spread function - then downsample the image by a factor of .


class: same cluster, base class: second -> first, new model class: second -> first
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
Because the <m> downsampling method </m> can influence the compression performance , we have made the used downsampled images available .


class: same cluster, base class: second -> first, new model class: second -> first
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
subsection : Spatial Sampling Let be the <m> downsampling function </m> that decimates an image to the corresponding image of size .


class: same cluster, base class: second -> first, new model class: second -> first
first:
The invention discloses a method for generating and playing a network video , which comprises the following steps : when a streaming media server receives a video command , storing an original code stream file from a front end encoder by the streaming media server ; decoding the original code stream file stored by the streaming media server by a transcription server to obtain an original image sequence , performing <m> N times downsampling </m> on the original image sequence to obtain a subsequence of a corresponding frame rate , and encoding the subsequence to obtain a corresponding code stream file ; and when a client requests multiple-speed playing of the network video , sending a code stream file corresponding to the requested multiple-speed to the client at a normal speed by the streaming media server .
second:
There are two technical hurdles in the application of DCNNs to image labeling tasks : <m> signal downsampling </m> , and spatial ‘


class: same cluster, base class: no relation, new model class: first -> second
first:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: same cluster, base class: no relation, new model class: first -> second
first:
It consists of the repeated application of two 3x3 convolutions ( unpadded convolutions ) , each followed by a rectified linear unit ( ReLU ) and a 2x2 max pooling operation with stride 2 for <m> downsampling </m> .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: same cluster, base class: no relation, new model class: first -> second
first:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: same cluster, base class: no relation, new model class: first -> second
first:
Many networks [ reference ] ) involve <m> downsampling layers </m> to gather global information and to reduce memory usage .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: same cluster, base class: no relation, new model class: first -> second
first:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .
second:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .


class: same cluster, base class: no relation, new model class: first -> second
first:
Previous works typically set multiplicative scaling of feature map dimension for <m> downsampling modules </m> , which is implemented to give a larger degree of freedom to the classification part by increasing the feature map dimension of the output - side layers .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: same cluster, base class: no relation, new model class: no relation
first:
This leaves open the question of whether severe <m> intermediate downsampling </m> was truly necessary .
second:
Since the hourglass network has a symmetrical structure , <m> delayed downsampling </m> results in higher resolution feature maps during the upsampling .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .


class: same cluster, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: same cluster, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
These features do not yet exist in the <m> chatbot application system </m> in other studies .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .


class: same cluster, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
Results The database search retrieved 1513 citations ; 17 articles ( 14 different <m> conversational agents </m> ) met the inclusion criteria .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: first -> second, new model class: first -> second
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .


class: same cluster, base class: first -> second, new model class: first -> second
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .


class: same cluster, base class: first -> second, new model class: first -> second
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .


class: same cluster, base class: first -> second, new model class: first -> second
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .


class: same cluster, base class: first -> second, new model class: first -> second
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .


class: same cluster, base class: first -> second, new model class: first -> second
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .


class: same cluster, base class: first -> second, new model class: first -> second
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: same cluster, base class: first -> second, new model class: first -> second
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
These features do not yet exist in the <m> chatbot application system </m> in other studies .


class: same cluster, base class: first -> second, new model class: first -> second
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper proposes the development of Chatbot which acts as a <m> conversation agent </m> that can play a role of as student candidate service .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .


class: same cluster, base class: no relation, new model class: first -> second
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .


class: same cluster, base class: no relation, new model class: first -> second
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .


class: same cluster, base class: no relation, new model class: first -> second
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .


class: same cluster, base class: no relation, new model class: first -> second
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .


class: same cluster, base class: no relation, new model class: first -> second
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: same cluster, base class: no relation, new model class: first -> second
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
These features do not yet exist in the <m> chatbot application system </m> in other studies .


class: same cluster, base class: no relation, new model class: first -> second
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe a <m> conversational virtual agent </m> that is designed to be used in conjunction with group medical visits to help treat individuals with chronic pain and depression using non-medical treatments including yoga , meditation , and self-massage .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .


class: same cluster, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: same cluster, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
These features do not yet exist in the <m> chatbot application system </m> in other studies .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .


class: same cluster, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
Drawing on these findings we discuss key challenges for conversational agent design , most notably the need to redefine the design parameters for <m> conversational agent interaction </m> .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: first -> second, new model class: first -> second
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .


class: same cluster, base class: first -> second, new model class: first -> second
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .


class: same cluster, base class: first -> second, new model class: first -> second
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .


class: same cluster, base class: first -> second, new model class: first -> second
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .


class: same cluster, base class: first -> second, new model class: first -> second
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .


class: same cluster, base class: first -> second, new model class: first -> second
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .


class: same cluster, base class: first -> second, new model class: first -> second
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .


class: same cluster, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: same cluster, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: same cluster, base class: first -> second, new model class: first -> second
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
These features do not yet exist in the <m> chatbot application system </m> in other studies .


class: same cluster, base class: first -> second, new model class: first -> second
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .


class: same cluster, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
The proposed model of empathy is illustrated in a <m> conversational agent scenario </m> involving the virtual humans MAX and EMMA .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: first -> second, new model class: first -> second
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .


class: same cluster, base class: first -> second, new model class: first -> second
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .


class: same cluster, base class: first -> second, new model class: first -> second
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .


class: same cluster, base class: first -> second, new model class: first -> second
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .


class: same cluster, base class: first -> second, new model class: first -> second
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .


class: same cluster, base class: first -> second, new model class: first -> second
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .


class: same cluster, base class: first -> second, new model class: first -> second
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: same cluster, base class: first -> second, new model class: first -> second
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
These features do not yet exist in the <m> chatbot application system </m> in other studies .


class: same cluster, base class: first -> second, new model class: first -> second
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we describe various examples undertaken using this approach along with initial experiments using a <m> conversational software agent </m> to enable the field user to interact with such information in full natural English language .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .


class: same cluster, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: same cluster, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
These features do not yet exist in the <m> chatbot application system </m> in other studies .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .


class: same cluster, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
We describe a <m> conversational agent-based system </m> designed to provide longitudinal social support to isolated older adults .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .


class: same cluster, base class: no relation, new model class: first -> second
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .


class: same cluster, base class: no relation, new model class: first -> second
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .


class: same cluster, base class: no relation, new model class: first -> second
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .


class: same cluster, base class: no relation, new model class: first -> second
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .


class: same cluster, base class: no relation, new model class: first -> second
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .


class: same cluster, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: same cluster, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: same cluster, base class: no relation, new model class: first -> second
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
These features do not yet exist in the <m> chatbot application system </m> in other studies .


class: same cluster, base class: no relation, new model class: first -> second
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .


class: same cluster, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
We investigate the dynamics of human game playing with a <m> conversational computational agent </m> ( Virtual Human ) .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .


class: same cluster, base class: no relation, new model class: first -> second
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .


class: same cluster, base class: no relation, new model class: first -> second
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .


class: same cluster, base class: no relation, new model class: first -> second
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .


class: same cluster, base class: no relation, new model class: first -> second
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .


class: same cluster, base class: no relation, new model class: first -> second
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .


class: same cluster, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: same cluster, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: same cluster, base class: no relation, new model class: first -> second
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
These features do not yet exist in the <m> chatbot application system </m> in other studies .


class: same cluster, base class: no relation, new model class: first -> second
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .


class: same cluster, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
Once matched , they are provided with a link to a chat room where they can work with their partner students on a synchronous collaboration activity , supported by a <m> conversational computer agent </m> .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: same cluster, base class: no relation, new model class: first -> second
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: same cluster, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
A <m> chatbot </m> is a software that is able to autonomously communicate with a human being through text and due to its usefulness , an increasing number of businesses are implementing such tools in order to provide timely communication to their clients .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: same cluster, base class: no relation, new model class: first -> second
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: same cluster, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
While these frameworks are efficient to design simple <m> chatbot applications </m> , they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs ( e.g. it is typically impossible to change the NL engine provider ) .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: same cluster, base class: no relation, new model class: first -> second
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: same cluster, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
After successful identification of emotions , a <m> chatbot framework </m> is presented which can adapt to interactive dialogues with the customer based on the emotion from the speech in the audio .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: same cluster, base class: no relation, new model class: first -> second
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: same cluster, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
Using these <m> chatbot platforms </m> , The University of Hong Kong ( HKU ) , through Technology-Enriched Learning Initiative , leverages on its experiences in integrating this technology to better enhance the learning experience of the students .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: same cluster, base class: no relation, new model class: first -> second
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: same cluster, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
In this chapter we describe different <m> chatbot architectures </m> , exploiting the use of ontologies in order to create clever information suppliers overcoming the main limits of chatbots : the knowledge base building and the rigidness of the dialogue mechanism .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: same cluster, base class: no relation, new model class: first -> second
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: same cluster, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
The first section of the chapter introduces the <m> chatbot concept </m> , followed by a section on implementing a basic rule-based chatbot system .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: same cluster, base class: no relation, new model class: first -> second
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: same cluster, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
Besides the original <m> chatbot function </m> for dialogue simulation , we have developed intelligent vocabulary assessment function and course management function .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .


class: same cluster, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: same cluster, base class: no relation, new model class: second -> first
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
These features do not yet exist in the <m> chatbot application system </m> in other studies .


class: same cluster, base class: no relation, new model class: second -> first
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .


class: same cluster, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
To explore the usefulness of the typology , we present four example chatbot purposes for which the typology may support <m> analysis of high-level chatbot interaction design </m> .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: second -> first, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
These features do not yet exist in the <m> chatbot application system </m> in other studies .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper , we introduced Jiao Xiao Tong , an <m> interactive question answering chatbot </m> designed in assisting students and faculties in Shanghai Jiao Tong University to accomplish various campus activities such as setting remainders , answering school trivia questions , route navigation and etc .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: no relation, new model class: second -> first
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
These features do not yet exist in the <m> chatbot application system </m> in other studies .


class: same cluster, base class: no relation, new model class: second -> first
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> intelligent chatbot tool </m> will make use of Natural Language Processing techniques to answer the queries by high school students .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
These features do not yet exist in the <m> chatbot application system </m> in other studies .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
This resarch helps applications like <m> virtual assistant </m> with classification of the sentence and giving the output of the class .


class: same cluster, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
The use <m> virtual assistant application </m> in teaching pronunciation ( An Experimental research at second grade of SMAN Krakatau Steel Cilegon ) This research is intended to obtain the empirical evidence about the effect of virtual assistant application in teaching pronunciation at the second grade of SMAN 2 Krakatau steel , the method of this research was a quasi-experimental research which classified as a quantitative research .


class: same cluster, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
To overcome such issues , in this paper we combine some of the most advanced techniques in computer vision , deep learning , speech generation and recognition , and artificial intelligence , into a <m> virtual assistant architecture </m> for smart home automation systems .


class: same cluster, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
The <m> virtual assistant concept </m> is one that many technology companies have taken on despite having other well-developed and popular user interfaces .


class: same cluster, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
Instant messaging solutions , mobile devices and the <m> virtual assistant paradigm </m> have also come into the picture .


class: same cluster, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
We provide a short state of the art on <m> virtual assistant 's technology </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
The <m> Virtual Assistant software </m> available in the market are not specifically catered for visually disabled .


class: same cluster, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
Following an upsurge in mobile device usage and improvements in speech recognition performance , multiple <m> virtual personal assistant systems </m> have emerged , and have been widely adopted by users .


class: same cluster, base class: no relation, new model class: no relation
first:
However , in many other practical applications , such as poem generation and <m> chatbot </m> , a task specific loss may not be directly available to score a generated sequence accurately .
second:
As a result , many <m> AI assistants </m> are available in the market such as Software and Hardware based ( Google Assistant , Amazon Alexa , and Apple Siri ) and Augmented Reality based ( Microsoft HoloLens ) .


class: same cluster, base class: no relation, new model class: second -> first
first:
This paper formulates two <m> stochastic nonautonomous SIRI epidemic systems </m> with nonlinear perturbations .
second:
The main aim of this study is to investigate stochastic dynamics of the two <m> SIRI epidemic systems </m> and obtain their thresholds .


class: same cluster, base class: no relation, new model class: first -> second
first:
The main aim of this study is to investigate stochastic dynamics of the two <m> SIRI epidemic systems </m> and obtain their thresholds .
second:
For the <m> nonautonomous stochastic SIRI epidemic system </m> with white noise , the authors provide analytic results regarding the stochastic boundedness , stochastic permanence and persistence in mean .


class: same cluster, base class: no relation, new model class: no relation
first:
Thanks to the dynamic viewpoint of <m> community detection </m> , Attractor has several potential attractive properties : ( a ) Attractor provides an intuitive solution to analyze the community structure of a network , and faithfully captures the natural communities ( with high quality ) .
second:
The first approach is based on a traditional machine learning method , and another approach exploits the <m> graph partitioning algorithm </m> and two deep neural models to generate the final clustering .


class: same cluster, base class: no relation, new model class: no relation
first:
Thanks to the dynamic viewpoint of <m> community detection </m> , Attractor has several potential attractive properties : ( a ) Attractor provides an intuitive solution to analyze the community structure of a network , and faithfully captures the natural communities ( with high quality ) .
second:
Our approach finds interesting mapping solutions in few milliseconds that makes it doable at regular time by translating it in an equivalent <m> graph partitioning problem </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Thanks to the dynamic viewpoint of <m> community detection </m> , Attractor has several potential attractive properties : ( a ) Attractor provides an intuitive solution to analyze the community structure of a network , and faithfully captures the natural communities ( with high quality ) .
second:
A <m> graph partitioning formulation </m> is presented and different ILP formulations are proposed , obtained by strengthening and/or relaxing constraints and by reducing the number of integer variables .


class: same cluster, base class: no relation, new model class: no relation
first:
Thanks to the dynamic viewpoint of <m> community detection </m> , Attractor has several potential attractive properties : ( a ) Attractor provides an intuitive solution to analyze the community structure of a network , and faithfully captures the natural communities ( with high quality ) .
second:
The authors define the characteristics that are needed in the service areas for digital cellular systems and state the service area design problem as a <m> graph partitioning optimization problem </m> where the goal is to minimize the diameter of the resulting subgraphs .


class: same cluster, base class: no relation, new model class: no relation
first:
Thanks to the dynamic viewpoint of <m> community detection </m> , Attractor has several potential attractive properties : ( a ) Attractor provides an intuitive solution to analyze the community structure of a network , and faithfully captures the natural communities ( with high quality ) .
second:
The most important issue of big data processing is the relevance of analytical data ; thought of this paper is to analyze the data as a <m> graph optimal partitioning problem </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Thanks to the dynamic viewpoint of <m> community detection </m> , Attractor has several potential attractive properties : ( a ) Attractor provides an intuitive solution to analyze the community structure of a network , and faithfully captures the natural communities ( with high quality ) .
second:
Furthermore , we show that existing <m> graph partitioning problem formulations </m> do not map to how big data jobs work , causing their solutions to miss opportunities for avoiding data movement .


class: same cluster, base class: no relation, new model class: no relation
first:
Thanks to the dynamic viewpoint of <m> community detection </m> , Attractor has several potential attractive properties : ( a ) Attractor provides an intuitive solution to analyze the community structure of a network , and faithfully captures the natural communities ( with high quality ) .
second:
The first problem is a <m> partitioning problem of graphs </m> , and second is a problem of measuring self similarity of a graph .


class: same cluster, base class: no relation, new model class: no relation
first:
Thanks to the dynamic viewpoint of <m> community detection </m> , Attractor has several potential attractive properties : ( a ) Attractor provides an intuitive solution to analyze the community structure of a network , and faithfully captures the natural communities ( with high quality ) .
second:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .


class: same cluster, base class: no relation, new model class: no relation
first:
Thanks to the dynamic viewpoint of <m> community detection </m> , Attractor has several potential attractive properties : ( a ) Attractor provides an intuitive solution to analyze the community structure of a network , and faithfully captures the natural communities ( with high quality ) .
second:
To that end we use the <m> graph partitioning formulation </m> that has been used for people tracking and pose estimation in the past , but has not been shown to enable articulated people tracking .


class: same cluster, base class: no relation, new model class: no relation
first:
Thanks to the dynamic viewpoint of <m> community detection </m> , Attractor has several potential attractive properties : ( a ) Attractor provides an intuitive solution to analyze the community structure of a network , and faithfully captures the natural communities ( with high quality ) .
second:
Graph simplification in TD / BU allows to further reduce the inference time for <m> graph partitioning </m> ( vs. for BU - sparse ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Thanks to the dynamic viewpoint of <m> community detection </m> , Attractor has several potential attractive properties : ( a ) Attractor provides an intuitive solution to analyze the community structure of a network , and faithfully captures the natural communities ( with high quality ) .
second:
Grouping is formulated as a <m> graph partitioning problem </m> that lends itself to efficient inference with recent local search techniques .


class: same cluster, base class: no relation, new model class: no relation
first:
Thanks to the dynamic viewpoint of <m> community detection </m> , Attractor has several potential attractive properties : ( a ) Attractor provides an intuitive solution to analyze the community structure of a network , and faithfully captures the natural communities ( with high quality ) .
second:
In addition , GPN formulates the inference procedure for joint configurations of human poses as a <m> graph partition problem </m> , and conducts local optimization for each person detection with reliable global affinity cues , leading to complexity reduction and performance improvement .


class: same cluster, base class: no relation, new model class: no relation
first:
Thanks to the dynamic viewpoint of <m> community detection </m> , Attractor has several potential attractive properties : ( a ) Attractor provides an intuitive solution to analyze the community structure of a network , and faithfully captures the natural communities ( with high quality ) .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: no relation
first:
Thanks to the dynamic viewpoint of <m> community detection </m> , Attractor has several potential attractive properties : ( a ) Attractor provides an intuitive solution to analyze the community structure of a network , and faithfully captures the natural communities ( with high quality ) .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: no relation
first:
Thanks to the dynamic viewpoint of <m> community detection </m> , Attractor has several potential attractive properties : ( a ) Attractor provides an intuitive solution to analyze the community structure of a network , and faithfully captures the natural communities ( with high quality ) .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: no relation
first:
Thanks to the dynamic viewpoint of <m> community detection </m> , Attractor has several potential attractive properties : ( a ) Attractor provides an intuitive solution to analyze the community structure of a network , and faithfully captures the natural communities ( with high quality ) .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Thanks to the dynamic viewpoint of <m> community detection </m> , Attractor has several potential attractive properties : ( a ) Attractor provides an intuitive solution to analyze the community structure of a network , and faithfully captures the natural communities ( with high quality ) .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: no relation
first:
Thanks to the dynamic viewpoint of <m> community detection </m> , Attractor has several potential attractive properties : ( a ) Attractor provides an intuitive solution to analyze the community structure of a network , and faithfully captures the natural communities ( with high quality ) .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we solve the <m> community detection problem </m> for bipartite networks by formulating a bipartite stochastic block model , which explicitly includes vertex type information and may be trivially extended to k-partite networks .
second:
The first approach is based on a traditional machine learning method , and another approach exploits the <m> graph partitioning algorithm </m> and two deep neural models to generate the final clustering .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we solve the <m> community detection problem </m> for bipartite networks by formulating a bipartite stochastic block model , which explicitly includes vertex type information and may be trivially extended to k-partite networks .
second:
Our approach finds interesting mapping solutions in few milliseconds that makes it doable at regular time by translating it in an equivalent <m> graph partitioning problem </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we solve the <m> community detection problem </m> for bipartite networks by formulating a bipartite stochastic block model , which explicitly includes vertex type information and may be trivially extended to k-partite networks .
second:
A <m> graph partitioning formulation </m> is presented and different ILP formulations are proposed , obtained by strengthening and/or relaxing constraints and by reducing the number of integer variables .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we solve the <m> community detection problem </m> for bipartite networks by formulating a bipartite stochastic block model , which explicitly includes vertex type information and may be trivially extended to k-partite networks .
second:
The authors define the characteristics that are needed in the service areas for digital cellular systems and state the service area design problem as a <m> graph partitioning optimization problem </m> where the goal is to minimize the diameter of the resulting subgraphs .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we solve the <m> community detection problem </m> for bipartite networks by formulating a bipartite stochastic block model , which explicitly includes vertex type information and may be trivially extended to k-partite networks .
second:
The most important issue of big data processing is the relevance of analytical data ; thought of this paper is to analyze the data as a <m> graph optimal partitioning problem </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we solve the <m> community detection problem </m> for bipartite networks by formulating a bipartite stochastic block model , which explicitly includes vertex type information and may be trivially extended to k-partite networks .
second:
Furthermore , we show that existing <m> graph partitioning problem formulations </m> do not map to how big data jobs work , causing their solutions to miss opportunities for avoiding data movement .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we solve the <m> community detection problem </m> for bipartite networks by formulating a bipartite stochastic block model , which explicitly includes vertex type information and may be trivially extended to k-partite networks .
second:
The first problem is a <m> partitioning problem of graphs </m> , and second is a problem of measuring self similarity of a graph .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we solve the <m> community detection problem </m> for bipartite networks by formulating a bipartite stochastic block model , which explicitly includes vertex type information and may be trivially extended to k-partite networks .
second:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we solve the <m> community detection problem </m> for bipartite networks by formulating a bipartite stochastic block model , which explicitly includes vertex type information and may be trivially extended to k-partite networks .
second:
To that end we use the <m> graph partitioning formulation </m> that has been used for people tracking and pose estimation in the past , but has not been shown to enable articulated people tracking .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we solve the <m> community detection problem </m> for bipartite networks by formulating a bipartite stochastic block model , which explicitly includes vertex type information and may be trivially extended to k-partite networks .
second:
Graph simplification in TD / BU allows to further reduce the inference time for <m> graph partitioning </m> ( vs. for BU - sparse ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we solve the <m> community detection problem </m> for bipartite networks by formulating a bipartite stochastic block model , which explicitly includes vertex type information and may be trivially extended to k-partite networks .
second:
Grouping is formulated as a <m> graph partitioning problem </m> that lends itself to efficient inference with recent local search techniques .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we solve the <m> community detection problem </m> for bipartite networks by formulating a bipartite stochastic block model , which explicitly includes vertex type information and may be trivially extended to k-partite networks .
second:
In addition , GPN formulates the inference procedure for joint configurations of human poses as a <m> graph partition problem </m> , and conducts local optimization for each person detection with reliable global affinity cues , leading to complexity reduction and performance improvement .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we solve the <m> community detection problem </m> for bipartite networks by formulating a bipartite stochastic block model , which explicitly includes vertex type information and may be trivially extended to k-partite networks .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we solve the <m> community detection problem </m> for bipartite networks by formulating a bipartite stochastic block model , which explicitly includes vertex type information and may be trivially extended to k-partite networks .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we solve the <m> community detection problem </m> for bipartite networks by formulating a bipartite stochastic block model , which explicitly includes vertex type information and may be trivially extended to k-partite networks .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we solve the <m> community detection problem </m> for bipartite networks by formulating a bipartite stochastic block model , which explicitly includes vertex type information and may be trivially extended to k-partite networks .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we solve the <m> community detection problem </m> for bipartite networks by formulating a bipartite stochastic block model , which explicitly includes vertex type information and may be trivially extended to k-partite networks .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we solve the <m> community detection problem </m> for bipartite networks by formulating a bipartite stochastic block model , which explicitly includes vertex type information and may be trivially extended to k-partite networks .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
However , existing community detecting algorithms are all based on pages as units , the topic drift phenomenon becomes the inherent problem of each existing <m> community detecting algorithm </m> since the web pages always have multiple topics in content .
second:
The first approach is based on a traditional machine learning method , and another approach exploits the <m> graph partitioning algorithm </m> and two deep neural models to generate the final clustering .


class: same cluster, base class: no relation, new model class: no relation
first:
However , existing community detecting algorithms are all based on pages as units , the topic drift phenomenon becomes the inherent problem of each existing <m> community detecting algorithm </m> since the web pages always have multiple topics in content .
second:
Our approach finds interesting mapping solutions in few milliseconds that makes it doable at regular time by translating it in an equivalent <m> graph partitioning problem </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
However , existing community detecting algorithms are all based on pages as units , the topic drift phenomenon becomes the inherent problem of each existing <m> community detecting algorithm </m> since the web pages always have multiple topics in content .
second:
A <m> graph partitioning formulation </m> is presented and different ILP formulations are proposed , obtained by strengthening and/or relaxing constraints and by reducing the number of integer variables .


class: same cluster, base class: no relation, new model class: no relation
first:
However , existing community detecting algorithms are all based on pages as units , the topic drift phenomenon becomes the inherent problem of each existing <m> community detecting algorithm </m> since the web pages always have multiple topics in content .
second:
The authors define the characteristics that are needed in the service areas for digital cellular systems and state the service area design problem as a <m> graph partitioning optimization problem </m> where the goal is to minimize the diameter of the resulting subgraphs .


class: same cluster, base class: no relation, new model class: no relation
first:
However , existing community detecting algorithms are all based on pages as units , the topic drift phenomenon becomes the inherent problem of each existing <m> community detecting algorithm </m> since the web pages always have multiple topics in content .
second:
The most important issue of big data processing is the relevance of analytical data ; thought of this paper is to analyze the data as a <m> graph optimal partitioning problem </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
However , existing community detecting algorithms are all based on pages as units , the topic drift phenomenon becomes the inherent problem of each existing <m> community detecting algorithm </m> since the web pages always have multiple topics in content .
second:
Furthermore , we show that existing <m> graph partitioning problem formulations </m> do not map to how big data jobs work , causing their solutions to miss opportunities for avoiding data movement .


class: same cluster, base class: no relation, new model class: no relation
first:
However , existing community detecting algorithms are all based on pages as units , the topic drift phenomenon becomes the inherent problem of each existing <m> community detecting algorithm </m> since the web pages always have multiple topics in content .
second:
The first problem is a <m> partitioning problem of graphs </m> , and second is a problem of measuring self similarity of a graph .


class: same cluster, base class: no relation, new model class: no relation
first:
However , existing community detecting algorithms are all based on pages as units , the topic drift phenomenon becomes the inherent problem of each existing <m> community detecting algorithm </m> since the web pages always have multiple topics in content .
second:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .


class: same cluster, base class: no relation, new model class: no relation
first:
However , existing community detecting algorithms are all based on pages as units , the topic drift phenomenon becomes the inherent problem of each existing <m> community detecting algorithm </m> since the web pages always have multiple topics in content .
second:
To that end we use the <m> graph partitioning formulation </m> that has been used for people tracking and pose estimation in the past , but has not been shown to enable articulated people tracking .


class: same cluster, base class: no relation, new model class: no relation
first:
However , existing community detecting algorithms are all based on pages as units , the topic drift phenomenon becomes the inherent problem of each existing <m> community detecting algorithm </m> since the web pages always have multiple topics in content .
second:
Graph simplification in TD / BU allows to further reduce the inference time for <m> graph partitioning </m> ( vs. for BU - sparse ) .


class: same cluster, base class: no relation, new model class: no relation
first:
However , existing community detecting algorithms are all based on pages as units , the topic drift phenomenon becomes the inherent problem of each existing <m> community detecting algorithm </m> since the web pages always have multiple topics in content .
second:
Grouping is formulated as a <m> graph partitioning problem </m> that lends itself to efficient inference with recent local search techniques .


class: same cluster, base class: no relation, new model class: no relation
first:
However , existing community detecting algorithms are all based on pages as units , the topic drift phenomenon becomes the inherent problem of each existing <m> community detecting algorithm </m> since the web pages always have multiple topics in content .
second:
In addition , GPN formulates the inference procedure for joint configurations of human poses as a <m> graph partition problem </m> , and conducts local optimization for each person detection with reliable global affinity cues , leading to complexity reduction and performance improvement .


class: same cluster, base class: no relation, new model class: no relation
first:
However , existing community detecting algorithms are all based on pages as units , the topic drift phenomenon becomes the inherent problem of each existing <m> community detecting algorithm </m> since the web pages always have multiple topics in content .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: no relation
first:
However , existing community detecting algorithms are all based on pages as units , the topic drift phenomenon becomes the inherent problem of each existing <m> community detecting algorithm </m> since the web pages always have multiple topics in content .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: no relation
first:
However , existing community detecting algorithms are all based on pages as units , the topic drift phenomenon becomes the inherent problem of each existing <m> community detecting algorithm </m> since the web pages always have multiple topics in content .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: no relation
first:
However , existing community detecting algorithms are all based on pages as units , the topic drift phenomenon becomes the inherent problem of each existing <m> community detecting algorithm </m> since the web pages always have multiple topics in content .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: no relation
first:
However , existing community detecting algorithms are all based on pages as units , the topic drift phenomenon becomes the inherent problem of each existing <m> community detecting algorithm </m> since the web pages always have multiple topics in content .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: no relation
first:
However , existing community detecting algorithms are all based on pages as units , the topic drift phenomenon becomes the inherent problem of each existing <m> community detecting algorithm </m> since the web pages always have multiple topics in content .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
However , with the development of recent applications , rich edge content can be available to give another view to the <m> community detection process </m> .
second:
The first approach is based on a traditional machine learning method , and another approach exploits the <m> graph partitioning algorithm </m> and two deep neural models to generate the final clustering .


class: same cluster, base class: no relation, new model class: no relation
first:
However , with the development of recent applications , rich edge content can be available to give another view to the <m> community detection process </m> .
second:
Our approach finds interesting mapping solutions in few milliseconds that makes it doable at regular time by translating it in an equivalent <m> graph partitioning problem </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
However , with the development of recent applications , rich edge content can be available to give another view to the <m> community detection process </m> .
second:
A <m> graph partitioning formulation </m> is presented and different ILP formulations are proposed , obtained by strengthening and/or relaxing constraints and by reducing the number of integer variables .


class: same cluster, base class: no relation, new model class: no relation
first:
However , with the development of recent applications , rich edge content can be available to give another view to the <m> community detection process </m> .
second:
The authors define the characteristics that are needed in the service areas for digital cellular systems and state the service area design problem as a <m> graph partitioning optimization problem </m> where the goal is to minimize the diameter of the resulting subgraphs .


class: same cluster, base class: no relation, new model class: no relation
first:
However , with the development of recent applications , rich edge content can be available to give another view to the <m> community detection process </m> .
second:
The most important issue of big data processing is the relevance of analytical data ; thought of this paper is to analyze the data as a <m> graph optimal partitioning problem </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
However , with the development of recent applications , rich edge content can be available to give another view to the <m> community detection process </m> .
second:
Furthermore , we show that existing <m> graph partitioning problem formulations </m> do not map to how big data jobs work , causing their solutions to miss opportunities for avoiding data movement .


class: same cluster, base class: no relation, new model class: no relation
first:
However , with the development of recent applications , rich edge content can be available to give another view to the <m> community detection process </m> .
second:
The first problem is a <m> partitioning problem of graphs </m> , and second is a problem of measuring self similarity of a graph .


class: same cluster, base class: no relation, new model class: no relation
first:
However , with the development of recent applications , rich edge content can be available to give another view to the <m> community detection process </m> .
second:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .


class: same cluster, base class: no relation, new model class: no relation
first:
However , with the development of recent applications , rich edge content can be available to give another view to the <m> community detection process </m> .
second:
To that end we use the <m> graph partitioning formulation </m> that has been used for people tracking and pose estimation in the past , but has not been shown to enable articulated people tracking .


class: same cluster, base class: no relation, new model class: no relation
first:
However , with the development of recent applications , rich edge content can be available to give another view to the <m> community detection process </m> .
second:
Graph simplification in TD / BU allows to further reduce the inference time for <m> graph partitioning </m> ( vs. for BU - sparse ) .


class: same cluster, base class: no relation, new model class: no relation
first:
However , with the development of recent applications , rich edge content can be available to give another view to the <m> community detection process </m> .
second:
Grouping is formulated as a <m> graph partitioning problem </m> that lends itself to efficient inference with recent local search techniques .


class: same cluster, base class: no relation, new model class: no relation
first:
However , with the development of recent applications , rich edge content can be available to give another view to the <m> community detection process </m> .
second:
In addition , GPN formulates the inference procedure for joint configurations of human poses as a <m> graph partition problem </m> , and conducts local optimization for each person detection with reliable global affinity cues , leading to complexity reduction and performance improvement .


class: same cluster, base class: no relation, new model class: no relation
first:
However , with the development of recent applications , rich edge content can be available to give another view to the <m> community detection process </m> .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: no relation
first:
However , with the development of recent applications , rich edge content can be available to give another view to the <m> community detection process </m> .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: no relation
first:
However , with the development of recent applications , rich edge content can be available to give another view to the <m> community detection process </m> .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: no relation
first:
However , with the development of recent applications , rich edge content can be available to give another view to the <m> community detection process </m> .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: no relation
first:
However , with the development of recent applications , rich edge content can be available to give another view to the <m> community detection process </m> .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: no relation
first:
However , with the development of recent applications , rich edge content can be available to give another view to the <m> community detection process </m> .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We present the results of a <m> community detection analysis </m> of the Wikipedia graph .
second:
The first approach is based on a traditional machine learning method , and another approach exploits the <m> graph partitioning algorithm </m> and two deep neural models to generate the final clustering .


class: same cluster, base class: no relation, new model class: no relation
first:
We present the results of a <m> community detection analysis </m> of the Wikipedia graph .
second:
Our approach finds interesting mapping solutions in few milliseconds that makes it doable at regular time by translating it in an equivalent <m> graph partitioning problem </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We present the results of a <m> community detection analysis </m> of the Wikipedia graph .
second:
A <m> graph partitioning formulation </m> is presented and different ILP formulations are proposed , obtained by strengthening and/or relaxing constraints and by reducing the number of integer variables .


class: same cluster, base class: no relation, new model class: no relation
first:
We present the results of a <m> community detection analysis </m> of the Wikipedia graph .
second:
The authors define the characteristics that are needed in the service areas for digital cellular systems and state the service area design problem as a <m> graph partitioning optimization problem </m> where the goal is to minimize the diameter of the resulting subgraphs .


class: same cluster, base class: no relation, new model class: no relation
first:
We present the results of a <m> community detection analysis </m> of the Wikipedia graph .
second:
The most important issue of big data processing is the relevance of analytical data ; thought of this paper is to analyze the data as a <m> graph optimal partitioning problem </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We present the results of a <m> community detection analysis </m> of the Wikipedia graph .
second:
Furthermore , we show that existing <m> graph partitioning problem formulations </m> do not map to how big data jobs work , causing their solutions to miss opportunities for avoiding data movement .


class: same cluster, base class: no relation, new model class: no relation
first:
We present the results of a <m> community detection analysis </m> of the Wikipedia graph .
second:
The first problem is a <m> partitioning problem of graphs </m> , and second is a problem of measuring self similarity of a graph .


class: same cluster, base class: no relation, new model class: no relation
first:
We present the results of a <m> community detection analysis </m> of the Wikipedia graph .
second:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .


class: same cluster, base class: no relation, new model class: no relation
first:
We present the results of a <m> community detection analysis </m> of the Wikipedia graph .
second:
To that end we use the <m> graph partitioning formulation </m> that has been used for people tracking and pose estimation in the past , but has not been shown to enable articulated people tracking .


class: same cluster, base class: no relation, new model class: no relation
first:
We present the results of a <m> community detection analysis </m> of the Wikipedia graph .
second:
Graph simplification in TD / BU allows to further reduce the inference time for <m> graph partitioning </m> ( vs. for BU - sparse ) .


class: same cluster, base class: no relation, new model class: no relation
first:
We present the results of a <m> community detection analysis </m> of the Wikipedia graph .
second:
Grouping is formulated as a <m> graph partitioning problem </m> that lends itself to efficient inference with recent local search techniques .


class: same cluster, base class: no relation, new model class: no relation
first:
We present the results of a <m> community detection analysis </m> of the Wikipedia graph .
second:
In addition , GPN formulates the inference procedure for joint configurations of human poses as a <m> graph partition problem </m> , and conducts local optimization for each person detection with reliable global affinity cues , leading to complexity reduction and performance improvement .


class: same cluster, base class: no relation, new model class: no relation
first:
We present the results of a <m> community detection analysis </m> of the Wikipedia graph .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: no relation
first:
We present the results of a <m> community detection analysis </m> of the Wikipedia graph .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: no relation
first:
We present the results of a <m> community detection analysis </m> of the Wikipedia graph .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: no relation
first:
We present the results of a <m> community detection analysis </m> of the Wikipedia graph .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: no relation
first:
We present the results of a <m> community detection analysis </m> of the Wikipedia graph .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: no relation
first:
We present the results of a <m> community detection analysis </m> of the Wikipedia graph .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We show that communities produced by a synthetic graph generator commonly used in <m> community detection research </m> are very dissimilar to those found in real graphs .
second:
The first approach is based on a traditional machine learning method , and another approach exploits the <m> graph partitioning algorithm </m> and two deep neural models to generate the final clustering .


class: same cluster, base class: no relation, new model class: no relation
first:
We show that communities produced by a synthetic graph generator commonly used in <m> community detection research </m> are very dissimilar to those found in real graphs .
second:
Our approach finds interesting mapping solutions in few milliseconds that makes it doable at regular time by translating it in an equivalent <m> graph partitioning problem </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We show that communities produced by a synthetic graph generator commonly used in <m> community detection research </m> are very dissimilar to those found in real graphs .
second:
A <m> graph partitioning formulation </m> is presented and different ILP formulations are proposed , obtained by strengthening and/or relaxing constraints and by reducing the number of integer variables .


class: same cluster, base class: no relation, new model class: no relation
first:
We show that communities produced by a synthetic graph generator commonly used in <m> community detection research </m> are very dissimilar to those found in real graphs .
second:
The authors define the characteristics that are needed in the service areas for digital cellular systems and state the service area design problem as a <m> graph partitioning optimization problem </m> where the goal is to minimize the diameter of the resulting subgraphs .


class: same cluster, base class: no relation, new model class: no relation
first:
We show that communities produced by a synthetic graph generator commonly used in <m> community detection research </m> are very dissimilar to those found in real graphs .
second:
The most important issue of big data processing is the relevance of analytical data ; thought of this paper is to analyze the data as a <m> graph optimal partitioning problem </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We show that communities produced by a synthetic graph generator commonly used in <m> community detection research </m> are very dissimilar to those found in real graphs .
second:
Furthermore , we show that existing <m> graph partitioning problem formulations </m> do not map to how big data jobs work , causing their solutions to miss opportunities for avoiding data movement .


class: same cluster, base class: no relation, new model class: no relation
first:
We show that communities produced by a synthetic graph generator commonly used in <m> community detection research </m> are very dissimilar to those found in real graphs .
second:
The first problem is a <m> partitioning problem of graphs </m> , and second is a problem of measuring self similarity of a graph .


class: same cluster, base class: no relation, new model class: no relation
first:
We show that communities produced by a synthetic graph generator commonly used in <m> community detection research </m> are very dissimilar to those found in real graphs .
second:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .


class: same cluster, base class: no relation, new model class: no relation
first:
We show that communities produced by a synthetic graph generator commonly used in <m> community detection research </m> are very dissimilar to those found in real graphs .
second:
To that end we use the <m> graph partitioning formulation </m> that has been used for people tracking and pose estimation in the past , but has not been shown to enable articulated people tracking .


class: same cluster, base class: no relation, new model class: no relation
first:
We show that communities produced by a synthetic graph generator commonly used in <m> community detection research </m> are very dissimilar to those found in real graphs .
second:
Graph simplification in TD / BU allows to further reduce the inference time for <m> graph partitioning </m> ( vs. for BU - sparse ) .


class: same cluster, base class: no relation, new model class: no relation
first:
We show that communities produced by a synthetic graph generator commonly used in <m> community detection research </m> are very dissimilar to those found in real graphs .
second:
Grouping is formulated as a <m> graph partitioning problem </m> that lends itself to efficient inference with recent local search techniques .


class: same cluster, base class: no relation, new model class: no relation
first:
We show that communities produced by a synthetic graph generator commonly used in <m> community detection research </m> are very dissimilar to those found in real graphs .
second:
In addition , GPN formulates the inference procedure for joint configurations of human poses as a <m> graph partition problem </m> , and conducts local optimization for each person detection with reliable global affinity cues , leading to complexity reduction and performance improvement .


class: same cluster, base class: no relation, new model class: no relation
first:
We show that communities produced by a synthetic graph generator commonly used in <m> community detection research </m> are very dissimilar to those found in real graphs .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: no relation
first:
We show that communities produced by a synthetic graph generator commonly used in <m> community detection research </m> are very dissimilar to those found in real graphs .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: no relation
first:
We show that communities produced by a synthetic graph generator commonly used in <m> community detection research </m> are very dissimilar to those found in real graphs .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: no relation
first:
We show that communities produced by a synthetic graph generator commonly used in <m> community detection research </m> are very dissimilar to those found in real graphs .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: no relation
first:
We show that communities produced by a synthetic graph generator commonly used in <m> community detection research </m> are very dissimilar to those found in real graphs .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: no relation
first:
We show that communities produced by a synthetic graph generator commonly used in <m> community detection research </m> are very dissimilar to those found in real graphs .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Previous <m> community detection studies </m> usually account for the structural factor of social networks to build their models .
second:
The first approach is based on a traditional machine learning method , and another approach exploits the <m> graph partitioning algorithm </m> and two deep neural models to generate the final clustering .


class: same cluster, base class: no relation, new model class: no relation
first:
Previous <m> community detection studies </m> usually account for the structural factor of social networks to build their models .
second:
Our approach finds interesting mapping solutions in few milliseconds that makes it doable at regular time by translating it in an equivalent <m> graph partitioning problem </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Previous <m> community detection studies </m> usually account for the structural factor of social networks to build their models .
second:
A <m> graph partitioning formulation </m> is presented and different ILP formulations are proposed , obtained by strengthening and/or relaxing constraints and by reducing the number of integer variables .


class: same cluster, base class: no relation, new model class: no relation
first:
Previous <m> community detection studies </m> usually account for the structural factor of social networks to build their models .
second:
The authors define the characteristics that are needed in the service areas for digital cellular systems and state the service area design problem as a <m> graph partitioning optimization problem </m> where the goal is to minimize the diameter of the resulting subgraphs .


class: same cluster, base class: no relation, new model class: no relation
first:
Previous <m> community detection studies </m> usually account for the structural factor of social networks to build their models .
second:
The most important issue of big data processing is the relevance of analytical data ; thought of this paper is to analyze the data as a <m> graph optimal partitioning problem </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Previous <m> community detection studies </m> usually account for the structural factor of social networks to build their models .
second:
Furthermore , we show that existing <m> graph partitioning problem formulations </m> do not map to how big data jobs work , causing their solutions to miss opportunities for avoiding data movement .


class: same cluster, base class: no relation, new model class: no relation
first:
Previous <m> community detection studies </m> usually account for the structural factor of social networks to build their models .
second:
The first problem is a <m> partitioning problem of graphs </m> , and second is a problem of measuring self similarity of a graph .


class: same cluster, base class: no relation, new model class: no relation
first:
Previous <m> community detection studies </m> usually account for the structural factor of social networks to build their models .
second:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .


class: same cluster, base class: no relation, new model class: no relation
first:
Previous <m> community detection studies </m> usually account for the structural factor of social networks to build their models .
second:
To that end we use the <m> graph partitioning formulation </m> that has been used for people tracking and pose estimation in the past , but has not been shown to enable articulated people tracking .


class: same cluster, base class: no relation, new model class: no relation
first:
Previous <m> community detection studies </m> usually account for the structural factor of social networks to build their models .
second:
Graph simplification in TD / BU allows to further reduce the inference time for <m> graph partitioning </m> ( vs. for BU - sparse ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Previous <m> community detection studies </m> usually account for the structural factor of social networks to build their models .
second:
Grouping is formulated as a <m> graph partitioning problem </m> that lends itself to efficient inference with recent local search techniques .


class: same cluster, base class: no relation, new model class: no relation
first:
Previous <m> community detection studies </m> usually account for the structural factor of social networks to build their models .
second:
In addition , GPN formulates the inference procedure for joint configurations of human poses as a <m> graph partition problem </m> , and conducts local optimization for each person detection with reliable global affinity cues , leading to complexity reduction and performance improvement .


class: same cluster, base class: no relation, new model class: no relation
first:
Previous <m> community detection studies </m> usually account for the structural factor of social networks to build their models .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: no relation
first:
Previous <m> community detection studies </m> usually account for the structural factor of social networks to build their models .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: no relation
first:
Previous <m> community detection studies </m> usually account for the structural factor of social networks to build their models .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: no relation
first:
Previous <m> community detection studies </m> usually account for the structural factor of social networks to build their models .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Previous <m> community detection studies </m> usually account for the structural factor of social networks to build their models .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: no relation
first:
Previous <m> community detection studies </m> usually account for the structural factor of social networks to build their models .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We consider diversity as a reverse measure of the average similarity between selected nodes , which can be specified using node embedding or <m> community detection results </m> .
second:
The first approach is based on a traditional machine learning method , and another approach exploits the <m> graph partitioning algorithm </m> and two deep neural models to generate the final clustering .


class: same cluster, base class: no relation, new model class: no relation
first:
We consider diversity as a reverse measure of the average similarity between selected nodes , which can be specified using node embedding or <m> community detection results </m> .
second:
Our approach finds interesting mapping solutions in few milliseconds that makes it doable at regular time by translating it in an equivalent <m> graph partitioning problem </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We consider diversity as a reverse measure of the average similarity between selected nodes , which can be specified using node embedding or <m> community detection results </m> .
second:
A <m> graph partitioning formulation </m> is presented and different ILP formulations are proposed , obtained by strengthening and/or relaxing constraints and by reducing the number of integer variables .


class: same cluster, base class: no relation, new model class: no relation
first:
We consider diversity as a reverse measure of the average similarity between selected nodes , which can be specified using node embedding or <m> community detection results </m> .
second:
The authors define the characteristics that are needed in the service areas for digital cellular systems and state the service area design problem as a <m> graph partitioning optimization problem </m> where the goal is to minimize the diameter of the resulting subgraphs .


class: same cluster, base class: no relation, new model class: no relation
first:
We consider diversity as a reverse measure of the average similarity between selected nodes , which can be specified using node embedding or <m> community detection results </m> .
second:
The most important issue of big data processing is the relevance of analytical data ; thought of this paper is to analyze the data as a <m> graph optimal partitioning problem </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We consider diversity as a reverse measure of the average similarity between selected nodes , which can be specified using node embedding or <m> community detection results </m> .
second:
Furthermore , we show that existing <m> graph partitioning problem formulations </m> do not map to how big data jobs work , causing their solutions to miss opportunities for avoiding data movement .


class: same cluster, base class: no relation, new model class: no relation
first:
We consider diversity as a reverse measure of the average similarity between selected nodes , which can be specified using node embedding or <m> community detection results </m> .
second:
The first problem is a <m> partitioning problem of graphs </m> , and second is a problem of measuring self similarity of a graph .


class: same cluster, base class: no relation, new model class: no relation
first:
We consider diversity as a reverse measure of the average similarity between selected nodes , which can be specified using node embedding or <m> community detection results </m> .
second:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .


class: same cluster, base class: no relation, new model class: no relation
first:
We consider diversity as a reverse measure of the average similarity between selected nodes , which can be specified using node embedding or <m> community detection results </m> .
second:
To that end we use the <m> graph partitioning formulation </m> that has been used for people tracking and pose estimation in the past , but has not been shown to enable articulated people tracking .


class: same cluster, base class: no relation, new model class: no relation
first:
We consider diversity as a reverse measure of the average similarity between selected nodes , which can be specified using node embedding or <m> community detection results </m> .
second:
Graph simplification in TD / BU allows to further reduce the inference time for <m> graph partitioning </m> ( vs. for BU - sparse ) .


class: same cluster, base class: no relation, new model class: no relation
first:
We consider diversity as a reverse measure of the average similarity between selected nodes , which can be specified using node embedding or <m> community detection results </m> .
second:
Grouping is formulated as a <m> graph partitioning problem </m> that lends itself to efficient inference with recent local search techniques .


class: same cluster, base class: no relation, new model class: no relation
first:
We consider diversity as a reverse measure of the average similarity between selected nodes , which can be specified using node embedding or <m> community detection results </m> .
second:
In addition , GPN formulates the inference procedure for joint configurations of human poses as a <m> graph partition problem </m> , and conducts local optimization for each person detection with reliable global affinity cues , leading to complexity reduction and performance improvement .


class: same cluster, base class: no relation, new model class: no relation
first:
We consider diversity as a reverse measure of the average similarity between selected nodes , which can be specified using node embedding or <m> community detection results </m> .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: no relation
first:
We consider diversity as a reverse measure of the average similarity between selected nodes , which can be specified using node embedding or <m> community detection results </m> .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: no relation
first:
We consider diversity as a reverse measure of the average similarity between selected nodes , which can be specified using node embedding or <m> community detection results </m> .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: no relation
first:
We consider diversity as a reverse measure of the average similarity between selected nodes , which can be specified using node embedding or <m> community detection results </m> .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: no relation
first:
We consider diversity as a reverse measure of the average similarity between selected nodes , which can be specified using node embedding or <m> community detection results </m> .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: no relation
first:
We consider diversity as a reverse measure of the average similarity between selected nodes , which can be specified using node embedding or <m> community detection results </m> .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Random walks have been used as a similarity measure for a variety of problems in content recommendation [ reference ] and <m> community detection </m> [ reference ] .
second:
The first approach is based on a traditional machine learning method , and another approach exploits the <m> graph partitioning algorithm </m> and two deep neural models to generate the final clustering .


class: same cluster, base class: no relation, new model class: no relation
first:
Random walks have been used as a similarity measure for a variety of problems in content recommendation [ reference ] and <m> community detection </m> [ reference ] .
second:
Our approach finds interesting mapping solutions in few milliseconds that makes it doable at regular time by translating it in an equivalent <m> graph partitioning problem </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Random walks have been used as a similarity measure for a variety of problems in content recommendation [ reference ] and <m> community detection </m> [ reference ] .
second:
A <m> graph partitioning formulation </m> is presented and different ILP formulations are proposed , obtained by strengthening and/or relaxing constraints and by reducing the number of integer variables .


class: same cluster, base class: no relation, new model class: no relation
first:
Random walks have been used as a similarity measure for a variety of problems in content recommendation [ reference ] and <m> community detection </m> [ reference ] .
second:
The authors define the characteristics that are needed in the service areas for digital cellular systems and state the service area design problem as a <m> graph partitioning optimization problem </m> where the goal is to minimize the diameter of the resulting subgraphs .


class: same cluster, base class: no relation, new model class: no relation
first:
Random walks have been used as a similarity measure for a variety of problems in content recommendation [ reference ] and <m> community detection </m> [ reference ] .
second:
The most important issue of big data processing is the relevance of analytical data ; thought of this paper is to analyze the data as a <m> graph optimal partitioning problem </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Random walks have been used as a similarity measure for a variety of problems in content recommendation [ reference ] and <m> community detection </m> [ reference ] .
second:
Furthermore , we show that existing <m> graph partitioning problem formulations </m> do not map to how big data jobs work , causing their solutions to miss opportunities for avoiding data movement .


class: same cluster, base class: no relation, new model class: no relation
first:
Random walks have been used as a similarity measure for a variety of problems in content recommendation [ reference ] and <m> community detection </m> [ reference ] .
second:
The first problem is a <m> partitioning problem of graphs </m> , and second is a problem of measuring self similarity of a graph .


class: same cluster, base class: no relation, new model class: no relation
first:
Random walks have been used as a similarity measure for a variety of problems in content recommendation [ reference ] and <m> community detection </m> [ reference ] .
second:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .


class: same cluster, base class: no relation, new model class: no relation
first:
Random walks have been used as a similarity measure for a variety of problems in content recommendation [ reference ] and <m> community detection </m> [ reference ] .
second:
To that end we use the <m> graph partitioning formulation </m> that has been used for people tracking and pose estimation in the past , but has not been shown to enable articulated people tracking .


class: same cluster, base class: no relation, new model class: no relation
first:
Random walks have been used as a similarity measure for a variety of problems in content recommendation [ reference ] and <m> community detection </m> [ reference ] .
second:
Graph simplification in TD / BU allows to further reduce the inference time for <m> graph partitioning </m> ( vs. for BU - sparse ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Random walks have been used as a similarity measure for a variety of problems in content recommendation [ reference ] and <m> community detection </m> [ reference ] .
second:
Grouping is formulated as a <m> graph partitioning problem </m> that lends itself to efficient inference with recent local search techniques .


class: same cluster, base class: no relation, new model class: no relation
first:
Random walks have been used as a similarity measure for a variety of problems in content recommendation [ reference ] and <m> community detection </m> [ reference ] .
second:
In addition , GPN formulates the inference procedure for joint configurations of human poses as a <m> graph partition problem </m> , and conducts local optimization for each person detection with reliable global affinity cues , leading to complexity reduction and performance improvement .


class: same cluster, base class: no relation, new model class: no relation
first:
Random walks have been used as a similarity measure for a variety of problems in content recommendation [ reference ] and <m> community detection </m> [ reference ] .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: no relation
first:
Random walks have been used as a similarity measure for a variety of problems in content recommendation [ reference ] and <m> community detection </m> [ reference ] .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: no relation
first:
Random walks have been used as a similarity measure for a variety of problems in content recommendation [ reference ] and <m> community detection </m> [ reference ] .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: no relation
first:
Random walks have been used as a similarity measure for a variety of problems in content recommendation [ reference ] and <m> community detection </m> [ reference ] .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Random walks have been used as a similarity measure for a variety of problems in content recommendation [ reference ] and <m> community detection </m> [ reference ] .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: no relation
first:
Random walks have been used as a similarity measure for a variety of problems in content recommendation [ reference ] and <m> community detection </m> [ reference ] .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: first -> second, new model class: first -> second
first:
The first approach is based on a traditional machine learning method , and another approach exploits the <m> graph partitioning algorithm </m> and two deep neural models to generate the final clustering .
second:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .


class: same cluster, base class: no relation, new model class: no relation
first:
The first approach is based on a traditional machine learning method , and another approach exploits the <m> graph partitioning algorithm </m> and two deep neural models to generate the final clustering .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: first -> second
first:
The first approach is based on a traditional machine learning method , and another approach exploits the <m> graph partitioning algorithm </m> and two deep neural models to generate the final clustering .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: first -> second
first:
The first approach is based on a traditional machine learning method , and another approach exploits the <m> graph partitioning algorithm </m> and two deep neural models to generate the final clustering .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: first -> second
first:
The first approach is based on a traditional machine learning method , and another approach exploits the <m> graph partitioning algorithm </m> and two deep neural models to generate the final clustering .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
The first approach is based on a traditional machine learning method , and another approach exploits the <m> graph partitioning algorithm </m> and two deep neural models to generate the final clustering .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: first -> second
first:
The first approach is based on a traditional machine learning method , and another approach exploits the <m> graph partitioning algorithm </m> and two deep neural models to generate the final clustering .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Our approach finds interesting mapping solutions in few milliseconds that makes it doable at regular time by translating it in an equivalent <m> graph partitioning problem </m> .
second:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .


class: same cluster, base class: no relation, new model class: no relation
first:
Our approach finds interesting mapping solutions in few milliseconds that makes it doable at regular time by translating it in an equivalent <m> graph partitioning problem </m> .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: first -> second
first:
Our approach finds interesting mapping solutions in few milliseconds that makes it doable at regular time by translating it in an equivalent <m> graph partitioning problem </m> .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: first -> second
first:
Our approach finds interesting mapping solutions in few milliseconds that makes it doable at regular time by translating it in an equivalent <m> graph partitioning problem </m> .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: first -> second
first:
Our approach finds interesting mapping solutions in few milliseconds that makes it doable at regular time by translating it in an equivalent <m> graph partitioning problem </m> .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
Our approach finds interesting mapping solutions in few milliseconds that makes it doable at regular time by translating it in an equivalent <m> graph partitioning problem </m> .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: first -> second
first:
Our approach finds interesting mapping solutions in few milliseconds that makes it doable at regular time by translating it in an equivalent <m> graph partitioning problem </m> .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: first -> second, new model class: first -> second
first:
A <m> graph partitioning formulation </m> is presented and different ILP formulations are proposed , obtained by strengthening and/or relaxing constraints and by reducing the number of integer variables .
second:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .


class: same cluster, base class: no relation, new model class: no relation
first:
A <m> graph partitioning formulation </m> is presented and different ILP formulations are proposed , obtained by strengthening and/or relaxing constraints and by reducing the number of integer variables .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: first -> second
first:
A <m> graph partitioning formulation </m> is presented and different ILP formulations are proposed , obtained by strengthening and/or relaxing constraints and by reducing the number of integer variables .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: first -> second
first:
A <m> graph partitioning formulation </m> is presented and different ILP formulations are proposed , obtained by strengthening and/or relaxing constraints and by reducing the number of integer variables .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: first -> second
first:
A <m> graph partitioning formulation </m> is presented and different ILP formulations are proposed , obtained by strengthening and/or relaxing constraints and by reducing the number of integer variables .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
A <m> graph partitioning formulation </m> is presented and different ILP formulations are proposed , obtained by strengthening and/or relaxing constraints and by reducing the number of integer variables .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: first -> second
first:
A <m> graph partitioning formulation </m> is presented and different ILP formulations are proposed , obtained by strengthening and/or relaxing constraints and by reducing the number of integer variables .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: first -> second, new model class: first -> second
first:
The authors define the characteristics that are needed in the service areas for digital cellular systems and state the service area design problem as a <m> graph partitioning optimization problem </m> where the goal is to minimize the diameter of the resulting subgraphs .
second:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .


class: same cluster, base class: no relation, new model class: no relation
first:
The authors define the characteristics that are needed in the service areas for digital cellular systems and state the service area design problem as a <m> graph partitioning optimization problem </m> where the goal is to minimize the diameter of the resulting subgraphs .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: first -> second
first:
The authors define the characteristics that are needed in the service areas for digital cellular systems and state the service area design problem as a <m> graph partitioning optimization problem </m> where the goal is to minimize the diameter of the resulting subgraphs .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: first -> second
first:
The authors define the characteristics that are needed in the service areas for digital cellular systems and state the service area design problem as a <m> graph partitioning optimization problem </m> where the goal is to minimize the diameter of the resulting subgraphs .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: first -> second
first:
The authors define the characteristics that are needed in the service areas for digital cellular systems and state the service area design problem as a <m> graph partitioning optimization problem </m> where the goal is to minimize the diameter of the resulting subgraphs .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
The authors define the characteristics that are needed in the service areas for digital cellular systems and state the service area design problem as a <m> graph partitioning optimization problem </m> where the goal is to minimize the diameter of the resulting subgraphs .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: first -> second
first:
The authors define the characteristics that are needed in the service areas for digital cellular systems and state the service area design problem as a <m> graph partitioning optimization problem </m> where the goal is to minimize the diameter of the resulting subgraphs .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: first -> second, new model class: first -> second
first:
The most important issue of big data processing is the relevance of analytical data ; thought of this paper is to analyze the data as a <m> graph optimal partitioning problem </m> .
second:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .


class: same cluster, base class: no relation, new model class: no relation
first:
The most important issue of big data processing is the relevance of analytical data ; thought of this paper is to analyze the data as a <m> graph optimal partitioning problem </m> .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: first -> second
first:
The most important issue of big data processing is the relevance of analytical data ; thought of this paper is to analyze the data as a <m> graph optimal partitioning problem </m> .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: first -> second
first:
The most important issue of big data processing is the relevance of analytical data ; thought of this paper is to analyze the data as a <m> graph optimal partitioning problem </m> .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: first -> second
first:
The most important issue of big data processing is the relevance of analytical data ; thought of this paper is to analyze the data as a <m> graph optimal partitioning problem </m> .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
The most important issue of big data processing is the relevance of analytical data ; thought of this paper is to analyze the data as a <m> graph optimal partitioning problem </m> .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: first -> second
first:
The most important issue of big data processing is the relevance of analytical data ; thought of this paper is to analyze the data as a <m> graph optimal partitioning problem </m> .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Furthermore , we show that existing <m> graph partitioning problem formulations </m> do not map to how big data jobs work , causing their solutions to miss opportunities for avoiding data movement .
second:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , we show that existing <m> graph partitioning problem formulations </m> do not map to how big data jobs work , causing their solutions to miss opportunities for avoiding data movement .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: first -> second
first:
Furthermore , we show that existing <m> graph partitioning problem formulations </m> do not map to how big data jobs work , causing their solutions to miss opportunities for avoiding data movement .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: first -> second
first:
Furthermore , we show that existing <m> graph partitioning problem formulations </m> do not map to how big data jobs work , causing their solutions to miss opportunities for avoiding data movement .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: first -> second
first:
Furthermore , we show that existing <m> graph partitioning problem formulations </m> do not map to how big data jobs work , causing their solutions to miss opportunities for avoiding data movement .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
Furthermore , we show that existing <m> graph partitioning problem formulations </m> do not map to how big data jobs work , causing their solutions to miss opportunities for avoiding data movement .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: first -> second
first:
Furthermore , we show that existing <m> graph partitioning problem formulations </m> do not map to how big data jobs work , causing their solutions to miss opportunities for avoiding data movement .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: first -> second, new model class: first -> second
first:
The first problem is a <m> partitioning problem of graphs </m> , and second is a problem of measuring self similarity of a graph .
second:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .


class: same cluster, base class: no relation, new model class: no relation
first:
The first problem is a <m> partitioning problem of graphs </m> , and second is a problem of measuring self similarity of a graph .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: first -> second
first:
The first problem is a <m> partitioning problem of graphs </m> , and second is a problem of measuring self similarity of a graph .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: first -> second
first:
The first problem is a <m> partitioning problem of graphs </m> , and second is a problem of measuring self similarity of a graph .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: first -> second
first:
The first problem is a <m> partitioning problem of graphs </m> , and second is a problem of measuring self similarity of a graph .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
The first problem is a <m> partitioning problem of graphs </m> , and second is a problem of measuring self similarity of a graph .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: first -> second
first:
The first problem is a <m> partitioning problem of graphs </m> , and second is a problem of measuring self similarity of a graph .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: second -> first, new model class: second -> first
first:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .
second:
To that end we use the <m> graph partitioning formulation </m> that has been used for people tracking and pose estimation in the past , but has not been shown to enable articulated people tracking .


class: same cluster, base class: second -> first, new model class: second -> first
first:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .
second:
Graph simplification in TD / BU allows to further reduce the inference time for <m> graph partitioning </m> ( vs. for BU - sparse ) .


class: same cluster, base class: second -> first, new model class: second -> first
first:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .
second:
Grouping is formulated as a <m> graph partitioning problem </m> that lends itself to efficient inference with recent local search techniques .


class: same cluster, base class: second -> first, new model class: second -> first
first:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .
second:
In addition , GPN formulates the inference procedure for joint configurations of human poses as a <m> graph partition problem </m> , and conducts local optimization for each person detection with reliable global affinity cues , leading to complexity reduction and performance improvement .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper , a <m> subgraph partitioning algorithm </m> for power grid graph data is developed .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
To that end we use the <m> graph partitioning formulation </m> that has been used for people tracking and pose estimation in the past , but has not been shown to enable articulated people tracking .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: first -> second
first:
To that end we use the <m> graph partitioning formulation </m> that has been used for people tracking and pose estimation in the past , but has not been shown to enable articulated people tracking .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: first -> second
first:
To that end we use the <m> graph partitioning formulation </m> that has been used for people tracking and pose estimation in the past , but has not been shown to enable articulated people tracking .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: first -> second
first:
To that end we use the <m> graph partitioning formulation </m> that has been used for people tracking and pose estimation in the past , but has not been shown to enable articulated people tracking .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
To that end we use the <m> graph partitioning formulation </m> that has been used for people tracking and pose estimation in the past , but has not been shown to enable articulated people tracking .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: first -> second
first:
To that end we use the <m> graph partitioning formulation </m> that has been used for people tracking and pose estimation in the past , but has not been shown to enable articulated people tracking .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Graph simplification in TD / BU allows to further reduce the inference time for <m> graph partitioning </m> ( vs. for BU - sparse ) .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: first -> second
first:
Graph simplification in TD / BU allows to further reduce the inference time for <m> graph partitioning </m> ( vs. for BU - sparse ) .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: first -> second
first:
Graph simplification in TD / BU allows to further reduce the inference time for <m> graph partitioning </m> ( vs. for BU - sparse ) .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: first -> second
first:
Graph simplification in TD / BU allows to further reduce the inference time for <m> graph partitioning </m> ( vs. for BU - sparse ) .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
Graph simplification in TD / BU allows to further reduce the inference time for <m> graph partitioning </m> ( vs. for BU - sparse ) .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: first -> second
first:
Graph simplification in TD / BU allows to further reduce the inference time for <m> graph partitioning </m> ( vs. for BU - sparse ) .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Grouping is formulated as a <m> graph partitioning problem </m> that lends itself to efficient inference with recent local search techniques .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: first -> second
first:
Grouping is formulated as a <m> graph partitioning problem </m> that lends itself to efficient inference with recent local search techniques .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: first -> second
first:
Grouping is formulated as a <m> graph partitioning problem </m> that lends itself to efficient inference with recent local search techniques .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: first -> second
first:
Grouping is formulated as a <m> graph partitioning problem </m> that lends itself to efficient inference with recent local search techniques .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
Grouping is formulated as a <m> graph partitioning problem </m> that lends itself to efficient inference with recent local search techniques .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: first -> second
first:
Grouping is formulated as a <m> graph partitioning problem </m> that lends itself to efficient inference with recent local search techniques .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In addition , GPN formulates the inference procedure for joint configurations of human poses as a <m> graph partition problem </m> , and conducts local optimization for each person detection with reliable global affinity cues , leading to complexity reduction and performance improvement .
second:
We address this problem by proposing a framework for detecting change in network structure based on separate pieces : a probabilistic model for <m> partitioning nodes </m> by their behavior , a label-unswitching heuristic , and an approach to change detection for sequences of complex objects .


class: same cluster, base class: no relation, new model class: first -> second
first:
In addition , GPN formulates the inference procedure for joint configurations of human poses as a <m> graph partition problem </m> , and conducts local optimization for each person detection with reliable global affinity cues , leading to complexity reduction and performance improvement .
second:
Because the link partition is conceptually natural for the problem of overlapping community detection , LinkLPA first transforms <m> node partition problem </m> into link partition problem and employs a new label propagation algorithm with preference on links instead of nodes to detect communities due to the simplicity and efficiency of label propagation algorithm .


class: same cluster, base class: no relation, new model class: first -> second
first:
In addition , GPN formulates the inference procedure for joint configurations of human poses as a <m> graph partition problem </m> , and conducts local optimization for each person detection with reliable global affinity cues , leading to complexity reduction and performance improvement .
second:
Our main message is that any <m> node-partitioning analysis </m> performed on aggregated networks should be interpreted with caution , as the outcome may be strongly influenced by the level of the aggregation .


class: same cluster, base class: no relation, new model class: first -> second
first:
In addition , GPN formulates the inference procedure for joint configurations of human poses as a <m> graph partition problem </m> , and conducts local optimization for each person detection with reliable global affinity cues , leading to complexity reduction and performance improvement .
second:
The group of classical graph-theoretic problems , including graph colouring , clique cover , and maximal clique , may be viewed as instances of a general <m> node partitioning problem </m> ( NPP ) .


class: same cluster, base class: no relation, new model class: first -> second
first:
In addition , GPN formulates the inference procedure for joint configurations of human poses as a <m> graph partition problem </m> , and conducts local optimization for each person detection with reliable global affinity cues , leading to complexity reduction and performance improvement .
second:
A <m> node partitioning scheme </m> to enhance concurrency for B-trees is presented .


class: same cluster, base class: no relation, new model class: first -> second
first:
In addition , GPN formulates the inference procedure for joint configurations of human poses as a <m> graph partition problem </m> , and conducts local optimization for each person detection with reliable global affinity cues , leading to complexity reduction and performance improvement .
second:
We write to denote the distance between and , that is , the length of the shortest path between and . is the - neighborhood of a node , that is , all nodes that are adjacent to . Labeling and <m> Node Partitions </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
The parallelism of link partitioning relative to <m> node partitioning </m> is examined in terms of an idealized execution using the well-known YAWNS synchronization algorithm .
second:
In this article , Based on the idea of coverage compensation a <m> distributed node partition algorithm </m> for random deployments is presented to generate a minimum cover set by using the optimal node distributions created by the centralized algorithms such as GA .


class: same cluster, base class: no relation, new model class: no relation
first:
Moreover , the scheme incorporates several source coding techniques , such as Wyner-Ziv coding , <m> binning </m> and superposition coding .
second:
The central task of automatic data visualization is , given a dataset , to visualize its compelling stories by transforming the data ( e.g. , selecting attributes , <m> grouping and binning values </m> ) and deciding the right type of visualization ( e.g. , bar or line charts ) .


class: same cluster, base class: first -> second, new model class: no relation
first:
Moreover , the scheme incorporates several source coding techniques , such as Wyner-Ziv coding , <m> binning </m> and superposition coding .
second:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .


class: same cluster, base class: no relation, new model class: no relation
first:
Moreover , the scheme incorporates several source coding techniques , such as Wyner-Ziv coding , <m> binning </m> and superposition coding .
second:
To overcome this problem , <m> bucketing </m> or binning techniques have been used .


class: same cluster, base class: no relation, new model class: no relation
first:
Moreover , the scheme incorporates several source coding techniques , such as Wyner-Ziv coding , <m> binning </m> and superposition coding .
second:
Köpf et al. adopted this approach to evaluate the contribution of a <m> bucketing technique </m> that was proposed by themselves .


class: same cluster, base class: no relation, new model class: no relation
first:
Moreover , the scheme incorporates several source coding techniques , such as Wyner-Ziv coding , <m> binning </m> and superposition coding .
second:
The algorithm is based on the <m> bucketing approach </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Moreover , the scheme incorporates several source coding techniques , such as Wyner-Ziv coding , <m> binning </m> and superposition coding .
second:
We also try to answer several questions , which are generally -and perhaps voluntarily-bypassed : " does the <m> bucketing strategy </m> influence the regression process ? " ; " how should the data be split into buckets to get the best fits both numerically and physically ? " ...


class: same cluster, base class: no relation, new model class: no relation
first:
Moreover , the scheme incorporates several source coding techniques , such as Wyner-Ziv coding , <m> binning </m> and superposition coding .
second:
In this paper , we propose a variant of the classical <m> bucketing algorithm </m> that ( 1 ) solves the convex-hull problem for any multiset of points , ( 2 ) uses O ( √ n ) words of extra space , ( 3 ) runs in O(n ) expected time on points drawn independently and uniformly from a rectangle , and ( 4 ) requires O(n lgn ) time in the worst case .


class: same cluster, base class: no relation, new model class: no relation
first:
Moreover , the scheme incorporates several source coding techniques , such as Wyner-Ziv coding , <m> binning </m> and superposition coding .
second:
This technique is extremely fast , polynomial in the number of days and stocks , and does not add any errors to those already incurred in the <m> companion bucketing scheme </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Moreover , the scheme incorporates several source coding techniques , such as Wyner-Ziv coding , <m> binning </m> and superposition coding .
second:
The pipeline , entirely written in CUDA , supports both fully conservative and thin voxelizations , multiple boolean , floating point , vector-typed render targets , user-defined vertex and fragment shaders , and a <m> bucketing mode </m> which can be used to generate 3D A-buffers containing the entire list of fragments belonging to each voxel .


class: same cluster, base class: no relation, new model class: first -> second
first:
The central task of automatic data visualization is , given a dataset , to visualize its compelling stories by transforming the data ( e.g. , selecting attributes , <m> grouping and binning values </m> ) and deciding the right type of visualization ( e.g. , bar or line charts ) .
second:
We describe a new <m> binning technic </m> for informed data hiding problem .


class: same cluster, base class: no relation, new model class: first -> second
first:
The central task of automatic data visualization is , given a dataset , to visualize its compelling stories by transforming the data ( e.g. , selecting attributes , <m> grouping and binning values </m> ) and deciding the right type of visualization ( e.g. , bar or line charts ) .
second:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .


class: same cluster, base class: no relation, new model class: no relation
first:
The central task of automatic data visualization is , given a dataset , to visualize its compelling stories by transforming the data ( e.g. , selecting attributes , <m> grouping and binning values </m> ) and deciding the right type of visualization ( e.g. , bar or line charts ) .
second:
To overcome this problem , <m> bucketing </m> or binning techniques have been used .


class: same cluster, base class: no relation, new model class: no relation
first:
The central task of automatic data visualization is , given a dataset , to visualize its compelling stories by transforming the data ( e.g. , selecting attributes , <m> grouping and binning values </m> ) and deciding the right type of visualization ( e.g. , bar or line charts ) .
second:
Köpf et al. adopted this approach to evaluate the contribution of a <m> bucketing technique </m> that was proposed by themselves .


class: same cluster, base class: no relation, new model class: no relation
first:
The central task of automatic data visualization is , given a dataset , to visualize its compelling stories by transforming the data ( e.g. , selecting attributes , <m> grouping and binning values </m> ) and deciding the right type of visualization ( e.g. , bar or line charts ) .
second:
The algorithm is based on the <m> bucketing approach </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The central task of automatic data visualization is , given a dataset , to visualize its compelling stories by transforming the data ( e.g. , selecting attributes , <m> grouping and binning values </m> ) and deciding the right type of visualization ( e.g. , bar or line charts ) .
second:
We also try to answer several questions , which are generally -and perhaps voluntarily-bypassed : " does the <m> bucketing strategy </m> influence the regression process ? " ; " how should the data be split into buckets to get the best fits both numerically and physically ? " ...


class: same cluster, base class: no relation, new model class: no relation
first:
The central task of automatic data visualization is , given a dataset , to visualize its compelling stories by transforming the data ( e.g. , selecting attributes , <m> grouping and binning values </m> ) and deciding the right type of visualization ( e.g. , bar or line charts ) .
second:
In this paper , we propose a variant of the classical <m> bucketing algorithm </m> that ( 1 ) solves the convex-hull problem for any multiset of points , ( 2 ) uses O ( √ n ) words of extra space , ( 3 ) runs in O(n ) expected time on points drawn independently and uniformly from a rectangle , and ( 4 ) requires O(n lgn ) time in the worst case .


class: same cluster, base class: no relation, new model class: no relation
first:
The central task of automatic data visualization is , given a dataset , to visualize its compelling stories by transforming the data ( e.g. , selecting attributes , <m> grouping and binning values </m> ) and deciding the right type of visualization ( e.g. , bar or line charts ) .
second:
This technique is extremely fast , polynomial in the number of days and stocks , and does not add any errors to those already incurred in the <m> companion bucketing scheme </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The central task of automatic data visualization is , given a dataset , to visualize its compelling stories by transforming the data ( e.g. , selecting attributes , <m> grouping and binning values </m> ) and deciding the right type of visualization ( e.g. , bar or line charts ) .
second:
The pipeline , entirely written in CUDA , supports both fully conservative and thin voxelizations , multiple boolean , floating point , vector-typed render targets , user-defined vertex and fragment shaders , and a <m> bucketing mode </m> which can be used to generate 3D A-buffers containing the entire list of fragments belonging to each voxel .


class: same cluster, base class: first -> second, new model class: no relation
first:
We describe a new <m> binning technic </m> for informed data hiding problem .
second:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .


class: same cluster, base class: no relation, new model class: no relation
first:
We describe a new <m> binning technic </m> for informed data hiding problem .
second:
To overcome this problem , <m> bucketing </m> or binning techniques have been used .


class: same cluster, base class: no relation, new model class: no relation
first:
We describe a new <m> binning technic </m> for informed data hiding problem .
second:
Köpf et al. adopted this approach to evaluate the contribution of a <m> bucketing technique </m> that was proposed by themselves .


class: same cluster, base class: no relation, new model class: no relation
first:
We describe a new <m> binning technic </m> for informed data hiding problem .
second:
The algorithm is based on the <m> bucketing approach </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We describe a new <m> binning technic </m> for informed data hiding problem .
second:
We also try to answer several questions , which are generally -and perhaps voluntarily-bypassed : " does the <m> bucketing strategy </m> influence the regression process ? " ; " how should the data be split into buckets to get the best fits both numerically and physically ? " ...


class: same cluster, base class: no relation, new model class: no relation
first:
We describe a new <m> binning technic </m> for informed data hiding problem .
second:
In this paper , we propose a variant of the classical <m> bucketing algorithm </m> that ( 1 ) solves the convex-hull problem for any multiset of points , ( 2 ) uses O ( √ n ) words of extra space , ( 3 ) runs in O(n ) expected time on points drawn independently and uniformly from a rectangle , and ( 4 ) requires O(n lgn ) time in the worst case .


class: same cluster, base class: no relation, new model class: no relation
first:
We describe a new <m> binning technic </m> for informed data hiding problem .
second:
This technique is extremely fast , polynomial in the number of days and stocks , and does not add any errors to those already incurred in the <m> companion bucketing scheme </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We describe a new <m> binning technic </m> for informed data hiding problem .
second:
The pipeline , entirely written in CUDA , supports both fully conservative and thin voxelizations , multiple boolean , floating point , vector-typed render targets , user-defined vertex and fragment shaders , and a <m> bucketing mode </m> which can be used to generate 3D A-buffers containing the entire list of fragments belonging to each voxel .


class: same cluster, base class: no relation, new model class: no relation
first:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .
second:
To overcome this problem , <m> bucketing </m> or binning techniques have been used .


class: same cluster, base class: no relation, new model class: no relation
first:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .
second:
Köpf et al. adopted this approach to evaluate the contribution of a <m> bucketing technique </m> that was proposed by themselves .


class: same cluster, base class: no relation, new model class: no relation
first:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .
second:
The algorithm is based on the <m> bucketing approach </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .
second:
We also try to answer several questions , which are generally -and perhaps voluntarily-bypassed : " does the <m> bucketing strategy </m> influence the regression process ? " ; " how should the data be split into buckets to get the best fits both numerically and physically ? " ...


class: same cluster, base class: no relation, new model class: no relation
first:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .
second:
In this paper , we propose a variant of the classical <m> bucketing algorithm </m> that ( 1 ) solves the convex-hull problem for any multiset of points , ( 2 ) uses O ( √ n ) words of extra space , ( 3 ) runs in O(n ) expected time on points drawn independently and uniformly from a rectangle , and ( 4 ) requires O(n lgn ) time in the worst case .


class: same cluster, base class: no relation, new model class: no relation
first:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .
second:
This technique is extremely fast , polynomial in the number of days and stocks , and does not add any errors to those already incurred in the <m> companion bucketing scheme </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The results showed that the new binning approach is well suited for <m> binning small datasets </m> and can be used in geoscience applications if needed .
second:
The pipeline , entirely written in CUDA , supports both fully conservative and thin voxelizations , multiple boolean , floating point , vector-typed render targets , user-defined vertex and fragment shaders , and a <m> bucketing mode </m> which can be used to generate 3D A-buffers containing the entire list of fragments belonging to each voxel .


class: same cluster, base class: no relation, new model class: no relation
first:
We establish a new achievable rate region for the ZC , using <m> Marton 's binning technique </m> .
second:
A feasible distributed data indexing algorithm is proposed for Hadoop data mining , based on <m> ZSCORE binning </m> and inverted indexing and on the Hadoop SequenceFile format .


class: same cluster, base class: second -> first, new model class: first -> second
first:
This candidate list was then compared with an expert-generated list of marijuana terms to assess the accuracy and efficacy of using <m> word-vector embeddings </m> to search for novel drug terminology .
second:
To that end , this paper proposes a <m> word embeddings based approach </m> to enhance the accuracy of PMM .


class: same cluster, base class: no relation, new model class: no relation
first:
On the basis of this principle , this study aims to verify whether the performance of <m> GloVe </m> can be improved after processing certain calibrations on the GloVe model .
second:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .


class: same cluster, base class: first -> second, new model class: no relation
first:
On the basis of this principle , this study aims to verify whether the performance of <m> GloVe </m> can be improved after processing certain calibrations on the GloVe model .
second:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .


class: same cluster, base class: first -> second, new model class: no relation
first:
On the basis of this principle , this study aims to verify whether the performance of <m> GloVe </m> can be improved after processing certain calibrations on the GloVe model .
second:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .


class: same cluster, base class: no relation, new model class: no relation
first:
On the basis of this principle , this study aims to verify whether the performance of <m> GloVe </m> can be improved after processing certain calibrations on the GloVe model .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: same cluster, base class: no relation, new model class: first -> second
first:
On the basis of this principle , this study aims to verify whether the performance of <m> GloVe </m> can be improved after processing certain calibrations on the GloVe model .
second:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .


class: same cluster, base class: no relation, new model class: first -> second
first:
On the basis of this principle , this study aims to verify whether the performance of <m> GloVe </m> can be improved after processing certain calibrations on the GloVe model .
second:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .


class: same cluster, base class: no relation, new model class: no relation
first:
We propose a new word embedding model , inspired by <m> GloVe </m> , which is formulated as a feasible least squares optimization problem .
second:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .


class: same cluster, base class: first -> second, new model class: no relation
first:
We propose a new word embedding model , inspired by <m> GloVe </m> , which is formulated as a feasible least squares optimization problem .
second:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .


class: same cluster, base class: first -> second, new model class: no relation
first:
We propose a new word embedding model , inspired by <m> GloVe </m> , which is formulated as a feasible least squares optimization problem .
second:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .


class: same cluster, base class: no relation, new model class: no relation
first:
We propose a new word embedding model , inspired by <m> GloVe </m> , which is formulated as a feasible least squares optimization problem .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: same cluster, base class: no relation, new model class: first -> second
first:
We propose a new word embedding model , inspired by <m> GloVe </m> , which is formulated as a feasible least squares optimization problem .
second:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .


class: same cluster, base class: no relation, new model class: first -> second
first:
We propose a new word embedding model , inspired by <m> GloVe </m> , which is formulated as a feasible least squares optimization problem .
second:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .


class: same cluster, base class: no relation, new model class: no relation
first:
We also show that distributed representations produced by the <m> GloVe model </m> are better than those produced by the Skip-gram model when being used in dependency parsing .
second:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .


class: same cluster, base class: first -> second, new model class: no relation
first:
We also show that distributed representations produced by the <m> GloVe model </m> are better than those produced by the Skip-gram model when being used in dependency parsing .
second:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .


class: same cluster, base class: first -> second, new model class: no relation
first:
We also show that distributed representations produced by the <m> GloVe model </m> are better than those produced by the Skip-gram model when being used in dependency parsing .
second:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .


class: same cluster, base class: no relation, new model class: no relation
first:
We also show that distributed representations produced by the <m> GloVe model </m> are better than those produced by the Skip-gram model when being used in dependency parsing .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: same cluster, base class: no relation, new model class: first -> second
first:
We also show that distributed representations produced by the <m> GloVe model </m> are better than those produced by the Skip-gram model when being used in dependency parsing .
second:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .


class: same cluster, base class: no relation, new model class: first -> second
first:
We also show that distributed representations produced by the <m> GloVe model </m> are better than those produced by the Skip-gram model when being used in dependency parsing .
second:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .


class: same cluster, base class: no relation, new model class: no relation
first:
For topic detection , we design a novel real-time topic model dubbed as a Cost-Effective And Scalable Embedding model ( CEASE ) based on improved <m> GloVe Models </m> and keyword frequency clustering algorithm .
second:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .


class: same cluster, base class: first -> second, new model class: no relation
first:
For topic detection , we design a novel real-time topic model dubbed as a Cost-Effective And Scalable Embedding model ( CEASE ) based on improved <m> GloVe Models </m> and keyword frequency clustering algorithm .
second:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .


class: same cluster, base class: first -> second, new model class: no relation
first:
For topic detection , we design a novel real-time topic model dubbed as a Cost-Effective And Scalable Embedding model ( CEASE ) based on improved <m> GloVe Models </m> and keyword frequency clustering algorithm .
second:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .


class: same cluster, base class: no relation, new model class: no relation
first:
For topic detection , we design a novel real-time topic model dubbed as a Cost-Effective And Scalable Embedding model ( CEASE ) based on improved <m> GloVe Models </m> and keyword frequency clustering algorithm .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: same cluster, base class: no relation, new model class: first -> second
first:
For topic detection , we design a novel real-time topic model dubbed as a Cost-Effective And Scalable Embedding model ( CEASE ) based on improved <m> GloVe Models </m> and keyword frequency clustering algorithm .
second:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .


class: same cluster, base class: no relation, new model class: first -> second
first:
For topic detection , we design a novel real-time topic model dubbed as a Cost-Effective And Scalable Embedding model ( CEASE ) based on improved <m> GloVe Models </m> and keyword frequency clustering algorithm .
second:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .


class: same cluster, base class: no relation, new model class: no relation
first:
The latent features consist of representations for a person learned by using a word2vec model and representations for profession/nationality values extracted from a <m> pre-trained GloVe embedding model </m> .
second:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .


class: same cluster, base class: first -> second, new model class: no relation
first:
The latent features consist of representations for a person learned by using a word2vec model and representations for profession/nationality values extracted from a <m> pre-trained GloVe embedding model </m> .
second:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .


class: same cluster, base class: first -> second, new model class: no relation
first:
The latent features consist of representations for a person learned by using a word2vec model and representations for profession/nationality values extracted from a <m> pre-trained GloVe embedding model </m> .
second:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .


class: same cluster, base class: no relation, new model class: no relation
first:
The latent features consist of representations for a person learned by using a word2vec model and representations for profession/nationality values extracted from a <m> pre-trained GloVe embedding model </m> .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: same cluster, base class: no relation, new model class: first -> second
first:
The latent features consist of representations for a person learned by using a word2vec model and representations for profession/nationality values extracted from a <m> pre-trained GloVe embedding model </m> .
second:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .


class: same cluster, base class: no relation, new model class: first -> second
first:
The latent features consist of representations for a person learned by using a word2vec model and representations for profession/nationality values extracted from a <m> pre-trained GloVe embedding model </m> .
second:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .


class: same cluster, base class: no relation, new model class: no relation
first:
Our work addresses the question : can affect lexica improve the word representations learnt from a corpus ? In this work , we propose techniques to incorporate affect lexica , which capture fine-grained information about a word 's psycholinguistic and emotional orientation , into the training process of Word2Vec SkipGram , <m> Word2Vec CBOW and GloVe methods </m> using a joint learning approach .
second:
Through large scale human evaluation , we report that our resulting word embedddings are much more interpretable than the original <m> GloVe and word2vec embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Recent approaches to word vector representations , e.g. , ‘ w2vec ’ and ‘ <m> GloVe ’ </m> , have been shown to be powerful methods for capturing the semantics and syntax of words in a text .
second:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .


class: same cluster, base class: first -> second, new model class: no relation
first:
Recent approaches to word vector representations , e.g. , ‘ w2vec ’ and ‘ <m> GloVe ’ </m> , have been shown to be powerful methods for capturing the semantics and syntax of words in a text .
second:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .


class: same cluster, base class: first -> second, new model class: no relation
first:
Recent approaches to word vector representations , e.g. , ‘ w2vec ’ and ‘ <m> GloVe ’ </m> , have been shown to be powerful methods for capturing the semantics and syntax of words in a text .
second:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .


class: same cluster, base class: no relation, new model class: no relation
first:
Recent approaches to word vector representations , e.g. , ‘ w2vec ’ and ‘ <m> GloVe ’ </m> , have been shown to be powerful methods for capturing the semantics and syntax of words in a text .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: same cluster, base class: no relation, new model class: first -> second
first:
Recent approaches to word vector representations , e.g. , ‘ w2vec ’ and ‘ <m> GloVe ’ </m> , have been shown to be powerful methods for capturing the semantics and syntax of words in a text .
second:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .


class: same cluster, base class: no relation, new model class: first -> second
first:
Recent approaches to word vector representations , e.g. , ‘ w2vec ’ and ‘ <m> GloVe ’ </m> , have been shown to be powerful methods for capturing the semantics and syntax of words in a text .
second:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .


class: same cluster, base class: no relation, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
First , we propose an online topic clustering algorithm based on keyword frequency to combine the extended TextRank with weighted titles and the improved <m> GloVe methods </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
The deep learning techniques such as long short term memory cell ( LSTM ) with and without word GloVe embeddings , a Convolution neural network ( CNN ) with or without GloVe are used , and <m> GloVe pretrained model </m> is used for classification


class: same cluster, base class: no relation, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
subsection : Parsing , POS and predicate detection We first report the labeled and unlabeled attachment scores ( LAS , UAS ) of our parsing models on the CoNLL - 2005 and 2012 test sets ( Table [ reference ] ) with <m> GloVe </m> ( ) and ELMo ( ) embeddings .


class: same cluster, base class: no relation, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
We use open - source <m> GloVe vectors </m> trained on Common Crawl 840B with 300 dimensions as fixed word embeddings .


class: same cluster, base class: no relation, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
<m> GloVe vectors </m> were trained on a large corpus of about 6 billion tokens pennington2014glove , and provide an important source of prior knowledge for the model .


class: same cluster, base class: no relation, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .


class: same cluster, base class: no relation, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .


class: same cluster, base class: no relation, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: same cluster, base class: no relation, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
The model exploits the 300 , 000 most frequent <m> pretrained GloVe embeddings </m> and improves them during the training process .


class: same cluster, base class: first -> second, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .


class: same cluster, base class: no relation, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .


class: same cluster, base class: no relation, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
We use pre - trained <m> word vectors GloVe </m> to obtain the fixed word embedding of each word .


class: same cluster, base class: no relation, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
On the textual side , we worked with <m> GloVe word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
<m> GloVe embeddings </m> [ Pennington et al. in : Empirical methods in natural language processing ( EMNLP ) , pp 1532–1543 , 2014 ] have been used to represent text data numerically .


class: same cluster, base class: no relation, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
For the unsupervised classifiers , we implemented several models of CNN and RNN classifiers utilizing <m> GloVe-based word embeddings </m> .


class: same cluster, base class: second -> first, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: second -> first, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: no relation, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
This model uses <m> GloVe embeddings </m> and the same optimization and procedure described above .


class: same cluster, base class: no relation, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
An initial embedding layer vectorises words in P and H using pretrained <m> GLoVe embeddings </m> , and passing them to a context representation layer , which uses bidirectional LSTMs ( BiLSTMs ) to encode context vectors for each time - step .


class: same cluster, base class: no relation, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
In particular , our experiments utilize the <m> GloVe embeddings </m> trained by pennington2014glove on 6 billion tokens of Wikipedia 2014 and Gigaword 5 .


class: same cluster, base class: no relation, new model class: no relation
first:
The model of embedding we use is the <m> GloVe [ 1 ] model </m> , with dimensionality size equal to 300 or 50 .
second:
We choose word embedding size , and use the - dimensional pre - trained <m> GloVe word embeddings </m> for initialization .


class: same cluster, base class: first -> second, new model class: no relation
first:
First , we propose an online topic clustering algorithm based on keyword frequency to combine the extended TextRank with weighted titles and the improved <m> GloVe methods </m> .
second:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .


class: same cluster, base class: first -> second, new model class: no relation
first:
First , we propose an online topic clustering algorithm based on keyword frequency to combine the extended TextRank with weighted titles and the improved <m> GloVe methods </m> .
second:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .


class: same cluster, base class: no relation, new model class: no relation
first:
First , we propose an online topic clustering algorithm based on keyword frequency to combine the extended TextRank with weighted titles and the improved <m> GloVe methods </m> .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: same cluster, base class: no relation, new model class: first -> second
first:
First , we propose an online topic clustering algorithm based on keyword frequency to combine the extended TextRank with weighted titles and the improved <m> GloVe methods </m> .
second:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .


class: same cluster, base class: no relation, new model class: first -> second
first:
First , we propose an online topic clustering algorithm based on keyword frequency to combine the extended TextRank with weighted titles and the improved <m> GloVe methods </m> .
second:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .


class: same cluster, base class: no relation, new model class: no relation
first:
The best result , $ 94\%$ of F1 , was obtained by the CRF classifier with the following combinations of features : ( i ) Wang2Vec -- Skip-gram of $ 100 $ dimension with the features from AZPort ; ( ii ) TF-IDF , AZPort and \textit{embeddings } extracted with the <m> Word2Vec -- Skip-gram and GloVe models </m> of dimensions $ 1000 $ and $ 300 $ , respectively .
second:
Through large scale human evaluation , we report that our resulting word embedddings are much more interpretable than the original <m> GloVe and word2vec embeddings </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
The deep learning techniques such as long short term memory cell ( LSTM ) with and without word GloVe embeddings , a Convolution neural network ( CNN ) with or without GloVe are used , and <m> GloVe pretrained model </m> is used for classification
second:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .


class: same cluster, base class: first -> second, new model class: no relation
first:
The deep learning techniques such as long short term memory cell ( LSTM ) with and without word GloVe embeddings , a Convolution neural network ( CNN ) with or without GloVe are used , and <m> GloVe pretrained model </m> is used for classification
second:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .


class: same cluster, base class: no relation, new model class: no relation
first:
The deep learning techniques such as long short term memory cell ( LSTM ) with and without word GloVe embeddings , a Convolution neural network ( CNN ) with or without GloVe are used , and <m> GloVe pretrained model </m> is used for classification
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: same cluster, base class: no relation, new model class: first -> second
first:
The deep learning techniques such as long short term memory cell ( LSTM ) with and without word GloVe embeddings , a Convolution neural network ( CNN ) with or without GloVe are used , and <m> GloVe pretrained model </m> is used for classification
second:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .


class: same cluster, base class: no relation, new model class: first -> second
first:
The deep learning techniques such as long short term memory cell ( LSTM ) with and without word GloVe embeddings , a Convolution neural network ( CNN ) with or without GloVe are used , and <m> GloVe pretrained model </m> is used for classification
second:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Parsing , POS and predicate detection We first report the labeled and unlabeled attachment scores ( LAS , UAS ) of our parsing models on the CoNLL - 2005 and 2012 test sets ( Table [ reference ] ) with <m> GloVe </m> ( ) and ELMo ( ) embeddings .
second:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Parsing , POS and predicate detection We first report the labeled and unlabeled attachment scores ( LAS , UAS ) of our parsing models on the CoNLL - 2005 and 2012 test sets ( Table [ reference ] ) with <m> GloVe </m> ( ) and ELMo ( ) embeddings .
second:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Parsing , POS and predicate detection We first report the labeled and unlabeled attachment scores ( LAS , UAS ) of our parsing models on the CoNLL - 2005 and 2012 test sets ( Table [ reference ] ) with <m> GloVe </m> ( ) and ELMo ( ) embeddings .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: same cluster, base class: no relation, new model class: first -> second
first:
subsection : Parsing , POS and predicate detection We first report the labeled and unlabeled attachment scores ( LAS , UAS ) of our parsing models on the CoNLL - 2005 and 2012 test sets ( Table [ reference ] ) with <m> GloVe </m> ( ) and ELMo ( ) embeddings .
second:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .


class: same cluster, base class: no relation, new model class: first -> second
first:
subsection : Parsing , POS and predicate detection We first report the labeled and unlabeled attachment scores ( LAS , UAS ) of our parsing models on the CoNLL - 2005 and 2012 test sets ( Table [ reference ] ) with <m> GloVe </m> ( ) and ELMo ( ) embeddings .
second:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .


class: same cluster, base class: first -> second, new model class: no relation
first:
We use open - source <m> GloVe vectors </m> trained on Common Crawl 840B with 300 dimensions as fixed word embeddings .
second:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .


class: same cluster, base class: first -> second, new model class: no relation
first:
We use open - source <m> GloVe vectors </m> trained on Common Crawl 840B with 300 dimensions as fixed word embeddings .
second:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .


class: same cluster, base class: no relation, new model class: no relation
first:
We use open - source <m> GloVe vectors </m> trained on Common Crawl 840B with 300 dimensions as fixed word embeddings .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: same cluster, base class: no relation, new model class: first -> second
first:
We use open - source <m> GloVe vectors </m> trained on Common Crawl 840B with 300 dimensions as fixed word embeddings .
second:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .


class: same cluster, base class: no relation, new model class: first -> second
first:
We use open - source <m> GloVe vectors </m> trained on Common Crawl 840B with 300 dimensions as fixed word embeddings .
second:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .


class: same cluster, base class: first -> second, new model class: no relation
first:
<m> GloVe vectors </m> were trained on a large corpus of about 6 billion tokens pennington2014glove , and provide an important source of prior knowledge for the model .
second:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .


class: same cluster, base class: first -> second, new model class: no relation
first:
<m> GloVe vectors </m> were trained on a large corpus of about 6 billion tokens pennington2014glove , and provide an important source of prior knowledge for the model .
second:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> GloVe vectors </m> were trained on a large corpus of about 6 billion tokens pennington2014glove , and provide an important source of prior knowledge for the model .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> GloVe vectors </m> were trained on a large corpus of about 6 billion tokens pennington2014glove , and provide an important source of prior knowledge for the model .
second:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> GloVe vectors </m> were trained on a large corpus of about 6 billion tokens pennington2014glove , and provide an important source of prior knowledge for the model .
second:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .


class: same cluster, base class: no relation, new model class: no relation
first:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: same cluster, base class: second -> first, new model class: no relation
first:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .
second:
The model exploits the 300 , 000 most frequent <m> pretrained GloVe embeddings </m> and improves them during the training process .


class: same cluster, base class: no relation, new model class: no relation
first:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .
second:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .


class: same cluster, base class: no relation, new model class: second -> first
first:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .
second:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .


class: same cluster, base class: second -> first, new model class: no relation
first:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .
second:
We use pre - trained <m> word vectors GloVe </m> to obtain the fixed word embedding of each word .


class: same cluster, base class: second -> first, new model class: no relation
first:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .
second:
On the textual side , we worked with <m> GloVe word embeddings </m> .


class: same cluster, base class: second -> first, new model class: no relation
first:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .
second:
<m> GloVe embeddings </m> [ Pennington et al. in : Empirical methods in natural language processing ( EMNLP ) , pp 1532–1543 , 2014 ] have been used to represent text data numerically .


class: same cluster, base class: second -> first, new model class: no relation
first:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .
second:
For the unsupervised classifiers , we implemented several models of CNN and RNN classifiers utilizing <m> GloVe-based word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: no relation, new model class: no relation
first:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: second -> first, new model class: no relation
first:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .
second:
This model uses <m> GloVe embeddings </m> and the same optimization and procedure described above .


class: same cluster, base class: second -> first, new model class: no relation
first:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .
second:
An initial embedding layer vectorises words in P and H using pretrained <m> GLoVe embeddings </m> , and passing them to a context representation layer , which uses bidirectional LSTMs ( BiLSTMs ) to encode context vectors for each time - step .


class: same cluster, base class: second -> first, new model class: no relation
first:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .
second:
In particular , our experiments utilize the <m> GloVe embeddings </m> trained by pennington2014glove on 6 billion tokens of Wikipedia 2014 and Gigaword 5 .


class: same cluster, base class: second -> first, new model class: no relation
first:
Embeddings : We use <m> 300 dimensional GloVe embeddings </m> to represent words .
second:
We choose word embedding size , and use the - dimensional pre - trained <m> GloVe word embeddings </m> for initialization .


class: same cluster, base class: no relation, new model class: no relation
first:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .
second:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .


class: same cluster, base class: second -> first, new model class: no relation
first:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .
second:
The model exploits the 300 , 000 most frequent <m> pretrained GloVe embeddings </m> and improves them during the training process .


class: same cluster, base class: no relation, new model class: no relation
first:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .
second:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .


class: same cluster, base class: no relation, new model class: second -> first
first:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .
second:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .


class: same cluster, base class: second -> first, new model class: no relation
first:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .
second:
We use pre - trained <m> word vectors GloVe </m> to obtain the fixed word embedding of each word .


class: same cluster, base class: second -> first, new model class: no relation
first:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .
second:
On the textual side , we worked with <m> GloVe word embeddings </m> .


class: same cluster, base class: second -> first, new model class: no relation
first:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .
second:
<m> GloVe embeddings </m> [ Pennington et al. in : Empirical methods in natural language processing ( EMNLP ) , pp 1532–1543 , 2014 ] have been used to represent text data numerically .


class: same cluster, base class: second -> first, new model class: no relation
first:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .
second:
For the unsupervised classifiers , we implemented several models of CNN and RNN classifiers utilizing <m> GloVe-based word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: no relation, new model class: no relation
first:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: second -> first, new model class: no relation
first:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .
second:
This model uses <m> GloVe embeddings </m> and the same optimization and procedure described above .


class: same cluster, base class: second -> first, new model class: no relation
first:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .
second:
An initial embedding layer vectorises words in P and H using pretrained <m> GLoVe embeddings </m> , and passing them to a context representation layer , which uses bidirectional LSTMs ( BiLSTMs ) to encode context vectors for each time - step .


class: same cluster, base class: second -> first, new model class: no relation
first:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .
second:
In particular , our experiments utilize the <m> GloVe embeddings </m> trained by pennington2014glove on 6 billion tokens of Wikipedia 2014 and Gigaword 5 .


class: same cluster, base class: second -> first, new model class: no relation
first:
Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pre - trained <m> 300 dimensional GloVe embeddings </m> that were fixed during training .
second:
We choose word embedding size , and use the - dimensional pre - trained <m> GloVe word embeddings </m> for initialization .


class: same cluster, base class: no relation, new model class: no relation
first:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .
second:
The model exploits the 300 , 000 most frequent <m> pretrained GloVe embeddings </m> and improves them during the training process .


class: same cluster, base class: no relation, new model class: no relation
first:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .
second:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .


class: same cluster, base class: no relation, new model class: no relation
first:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .
second:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .


class: same cluster, base class: no relation, new model class: no relation
first:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .
second:
We use pre - trained <m> word vectors GloVe </m> to obtain the fixed word embedding of each word .


class: same cluster, base class: no relation, new model class: no relation
first:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .
second:
On the textual side , we worked with <m> GloVe word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .
second:
<m> GloVe embeddings </m> [ Pennington et al. in : Empirical methods in natural language processing ( EMNLP ) , pp 1532–1543 , 2014 ] have been used to represent text data numerically .


class: same cluster, base class: no relation, new model class: no relation
first:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .
second:
For the unsupervised classifiers , we implemented several models of CNN and RNN classifiers utilizing <m> GloVe-based word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: no relation, new model class: no relation
first:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: no relation, new model class: no relation
first:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .
second:
This model uses <m> GloVe embeddings </m> and the same optimization and procedure described above .


class: same cluster, base class: no relation, new model class: no relation
first:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .
second:
An initial embedding layer vectorises words in P and H using pretrained <m> GLoVe embeddings </m> , and passing them to a context representation layer , which uses bidirectional LSTMs ( BiLSTMs ) to encode context vectors for each time - step .


class: same cluster, base class: no relation, new model class: no relation
first:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .
second:
In particular , our experiments utilize the <m> GloVe embeddings </m> trained by pennington2014glove on 6 billion tokens of Wikipedia 2014 and Gigaword 5 .


class: same cluster, base class: no relation, new model class: no relation
first:
Words are represented using a concatenation of 100 dimensional vector representations , initialized using <m> GloVe Pennington2014GloveGV </m> and a binary , per - word predicate feature , represented using an 100 dimensional embedding .
second:
We choose word embedding size , and use the - dimensional pre - trained <m> GloVe word embeddings </m> for initialization .


class: same cluster, base class: no relation, new model class: first -> second
first:
The model exploits the 300 , 000 most frequent <m> pretrained GloVe embeddings </m> and improves them during the training process .
second:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .


class: same cluster, base class: no relation, new model class: first -> second
first:
The model exploits the 300 , 000 most frequent <m> pretrained GloVe embeddings </m> and improves them during the training process .
second:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .


class: same cluster, base class: no relation, new model class: no relation
first:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .
second:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .


class: same cluster, base class: no relation, new model class: second -> first
first:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .
second:
We use pre - trained <m> word vectors GloVe </m> to obtain the fixed word embedding of each word .


class: same cluster, base class: no relation, new model class: second -> first
first:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .
second:
On the textual side , we worked with <m> GloVe word embeddings </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .
second:
<m> GloVe embeddings </m> [ Pennington et al. in : Empirical methods in natural language processing ( EMNLP ) , pp 1532–1543 , 2014 ] have been used to represent text data numerically .


class: same cluster, base class: no relation, new model class: second -> first
first:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .
second:
For the unsupervised classifiers , we implemented several models of CNN and RNN classifiers utilizing <m> GloVe-based word embeddings </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: no relation, new model class: second -> first
first:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: no relation, new model class: second -> first
first:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .
second:
This model uses <m> GloVe embeddings </m> and the same optimization and procedure described above .


class: same cluster, base class: no relation, new model class: second -> first
first:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .
second:
An initial embedding layer vectorises words in P and H using pretrained <m> GLoVe embeddings </m> , and passing them to a context representation layer , which uses bidirectional LSTMs ( BiLSTMs ) to encode context vectors for each time - step .


class: same cluster, base class: no relation, new model class: second -> first
first:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .
second:
In particular , our experiments utilize the <m> GloVe embeddings </m> trained by pennington2014glove on 6 billion tokens of Wikipedia 2014 and Gigaword 5 .


class: same cluster, base class: no relation, new model class: second -> first
first:
We initialize our word embeddings with pre - trained <m> 300D GloVe 840B vectors </m> while the out - of - vocabulary word are randomly initialized with uniform distribution .
second:
We choose word embedding size , and use the - dimensional pre - trained <m> GloVe word embeddings </m> for initialization .


class: same cluster, base class: no relation, new model class: second -> first
first:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .
second:
We use pre - trained <m> word vectors GloVe </m> to obtain the fixed word embedding of each word .


class: same cluster, base class: no relation, new model class: second -> first
first:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .
second:
On the textual side , we worked with <m> GloVe word embeddings </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .
second:
<m> GloVe embeddings </m> [ Pennington et al. in : Empirical methods in natural language processing ( EMNLP ) , pp 1532–1543 , 2014 ] have been used to represent text data numerically .


class: same cluster, base class: no relation, new model class: second -> first
first:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .
second:
For the unsupervised classifiers , we implemented several models of CNN and RNN classifiers utilizing <m> GloVe-based word embeddings </m> .


class: same cluster, base class: second -> first, new model class: second -> first
first:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .
second:
The results indicate that our proposed method based on <m> Glove model word embedding </m> can significantly improve query expansion methods using Arberry dataset .


class: same cluster, base class: second -> first, new model class: second -> first
first:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .
second:
The second system uses a multi-branch 1D CNN classifier with <m> Glove pre-trained embedding layer </m> for the first level of classification and string matching for the second level of classification .


class: same cluster, base class: no relation, new model class: second -> first
first:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .
second:
This model uses <m> GloVe embeddings </m> and the same optimization and procedure described above .


class: same cluster, base class: no relation, new model class: second -> first
first:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .
second:
An initial embedding layer vectorises words in P and H using pretrained <m> GLoVe embeddings </m> , and passing them to a context representation layer , which uses bidirectional LSTMs ( BiLSTMs ) to encode context vectors for each time - step .


class: same cluster, base class: no relation, new model class: second -> first
first:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .
second:
In particular , our experiments utilize the <m> GloVe embeddings </m> trained by pennington2014glove on 6 billion tokens of Wikipedia 2014 and Gigaword 5 .


class: same cluster, base class: no relation, new model class: second -> first
first:
We use pre - trained word vectors , <m> GloVe glove </m> , to obtain the fixed word embedding of each word .
second:
We choose word embedding size , and use the - dimensional pre - trained <m> GloVe word embeddings </m> for initialization .


class: same cluster, base class: no relation, new model class: no relation
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .


class: same cluster, base class: first -> second, new model class: no relation
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .


class: same cluster, base class: no relation, new model class: no relation
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
<m> Distributed Representations of Words </m> , Documents and Class Labels .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
Word vectors are <m> distributed representations of word features </m> .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .


class: same cluster, base class: first -> second, new model class: no relation
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: second -> first
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: no relation, new model class: no relation
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: no relation
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: no relation
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: no relation
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: no relation
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: first -> second
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: no relation
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: no relation
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: second -> first
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: second -> first
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: second -> first
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: second -> first
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: second -> first
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: second -> first
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: second -> first
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: second -> first
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: second -> first
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: second -> first
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: second -> first
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: second -> first
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: second -> first
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: second -> first
first:
Advanced <m> word embedding models </m> are then compared on the collected data and it is demonstrated that they often fail in the cases of complex word associations that go beyond simple contextual interchangeability .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .


class: same cluster, base class: first -> second, new model class: no relation
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .


class: same cluster, base class: first -> second, new model class: first -> second
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .


class: same cluster, base class: first -> second, new model class: first -> second
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
<m> Distributed Representations of Words </m> , Documents and Class Labels .


class: same cluster, base class: first -> second, new model class: first -> second
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .


class: same cluster, base class: first -> second, new model class: first -> second
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
Word vectors are <m> distributed representations of word features </m> .


class: same cluster, base class: first -> second, new model class: first -> second
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .


class: same cluster, base class: first -> second, new model class: first -> second
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .


class: same cluster, base class: first -> second, new model class: first -> second
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .


class: same cluster, base class: first -> second, new model class: first -> second
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: first -> second
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .


class: same cluster, base class: first -> second, new model class: first -> second
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .


class: same cluster, base class: first -> second, new model class: no relation
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: second -> first
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: first -> second
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: second -> first
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: second -> first
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: second -> first
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: second -> first
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: second -> first
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: second -> first
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: second -> first
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: second -> first
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: second -> first
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: second -> first
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: second -> first
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: second -> first
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: second -> first
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: second -> first
first:
This paper presents <m> word embedding-based approach </m> to text classification .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .


class: same cluster, base class: first -> second, new model class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .


class: same cluster, base class: no relation, new model class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .


class: same cluster, base class: first -> second, new model class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
<m> Distributed Representations of Words </m> , Documents and Class Labels .


class: same cluster, base class: first -> second, new model class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .


class: same cluster, base class: first -> second, new model class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
Word vectors are <m> distributed representations of word features </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .


class: same cluster, base class: first -> second, new model class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .


class: same cluster, base class: first -> second, new model class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .


class: same cluster, base class: first -> second, new model class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .


class: same cluster, base class: first -> second, new model class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: first -> second
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: first -> second
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: first -> second
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: first -> second
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: first -> second
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: first -> second
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: first -> second
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: first -> second
first:
In the standard setting , two kinds of training corpora are used : a very large unlabeled corpus for learning the <m> word embedding representations </m> ; and an in-domain training corpus with gold labels for training classifiers on the target NLP task .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .


class: same cluster, base class: first -> second, new model class: no relation
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .


class: same cluster, base class: no relation, new model class: no relation
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
<m> Distributed Representations of Words </m> , Documents and Class Labels .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
Word vectors are <m> distributed representations of word features </m> .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .


class: same cluster, base class: first -> second, new model class: no relation
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: second -> first
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: no relation, new model class: no relation
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: no relation
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: no relation
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: no relation
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: no relation
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: first -> second
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: no relation
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: no relation
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: second -> first
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: second -> first
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: second -> first
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: second -> first
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: second -> first
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: second -> first
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: second -> first
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: second -> first
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: second -> first
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: second -> first
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: second -> first
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: second -> first
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: second -> first
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: second -> first
first:
We assessed scalable statistical models for high dimensional discrete data , including fitting , assessing and exploring models from three broad statistical areas : i ) matrix factorization/decomposition models ii ) probabilistic topic models and iii ) <m> word-vector embedding models </m> .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
<m> Distributed Representations of Words </m> , Documents and Class Labels .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
Word vectors are <m> distributed representations of word features </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: same cluster, base class: second -> first, new model class: second -> first
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: second -> first, new model class: second -> first
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: second -> first
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: second -> first
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: second -> first
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: second -> first, new model class: second -> first
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: second -> first, new model class: second -> first
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: no relation
first:
Different from traditional <m> word-based embedding </m> , single word embedding was used as input of CNN and LSTM .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .


class: same cluster, base class: no relation, new model class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .


class: same cluster, base class: first -> second, new model class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
<m> Distributed Representations of Words </m> , Documents and Class Labels .


class: same cluster, base class: first -> second, new model class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .


class: same cluster, base class: first -> second, new model class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
Word vectors are <m> distributed representations of word features </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .


class: same cluster, base class: first -> second, new model class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .


class: same cluster, base class: first -> second, new model class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .


class: same cluster, base class: first -> second, new model class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .


class: same cluster, base class: first -> second, new model class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: first -> second
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: first -> second
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: first -> second
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: first -> second
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: first -> second
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: first -> second
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: first -> second
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: first -> second
first:
We showed that our segment LSTM model , with only <m> word embedding feature </m> and no manual feature engineering , achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations , 0.800 for medical problem-test relations , and 0.683 for medical problem-medical problem relations .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .


class: same cluster, base class: no relation, new model class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .


class: same cluster, base class: first -> second, new model class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
<m> Distributed Representations of Words </m> , Documents and Class Labels .


class: same cluster, base class: first -> second, new model class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .


class: same cluster, base class: first -> second, new model class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
Word vectors are <m> distributed representations of word features </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .


class: same cluster, base class: first -> second, new model class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .


class: same cluster, base class: first -> second, new model class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .


class: same cluster, base class: first -> second, new model class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .


class: same cluster, base class: first -> second, new model class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: first -> second
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: first -> second
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: first -> second
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: first -> second
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: first -> second
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: first -> second
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: first -> second
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: first -> second
first:
While the <m> word embedding vectors </m> like Word2Vec were successfully employed in many NLP approaches .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .


class: same cluster, base class: no relation, new model class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .


class: same cluster, base class: first -> second, new model class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
<m> Distributed Representations of Words </m> , Documents and Class Labels .


class: same cluster, base class: first -> second, new model class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .


class: same cluster, base class: first -> second, new model class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
Word vectors are <m> distributed representations of word features </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .


class: same cluster, base class: first -> second, new model class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .


class: same cluster, base class: first -> second, new model class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .


class: same cluster, base class: first -> second, new model class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .


class: same cluster, base class: first -> second, new model class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: first -> second
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: first -> second
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: first -> second
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: first -> second
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: first -> second
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: first -> second
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: first -> second
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: first -> second
first:
Our results also suggested that the Global Vectors for <m> word representation embedding </m> in general domain provides a very strong baseline , which can be further improved by applying the principal component analysis to generate more isotropic vectors .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .


class: same cluster, base class: no relation, new model class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .


class: same cluster, base class: first -> second, new model class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
<m> Distributed Representations of Words </m> , Documents and Class Labels .


class: same cluster, base class: first -> second, new model class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .


class: same cluster, base class: first -> second, new model class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
Word vectors are <m> distributed representations of word features </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .


class: same cluster, base class: first -> second, new model class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .


class: same cluster, base class: first -> second, new model class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .


class: same cluster, base class: first -> second, new model class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .


class: same cluster, base class: first -> second, new model class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: first -> second
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: first -> second
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: first -> second
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: first -> second
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: first -> second
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: first -> second
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: first -> second
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: first -> second
first:
The input to the character - level GRUs is character embeddings , while the input to the word - level GRUs is the concatenation of character - level GRU hidden states and <m> word embeddings </m> .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .


class: same cluster, base class: first -> second, new model class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
<m> Distributed Representations of Words </m> , Documents and Class Labels .


class: same cluster, base class: first -> second, new model class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .


class: same cluster, base class: first -> second, new model class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
Word vectors are <m> distributed representations of word features </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .


class: same cluster, base class: first -> second, new model class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .


class: same cluster, base class: first -> second, new model class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .


class: same cluster, base class: first -> second, new model class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .


class: same cluster, base class: first -> second, new model class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Word embedding layer </m> also maps each word to a high - dimensional vector space .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: second -> first, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .


class: same cluster, base class: second -> first, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .


class: same cluster, base class: second -> first, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .


class: same cluster, base class: second -> first, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
<m> Distributed Representations of Words </m> , Documents and Class Labels .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
Word vectors are <m> distributed representations of word features </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .


class: same cluster, base class: second -> first, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .


class: same cluster, base class: second -> first, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: second -> first, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: same cluster, base class: second -> first, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: same cluster, base class: second -> first, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: same cluster, base class: no relation, new model class: second -> first
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: second -> first
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: second -> first
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: second -> first
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: second -> first
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: second -> first
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: second -> first
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: no relation
first:
Then the RNN language models predict probability distribution by the following equation : where is a weight matrix , is a <m> word embedding matrix </m> , is a one - hot vector of input word at timestep , and is the hidden state of the - th RNN at timestep .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
<m> Distributed Representations of Words </m> , Documents and Class Labels .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
Word vectors are <m> distributed representations of word features </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: first -> second
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: first -> second
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: first -> second
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: first -> second
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: first -> second
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: first -> second
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: first -> second
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: first -> second
first:
subsection : Experimental Setup We use pre - trained - Glove vectors to initialize our <m> word embedding vectors </m> .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
<m> Distributed Representations of Words </m> , Documents and Class Labels .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
Word vectors are <m> distributed representations of word features </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: same cluster, base class: second -> first, new model class: second -> first
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: second -> first, new model class: second -> first
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: second -> first
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: second -> first
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: second -> first
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: second -> first, new model class: second -> first
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: second -> first, new model class: second -> first
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: no relation
first:
Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to traditional <m> word - based embeddings </m> .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
<m> Distributed Representations of Words </m> , Documents and Class Labels .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
Word vectors are <m> distributed representations of word features </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: first -> second
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: first -> second
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: first -> second
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: first -> second
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: first -> second
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: first -> second
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: first -> second
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: first -> second
first:
subsection : Formal Description Our model uses one <m> word embedding function </m> and two encoder functions .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .


class: same cluster, base class: first -> second, new model class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
<m> Distributed Representations of Words </m> , Documents and Class Labels .


class: same cluster, base class: first -> second, new model class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .


class: same cluster, base class: first -> second, new model class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
Word vectors are <m> distributed representations of word features </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .


class: same cluster, base class: first -> second, new model class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .


class: same cluster, base class: first -> second, new model class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .


class: same cluster, base class: first -> second, new model class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .


class: same cluster, base class: first -> second, new model class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: first -> second
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: first -> second
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: first -> second
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: first -> second
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: first -> second
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: first -> second
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: first -> second
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: first -> second
first:
We study their effectiveness under various learning scenarios ( point - wise and pair - wise models ) and using different input representations ( i.e. , from encoding query - document pairs into dense / sparse vectors to using <m> word embedding representation </m> ) .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .


class: same cluster, base class: first -> second, new model class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
<m> Distributed Representations of Words </m> , Documents and Class Labels .


class: same cluster, base class: first -> second, new model class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .


class: same cluster, base class: first -> second, new model class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
Word vectors are <m> distributed representations of word features </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .


class: same cluster, base class: first -> second, new model class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .


class: same cluster, base class: first -> second, new model class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .


class: same cluster, base class: first -> second, new model class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .


class: same cluster, base class: first -> second, new model class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .


class: same cluster, base class: first -> second, new model class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: first -> second
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: first -> second
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: first -> second
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: first -> second
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: first -> second
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: first -> second
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: first -> second
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: first -> second
first:
As described in the previous section , we initialized the <m> word - vector embedding </m> with pre - trained word vectors obtained from word2vec .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: second -> first, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .


class: same cluster, base class: second -> first, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: second -> first, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: same cluster, base class: second -> first, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: same cluster, base class: second -> first, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: second -> first
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word representations </m> are now claimed ( and on a reasonable basis ) to be one of the most promising distributional semantics models .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: second -> first, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .


class: same cluster, base class: second -> first, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: second -> first, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: same cluster, base class: second -> first, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: same cluster, base class: second -> first, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: second -> first
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed Representations of Words </m> , Documents and Class Labels .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: second -> first, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .


class: same cluster, base class: second -> first, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: second -> first, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: same cluster, base class: second -> first, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: same cluster, base class: second -> first, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: second -> first
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Distributed word vector representations </m> have been shown to improve Natural Language Processing ( NLP ) tasks , especially Automatic Text Summarization ( ATS ) .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: second -> first, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .


class: same cluster, base class: second -> first, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: second -> first, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: same cluster, base class: second -> first, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: same cluster, base class: second -> first, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: second -> first
first:
Word vectors are <m> distributed representations of word features </m> .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: no relation
first:
Word vectors are <m> distributed representations of word features </m> .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: second -> first, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .


class: same cluster, base class: second -> first, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: second -> first, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: same cluster, base class: second -> first, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: same cluster, base class: second -> first, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: second -> first
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: no relation
first:
Word embeddings are the <m> distributed representation of the words </m> in numerical form .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: second -> first, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .


class: same cluster, base class: second -> first, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: second -> first, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: same cluster, base class: second -> first, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: same cluster, base class: second -> first, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: second -> first
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: no relation
first:
RNN can utilize <m> distributed representations of words </m> by first converting the tokens comprising each text into vectors , which form a matrix . And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: second -> first, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .


class: same cluster, base class: second -> first, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: second -> first, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: same cluster, base class: second -> first, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: same cluster, base class: second -> first, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: second -> first
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Learning Vector Representation of Words This section introduces the concept of <m> distributed vector representation of words </m> .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In our experiments , each paragraph vector is a combination of two vectors : one learned by the standard paragraph vector with distributed memory ( PV - DM ) and one learned by the paragraph vector with <m> distributed bag of words </m> ( PV - DBOW ) .
second:
Our experiments , on a dataset of 321k news titles created for the WSDM 2019 challenge , show that the BERT-based models significantly outperform BiLSTM , which in-turn significantly outperforms a simpler <m> embedding-based representation </m> .


class: same cluster, base class: second -> first, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .


class: same cluster, base class: second -> first, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: second -> first, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: same cluster, base class: second -> first, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: same cluster, base class: second -> first, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: second -> first
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: no relation
first:
Given that natural language is highly context - dependent , this further implies that in practice Softmax with <m> distributed word embeddings </m> does not have enough capacity to model natural language .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: second -> first, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .


class: same cluster, base class: second -> first, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: second -> first, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: same cluster, base class: second -> first, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: same cluster, base class: second -> first, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: second -> first
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: no relation
first:
This further implies that standard Softmax - based language models with <m> distributed ( output ) word embeddings </m> do not have enough capacity to model natural language .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: second -> first, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .


class: same cluster, base class: second -> first, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: second -> first, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: same cluster, base class: second -> first, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: same cluster, base class: second -> first, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: no relation
first:
In recent years , it has become an important testing ground for approaches employing <m> distributed word </m> and phrase representations .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: second -> first, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .


class: same cluster, base class: second -> first, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: second -> first, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: same cluster, base class: second -> first, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: same cluster, base class: second -> first, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: second -> first
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: no relation
first:
Comparing with traditional models , this suggests such a simple use of a <m> distributed word representation </m> may not give us an advantage to text classification .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Moreover , VDA constructs condensed domain invariant clusters in the <m> embedding representation </m> to separate various classes alongside the domain transfer .
second:
subsection : <m> Embedding Dimensionality </m> We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in Table [ reference ] .


class: same cluster, base class: first -> second, new model class: no relation
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: first -> second
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: no relation, new model class: no relation
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: no relation
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: no relation
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: no relation
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: no relation
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: no relation
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: no relation
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: first -> second
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: no relation
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: first -> second
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: first -> second
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: first -> second
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: first -> second
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: first -> second
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: first -> second
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: first -> second
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: first -> second
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: first -> second
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: first -> second
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: first -> second
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: first -> second
first:
Then by employing the log-linear word production model for relating random variables to their <m> embedding space representation </m> and making use of the convexity of natural exponential function , we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors , representing its context-free and context-sensitive parts , respectively .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .


class: same cluster, base class: no relation, new model class: first -> second
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: first -> second
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: first -> second
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: first -> second
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: first -> second
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: first -> second
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: first -> second
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: first -> second
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: first -> second
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: first -> second
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: first -> second
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: first -> second
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: first -> second
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: first -> second
first:
The <m> embedding layer </m> for our 90 K vocabulary can be initialized randomly or using pre - trained word - vector embeddings .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: second -> first, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: same cluster, base class: second -> first, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: same cluster, base class: second -> first, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: same cluster, base class: no relation, new model class: second -> first
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: second -> first
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: second -> first
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: second -> first
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: second -> first
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: second -> first
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: second -> first
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: no relation
first:
For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two <m> embedding matrices </m> .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .


class: same cluster, base class: second -> first, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: same cluster, base class: second -> first, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: same cluster, base class: second -> first, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: same cluster, base class: no relation, new model class: second -> first
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: second -> first
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: second -> first
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: second -> first
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: second -> first
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: second -> first
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: second -> first
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: no relation
first:
section : EMBEDDING COMPONENT We first embed words in each x i = { x i1 , x i2 , x i3 , ... , x ini } and u to a continuous space multiplying an <m> embedding matrix </m> A ∈ R d×V .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .


class: same cluster, base class: no relation, new model class: second -> first
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .


class: same cluster, base class: no relation, new model class: second -> first
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .


class: same cluster, base class: no relation, new model class: first -> second
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: first -> second
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: first -> second
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: first -> second
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: first -> second
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: first -> second
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: first -> second
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: first -> second
first:
Given two sequences and , we let denote the <m> embedded representation </m> of the word .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Embedding vector representation </m> ( Embed ) : The major weakness of the previous input representation is that words are treated as discrete units , hence prohibiting the network from performing soft matching between semantically similar words in queries and documents .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: no relation
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: no relation
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: no relation
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: no relation
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: no relation
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: no relation
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: first -> second
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: no relation
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: first -> second
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: first -> second
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: first -> second
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: first -> second
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: first -> second
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: first -> second
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: first -> second
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: first -> second
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: first -> second
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: first -> second
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: first -> second
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: first -> second
first:
Considering that off - the - shelf RankSVM is not able to learn <m> embedding representations </m> during training , for embedding vector representation , instead of learning embeddings we use a pre - trained embedding matrix trained on Google News and fixed IDF weights .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .


class: same cluster, base class: no relation, new model class: no relation
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .


class: same cluster, base class: no relation, new model class: no relation
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .


class: same cluster, base class: no relation, new model class: no relation
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .


class: same cluster, base class: no relation, new model class: no relation
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .


class: same cluster, base class: no relation, new model class: no relation
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: no relation
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: no relation, new model class: no relation
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: first -> second
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: no relation
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: first -> second
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: first -> second
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: first -> second
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: first -> second
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: first -> second
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: first -> second
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: first -> second
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: first -> second
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: first -> second
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: first -> second
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: first -> second
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: first -> second
first:
In total , our <m> embedding vector </m> grew from the original 100 to 155 , and produced incremental gains compared to its counterpart words - lvt2k - 2sent as shown in Table [ reference ] , demonstrating the utility of syntax based features in this task .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: second -> first
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: first -> second, new model class: no relation
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: second -> first
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: second -> first
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: second -> first
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we introduce a new approach to capture analogies in <m> continuous word representations </m> , based on modeling not just individual word vectors , but rather the subspaces spanned by groups of words .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: second -> first
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: first -> second, new model class: no relation
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: second -> first
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: second -> first
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: second -> first
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In this paper , we present a weighted average word embeddings method which incorporates sentiment information in the <m> continuous representation of words </m> based on an adapted version of the delta TFIDF measure .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: second -> first
first:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: second -> first
first:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: second -> first
first:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: second -> first
first:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: second -> first
first:
The evaluations on multi-document and multilingual datasets prove the effectiveness of the <m> continuous vector representation of words </m> compared to the bag-of-words model .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: second -> first
first:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: second -> first
first:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: second -> first
first:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: second -> first
first:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: second -> first
first:
Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of <m> continuous word vector representations </m> for automatic summarization .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .
second:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .


class: same cluster, base class: no relation, new model class: second -> first
first:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: second -> first
first:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: second -> first
first:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: second -> first
first:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: second -> first
first:
Given the recent success of <m> continuous vector space word representations </m> , we provide such an inference procedure for continuous states , where words ' representations are given by the posterior mean of a linear dynamical system .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: second -> first, new model class: no relation
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .


class: same cluster, base class: second -> first, new model class: no relation
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
They first learn <m> continuous word vector embeddings </m> from data .


class: same cluster, base class: no relation, new model class: no relation
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: second -> first
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: no relation
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: no relation, new model class: no relation
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: no relation, new model class: no relation
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: no relation, new model class: no relation
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: no relation, new model class: no relation
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: no relation, new model class: no relation
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: no relation
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: no relation
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: no relation
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: no relation
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: no relation, new model class: no relation
first:
As a result , the use of <m> continuous valued word representation </m> was proposed in literature .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: first -> second, new model class: no relation
first:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: first -> second, new model class: second -> first
first:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: first -> second, new model class: second -> first
first:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: first -> second, new model class: second -> first
first:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: first -> second, new model class: second -> first
first:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: first -> second, new model class: second -> first
first:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: first -> second, new model class: second -> first
first:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: first -> second, new model class: second -> first
first:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: first -> second, new model class: second -> first
first:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: second -> first
first:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: second -> first
first:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: first -> second, new model class: second -> first
first:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: second -> first
first:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: first -> second, new model class: second -> first
first:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: first -> second, new model class: second -> first
first:
We present a technique to overcome this gap by using <m> continuous word space representations </m> to explicitly compute query and detector concept similarity .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: first -> second, new model class: no relation
first:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .
second:
We also show the effect of varying the vector dimension and context window for two different approaches of <m> learning word vectors </m> .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .
second:
However , <m> learning of word vectors </m> is crucial to obtain good results .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .
second:
While previous studies show that modeling the minimum meaning-bearing units ( characters or morphemes ) benefits <m> learning vector representations of words </m> , they ignore the semantic dependencies across these units when deriving word vectors .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .
second:
<m> Learning vector space representation of words </m> ( i.e. , word embeddings ) has recently attracted wide research interests , and has been extended to cross-lingual scenario .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .
second:
We survey its use in both human-facing applications and downstream NLP tasks , including event schema induction , sentence similarity , text comprehension , <m> learning word vector embeddings </m> , and more .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .
second:
In recent years , <m> learning word vector representations </m> has attracted much interest in Natural Language Processing .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .
second:
levy2014neural also considered the matrix factorization perspective , but in the context of <m> learning word embeddings </m> .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .
second:
However , these approaches for <m> learning word vectors </m> only allow a single context - independent representation for each word .


class: same cluster, base class: no relation, new model class: second -> first
first:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: second -> first
first:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .
second:
subsection : <m> Learning Vector Representation of Words </m> This section introduces the concept of distributed vector representation of words .


class: same cluster, base class: no relation, new model class: second -> first
first:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .
second:
In that sense , a <m> word vector learning layer </m> is a special ( and unusual ) case of convolution layer with region size one .


class: same cluster, base class: first -> second, new model class: second -> first
first:
In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of <m> continuous word representations </m> .
second:
Also , with our approach , a system is simpler with one fewer layer – no need to tune the dimensionality of word vectors or meta - parameters for <m> word vector learning </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
They first learn <m> continuous word vector embeddings </m> from data .
second:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .


class: same cluster, base class: no relation, new model class: second -> first
first:
They first learn <m> continuous word vector embeddings </m> from data .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: second -> first
first:
They first learn <m> continuous word vector embeddings </m> from data .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: second -> first
first:
They first learn <m> continuous word vector embeddings </m> from data .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: second -> first
first:
They first learn <m> continuous word vector embeddings </m> from data .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: first -> second
first:
Recent work for <m> learning word representations </m> has applied successfully to many NLP applications , such as sentiment analysis and question answering .
second:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .
second:
<m> Learning representations of words </m> is a pioneering study in this school of research .


class: same cluster, base class: no relation, new model class: second -> first
first:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .
second:
In the next section , we depart from previous work by introducing a new approach for <m> learning word representations </m> that are a linear combination of the biLM layers .


class: same cluster, base class: no relation, new model class: second -> first
first:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .
second:
subsubsection : Vector arithmetic on face samples In the context of evaluating <m> learned representations of words </m> mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space .


class: same cluster, base class: no relation, new model class: second -> first
first:
We define a semantic relationship among cross-language libraries and API methods ( both local and third party ) using functional descriptions and a <m> word-vector learning model </m> .
second:
cluster tightness Next , we perform an analysis on <m> representation learning of words </m> ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
The experimental results obtained , show that the overall accuracy offered by the employed machine learning techniques is high compared with other machine learning techniques including decision trees , rough sets , <m> neural networks </m> , and fuzzy artmap .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: same cluster, base class: no relation, new model class: no relation
first:
Since neural networks have the capabilities of learning and self-organizing and parallel computing mechanism , with the great increasing of digital images and video databases , <m> neural networks based techniques </m> become more efficient and popular tools for multimedia processing .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: same cluster, base class: no relation, new model class: no relation
first:
This <m> neural network systems approach </m> successfully identified and located objects in digitized images .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: same cluster, base class: no relation, new model class: no relation
first:
iIt I Modelling time-dependent nonlinear systems isatopic ofgrowing interest <m> inneural networks </m> .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: same cluster, base class: no relation, new model class: no relation
first:
The recognition of massive parallelism in natural vision has led to proposals for emulating aspects of <m> neural networks in technology </m> .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: same cluster, base class: no relation, new model class: no relation
first:
Our results indicate clear improvements in performance for <m> neural networks networks </m> incorporating memory into their structure
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: same cluster, base class: no relation, new model class: no relation
first:
We find that two models achieve comparable performance : a feature - rich classifier model and a <m> neural network model </m> centered around a Long Short - Term Memory network ( LSTM ; ? ) .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: same cluster, base class: no relation, new model class: no relation
first:
Abstract In this paper , we introduce a two-stage procedure based on <m> artificial neural networks </m> for the automatic recognition of sleep spindles ( SSs ) in a multi-channel electroencephalographic signal .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: same cluster, base class: no relation, new model class: no relation
first:
In the present study , an <m> artificial neural networks-based model </m> ( ANNs ) was developed to predict the Vickers microhardness of low-carbon Nb microalloyed steels .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Artificial neural network Artificial Neural Networks </m> ( ANN ) has the characteristics of adaptive , self-organization and self-learning .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: same cluster, base class: no relation, new model class: no relation
first:
Keywords — Fault tolerance , <m> Artificial Neural Network neural network </m> , sigmoid function , stuck-at-faults , VLSI realization .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: same cluster, base class: no relation, new model class: no relation
first:
Whereas , a third PV detection algorithm is based on <m> artificial neural networks ( ANN ) networks </m> .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: same cluster, base class: no relation, new model class: no relation
first:
Specifically we target polynomial approximation , general additive models ( Gam ) , local regression ( Loess ) , multivariate additive regression splines ( Mars ) and <m> artificial neural networks (Ann).Neural networks </m> can be viewed as models of real systems , built by tuning parameters known as weights .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Theartificial neural networks </m> becomepopular tosolve FR andFDproblems .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: same cluster, base class: no relation, new model class: no relation
first:
In , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the <m> actions of neural networks </m> .
second:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .


class: same cluster, base class: first -> second, new model class: no relation
first:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .
second:
Deep Bi-LSTM has great performance in the cases with relative long latency , average MAE of 0.074 mm , RMSE of 0.097 mm , and normalized root mean square error ( nRMSE ) of 0.081 with latency about 400 ms are obtained from predictive results of <m> Deep Bi-LSTM </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The final layer is defined as : where , , is the dimension of the hidden state of the last <m> forward and backward LSTM layers </m> , and is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .
second:
In addition , we experiment with a <m> deep bidirectional LSTM </m> , which includes two consecutive layers of bidirectional LSTMs , modeling even more complex features and performing multiple passes through the sentence .


class: same cluster, base class: no relation, new model class: no relation
first:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .
second:
For learning such saliency models , we introduce a <m> neural network architecture </m> , which has fully connected layers on top of CNNs responsible for feature extraction at three different scales .


class: same cluster, base class: no relation, new model class: no relation
first:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .
second:
In this paper , we introduce a Wavelet and Gabor based Image Retrieval ( WAGBIR ) system , a <m> Neural Network based architecture </m> for content based image retrieval .


class: same cluster, base class: no relation, new model class: no relation
first:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .
second:
However , a new challenge of <m> neural network architecture design </m> has also attracted attention : due to the high dimension of fMRI volume images , the manual process of network model design is very time-consuming and error prone .


class: same cluster, base class: no relation, new model class: no relation
first:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .
second:
In this paper we propose a new <m> neural network model architecture </m> , namely multi-stream self-attention , to address the issue thus make the self-attention mechanism more effective for speech recognition .


class: same cluster, base class: no relation, new model class: no relation
first:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .
second:
U-Nets have been established as a standard <m> neural network design architecture </m> for image-to-image learning problems such as segmentation and inverse problems in imaging .


class: same cluster, base class: no relation, new model class: no relation
first:
All of our experiments except ImageNet are based on <m> architecture </m> with pre - activation residual blocks and we use it as baseline .
second:
section : Conclusions We introduced a <m> neural network architecture </m> and training procedure for learning compact representations of image sets for template - based face recognition .


class: same cluster, base class: no relation, new model class: no relation
first:
Deep Bi-LSTM has great performance in the cases with relative long latency , average MAE of 0.074 mm , RMSE of 0.097 mm , and normalized root mean square error ( nRMSE ) of 0.081 with latency about 400 ms are obtained from predictive results of <m> Deep Bi-LSTM </m> .
second:
In addition , we experiment with a <m> deep bidirectional LSTM </m> , which includes two consecutive layers of bidirectional LSTMs , modeling even more complex features and performing multiple passes through the sentence .


class: same cluster, base class: second -> first, new model class: second -> first
first:
This system achieves 79.5 % correct prediction using the " hard " CASP 3-class assignment , and 81.4 % with a more lenient assignment , outperforming a sophisticated state-of-the-art <m> predictor </m> ( Porter ) trained in the same experimental conditions .
second:
In both approaches , the actual <m> classifier / predictor </m> is learned in a separate step using the feature representation learned by autoencoder ( s ) .


class: same cluster, base class: first -> second, new model class: second -> first
first:
Several studies have also confirmed that this philosophy of making the <m> predictor </m> robust against random and local perturbation is effective in semi - supervised learning .
second:
In both approaches , the actual <m> classifier / predictor </m> is learned in a separate step using the feature representation learned by autoencoder ( s ) .


class: same cluster, base class: no relation, new model class: no relation
first:
We apply domainadversarial learning , as we consider a <m> descriptor predictor </m> trained with a Siamese - like loss instead of the label predictor trained with a classification loss .
second:
Fang et al . won the 2015 COCO Captioning Challenge with an approach that is similar to ours in as much as it applies a visual concept ( i.e. , <m> attribute ) detection process </m> before generating sentences .


class: same cluster, base class: no relation, new model class: no relation
first:
We apply domainadversarial learning , as we consider a <m> descriptor predictor </m> trained with a Siamese - like loss instead of the label predictor trained with a classification loss .
second:
Since we formulate the <m> attribute prediction </m> as a multi - label problem , our attributes prediction network can be replaced by any other multi - label classification framework and it also can be benefit from the development of the multi - label classification researches .


class: same cluster, base class: second -> first, new model class: first -> second
first:
In both approaches , the actual <m> classifier / predictor </m> is learned in a separate step using the feature representation learned by autoencoder ( s ) .
second:
As in Fig 2 ( top ) , our network has an encoder stage and a <m> predictor stage </m> .


class: same cluster, base class: first -> second, new model class: first -> second
first:
However , SVM has a weakness for parameter selection or suitable <m> feature </m> .
second:
In prediction problems on networks this means that one has to construct a <m> feature vector representation </m> for the nodes and edges .


class: same cluster, base class: no relation, new model class: no relation
first:
However , SVM has a weakness for parameter selection or suitable <m> feature </m> .
second:
The authors show that if the trail arrival process is modeled by three <m> independent random variables </m> and the decay factor is modeled as a random process , the observed distribution functions for the amplitude and duration of underdense meteor trails can be more accurately synthesized .


class: same cluster, base class: no relation, new model class: no relation
first:
However , SVM has a weakness for parameter selection or suitable <m> feature </m> .
second:
The empirical results show that both the confirmation data of the <m> independent variable system </m> and the confirmation data of the dependent variable have the satisfying reliability .


class: same cluster, base class: no relation, new model class: no relation
first:
However , SVM has a weakness for parameter selection or suitable <m> feature </m> .
second:
Up to date in the area of feature extraction , image feature representation method based on the spatial gradient is still lacking in efficiency especially for the <m> covariate case </m> like carrying bag and wearing a coat .


class: same cluster, base class: no relation, new model class: no relation
first:
However , SVM has a weakness for parameter selection or suitable <m> feature </m> .
second:
I should like to express my gratitude to Keith Rapley ( now Innovation Manager ) at British Airways for arranging sponsorship of this work , to Mark Raskino and Neil Morrison for early stimulation of ideas concerning neural networks ; and to Dr Maurice Barr , Louis Busuttil , Paul Summerbell , Dr Himadri Chatterjee and Rupert Blackley for making possible the forecasting and <m> attribute </m> work .


class: same cluster, base class: no relation, new model class: second -> first
first:
In prediction problems on networks this means that one has to construct a <m> feature vector representation </m> for the nodes and edges .
second:
This would amount to an additional <m> feature level </m> GAN loss ( see Figure [ reference ] orange portion ) : Taken together , these loss functions form our complete objective : This ultimately corresponds to solving for a target model according to the optimization problem We have introduced a method for unsupervised adaptation which generalizes adversarial objectives to be viewed as operating at the pixel or feature level .


class: same cluster, base class: no relation, new model class: no relation
first:
In prediction problems on networks this means that one has to construct a <m> feature vector representation </m> for the nodes and edges .
second:
The authors show that if the trail arrival process is modeled by three <m> independent random variables </m> and the decay factor is modeled as a random process , the observed distribution functions for the amplitude and duration of underdense meteor trails can be more accurately synthesized .


class: same cluster, base class: no relation, new model class: no relation
first:
In prediction problems on networks this means that one has to construct a <m> feature vector representation </m> for the nodes and edges .
second:
The empirical results show that both the confirmation data of the <m> independent variable system </m> and the confirmation data of the dependent variable have the satisfying reliability .


class: same cluster, base class: no relation, new model class: no relation
first:
In prediction problems on networks this means that one has to construct a <m> feature vector representation </m> for the nodes and edges .
second:
Up to date in the area of feature extraction , image feature representation method based on the spatial gradient is still lacking in efficiency especially for the <m> covariate case </m> like carrying bag and wearing a coat .


class: same cluster, base class: no relation, new model class: no relation
first:
In prediction problems on networks this means that one has to construct a <m> feature vector representation </m> for the nodes and edges .
second:
I should like to express my gratitude to Keith Rapley ( now Innovation Manager ) at British Airways for arranging sponsorship of this work , to Mark Raskino and Neil Morrison for early stimulation of ideas concerning neural networks ; and to Dr Maurice Barr , Louis Busuttil , Paul Summerbell , Dr Himadri Chatterjee and Rupert Blackley for making possible the forecasting and <m> attribute </m> work .


class: same cluster, base class: first -> second, new model class: first -> second
first:
subsection : Locality Sensitive Hashing ( LSH ) An alternative approach to BASS and DISCO is to use well - established <m> feature generation methods </m> that are agnostic about the type of input they receive .
second:
When used as <m> generic feature extractors </m> , feature aggregation techniques designed for hand - crafted features can also work with CNN embeddings and achieve competitive performance .


class: same cluster, base class: first -> second, new model class: first -> second
first:
subsection : Unimodal Feature Extraction For a fair comparison with the state - of - the - art method , conversational memory networks ( CMN ) , we follow identical <m> feature extraction procedures </m> .
second:
When used as <m> generic feature extractors </m> , feature aggregation techniques designed for hand - crafted features can also work with CNN embeddings and achieve competitive performance .


class: same cluster, base class: no relation, new model class: no relation
first:
This would amount to an additional <m> feature level </m> GAN loss ( see Figure [ reference ] orange portion ) : Taken together , these loss functions form our complete objective : This ultimately corresponds to solving for a target model according to the optimization problem We have introduced a method for unsupervised adaptation which generalizes adversarial objectives to be viewed as operating at the pixel or feature level .
second:
The authors show that if the trail arrival process is modeled by three <m> independent random variables </m> and the decay factor is modeled as a random process , the observed distribution functions for the amplitude and duration of underdense meteor trails can be more accurately synthesized .


class: same cluster, base class: no relation, new model class: no relation
first:
This would amount to an additional <m> feature level </m> GAN loss ( see Figure [ reference ] orange portion ) : Taken together , these loss functions form our complete objective : This ultimately corresponds to solving for a target model according to the optimization problem We have introduced a method for unsupervised adaptation which generalizes adversarial objectives to be viewed as operating at the pixel or feature level .
second:
The empirical results show that both the confirmation data of the <m> independent variable system </m> and the confirmation data of the dependent variable have the satisfying reliability .


class: same cluster, base class: no relation, new model class: no relation
first:
This would amount to an additional <m> feature level </m> GAN loss ( see Figure [ reference ] orange portion ) : Taken together , these loss functions form our complete objective : This ultimately corresponds to solving for a target model according to the optimization problem We have introduced a method for unsupervised adaptation which generalizes adversarial objectives to be viewed as operating at the pixel or feature level .
second:
Up to date in the area of feature extraction , image feature representation method based on the spatial gradient is still lacking in efficiency especially for the <m> covariate case </m> like carrying bag and wearing a coat .


class: same cluster, base class: no relation, new model class: no relation
first:
This would amount to an additional <m> feature level </m> GAN loss ( see Figure [ reference ] orange portion ) : Taken together , these loss functions form our complete objective : This ultimately corresponds to solving for a target model according to the optimization problem We have introduced a method for unsupervised adaptation which generalizes adversarial objectives to be viewed as operating at the pixel or feature level .
second:
I should like to express my gratitude to Keith Rapley ( now Innovation Manager ) at British Airways for arranging sponsorship of this work , to Mark Raskino and Neil Morrison for early stimulation of ideas concerning neural networks ; and to Dr Maurice Barr , Louis Busuttil , Paul Summerbell , Dr Himadri Chatterjee and Rupert Blackley for making possible the forecasting and <m> attribute </m> work .


class: same cluster, base class: no relation, new model class: no relation
first:
The authors show that if the trail arrival process is modeled by three <m> independent random variables </m> and the decay factor is modeled as a random process , the observed distribution functions for the amplitude and duration of underdense meteor trails can be more accurately synthesized .
second:
The empirical results show that both the confirmation data of the <m> independent variable system </m> and the confirmation data of the dependent variable have the satisfying reliability .


class: same cluster, base class: no relation, new model class: no relation
first:
The authors show that if the trail arrival process is modeled by three <m> independent random variables </m> and the decay factor is modeled as a random process , the observed distribution functions for the amplitude and duration of underdense meteor trails can be more accurately synthesized .
second:
Up to date in the area of feature extraction , image feature representation method based on the spatial gradient is still lacking in efficiency especially for the <m> covariate case </m> like carrying bag and wearing a coat .


class: same cluster, base class: no relation, new model class: no relation
first:
The authors show that if the trail arrival process is modeled by three <m> independent random variables </m> and the decay factor is modeled as a random process , the observed distribution functions for the amplitude and duration of underdense meteor trails can be more accurately synthesized .
second:
I should like to express my gratitude to Keith Rapley ( now Innovation Manager ) at British Airways for arranging sponsorship of this work , to Mark Raskino and Neil Morrison for early stimulation of ideas concerning neural networks ; and to Dr Maurice Barr , Louis Busuttil , Paul Summerbell , Dr Himadri Chatterjee and Rupert Blackley for making possible the forecasting and <m> attribute </m> work .


class: same cluster, base class: no relation, new model class: no relation
first:
Domestic learners establish financial risk forecasting system taking ST ( special treatment for abnormal situation including finance and others ) companies in domestic stock market as studying objects , adopting <m> single variable analysis </m> , Multiple Linear Regression Discriminant Model and multielement logical aggressive model [ 7 - 13 ] . But the current research has the following problems :  A serial of mathematical statistics is taken before model establishing , but this affects the effectiveness .
second:
While we ensure the causal variables of the previous study are considered for inclusion in the models developed , this paper focuses on the ability to accurately predict conflict over individual <m> independent variable analysis </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Domestic learners establish financial risk forecasting system taking ST ( special treatment for abnormal situation including finance and others ) companies in domestic stock market as studying objects , adopting <m> single variable analysis </m> , Multiple Linear Regression Discriminant Model and multielement logical aggressive model [ 7 - 13 ] . But the current research has the following problems :  A serial of mathematical statistics is taken before model establishing , but this affects the effectiveness .
second:
The <m> single random variable case </m> is treated here .


class: same cluster, base class: no relation, new model class: no relation
first:
The empirical results show that both the confirmation data of the <m> independent variable system </m> and the confirmation data of the dependent variable have the satisfying reliability .
second:
Up to date in the area of feature extraction , image feature representation method based on the spatial gradient is still lacking in efficiency especially for the <m> covariate case </m> like carrying bag and wearing a coat .


class: same cluster, base class: no relation, new model class: no relation
first:
The empirical results show that both the confirmation data of the <m> independent variable system </m> and the confirmation data of the dependent variable have the satisfying reliability .
second:
I should like to express my gratitude to Keith Rapley ( now Innovation Manager ) at British Airways for arranging sponsorship of this work , to Mark Raskino and Neil Morrison for early stimulation of ideas concerning neural networks ; and to Dr Maurice Barr , Louis Busuttil , Paul Summerbell , Dr Himadri Chatterjee and Rupert Blackley for making possible the forecasting and <m> attribute </m> work .


class: same cluster, base class: no relation, new model class: no relation
first:
While we ensure the causal variables of the previous study are considered for inclusion in the models developed , this paper focuses on the ability to accurately predict conflict over individual <m> independent variable analysis </m> .
second:
Thus , <m> one variable analysis </m> is used in order to see the value of the central tendency and then tested by using the rate value .


class: same cluster, base class: no relation, new model class: no relation
first:
While we ensure the causal variables of the previous study are considered for inclusion in the models developed , this paper focuses on the ability to accurately predict conflict over individual <m> independent variable analysis </m> .
second:
The data types of software defect attributes are discussed , and then the basic analysis methods of software defect data , including <m> one-variable data analysis </m> and multiple-variable data analysis , are proposed .


class: same cluster, base class: no relation, new model class: no relation
first:
While we ensure the causal variables of the previous study are considered for inclusion in the models developed , this paper focuses on the ability to accurately predict conflict over individual <m> independent variable analysis </m> .
second:
The <m> single random variable case </m> is treated here .


class: same cluster, base class: no relation, new model class: no relation
first:
Thus , <m> one variable analysis </m> is used in order to see the value of the central tendency and then tested by using the rate value .
second:
The <m> single random variable case </m> is treated here .


class: same cluster, base class: no relation, new model class: no relation
first:
The data types of software defect attributes are discussed , and then the basic analysis methods of software defect data , including <m> one-variable data analysis </m> and multiple-variable data analysis , are proposed .
second:
The <m> single random variable case </m> is treated here .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Independent variable selection </m> Generally in a classification problem , the variable that is to be predicted is known as the dependent variable ( transportation mode in our case ) because its value depends upon , or is decided by , the values of all the other attributes .
second:
Objectives : This article aims to produce a <m> covariate adjustment method </m> that allows for automatic variable selection , so that practitioners need not commit to any specific set of covariates prior to seeing the data .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Independent variable selection </m> Generally in a classification problem , the variable that is to be predicted is known as the dependent variable ( transportation mode in our case ) because its value depends upon , or is decided by , the values of all the other attributes .
second:
The present paper proposes an organized and smart fault classifier with a combination of reliable preprocessing technique for <m> key attribute selection </m> from the power system recorded waveforms and employs a dependable decision tree based classification algorithm to acquire fault detection precision , even when the power network under consideration is large .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Independent variable selection </m> Generally in a classification problem , the variable that is to be predicted is known as the dependent variable ( transportation mode in our case ) because its value depends upon , or is decided by , the values of all the other attributes .
second:
Finally , comparative experiments on different real-life data sets are conducted to demonstrate the effectiveness and efficiency of the proposed incremental algorithms for <m> updating attribute reducts </m> with the variation of multiple objects in incomplete decision systems .


class: same cluster, base class: no relation, new model class: no relation
first:
Objectives : This article aims to produce a <m> covariate adjustment method </m> that allows for automatic variable selection , so that practitioners need not commit to any specific set of covariates prior to seeing the data .
second:
The present paper proposes an organized and smart fault classifier with a combination of reliable preprocessing technique for <m> key attribute selection </m> from the power system recorded waveforms and employs a dependable decision tree based classification algorithm to acquire fault detection precision , even when the power network under consideration is large .


class: same cluster, base class: no relation, new model class: no relation
first:
Objectives : This article aims to produce a <m> covariate adjustment method </m> that allows for automatic variable selection , so that practitioners need not commit to any specific set of covariates prior to seeing the data .
second:
Finally , comparative experiments on different real-life data sets are conducted to demonstrate the effectiveness and efficiency of the proposed incremental algorithms for <m> updating attribute reducts </m> with the variation of multiple objects in incomplete decision systems .


class: same cluster, base class: no relation, new model class: no relation
first:
Up to date in the area of feature extraction , image feature representation method based on the spatial gradient is still lacking in efficiency especially for the <m> covariate case </m> like carrying bag and wearing a coat .
second:
I should like to express my gratitude to Keith Rapley ( now Innovation Manager ) at British Airways for arranging sponsorship of this work , to Mark Raskino and Neil Morrison for early stimulation of ideas concerning neural networks ; and to Dr Maurice Barr , Louis Busuttil , Paul Summerbell , Dr Himadri Chatterjee and Rupert Blackley for making possible the forecasting and <m> attribute </m> work .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Batch Normalization Review Batch Normalization has been introduced in as an effective tool to reduce <m> internal covariate shift </m> in deep networks and accelerate the training process .
second:
Under the <m> covariate shift assumption </m> , this would make the label prediction accuracy on the target domain to be the same as on the source domain .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : Batch Normalization Batch normalization has been recently proposed in the machine learning community and addresses the so - called <m> internal covariate shift problem </m> by normalizing the mean and the variance of each layer ’s pre - activations for each training mini - batch .
second:
Under the <m> covariate shift assumption </m> , this would make the label prediction accuracy on the target domain to be the same as on the source domain .


class: same cluster, base class: no relation, new model class: no relation
first:
The present paper proposes an organized and smart fault classifier with a combination of reliable preprocessing technique for <m> key attribute selection </m> from the power system recorded waveforms and employs a dependable decision tree based classification algorithm to acquire fault detection precision , even when the power network under consideration is large .
second:
Finally , comparative experiments on different real-life data sets are conducted to demonstrate the effectiveness and efficiency of the proposed incremental algorithms for <m> updating attribute reducts </m> with the variation of multiple objects in incomplete decision systems .


class: same cluster, base class: no relation, new model class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .


class: same cluster, base class: no relation, new model class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .


class: same cluster, base class: no relation, new model class: first -> second
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .


class: same cluster, base class: first -> second, new model class: first -> second
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .


class: same cluster, base class: no relation, new model class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .


class: same cluster, base class: no relation, new model class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: same cluster, base class: no relation, new model class: second -> first
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: same cluster, base class: no relation, new model class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: same cluster, base class: no relation, new model class: second -> first
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: same cluster, base class: no relation, new model class: second -> first
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: same cluster, base class: no relation, new model class: second -> first
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: same cluster, base class: no relation, new model class: second -> first
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: same cluster, base class: no relation, new model class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: second -> first
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: same cluster, base class: no relation, new model class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: second -> first
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: same cluster, base class: no relation, new model class: no relation
first:
But this study modified the Random Forest Algorithm along the basis of signal characteristics and comparatively analyzed the accuracies of modified algorithm with those of SVM and <m> MLP </m> to prove the ability of modified algorithm .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .


class: same cluster, base class: no relation, new model class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .


class: same cluster, base class: no relation, new model class: first -> second
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .


class: same cluster, base class: no relation, new model class: first -> second
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .


class: same cluster, base class: no relation, new model class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .


class: same cluster, base class: no relation, new model class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: same cluster, base class: no relation, new model class: second -> first
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: same cluster, base class: no relation, new model class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: same cluster, base class: no relation, new model class: second -> first
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: same cluster, base class: no relation, new model class: second -> first
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: same cluster, base class: no relation, new model class: second -> first
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: same cluster, base class: no relation, new model class: second -> first
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: same cluster, base class: no relation, new model class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: second -> first
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: same cluster, base class: no relation, new model class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: second -> first
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: same cluster, base class: no relation, new model class: no relation
first:
Roughly , the principle is based on <m> main MLP </m> ( Multilayered Perceptron ) in which each synaptic weight connection value is estimated by another MLP ( an OWE ) with respect to context representation .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .


class: same cluster, base class: no relation, new model class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .


class: same cluster, base class: no relation, new model class: first -> second
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .


class: same cluster, base class: first -> second, new model class: first -> second
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .


class: same cluster, base class: no relation, new model class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .


class: same cluster, base class: no relation, new model class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: same cluster, base class: no relation, new model class: second -> first
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: same cluster, base class: no relation, new model class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: same cluster, base class: no relation, new model class: second -> first
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: same cluster, base class: no relation, new model class: second -> first
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: same cluster, base class: no relation, new model class: second -> first
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: same cluster, base class: no relation, new model class: second -> first
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: same cluster, base class: no relation, new model class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: second -> first
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: same cluster, base class: no relation, new model class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: second -> first
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: same cluster, base class: no relation, new model class: no relation
first:
While <m> MLP processing </m> alone did not have a large effect on the overall performance , the best results were obtained for MLP-processed phase sensitive filters and added MFCCs , with relative error reductions of over 40 % for both noisy and clean training .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .


class: same cluster, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .


class: same cluster, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .


class: same cluster, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .


class: same cluster, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .


class: same cluster, base class: first -> second, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .


class: same cluster, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: same cluster, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: same cluster, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: same cluster, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: same cluster, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: same cluster, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: same cluster, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: same cluster, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: same cluster, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: same cluster, base class: no relation, new model class: no relation
first:
This study employs a <m> multilayer perceptions ( MLP ) neural network </m> with genetic algorithm ( GA ) to predict the New Taiwan dollar (NTD)/U.S. dollar ( USD ) exchange rate .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: same cluster, base class: no relation, new model class: no relation
first:
In terms of model parsimony , the B-spline neurofuzzy model is found to be more parsimonious than the <m> back-propagation MLP model </m> .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: first -> second
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .


class: same cluster, base class: no relation, new model class: first -> second
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .


class: same cluster, base class: no relation, new model class: first -> second
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .


class: same cluster, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .


class: same cluster, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: same cluster, base class: no relation, new model class: second -> first
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: same cluster, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: same cluster, base class: no relation, new model class: second -> first
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: same cluster, base class: no relation, new model class: second -> first
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: same cluster, base class: no relation, new model class: second -> first
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: same cluster, base class: no relation, new model class: second -> first
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: same cluster, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: second -> first
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: same cluster, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: second -> first
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: same cluster, base class: no relation, new model class: no relation
first:
The selected MFCC features were then used to train several <m> ANN Multi-Layer Perceptron ( MLP </m> ) .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: first -> second
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .


class: same cluster, base class: no relation, new model class: first -> second
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .


class: same cluster, base class: first -> second, new model class: first -> second
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: same cluster, base class: no relation, new model class: second -> first
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: same cluster, base class: no relation, new model class: second -> first
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: same cluster, base class: no relation, new model class: second -> first
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: same cluster, base class: no relation, new model class: second -> first
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: same cluster, base class: no relation, new model class: second -> first
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: second -> first
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: second -> first
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> MLP </m> has one hidden layer with tanh activation and softmax output layer in our experiments .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: first -> second
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .


class: same cluster, base class: no relation, new model class: first -> second
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .


class: same cluster, base class: no relation, new model class: no relation
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .


class: same cluster, base class: no relation, new model class: no relation
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: same cluster, base class: no relation, new model class: second -> first
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: same cluster, base class: no relation, new model class: no relation
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: same cluster, base class: no relation, new model class: second -> first
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: same cluster, base class: no relation, new model class: second -> first
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: same cluster, base class: no relation, new model class: second -> first
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: same cluster, base class: no relation, new model class: second -> first
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: same cluster, base class: no relation, new model class: no relation
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: second -> first
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: same cluster, base class: no relation, new model class: no relation
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: second -> first
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: same cluster, base class: no relation, new model class: no relation
first:
We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the <m> MLP layer </m> .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: second -> first
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .


class: same cluster, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .


class: same cluster, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .


class: same cluster, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .


class: same cluster, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: same cluster, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: same cluster, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: same cluster, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: same cluster, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: same cluster, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: same cluster, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: same cluster, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: same cluster, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: same cluster, base class: no relation, new model class: no relation
first:
The dimension of encoder states ( ) is set to 300 and a <m> 1024D MLP </m> with one or two hidden layers is used .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: first -> second
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .


class: same cluster, base class: first -> second, new model class: first -> second
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .


class: same cluster, base class: no relation, new model class: no relation
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .


class: same cluster, base class: no relation, new model class: no relation
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: same cluster, base class: no relation, new model class: second -> first
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: same cluster, base class: no relation, new model class: no relation
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: same cluster, base class: no relation, new model class: second -> first
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: same cluster, base class: no relation, new model class: second -> first
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: same cluster, base class: no relation, new model class: second -> first
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: same cluster, base class: no relation, new model class: second -> first
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: same cluster, base class: no relation, new model class: no relation
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: second -> first
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: same cluster, base class: no relation, new model class: no relation
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: second -> first
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: same cluster, base class: no relation, new model class: no relation
first:
As introduced in section [ reference ] , BaseModel follows the Embedding & <m> MLP architecture </m> and is the base of most of subsequently developed deep networks for CTR modeling .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .


class: same cluster, base class: no relation, new model class: first -> second
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .


class: same cluster, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: same cluster, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: same cluster, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: same cluster, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: same cluster, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: same cluster, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: same cluster, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: same cluster, base class: no relation, new model class: no relation
first:
Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) : A tag can then be predicted with a linear classifier that takes as input the output of the <m> MLP enized </m> / segmented . , applies a softmax function and chooses for each word the tag with highest probability .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .


class: same cluster, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: same cluster, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: same cluster, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: same cluster, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: same cluster, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: same cluster, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: same cluster, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: same cluster, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: same cluster, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: same cluster, base class: no relation, new model class: no relation
first:
The labeling of an arc is performed using the same feature representation fed into a different <m> MLP predictor </m> : As before we use a margin based hinge loss .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .


class: same cluster, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: same cluster, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: same cluster, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: same cluster, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: same cluster, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: same cluster, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: same cluster, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: same cluster, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: same cluster, base class: no relation, new model class: no relation
first:
On top of each level of the feature pyramid , we apply a small <m> 5×5 MLP </m> to predict 14×14 masks and object scores in a fully convolutional fashion , see Fig .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .


class: same cluster, base class: no relation, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: same cluster, base class: second -> first, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: same cluster, base class: no relation, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: same cluster, base class: no relation, new model class: no relation
first:
The most novel characteristics of our machine learning model compared to previous ones are as follows : 1 ) multi-tasking , in the sense that our deep learning model jointly learns to simultaneously predict both MCI to AD conversion , and AD vs healthy classification which facilitates the relevant feature extraction for prognostication ; 2 ) the <m> neural network classifier </m> employs relatively few parameters compared to other deep learning architectures ( we use ~550,000 network parameters , orders of magnitude lower than other network designs ) without compromising network complexity and hence significantly limits data-overfitting ; 3 ) both structural MRI images and warp field characteristics , which quantify the amount of volumetric change compared to the common template , were used as separate input streams to extract as much information as possible from the MRI data .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: same cluster, base class: no relation, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .


class: same cluster, base class: no relation, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: same cluster, base class: second -> first, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: same cluster, base class: no relation, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: same cluster, base class: no relation, new model class: no relation
first:
Finally , a <m> neural network based classifier </m> was trained with features extracted from 66 chest images to distinguish pneumoconiosis patients from normal cases .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: same cluster, base class: second -> first, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Neural network classifying method </m> is used in this work to perform facial expression recognition .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: same cluster, base class: no relation, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .


class: same cluster, base class: no relation, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: same cluster, base class: second -> first, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: same cluster, base class: no relation, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: same cluster, base class: no relation, new model class: no relation
first:
The results of speaker dependent and hybrid methods show that <m> theneural network classifier </m> provides the best results .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: same cluster, base class: no relation, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .


class: same cluster, base class: no relation, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: same cluster, base class: second -> first, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: same cluster, base class: no relation, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: same cluster, base class: no relation, new model class: no relation
first:
Based on the multi-spectrum samples of stationary meteorology satellite cloud images , the best distinguishing factors of cloud classification were distilled and a synthetical optimized cloud classifier combining the advantages of both self-organizing feature map ( SOFM ) and probabilistic neural network ( PNN ) was established by computing and analyzing the gray- gradient co-occurrence matrix and texture characters of satellite cloud image samples to overcome the shortcoming of single a <m> neural network ( ANN ) classifier </m> difficult to identify and classify complex cloud characters accurately and effectively .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: same cluster, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .


class: same cluster, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .


class: same cluster, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: same cluster, base class: second -> first, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: same cluster, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: same cluster, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: same cluster, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: same cluster, base class: no relation, new model class: no relation
first:
Advanced <m> neural network-oriented classifier </m> is able to achieve competing result on the benchmark streams via an aggregated recurrent unit incorporated with sophis- ticated convolving layer .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: same cluster, base class: no relation, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: same cluster, base class: second -> first, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: same cluster, base class: no relation, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: same cluster, base class: no relation, new model class: no relation
first:
As a <m> classification network </m> , it employs the Softmax loss regularized with its proposed center loss . But it is difficult to directly fine - tune the network for pain intensity classification due to limited face images with pain labels .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: same cluster, base class: no relation, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: same cluster, base class: second -> first, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: same cluster, base class: no relation, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: same cluster, base class: no relation, new model class: no relation
first:
section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a <m> neural network classifier </m> .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: same cluster, base class: no relation, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: same cluster, base class: second -> first, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: same cluster, base class: no relation, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: same cluster, base class: no relation, new model class: no relation
first:
To test this , we perform class - wise nearest neighbors analysis in pixel space and the feature space of pre - trained <m> classifier networks </m> ( Appendix [ reference ] ) .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: same cluster, base class: no relation, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .


class: same cluster, base class: second -> first, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: same cluster, base class: no relation, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: same cluster, base class: no relation, new model class: no relation
first:
section : Localization Starting from our <m> classification - trained network </m> , we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: same cluster, base class: no relation, new model class: no relation
first:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .
second:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .


class: same cluster, base class: no relation, new model class: no relation
first:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: same cluster, base class: no relation, new model class: no relation
first:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: same cluster, base class: no relation, new model class: no relation
first:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: same cluster, base class: no relation, new model class: no relation
first:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: same cluster, base class: no relation, new model class: no relation
first:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: same cluster, base class: no relation, new model class: no relation
first:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: no relation
first:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: same cluster, base class: no relation, new model class: no relation
first:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: no relation
first:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: same cluster, base class: no relation, new model class: no relation
first:
We then train a <m> feed - forward neural network classifier </m> that uses the lexical and syntactic features extracted from the query on this data ( § [ reference ] ) .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: second -> first, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .


class: same cluster, base class: no relation, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: same cluster, base class: no relation, new model class: second -> first
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: same cluster, base class: no relation, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: same cluster, base class: no relation, new model class: no relation
first:
In MMD - GAN , the <m> neural - network discriminator </m> works in a similar way as LDA .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: same cluster, base class: no relation, new model class: no relation
first:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .
second:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .


class: same cluster, base class: no relation, new model class: first -> second
first:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: no relation
first:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: no relation
first:
A <m> Fully Connected Neural Network </m> was used to build the ensemble model , and it achieved an accuracy of 94.02 % .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: no relation
first:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .
second:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .
second:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .


class: same cluster, base class: no relation, new model class: no relation
first:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .
second:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .


class: same cluster, base class: no relation, new model class: no relation
first:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .
second:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .
second:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .


class: same cluster, base class: no relation, new model class: no relation
first:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: no relation
first:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: same cluster, base class: no relation, new model class: no relation
first:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: same cluster, base class: no relation, new model class: no relation
first:
Through computer vision and deep learning , the final <m> fully-connected feed-forward neural network </m> achieves competitive results compared to Dräger JM-103 , a state-of-the-art bilirubinometer .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: first -> second
first:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: no relation
first:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: no relation
first:
The proposed system ( EmoDet ) ensembles a <m> fully connected neural network architecture </m> and LSTM neural network to obtain performance results that show substantial improvements ( F1-Score 0.67 ) over the baseline model provided by Task 3 organizers ( F1score 0.58 ) .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: first -> second
first:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: no relation
first:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: no relation
first:
The residual errors of both encoders are then fed into a <m> fully-connected neural network layer </m> to detect whether a post was published by the specified user or by a hacker .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: first -> second
first:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: no relation
first:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: no relation
first:
According to traditional multi-layer feed-forward neural network , this paper elaborates the concept of <m> completely-fully connected neural network </m> and then puts forward a cross-connected multi-layer feed-forward neural network algorithm .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: first -> second
first:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: no relation
first:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: no relation
first:
We attempt to replace the hierarchical architecture by a <m> fully connected layer </m> .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: first -> second
first:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .
second:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .


class: same cluster, base class: no relation, new model class: no relation
first:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: no relation
first:
In summary , we use the sub - vectors to model and according to : where denotes inner product , is a <m> fully - connected neural network </m> with model parameter , denotes the element - wise sigmoid function , and is the softmax function with temperature parameter .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: no relation
first:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: same cluster, base class: first -> second, new model class: no relation
first:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: same cluster, base class: first -> second, new model class: no relation
first:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: same cluster, base class: first -> second, new model class: no relation
first:
In the Neural Module Networks , the question is processed by a dependency parser , and fragments of the parse , selected with ad hoc fixed rules are associated with modules , are assembled into a <m> full neural network </m> .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: same cluster, base class: no relation, new model class: second -> first
first:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .
second:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .


class: same cluster, base class: no relation, new model class: no relation
first:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: second -> first
first:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: same cluster, base class: no relation, new model class: no relation
first:
We trained a <m> fully - connected 2 - layer neural networks </m> with 50 rectified linear units ( ReLU ) in each layer on 50 bootstrapped samples from the data .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .
second:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Fully connected deep neural networks </m> with ELUs ( ) , ReLUs , and LReLUs ( ) were trained on the MNIST digit classification dataset while each hidden unit ’s activation was tracked .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: no relation
first:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .
second:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .


class: same cluster, base class: no relation, new model class: no relation
first:
For experiments with the synthetic datasets , the following <m> fully - connected feed forward neural networks </m> are employed where FC and BN denote fully - connected layer and batch normalization layer respectively .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: no relation
first:
To experiment with <m> fully - connected neural nets </m> , as in CNN , we minimized square loss with regularization and optional dropout by SGD , and activation was fixed to rectifier .
second:
The proposed research designs a <m> three layer neuron model </m> using supervised learning technique .


class: same cluster, base class: no relation, new model class: no relation
first:
In our work , activation function of <m> neuron </m> is implemented with simple CMOS inverter to save area overhead .
second:
A stochastic resonance behavior is detected for the <m> processing neuron </m> in correspondence with the " ghost " frequencies both in the harmonic and in the anharmonic case .


class: same cluster, base class: first -> second, new model class: no relation
first:
In our work , activation function of <m> neuron </m> is implemented with simple CMOS inverter to save area overhead .
second:
Results indicate that , the stochastic resonance response of the double layers FHN neuron network is better than the <m> single FHN neuron model </m> , and has better stability , and can be effectively detected for input signal at a wider range of noise intensity .


class: same cluster, base class: no relation, new model class: no relation
first:
In our work , activation function of <m> neuron </m> is implemented with simple CMOS inverter to save area overhead .
second:
Here we review recent progress in our understanding of how intrinsic correlations arise in simple <m> biophysically justified neuron models </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In our work , activation function of <m> neuron </m> is implemented with simple CMOS inverter to save area overhead .
second:
In this algorithm , the <m> aggregation computation of process neuron </m> can be simplified and the complex integration procedure can be avoided , and the training of process neuron networks can equate to the training of common networks , namely the arbitrary function approximating problems are converted to the function optimization problems .


class: same cluster, base class: no relation, new model class: no relation
first:
A stochastic resonance behavior is detected for the <m> processing neuron </m> in correspondence with the " ghost " frequencies both in the harmonic and in the anharmonic case .
second:
Results indicate that , the stochastic resonance response of the double layers FHN neuron network is better than the <m> single FHN neuron model </m> , and has better stability , and can be effectively detected for input signal at a wider range of noise intensity .


class: same cluster, base class: no relation, new model class: no relation
first:
A stochastic resonance behavior is detected for the <m> processing neuron </m> in correspondence with the " ghost " frequencies both in the harmonic and in the anharmonic case .
second:
Here we review recent progress in our understanding of how intrinsic correlations arise in simple <m> biophysically justified neuron models </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
A stochastic resonance behavior is detected for the <m> processing neuron </m> in correspondence with the " ghost " frequencies both in the harmonic and in the anharmonic case .
second:
In this algorithm , the <m> aggregation computation of process neuron </m> can be simplified and the complex integration procedure can be avoided , and the training of process neuron networks can equate to the training of common networks , namely the arbitrary function approximating problems are converted to the function optimization problems .


class: same cluster, base class: no relation, new model class: no relation
first:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .
second:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .


class: same cluster, base class: no relation, new model class: no relation
first:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: same cluster, base class: no relation, new model class: no relation
first:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: same cluster, base class: no relation, new model class: no relation
first:
Based on the <m> neuron network technique </m> , methods of interpolation to the charted depth are suggested in this paper .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: same cluster, base class: no relation, new model class: no relation
first:
Results indicate that , the stochastic resonance response of the double layers FHN neuron network is better than the <m> single FHN neuron model </m> , and has better stability , and can be effectively detected for input signal at a wider range of noise intensity .
second:
Here we review recent progress in our understanding of how intrinsic correlations arise in simple <m> biophysically justified neuron models </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Results indicate that , the stochastic resonance response of the double layers FHN neuron network is better than the <m> single FHN neuron model </m> , and has better stability , and can be effectively detected for input signal at a wider range of noise intensity .
second:
In this algorithm , the <m> aggregation computation of process neuron </m> can be simplified and the complex integration procedure can be avoided , and the training of process neuron networks can equate to the training of common networks , namely the arbitrary function approximating problems are converted to the function optimization problems .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we review recent progress in our understanding of how intrinsic correlations arise in simple <m> biophysically justified neuron models </m> .
second:
In this algorithm , the <m> aggregation computation of process neuron </m> can be simplified and the complex integration procedure can be avoided , and the training of process neuron networks can equate to the training of common networks , namely the arbitrary function approximating problems are converted to the function optimization problems .


class: same cluster, base class: no relation, new model class: no relation
first:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .
second:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .


class: same cluster, base class: no relation, new model class: no relation
first:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: same cluster, base class: no relation, new model class: no relation
first:
The proposed <m> TROIKA neuron </m> is based on nonlinear aggregation functions which enables our hybrid neural classifier to inculcate the benefits of its computational power resulting in superior learning and generalization .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: same cluster, base class: no relation, new model class: no relation
first:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .
second:
Inadequate amounts of defective samples in some rare cases poses a major challenge for the development of defect detection and <m> classification deep learning models </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .
second:
So a popular deep learning method for re - ID consists of 1 ) training a <m> classification deep model </m> on the training set , 2 ) extracting image descriptors using the fully - connected ( FC ) layer for the query and gallery images , and 3 ) computing similarities based on Euclidean distance before returning the sorted list .


class: same cluster, base class: no relation, new model class: no relation
first:
We are not the first to consider alternatives to traditional <m> neuron models </m> in CNNs .
second:
Unlike these existing methods , we adapt and extend <m> deep classification architectures </m> , using image classification as supervised pre - training , and fine - tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths .


class: same cluster, base class: no relation, new model class: no relation
first:
We evaluate the accessibility of our approach with a <m> user study </m> .
second:
The usability and ease of learning of this web interface were evaluated in a <m> user-based study </m> , the results of which are also reported .


class: same cluster, base class: no relation, new model class: no relation
first:
The usability and ease of learning of this web interface were evaluated in a <m> user-based study </m> , the results of which are also reported .
second:
The experimental results demonstrated that our work outperformed the state-of-the-art methods not only in several objective quality measures but also in a <m> user study analysis </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The usability and ease of learning of this web interface were evaluated in a <m> user-based study </m> , the results of which are also reported .
second:
The main contributions of this paper are two-fold and materialize into i ) a <m> user research study </m> that covers aspects such as perception , cognition , mental and psychosocial changes that occur with age and ii ) an example-based description of the process of creating personas .


class: same cluster, base class: no relation, new model class: no relation
first:
The usability and ease of learning of this web interface were evaluated in a <m> user-based study </m> , the results of which are also reported .
second:
In the core of our paper , we survey colormap generation techniques , including the latest advances in the field by grouping these techniques into four classes : procedural methods , <m> user-study based methods </m> , rule-based methods , and data-driven methods ; we also include a section on methods that are beyond pure data comprehension purposes .


class: same cluster, base class: no relation, new model class: no relation
first:
The usability and ease of learning of this web interface were evaluated in a <m> user-based study </m> , the results of which are also reported .
second:
The laboratory setting allows evaluators to create not only the environment , but also the scenario in which a <m> user study of system </m> is conducted .


class: same cluster, base class: no relation, new model class: no relation
first:
The usability and ease of learning of this web interface were evaluated in a <m> user-based study </m> , the results of which are also reported .
second:
In this paper , we present the results of a <m> user study of a technique </m> used to pair two devices ( such as two cell phones ) which have good quality output interfaces in the form of a display , speaker , and/or vibration .


class: same cluster, base class: no relation, new model class: no relation
first:
The usability and ease of learning of this web interface were evaluated in a <m> user-based study </m> , the results of which are also reported .
second:
System evaluation and <m> user study research </m> thus can play a significant role in infectious disease information research .


class: same cluster, base class: no relation, new model class: no relation
first:
The usability and ease of learning of this web interface were evaluated in a <m> user-based study </m> , the results of which are also reported .
second:
To evaluate the green navigation service efficiency , we conducted a <m> user subject study </m> consisting of 22 users driving different vehicles over the course of several months in Urbana-Champaign , IL .


class: same cluster, base class: no relation, new model class: no relation
first:
The usability and ease of learning of this web interface were evaluated in a <m> user-based study </m> , the results of which are also reported .
second:
For realism , we conduct a <m> user study </m> using pairwise comparison .


class: same cluster, base class: no relation, new model class: no relation
first:
The experimental results demonstrated that our work outperformed the state-of-the-art methods not only in several objective quality measures but also in a <m> user study analysis </m> .
second:
The main contributions of this paper are two-fold and materialize into i ) a <m> user research study </m> that covers aspects such as perception , cognition , mental and psychosocial changes that occur with age and ii ) an example-based description of the process of creating personas .


class: same cluster, base class: no relation, new model class: no relation
first:
The experimental results demonstrated that our work outperformed the state-of-the-art methods not only in several objective quality measures but also in a <m> user study analysis </m> .
second:
In the core of our paper , we survey colormap generation techniques , including the latest advances in the field by grouping these techniques into four classes : procedural methods , <m> user-study based methods </m> , rule-based methods , and data-driven methods ; we also include a section on methods that are beyond pure data comprehension purposes .


class: same cluster, base class: no relation, new model class: no relation
first:
The experimental results demonstrated that our work outperformed the state-of-the-art methods not only in several objective quality measures but also in a <m> user study analysis </m> .
second:
The laboratory setting allows evaluators to create not only the environment , but also the scenario in which a <m> user study of system </m> is conducted .


class: same cluster, base class: no relation, new model class: no relation
first:
The experimental results demonstrated that our work outperformed the state-of-the-art methods not only in several objective quality measures but also in a <m> user study analysis </m> .
second:
In this paper , we present the results of a <m> user study of a technique </m> used to pair two devices ( such as two cell phones ) which have good quality output interfaces in the form of a display , speaker , and/or vibration .


class: same cluster, base class: no relation, new model class: no relation
first:
The experimental results demonstrated that our work outperformed the state-of-the-art methods not only in several objective quality measures but also in a <m> user study analysis </m> .
second:
System evaluation and <m> user study research </m> thus can play a significant role in infectious disease information research .


class: same cluster, base class: no relation, new model class: no relation
first:
The experimental results demonstrated that our work outperformed the state-of-the-art methods not only in several objective quality measures but also in a <m> user study analysis </m> .
second:
To evaluate the green navigation service efficiency , we conducted a <m> user subject study </m> consisting of 22 users driving different vehicles over the course of several months in Urbana-Champaign , IL .


class: same cluster, base class: second -> first, new model class: no relation
first:
A standard <m> survey </m> was applied in five experiments .
second:
In the <m> survey and exploration phase </m> , a number of studies have been proposed to carry out mineral potential mapping by integrating geochemical and geophysical explorations , geological maps , fault lines , and topographic information .


class: same cluster, base class: no relation, new model class: no relation
first:
Nowadays , attempts have been made to automatically infer transportation modes from positional data , such as the data collected by using GPS devices so that the cost in time and budget of conventional <m> travel diary survey </m> could be significantly reduced .
second:
<m> Travel survey definitions </m> In order to de - construct a GPS track , some definitions have been standardised to be used for the description of different fragments of the trip .


class: same cluster, base class: no relation, new model class: no relation
first:
The paper finally presents a <m> user experiment </m> , carried out with 30 participants according to a between-subject protocol , to evaluate the usability of ImAtHome and compare it with the official app for home automation recently released by Apple .
second:
Keyword Outdoor Mixed-Reality , Augmented-Reality annotation , <m> User experiment 1 </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
This tutorial is for researchers and engineers , working in the field of Virtual Reality ( VR ) and Augmented Reality ( AR ) , who wish to conduct <m> user-based experiments </m> with a specific aim of promoting both traditional quantitative human-subject experiments and qualitative methods for assessing usability .
second:
Keyword Outdoor Mixed-Reality , Augmented-Reality annotation , <m> User experiment 1 </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Keyword Outdoor Mixed-Reality , Augmented-Reality annotation , <m> User experiment 1 </m> .
second:
We evaluate four interactive segmentation algorithms using these strategies , and compare the results with our previous <m> user experiment-based evaluation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .
second:
The added advantage of <m> hidden layer </m> for lip reading is that it takes into account the nonlinear variation of lip features while speaking .


class: same cluster, base class: no relation, new model class: no relation
first:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .
second:
The experiment showed that the best <m> hidden layer architecture </m> ( HLA ) is 5 - 10 - 11 - 12 - 13 - 1 with learning function ( LF ) of trainlm , activation function ( AF ) of logsig and purelin , and learning rate ( LR ) of 0.5 .


class: same cluster, base class: no relation, new model class: no relation
first:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .
second:
Further <m> Hidden layer 1 </m> helps to extract data to estimate the forecasting equation and also it helps for evaluating estimated Model using different measures .


class: same cluster, base class: no relation, new model class: no relation
first:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .
second:
where and are the question and the answer encoded vectors and denotes the output of the <m> hidden layer </m> of the MLP .


class: same cluster, base class: no relation, new model class: no relation
first:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .
second:
Similar to our LoNGAE model , the GCN model can learn <m> hidden layer representations </m> that encode both local graph structure and features of nodes .


class: same cluster, base class: no relation, new model class: no relation
first:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .
second:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .


class: same cluster, base class: no relation, new model class: no relation
first:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .
second:
PNN : For the purpose of capturing high - order feature interactions , PNN imposes a product layer between the embedding layer and the <m> first hidden layer </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .
second:
Their method simultaneously minimizes classification error while improving the directness and transparency of the <m> hidden layer learning process </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The computation from the hidden state in the decoder to the output is implemented as a deep neural network with a single <m> intermediate layer </m> having 500 maxout units each pooling 2 inputs .
second:
This was found sufficient , with the following <m> hidden layers learning </m> to combine the multi - scale features .


class: same cluster, base class: no relation, new model class: no relation
first:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .
second:
The added advantage of <m> hidden layer </m> for lip reading is that it takes into account the nonlinear variation of lip features while speaking .


class: same cluster, base class: no relation, new model class: no relation
first:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .
second:
The experiment showed that the best <m> hidden layer architecture </m> ( HLA ) is 5 - 10 - 11 - 12 - 13 - 1 with learning function ( LF ) of trainlm , activation function ( AF ) of logsig and purelin , and learning rate ( LR ) of 0.5 .


class: same cluster, base class: no relation, new model class: no relation
first:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .
second:
Further <m> Hidden layer 1 </m> helps to extract data to estimate the forecasting equation and also it helps for evaluating estimated Model using different measures .


class: same cluster, base class: no relation, new model class: no relation
first:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .
second:
where and are the question and the answer encoded vectors and denotes the output of the <m> hidden layer </m> of the MLP .


class: same cluster, base class: no relation, new model class: no relation
first:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .
second:
Similar to our LoNGAE model , the GCN model can learn <m> hidden layer representations </m> that encode both local graph structure and features of nodes .


class: same cluster, base class: no relation, new model class: no relation
first:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .
second:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .


class: same cluster, base class: no relation, new model class: no relation
first:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .
second:
PNN : For the purpose of capturing high - order feature interactions , PNN imposes a product layer between the embedding layer and the <m> first hidden layer </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .
second:
Their method simultaneously minimizes classification error while improving the directness and transparency of the <m> hidden layer learning process </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
One of the drawbacks of this approach is that all <m> intermediate feature layers </m> are at full image resolution and have a high memory footprint .
second:
This was found sufficient , with the following <m> hidden layers learning </m> to combine the multi - scale features .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .
second:
The added advantage of <m> hidden layer </m> for lip reading is that it takes into account the nonlinear variation of lip features while speaking .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .
second:
The experiment showed that the best <m> hidden layer architecture </m> ( HLA ) is 5 - 10 - 11 - 12 - 13 - 1 with learning function ( LF ) of trainlm , activation function ( AF ) of logsig and purelin , and learning rate ( LR ) of 0.5 .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .
second:
Further <m> Hidden layer 1 </m> helps to extract data to estimate the forecasting equation and also it helps for evaluating estimated Model using different measures .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .
second:
where and are the question and the answer encoded vectors and denotes the output of the <m> hidden layer </m> of the MLP .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .
second:
Similar to our LoNGAE model , the GCN model can learn <m> hidden layer representations </m> that encode both local graph structure and features of nodes .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .
second:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .
second:
PNN : For the purpose of capturing high - order feature interactions , PNN imposes a product layer between the embedding layer and the <m> first hidden layer </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .
second:
Their method simultaneously minimizes classification error while improving the directness and transparency of the <m> hidden layer learning process </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
subsection : ELMo ELMo is a task specific combination of the <m> intermediate layer representations </m> in the biLM .
second:
This was found sufficient , with the following <m> hidden layers learning </m> to combine the multi - scale features .


class: same cluster, base class: no relation, new model class: no relation
first:
The added advantage of <m> hidden layer </m> for lip reading is that it takes into account the nonlinear variation of lip features while speaking .
second:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .


class: same cluster, base class: no relation, new model class: no relation
first:
The experiment showed that the best <m> hidden layer architecture </m> ( HLA ) is 5 - 10 - 11 - 12 - 13 - 1 with learning function ( LF ) of trainlm , activation function ( AF ) of logsig and purelin , and learning rate ( LR ) of 0.5 .
second:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .


class: same cluster, base class: no relation, new model class: no relation
first:
Further <m> Hidden layer 1 </m> helps to extract data to estimate the forecasting equation and also it helps for evaluating estimated Model using different measures .
second:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .


class: same cluster, base class: no relation, new model class: no relation
first:
where and are the question and the answer encoded vectors and denotes the output of the <m> hidden layer </m> of the MLP .
second:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .


class: same cluster, base class: no relation, new model class: no relation
first:
Similar to our LoNGAE model , the GCN model can learn <m> hidden layer representations </m> that encode both local graph structure and features of nodes .
second:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .


class: same cluster, base class: no relation, new model class: second -> first
first:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .
second:
PNN : For the purpose of capturing high - order feature interactions , PNN imposes a product layer between the embedding layer and the <m> first hidden layer </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .
second:
Their method simultaneously minimizes classification error while improving the directness and transparency of the <m> hidden layer learning process </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The RNNs and LSTMs use 50 dimensional embeddings and 50 <m> dimensional hidden layers </m> ; they predict a single output at the end of the sequences and the output is treated as a classification problem , the loss is cross entropy .
second:
This was found sufficient , with the following <m> hidden layers learning </m> to combine the multi - scale features .


class: same cluster, base class: second -> first, new model class: second -> first
first:
We review how data-analysis concepts and techniques developed by computational neuroscientists can be useful for analyzing representations in DNNs , and in turn , how recently developed techniques for <m> analysis of DNNs </m> can be useful for understanding representations in biological neural networks .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: same cluster, base class: second -> first, new model class: second -> first
first:
We review how data-analysis concepts and techniques developed by computational neuroscientists can be useful for analyzing representations in DNNs , and in turn , how recently developed techniques for <m> analysis of DNNs </m> can be useful for understanding representations in biological neural networks .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: same cluster, base class: second -> first, new model class: second -> first
first:
We review how data-analysis concepts and techniques developed by computational neuroscientists can be useful for analyzing representations in DNNs , and in turn , how recently developed techniques for <m> analysis of DNNs </m> can be useful for understanding representations in biological neural networks .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: same cluster, base class: second -> first, new model class: second -> first
first:
Accordingly , techniques that enable efficient <m> processing of DNNs </m> to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI system .
second:
section : Abstract - We propose a <m> deep learning method </m> for single image super - resolution ( SR ) .


class: same cluster, base class: second -> first, new model class: second -> first
first:
Accordingly , techniques that enable efficient <m> processing of DNNs </m> to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI system .
second:
In , authors present a <m> deep learning architecture </m> for learning on Riemannian manifold which can be employed for covariance pooling .


class: same cluster, base class: second -> first, new model class: second -> first
first:
Accordingly , techniques that enable efficient <m> processing of DNNs </m> to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI system .
second:
By combining the <m> deep learning algorithm </m> , the discriminative scheme has achieved substantial improvement on AIFR .


class: same cluster, base class: no relation, new model class: no relation
first:
Very recently , Ma et al. ( ICLR 2018 ) proposed to use local intrinsic dimensionality ( LID ) in <m> layer-wise hidden representations of DNNs </m> to study adversarial subspaces .
second:
Our work is also related to the recent efforts in visualizing the <m> internal representations of DNNs </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In a first phase , we train independent classifiers from image data , using a <m> deep learning representation </m> , and from text data , using a bag-of-words representation .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Recently , <m> deep learning-based representation learning methods </m> have attracted increasing attention in machine fault diagnosis .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Conventional <m> deep models of representation learning </m> used for HRRP target recognition commonly use the vanilla autoen-coder and deep belief net ( DBN ) , moreover , the simulated HRRP samples used in these related work are mostly under the free space condition which is different from the real world situation .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Abstract : We have developed what is , to the best of our knowledge , the first complete theory of representation for decision and control task , which has shown not only to encompass and explain all known phenomenology in <m> deep neural network-based representation learning </m> , but also to predict phenomena that were thus far unexplained .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Inspired by the recent advances in the area of <m> deep neural representation learning </m> , for the first time in the literature , in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Unlike most previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and <m> deep feature learning </m> within one training process ( deep domain adaptation ) .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
section : Conclusion This paper have shown that the effect of the face identification and verification supervisory signals on <m> deep feature representation </m> coincide with the two aspects of constructing ideal features for face recognition , _ meaning : NTF .
second:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .
second:
While , <m> neural-based representations </m> does not require a dedicated calibration process to solve these tasks .


class: same cluster, base class: no relation, new model class: no relation
first:
This body of evidence , which is reviewed in this report , suggests that CANNs may serve as a canonical model for <m> neural information representation </m> .
second:
More recently , non - linear representations have become increasingly studied , including <m> neural network representations </m> [ reference ] and most notably the state - of - the - art mSDA [ reference ] .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Towards this aim , we use Bingham distributions , to model the orientation of the camera pose , and a multivariate Gaussian to model the position , with an <m> end-to-end deep neural network </m> .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: same cluster, base class: first -> second, new model class: first -> second
first:
In addition , we also present an alternate <m> end-to-end deep neural network architecture </m> for variational synthesis of linkages .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: same cluster, base class: first -> second, new model class: first -> second
first:
This paper presents a novel <m> end-to-end deep learning neural network </m> that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: same cluster, base class: first -> second, new model class: first -> second
first:
In this paper , we propose an <m> end-to-end deep neural network solution </m> to the prediction task .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: same cluster, base class: first -> second, new model class: first -> second
first:
To achieve this goal , we have designed an <m> end to end deep neural network structure </m> consisting of fully connected layers with cascaded convolutional layers to be trained and tested over a publicly available image dataset .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive <m> deep end-to-end neural network models </m> that generate driving control signals directly from input images .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: same cluster, base class: no relation, new model class: first -> second
first:
We then introduce a unified hierarchical model , which combines methods from Bayesian inference , invertible latent density inference , and discriminative classification in a single <m> end-to-end deep neural network topology </m> to yield efficient per-sample uncertainty estimation .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We propose to directly learn the discrete codes in an <m> end - to - end neural network </m> by applying the Gumbel - softmax trick .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: same cluster, base class: first -> second, new model class: first -> second
first:
More recently , several end - to - <m> end deep neural networks </m> that learn features directly from data have been applied to this problem .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Bilen and Vedaldi presents an <m> end - to - end deep network </m> for WSOD , in which final image classification score is the weighted sum of proposal scores , that is , each proposal contributes a percentage to the final image classification .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: same cluster, base class: first -> second, new model class: first -> second
first:
The model is an <m> end - to - end deep neural network </m> that can generate answers conditioned on a given style .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: same cluster, base class: first -> second, new model class: first -> second
first:
First , we propose a novel <m> end - to - end neural network architecture </m> that generates a 3D mesh model from a single RGB image .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Recently , <m> end - to - end neural network - based models </m> have drawn worldwide attention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: same cluster, base class: first -> second, new model class: first -> second
first:
We demonstrate the use of this approach by utilizing it for the semantic segmentation task : we form an end - to - <m> end trainable deep network </m> by combining a fully convolutional neural network with the CRF - RNN .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: same cluster, base class: first -> second, new model class: first -> second
first:
To ease these issues , we , therefore , propose a novel <m> end-to-end neural framework </m> – TDRB , which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference , thus achieving more accurate recommendations .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: same cluster, base class: first -> second, new model class: first -> second
first:
To address the above problems , we propose Coda , the first <m> end-to-end neural-based framework </m> for code decompilation .
second:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .


class: same cluster, base class: no relation, new model class: second -> first
first:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .
second:
However , <m> emergent end-to-end neural methods </m> enable to optimize the speech enhancement system with more application-oriented objectives .


class: same cluster, base class: second -> first, new model class: second -> first
first:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .
second:
To address this issue , we propose an endto- <m> end neural model </m> , TGMN-CR , which makes better use of target content information .


class: same cluster, base class: second -> first, new model class: second -> first
first:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .
second:
In this paper , we delve into an effective <m> end-to-end neural network structure </m> for stronger feature expression and spatial correlation learning .


class: same cluster, base class: second -> first, new model class: second -> first
first:
In recent years , <m> all-neural end-to-end approaches </m> have obtained state-of-the-art results on several challenging automatic speech recognition ( ASR ) tasks .
second:
To address this problem , we propose an <m> end - to - end neural model </m> that enables those answer candidates from different passages to verify each other based on their content representations .


class: same cluster, base class: no relation, new model class: no relation
first:
What is Bootstrapping ? Estimation Confidence Sets and Hypothesis Testing Regression Analysis Forecasting and <m> Time Series Analysis </m> Which Resampling Method Should You Use ?
second:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In this paper we present the different approaches to modelling dynamic regression and structural equations• First , we give a survey of the traditional econometrie and the <m> time series approaches </m> to specification analysis of dynamic econometrie models .
second:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
As in the case of traditional data base systems , in <m> time series data systems </m> too the methods employed in capturing , indexing , representing and storing the data remains as the key issue .
second:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In order to evaluate the ability of this proposed algorithm , the traditional statistical models are used as a benchmark vis-a-vis their <m> time series counterparts </m> .
second:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The judicature process is induced into the de-noising disposal of <m> time series in algorithm </m> , which builds a new de-nosing algorithm .
second:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Through the numerical experiments , we showed that the quality of the inferred genetic network is slightly improved by giving greater importance to static data than <m> time-series ones </m> .
second:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In addition , searching time series databases via examples , which may be either <m> time series or models </m> , is also part of this work .
second:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Considering statistical methods , we propose the <m> Time Series ( TIME ) model </m> based on the analysis of users ' time behavior characteristics . And thinking from the theme of the users ' session , the improved Latent Dirichlet Allocation ( LDA ) malicious topic model is designed which has improved the sensitivity of the model to detect malicious topics .
second:
We have used various level set methods in order to identify novelty data for <m> temporal data time series </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: same cluster, base class: first -> second, new model class: no relation
first:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: same cluster, base class: first -> second, new model class: no relation
first:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: same cluster, base class: first -> second, new model class: no relation
first:
However , as illustrated above , an additional hurdle stems from the fact that many ( early ) temporal query languages allowed the users to manipulate a finite underlying representation of temporal databases rather than the actual temporal values/objects in the associated <m> temporal data model </m> .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: same cluster, base class: first -> second, new model class: no relation
first:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: same cluster, base class: first -> second, new model class: no relation
first:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: same cluster, base class: first -> second, new model class: no relation
first:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: same cluster, base class: first -> second, new model class: no relation
first:
A comparison of the proposed method with other algorithms using <m> temporal data analysis </m> suggested that the hand gesture prediction system using ANNs would be able to forecast various types of hand gestures using resistance data obtained from wearable devices based on PGSs .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: same cluster, base class: first -> second, new model class: no relation
first:
Tuple-versioning of relations is the adopted method of <m> temporal data representation </m> .
second:
This user modeling approach comprises the following new novel aspects ( 1 ) Modeling microblog users behavior evolution by considering the different event phases ( 2 ) Characterizing users activity over time through a <m> temporal sequence representation </m> ( 3 ) Time-series-based selection of the most discriminative features characterizing users at each event phase .


class: same cluster, base class: first -> second, new model class: no relation
first:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: same cluster, base class: first -> second, new model class: no relation
first:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: same cluster, base class: first -> second, new model class: no relation
first:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: same cluster, base class: first -> second, new model class: no relation
first:
The system provides the <m> temporal data collection </m> , data storage , data management and data display and query functions .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: same cluster, base class: first -> second, new model class: no relation
first:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: same cluster, base class: first -> second, new model class: no relation
first:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: same cluster, base class: first -> second, new model class: no relation
first:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: same cluster, base class: first -> second, new model class: no relation
first:
The thesis posits that <m> temporal analysis of data </m> , from social media in particular , provides us with insights into real-world dynamics .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: same cluster, base class: first -> second, new model class: no relation
first:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: same cluster, base class: first -> second, new model class: no relation
first:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: same cluster, base class: first -> second, new model class: no relation
first:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: same cluster, base class: first -> second, new model class: no relation
first:
This paper explores FSM in <m> temporal data domain </m> or FTP mining and proposes an efficient algorithm for the same .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: same cluster, base class: first -> second, new model class: no relation
first:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: same cluster, base class: first -> second, new model class: no relation
first:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: same cluster, base class: first -> second, new model class: no relation
first:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: same cluster, base class: first -> second, new model class: no relation
first:
The <m> temporal data format </m> is implemented using the GML temporal data type , i.e. , the format to define temporal data types for temporal attributes in the Nexus context model is based on the ISO 8601 standard for temporal values .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: same cluster, base class: first -> second, new model class: no relation
first:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: same cluster, base class: first -> second, new model class: no relation
first:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: same cluster, base class: first -> second, new model class: no relation
first:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: same cluster, base class: first -> second, new model class: no relation
first:
In this paper , we are concerned with an effective framework for <m> temporal data set </m> that does not scarify on-line query performance and is specifically designed for very large sensor network database .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .
second:
This paper presents a recurrent self-organizing map ( RSOM ) for <m> temporal sequence processing </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .
second:
<m> Temporal sequence analysis </m> is a pracrical technique popular in economic studies both at home and abroad .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .
second:
We develop and illustrate the framework in the context of <m> temporal sequence models </m> with examples .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .
second:
This paper presented a <m> temporal sequence analyzing method </m> , aiming at the extraction of typical sequences from an unlabeled dataset .


class: same cluster, base class: first -> second, new model class: no relation
first:
subsection : <m> Temporal Model </m> Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames .
second:
This paper is to analyse the typicality layer of Serial Verb Construction in Qiminyaoshu according to the similarity of semantic characters of prototypical Serial Verb Construction and comes to a conclusion that <m> Temporal Sequence Principle </m> , which divides the typicality layer , is the cognitive basis of Serial Verb Construction .


class: same cluster, base class: first -> second, new model class: no relation
first:
We claim that these refinement iterations should have a more direct and shared <m> temporal representation </m> .
second:
This user modeling approach comprises the following new novel aspects ( 1 ) Modeling microblog users behavior evolution by considering the different event phases ( 2 ) Characterizing users activity over time through a <m> temporal sequence representation </m> ( 3 ) Time-series-based selection of the most discriminative features characterizing users at each event phase .


class: same cluster, base class: no relation, new model class: second -> first
first:
Furthermore , the optimal organization of <m> numerical data processing </m> in enterprises is analyzed on the example of predicting demand in the firm .
second:
The fourth and last module , entitled Backtesting and <m> Numerical Results </m> , evaluates the accuracy of the trend predictive numerical models over a " significant " test set via two generic backtesting plans .


class: same cluster, base class: second -> first, new model class: second -> first
first:
Furthermore , the optimal organization of <m> numerical data processing </m> in enterprises is analyzed on the example of predicting demand in the firm .
second:
On the downside , todayâs computing infrastructures are very inefficient when it comes to <m> numerical calculation </m> on non - uniform sparse data structures .


class: same cluster, base class: second -> first, new model class: second -> first
first:
Furthermore , the optimal organization of <m> numerical data processing </m> in enterprises is analyzed on the example of predicting demand in the firm .
second:
Because sea surface propagation causes multipath effect , it 's necessary to study the multipath ray model and the <m> numerical characteristics </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
It is designed to support distributed computing and data sharing on the Internet through the use of distributed objects and a very general <m> numerical data model </m> .
second:
The fourth and last module , entitled Backtesting and <m> Numerical Results </m> , evaluates the accuracy of the trend predictive numerical models over a " significant " test set via two generic backtesting plans .


class: same cluster, base class: second -> first, new model class: second -> first
first:
It is designed to support distributed computing and data sharing on the Internet through the use of distributed objects and a very general <m> numerical data model </m> .
second:
On the downside , todayâs computing infrastructures are very inefficient when it comes to <m> numerical calculation </m> on non - uniform sparse data structures .


class: same cluster, base class: second -> first, new model class: second -> first
first:
It is designed to support distributed computing and data sharing on the Internet through the use of distributed objects and a very general <m> numerical data model </m> .
second:
Because sea surface propagation causes multipath effect , it 's necessary to study the multipath ray model and the <m> numerical characteristics </m> .


class: same cluster, base class: first -> second, new model class: second -> first
first:
The rapidly increasing power and availability of small computers have made sophisticated <m> numerical analysis of data </m> a routine feature of modem chemistry .
second:
The fourth and last module , entitled Backtesting and <m> Numerical Results </m> , evaluates the accuracy of the trend predictive numerical models over a " significant " test set via two generic backtesting plans .


class: same cluster, base class: no relation, new model class: first -> second
first:
The fourth and last module , entitled Backtesting and <m> Numerical Results </m> , evaluates the accuracy of the trend predictive numerical models over a " significant " test set via two generic backtesting plans .
second:
Formation features of the <m> numerical data source model </m> were examined .


class: same cluster, base class: second -> first, new model class: second -> first
first:
Formation features of the <m> numerical data source model </m> were examined .
second:
On the downside , todayâs computing infrastructures are very inefficient when it comes to <m> numerical calculation </m> on non - uniform sparse data structures .


class: same cluster, base class: second -> first, new model class: second -> first
first:
Formation features of the <m> numerical data source model </m> were examined .
second:
Because sea surface propagation causes multipath effect , it 's necessary to study the multipath ray model and the <m> numerical characteristics </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .


class: same cluster, base class: no relation, new model class: first -> second
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .


class: same cluster, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .


class: same cluster, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .


class: same cluster, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .


class: same cluster, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: same cluster, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: same cluster, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: same cluster, base class: no relation, new model class: no relation
first:
Our findings suggest a complex interplay between visual feature coding and <m> categorical representations </m> that is mediated by the visual system 's capacity to use image features to resolve a recognisable object .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: same cluster, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .


class: same cluster, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .


class: same cluster, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .


class: same cluster, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .


class: same cluster, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .


class: same cluster, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: same cluster, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: same cluster, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: same cluster, base class: no relation, new model class: no relation
first:
When evaluating the fitness function , we use different mechanisms to interpret the particle positions in the description space , based on data type ; as will be described , rounding is used for integer attributes while a frequency measure is used for <m> categorical descriptors </m> .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: same cluster, base class: no relation, new model class: no relation
first:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .
second:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .


class: same cluster, base class: no relation, new model class: no relation
first:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .
second:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .


class: same cluster, base class: no relation, new model class: no relation
first:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .
second:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .


class: same cluster, base class: no relation, new model class: no relation
first:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: same cluster, base class: no relation, new model class: no relation
first:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: same cluster, base class: no relation, new model class: no relation
first:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: same cluster, base class: no relation, new model class: no relation
first:
Great efforts should be made to RP research in terms of varies of materials , combination with other methods , learning mechanism of <m> categorical feature processing </m> .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .
second:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .
second:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .
second:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .
second:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Categorical feature selection </m> for clustering has rarely been addressed in the literature , with most of the proposed approaches having focused on numerical data .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: same cluster, base class: no relation, new model class: no relation
first:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .
second:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .


class: same cluster, base class: no relation, new model class: no relation
first:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .
second:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .


class: same cluster, base class: no relation, new model class: no relation
first:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .
second:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .


class: same cluster, base class: no relation, new model class: no relation
first:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .
second:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .


class: same cluster, base class: no relation, new model class: no relation
first:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: same cluster, base class: no relation, new model class: no relation
first:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: same cluster, base class: no relation, new model class: no relation
first:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: same cluster, base class: no relation, new model class: no relation
first:
In our method , each data source is represented by either a functional linkage graph or a <m> categorical feature vector </m> .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: same cluster, base class: no relation, new model class: no relation
first:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .
second:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .


class: same cluster, base class: no relation, new model class: no relation
first:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: same cluster, base class: no relation, new model class: no relation
first:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: same cluster, base class: no relation, new model class: no relation
first:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: same cluster, base class: no relation, new model class: no relation
first:
A technique which called Maximum Total Attribute Relative ( MTAR ) is able to determine the partition attribute of the <m> categorical information system </m> at the category level without compromising the computational cost and at the same time enhance the legitimacy of the resulting clusters .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: same cluster, base class: no relation, new model class: no relation
first:
This paper introduces a <m> categorical data analysis technique </m> called Hildebrand 's del .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: same cluster, base class: no relation, new model class: no relation
first:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: same cluster, base class: no relation, new model class: no relation
first:
While some statistic courses give cursory coverage of contingency tables and a few nonparametric techniques , most psychologists are unaware of the rich possibilities that other research fields have enjoyed through the use of <m> categorical data method </m> .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: same cluster, base class: no relation, new model class: no relation
first:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .
second:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .


class: same cluster, base class: no relation, new model class: no relation
first:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: same cluster, base class: no relation, new model class: no relation
first:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: same cluster, base class: no relation, new model class: no relation
first:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: same cluster, base class: no relation, new model class: no relation
first:
Extensive experiments of classification with the embedded representation on 17 data sets demonstrate that the proposed framework can significantly improve the <m> categorical data representation </m> performance compared with state-of-the-art competitors .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> categorical data type approach </m> is an extension of the abstract data type approach in a way that seems particularly useful for parallel computation .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: same cluster, base class: no relation, new model class: no relation
first:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .
second:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .


class: same cluster, base class: no relation, new model class: no relation
first:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: same cluster, base class: no relation, new model class: no relation
first:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: same cluster, base class: no relation, new model class: no relation
first:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: same cluster, base class: no relation, new model class: no relation
first:
For the <m> categorical data case </m> , special attention is given to measures of inter- , observer agreement and tests of hypotheses concerning inter-observer bias .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .
second:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .
second:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .
second:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> categorical distribution </m> ( cat ) captures each intensity value as a discrete outcome and factorizes across channels .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: same cluster, base class: no relation, new model class: no relation
first:
section : Inference Across all of the presented experiments , we use <m> categorical sampling </m> during decoding with a tempered PixelRecursiveSuperResolution .
second:
This is due to the generator network in GAN is designed to be able to adjust the output continuously , which does not work on <m> discrete data generation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .
second:
Specifically , to simulate the feature templates of traditional <m> discrete feature based models </m> , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer .


class: same cluster, base class: no relation, new model class: no relation
first:
Concurrently , rowland2018analysis showed the original class of <m> categorical algorithms </m> are a contraction in the Cramér distance , the metric on cumulative distribution functions .
second:
Properties with a distribution supporting a <m> discrete feature analysis </m> in many languages are likelier to be represented in WALS and to be represented accurately .


class: same cluster, base class: no relation, new model class: no relation
first:
These motifs ( <m> numerical descriptors </m> ) are automatically targeted from the human genome without any clear definition .
second:
Conventional analyses address discriminative tasks of behaviors , e.g. , classification and clustering typically using the subsequences extracted from the trajectory of an object as a <m> numerical feature representation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
These motifs ( <m> numerical descriptors </m> ) are automatically targeted from the human genome without any clear definition .
second:
Experimental results show that the proposed method presents an efficient and effective solution for <m> numerical feature analysis </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
These motifs ( <m> numerical descriptors </m> ) are automatically targeted from the human genome without any clear definition .
second:
Here , we present iFeature , a versatile Python‐based toolkit for generating various <m> numerical feature representation schemes </m> for both protein and peptide sequences .


class: same cluster, base class: no relation, new model class: no relation
first:
These motifs ( <m> numerical descriptors </m> ) are automatically targeted from the human genome without any clear definition .
second:
Experimental comparisons also show the new method can significantly outperform standard <m> numerical feature-type methods </m> in terms of agreement with the clinical diagnosis gold standard .


class: same cluster, base class: no relation, new model class: no relation
first:
These motifs ( <m> numerical descriptors </m> ) are automatically targeted from the human genome without any clear definition .
second:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .


class: same cluster, base class: no relation, new model class: no relation
first:
These motifs ( <m> numerical descriptors </m> ) are automatically targeted from the human genome without any clear definition .
second:
MPEG-7 visual descriptors are <m> numerical representations of features </m> - such as : texture , shape and color - extracted from an image .


class: same cluster, base class: no relation, new model class: no relation
first:
These motifs ( <m> numerical descriptors </m> ) are automatically targeted from the human genome without any clear definition .
second:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .


class: same cluster, base class: no relation, new model class: no relation
first:
These motifs ( <m> numerical descriptors </m> ) are automatically targeted from the human genome without any clear definition .
second:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .


class: same cluster, base class: second -> first, new model class: no relation
first:
These motifs ( <m> numerical descriptors </m> ) are automatically targeted from the human genome without any clear definition .
second:
While previous studies have considered categorical emotions , expressive speech during human interaction conveys subtle behaviors that are better characterized with <m> continuous descriptors </m> ( e.g. , attributes such as arousal , valence , dominance ) .


class: same cluster, base class: no relation, new model class: no relation
first:
These motifs ( <m> numerical descriptors </m> ) are automatically targeted from the human genome without any clear definition .
second:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
These motifs ( <m> numerical descriptors </m> ) are automatically targeted from the human genome without any clear definition .
second:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
These motifs ( <m> numerical descriptors </m> ) are automatically targeted from the human genome without any clear definition .
second:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .


class: same cluster, base class: no relation, new model class: no relation
first:
These motifs ( <m> numerical descriptors </m> ) are automatically targeted from the human genome without any clear definition .
second:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .


class: same cluster, base class: no relation, new model class: no relation
first:
These motifs ( <m> numerical descriptors </m> ) are automatically targeted from the human genome without any clear definition .
second:
The main goal of this dissertation work is to provide syntactically and semantically related words based on <m> continuous feature vector representation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
These motifs ( <m> numerical descriptors </m> ) are automatically targeted from the human genome without any clear definition .
second:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: same cluster, base class: no relation, new model class: no relation
first:
These motifs ( <m> numerical descriptors </m> ) are automatically targeted from the human genome without any clear definition .
second:
Here , the encoder maps an input sequence of symbol representations to a sequence of <m> continuous representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Aiming at this problem , a novel <m> numerical feature extraction approach </m> EPLS is proposed .
second:
For a <m> continuous feature extraction </m> , we use a template matching of the nose bridge in combination with selected features derived from the optical flow .


class: same cluster, base class: no relation, new model class: first -> second
first:
Conventional analyses address discriminative tasks of behaviors , e.g. , classification and clustering typically using the subsequences extracted from the trajectory of an object as a <m> numerical feature representation </m> .
second:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .


class: same cluster, base class: no relation, new model class: no relation
first:
Conventional analyses address discriminative tasks of behaviors , e.g. , classification and clustering typically using the subsequences extracted from the trajectory of an object as a <m> numerical feature representation </m> .
second:
A text parser to match keywords against vocabularies and <m> numerical value descriptors </m> is introduced .


class: same cluster, base class: no relation, new model class: no relation
first:
Conventional analyses address discriminative tasks of behaviors , e.g. , classification and clustering typically using the subsequences extracted from the trajectory of an object as a <m> numerical feature representation </m> .
second:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .


class: same cluster, base class: no relation, new model class: no relation
first:
Conventional analyses address discriminative tasks of behaviors , e.g. , classification and clustering typically using the subsequences extracted from the trajectory of an object as a <m> numerical feature representation </m> .
second:
While previous studies have considered categorical emotions , expressive speech during human interaction conveys subtle behaviors that are better characterized with <m> continuous descriptors </m> ( e.g. , attributes such as arousal , valence , dominance ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Conventional analyses address discriminative tasks of behaviors , e.g. , classification and clustering typically using the subsequences extracted from the trajectory of an object as a <m> numerical feature representation </m> .
second:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Conventional analyses address discriminative tasks of behaviors , e.g. , classification and clustering typically using the subsequences extracted from the trajectory of an object as a <m> numerical feature representation </m> .
second:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Conventional analyses address discriminative tasks of behaviors , e.g. , classification and clustering typically using the subsequences extracted from the trajectory of an object as a <m> numerical feature representation </m> .
second:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .


class: same cluster, base class: no relation, new model class: no relation
first:
Conventional analyses address discriminative tasks of behaviors , e.g. , classification and clustering typically using the subsequences extracted from the trajectory of an object as a <m> numerical feature representation </m> .
second:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .


class: same cluster, base class: no relation, new model class: no relation
first:
Conventional analyses address discriminative tasks of behaviors , e.g. , classification and clustering typically using the subsequences extracted from the trajectory of an object as a <m> numerical feature representation </m> .
second:
The main goal of this dissertation work is to provide syntactically and semantically related words based on <m> continuous feature vector representation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Conventional analyses address discriminative tasks of behaviors , e.g. , classification and clustering typically using the subsequences extracted from the trajectory of an object as a <m> numerical feature representation </m> .
second:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: same cluster, base class: no relation, new model class: no relation
first:
Conventional analyses address discriminative tasks of behaviors , e.g. , classification and clustering typically using the subsequences extracted from the trajectory of an object as a <m> numerical feature representation </m> .
second:
Here , the encoder maps an input sequence of symbol representations to a sequence of <m> continuous representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental results show that the proposed method presents an efficient and effective solution for <m> numerical feature analysis </m> .
second:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental results show that the proposed method presents an efficient and effective solution for <m> numerical feature analysis </m> .
second:
A text parser to match keywords against vocabularies and <m> numerical value descriptors </m> is introduced .


class: same cluster, base class: second -> first, new model class: first -> second
first:
Experimental results show that the proposed method presents an efficient and effective solution for <m> numerical feature analysis </m> .
second:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental results show that the proposed method presents an efficient and effective solution for <m> numerical feature analysis </m> .
second:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental results show that the proposed method presents an efficient and effective solution for <m> numerical feature analysis </m> .
second:
While previous studies have considered categorical emotions , expressive speech during human interaction conveys subtle behaviors that are better characterized with <m> continuous descriptors </m> ( e.g. , attributes such as arousal , valence , dominance ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental results show that the proposed method presents an efficient and effective solution for <m> numerical feature analysis </m> .
second:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental results show that the proposed method presents an efficient and effective solution for <m> numerical feature analysis </m> .
second:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental results show that the proposed method presents an efficient and effective solution for <m> numerical feature analysis </m> .
second:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental results show that the proposed method presents an efficient and effective solution for <m> numerical feature analysis </m> .
second:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental results show that the proposed method presents an efficient and effective solution for <m> numerical feature analysis </m> .
second:
The main goal of this dissertation work is to provide syntactically and semantically related words based on <m> continuous feature vector representation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental results show that the proposed method presents an efficient and effective solution for <m> numerical feature analysis </m> .
second:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental results show that the proposed method presents an efficient and effective solution for <m> numerical feature analysis </m> .
second:
Here , the encoder maps an input sequence of symbol representations to a sequence of <m> continuous representations </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
Here , we present iFeature , a versatile Python‐based toolkit for generating various <m> numerical feature representation schemes </m> for both protein and peptide sequences .
second:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .


class: same cluster, base class: no relation, new model class: no relation
first:
Here , we present iFeature , a versatile Python‐based toolkit for generating various <m> numerical feature representation schemes </m> for both protein and peptide sequences .
second:
A text parser to match keywords against vocabularies and <m> numerical value descriptors </m> is introduced .


class: same cluster, base class: no relation, new model class: no relation
first:
Here , we present iFeature , a versatile Python‐based toolkit for generating various <m> numerical feature representation schemes </m> for both protein and peptide sequences .
second:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .


class: same cluster, base class: no relation, new model class: no relation
first:
Here , we present iFeature , a versatile Python‐based toolkit for generating various <m> numerical feature representation schemes </m> for both protein and peptide sequences .
second:
While previous studies have considered categorical emotions , expressive speech during human interaction conveys subtle behaviors that are better characterized with <m> continuous descriptors </m> ( e.g. , attributes such as arousal , valence , dominance ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Here , we present iFeature , a versatile Python‐based toolkit for generating various <m> numerical feature representation schemes </m> for both protein and peptide sequences .
second:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Here , we present iFeature , a versatile Python‐based toolkit for generating various <m> numerical feature representation schemes </m> for both protein and peptide sequences .
second:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Here , we present iFeature , a versatile Python‐based toolkit for generating various <m> numerical feature representation schemes </m> for both protein and peptide sequences .
second:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .


class: same cluster, base class: no relation, new model class: no relation
first:
Here , we present iFeature , a versatile Python‐based toolkit for generating various <m> numerical feature representation schemes </m> for both protein and peptide sequences .
second:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .


class: same cluster, base class: no relation, new model class: no relation
first:
Here , we present iFeature , a versatile Python‐based toolkit for generating various <m> numerical feature representation schemes </m> for both protein and peptide sequences .
second:
The main goal of this dissertation work is to provide syntactically and semantically related words based on <m> continuous feature vector representation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Here , we present iFeature , a versatile Python‐based toolkit for generating various <m> numerical feature representation schemes </m> for both protein and peptide sequences .
second:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: same cluster, base class: no relation, new model class: no relation
first:
Here , we present iFeature , a versatile Python‐based toolkit for generating various <m> numerical feature representation schemes </m> for both protein and peptide sequences .
second:
Here , the encoder maps an input sequence of symbol representations to a sequence of <m> continuous representations </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
Experimental comparisons also show the new method can significantly outperform standard <m> numerical feature-type methods </m> in terms of agreement with the clinical diagnosis gold standard .
second:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental comparisons also show the new method can significantly outperform standard <m> numerical feature-type methods </m> in terms of agreement with the clinical diagnosis gold standard .
second:
A text parser to match keywords against vocabularies and <m> numerical value descriptors </m> is introduced .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental comparisons also show the new method can significantly outperform standard <m> numerical feature-type methods </m> in terms of agreement with the clinical diagnosis gold standard .
second:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental comparisons also show the new method can significantly outperform standard <m> numerical feature-type methods </m> in terms of agreement with the clinical diagnosis gold standard .
second:
While previous studies have considered categorical emotions , expressive speech during human interaction conveys subtle behaviors that are better characterized with <m> continuous descriptors </m> ( e.g. , attributes such as arousal , valence , dominance ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental comparisons also show the new method can significantly outperform standard <m> numerical feature-type methods </m> in terms of agreement with the clinical diagnosis gold standard .
second:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental comparisons also show the new method can significantly outperform standard <m> numerical feature-type methods </m> in terms of agreement with the clinical diagnosis gold standard .
second:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental comparisons also show the new method can significantly outperform standard <m> numerical feature-type methods </m> in terms of agreement with the clinical diagnosis gold standard .
second:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental comparisons also show the new method can significantly outperform standard <m> numerical feature-type methods </m> in terms of agreement with the clinical diagnosis gold standard .
second:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental comparisons also show the new method can significantly outperform standard <m> numerical feature-type methods </m> in terms of agreement with the clinical diagnosis gold standard .
second:
The main goal of this dissertation work is to provide syntactically and semantically related words based on <m> continuous feature vector representation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental comparisons also show the new method can significantly outperform standard <m> numerical feature-type methods </m> in terms of agreement with the clinical diagnosis gold standard .
second:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: same cluster, base class: no relation, new model class: no relation
first:
Experimental comparisons also show the new method can significantly outperform standard <m> numerical feature-type methods </m> in terms of agreement with the clinical diagnosis gold standard .
second:
Here , the encoder maps an input sequence of symbol representations to a sequence of <m> continuous representations </m> .


class: same cluster, base class: no relation, new model class: second -> first
first:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .
second:
MPEG-7 visual descriptors are <m> numerical representations of features </m> - such as : texture , shape and color - extracted from an image .


class: same cluster, base class: no relation, new model class: no relation
first:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .
second:
A text parser to match keywords against vocabularies and <m> numerical value descriptors </m> is introduced .


class: same cluster, base class: no relation, new model class: second -> first
first:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .
second:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .


class: same cluster, base class: no relation, new model class: no relation
first:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .
second:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .


class: same cluster, base class: no relation, new model class: no relation
first:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .
second:
While previous studies have considered categorical emotions , expressive speech during human interaction conveys subtle behaviors that are better characterized with <m> continuous descriptors </m> ( e.g. , attributes such as arousal , valence , dominance ) .


class: same cluster, base class: no relation, new model class: no relation
first:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .
second:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .
second:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .
second:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .


class: same cluster, base class: no relation, new model class: no relation
first:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .
second:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .


class: same cluster, base class: no relation, new model class: no relation
first:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .
second:
The main goal of this dissertation work is to provide syntactically and semantically related words based on <m> continuous feature vector representation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .
second:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: same cluster, base class: no relation, new model class: no relation
first:
In CB3DR , shape descriptors are used to provide a <m> numerical representation of the salient features </m> of the data , while similarity functions capture the high level semantic concepts .
second:
Here , the encoder maps an input sequence of symbol representations to a sequence of <m> continuous representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
MPEG-7 visual descriptors are <m> numerical representations of features </m> - such as : texture , shape and color - extracted from an image .
second:
A text parser to match keywords against vocabularies and <m> numerical value descriptors </m> is introduced .


class: same cluster, base class: no relation, new model class: no relation
first:
MPEG-7 visual descriptors are <m> numerical representations of features </m> - such as : texture , shape and color - extracted from an image .
second:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .


class: same cluster, base class: no relation, new model class: no relation
first:
MPEG-7 visual descriptors are <m> numerical representations of features </m> - such as : texture , shape and color - extracted from an image .
second:
While previous studies have considered categorical emotions , expressive speech during human interaction conveys subtle behaviors that are better characterized with <m> continuous descriptors </m> ( e.g. , attributes such as arousal , valence , dominance ) .


class: same cluster, base class: no relation, new model class: no relation
first:
MPEG-7 visual descriptors are <m> numerical representations of features </m> - such as : texture , shape and color - extracted from an image .
second:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
MPEG-7 visual descriptors are <m> numerical representations of features </m> - such as : texture , shape and color - extracted from an image .
second:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
MPEG-7 visual descriptors are <m> numerical representations of features </m> - such as : texture , shape and color - extracted from an image .
second:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .


class: same cluster, base class: no relation, new model class: no relation
first:
MPEG-7 visual descriptors are <m> numerical representations of features </m> - such as : texture , shape and color - extracted from an image .
second:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .


class: same cluster, base class: no relation, new model class: no relation
first:
MPEG-7 visual descriptors are <m> numerical representations of features </m> - such as : texture , shape and color - extracted from an image .
second:
The main goal of this dissertation work is to provide syntactically and semantically related words based on <m> continuous feature vector representation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
MPEG-7 visual descriptors are <m> numerical representations of features </m> - such as : texture , shape and color - extracted from an image .
second:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: same cluster, base class: no relation, new model class: no relation
first:
MPEG-7 visual descriptors are <m> numerical representations of features </m> - such as : texture , shape and color - extracted from an image .
second:
Here , the encoder maps an input sequence of symbol representations to a sequence of <m> continuous representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
A text parser to match keywords against vocabularies and <m> numerical value descriptors </m> is introduced .
second:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .


class: same cluster, base class: no relation, new model class: no relation
first:
A text parser to match keywords against vocabularies and <m> numerical value descriptors </m> is introduced .
second:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .


class: same cluster, base class: second -> first, new model class: no relation
first:
A text parser to match keywords against vocabularies and <m> numerical value descriptors </m> is introduced .
second:
While previous studies have considered categorical emotions , expressive speech during human interaction conveys subtle behaviors that are better characterized with <m> continuous descriptors </m> ( e.g. , attributes such as arousal , valence , dominance ) .


class: same cluster, base class: no relation, new model class: no relation
first:
A text parser to match keywords against vocabularies and <m> numerical value descriptors </m> is introduced .
second:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
A text parser to match keywords against vocabularies and <m> numerical value descriptors </m> is introduced .
second:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
A text parser to match keywords against vocabularies and <m> numerical value descriptors </m> is introduced .
second:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .


class: same cluster, base class: no relation, new model class: no relation
first:
A text parser to match keywords against vocabularies and <m> numerical value descriptors </m> is introduced .
second:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .


class: same cluster, base class: no relation, new model class: no relation
first:
A text parser to match keywords against vocabularies and <m> numerical value descriptors </m> is introduced .
second:
The main goal of this dissertation work is to provide syntactically and semantically related words based on <m> continuous feature vector representation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
A text parser to match keywords against vocabularies and <m> numerical value descriptors </m> is introduced .
second:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: same cluster, base class: no relation, new model class: no relation
first:
A text parser to match keywords against vocabularies and <m> numerical value descriptors </m> is introduced .
second:
Here , the encoder maps an input sequence of symbol representations to a sequence of <m> continuous representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .
second:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .
second:
While previous studies have considered categorical emotions , expressive speech during human interaction conveys subtle behaviors that are better characterized with <m> continuous descriptors </m> ( e.g. , attributes such as arousal , valence , dominance ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .
second:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .
second:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .
second:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .
second:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .
second:
The main goal of this dissertation work is to provide syntactically and semantically related words based on <m> continuous feature vector representation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .
second:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , it is necessary to generate a <m> numerical representation </m> from the text so the network can be trained using this representation .
second:
Here , the encoder maps an input sequence of symbol representations to a sequence of <m> continuous representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .
second:
While previous studies have considered categorical emotions , expressive speech during human interaction conveys subtle behaviors that are better characterized with <m> continuous descriptors </m> ( e.g. , attributes such as arousal , valence , dominance ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we propose node2vec , an algorithmic framework for learning <m> continuous feature representations </m> for nodes in networks .
second:
Here , the encoder maps an input sequence of symbol representations to a sequence of <m> continuous representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
While previous studies have considered categorical emotions , expressive speech during human interaction conveys subtle behaviors that are better characterized with <m> continuous descriptors </m> ( e.g. , attributes such as arousal , valence , dominance ) .
second:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
While previous studies have considered categorical emotions , expressive speech during human interaction conveys subtle behaviors that are better characterized with <m> continuous descriptors </m> ( e.g. , attributes such as arousal , valence , dominance ) .
second:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
While previous studies have considered categorical emotions , expressive speech during human interaction conveys subtle behaviors that are better characterized with <m> continuous descriptors </m> ( e.g. , attributes such as arousal , valence , dominance ) .
second:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .


class: same cluster, base class: no relation, new model class: no relation
first:
While previous studies have considered categorical emotions , expressive speech during human interaction conveys subtle behaviors that are better characterized with <m> continuous descriptors </m> ( e.g. , attributes such as arousal , valence , dominance ) .
second:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .


class: same cluster, base class: no relation, new model class: no relation
first:
While previous studies have considered categorical emotions , expressive speech during human interaction conveys subtle behaviors that are better characterized with <m> continuous descriptors </m> ( e.g. , attributes such as arousal , valence , dominance ) .
second:
The main goal of this dissertation work is to provide syntactically and semantically related words based on <m> continuous feature vector representation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
While previous studies have considered categorical emotions , expressive speech during human interaction conveys subtle behaviors that are better characterized with <m> continuous descriptors </m> ( e.g. , attributes such as arousal , valence , dominance ) .
second:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .


class: same cluster, base class: no relation, new model class: no relation
first:
While previous studies have considered categorical emotions , expressive speech during human interaction conveys subtle behaviors that are better characterized with <m> continuous descriptors </m> ( e.g. , attributes such as arousal , valence , dominance ) .
second:
Here , the encoder maps an input sequence of symbol representations to a sequence of <m> continuous representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing ( <m> continuous feature ) models </m> .
second:
Here , the encoder maps an input sequence of symbol representations to a sequence of <m> continuous representations </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .
second:
The main goal of this dissertation work is to provide syntactically and semantically related words based on <m> continuous feature vector representation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
First , network search data , meteorological data , and other data are constructed into <m> continuous feature maps </m> .
second:
Here , the encoder maps an input sequence of symbol representations to a sequence of <m> continuous representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Abstract This paper proposes a unified approach to learning from constraints , which integrates the ability of classical machine learning techniques to learn from <m> continuous feature-based representations </m> with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning .
second:
Here , the encoder maps an input sequence of symbol representations to a sequence of <m> continuous representations </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .
second:
The main goal of this dissertation work is to provide syntactically and semantically related words based on <m> continuous feature vector representation </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
It is shown that this approach is well-suited to <m> continuous feature information analysis </m> and , furthermore , can be used to estimate redundancy in multiple features and information transmitted by combinations of features .
second:
Here , the encoder maps an input sequence of symbol representations to a sequence of <m> continuous representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
The main goal of this dissertation work is to provide syntactically and semantically related words based on <m> continuous feature vector representation </m> .
second:
Here , the encoder maps an input sequence of symbol representations to a sequence of <m> continuous representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Continuous feature representations </m> are the backbone of many deep learning algorithms , and it would be interesting to use node2vec representations as building blocks for end - to - end deep learning on graphs .
second:
Here , the encoder maps an input sequence of symbol representations to a sequence of <m> continuous representations </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
If we compare the widely used Conditional Random Fields ( CRF ) with newly proposed “ deep architecture ” sequence models ( Collobert et al. , 2011 ) , there are two things changing : from linear architecture to non-linear , and from <m> discrete feature representation </m> to distributional .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: same cluster, base class: no relation, new model class: no relation
first:
First , feature selection strategy is explicitly introduced , including <m> discrete feature method </m> and continuous feature method .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: same cluster, base class: no relation, new model class: no relation
first:
With these distortion models and the <m> discrete representation of the features </m> , we design a cost function whose minimization yields the parameters of the distortion model .
second:
While such methods offer a number of appealing guarantees , such as near - Bayesian exploration in polynomial time , they require a concise , often <m> discrete representation </m> of the agent ’s state - action space to measure state visitation frequencies .


class: same cluster, base class: no relation, new model class: first -> second
first:
For this reason , companies have limited options to analyse manufacturing data , despite the capability of advanced <m> machine learning techniques </m> in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: same cluster, base class: no relation, new model class: first -> second
first:
The approach is based on the <m> machine learning paradigm </m> called LUPI - “ Learning Using Privileged Information ” .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: same cluster, base class: no relation, new model class: first -> second
first:
A novel method employing <m> machine-based learning </m> to identify messages related to other messages is described and evaluated .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: same cluster, base class: first -> second, new model class: first -> second
first:
The first was the collection of considerable bridge design data and set up data sets , and the second was data reduction , including raw data processing and debugging or developing a reasonable <m> machine learning algorithm model </m> .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: same cluster, base class: no relation, new model class: first -> second
first:
Eventually , eight papers were accepted for the special issue , spanning a variety of topics in terms <m> ofmachine learning </m> and its utilization for analyzingmotion includingbiological trajectorymatching .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: same cluster, base class: no relation, new model class: first -> second
first:
Using <m> Machine learning in </m> , a strategy for information investigation that iteratively gain from information and enables PCs to discover shrouded bits of knowledge without being any express customized .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: same cluster, base class: no relation, new model class: first -> second
first:
The proposed model exploits 1 ) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2 ) gives <m> machine learning based learning models </m> and their results during learning model phase to support the decision making accurately .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: same cluster, base class: no relation, new model class: first -> second
first:
Citizens ' sketches are synthesized , enhanced to be more vivid through <m> machine machine learning algorithms </m> , and projected on a screen , forming a participatory artwork .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: same cluster, base class: no relation, new model class: first -> second
first:
The time is ripe to apply more powerful and flexible <m> machine learning methods </m> to these problems , assuming we can find models with suitable inductive biases .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: same cluster, base class: no relation, new model class: first -> second
first:
In the <m> machine learning community </m> , the research of novel and powerful RNN models is a very active research topic .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: same cluster, base class: no relation, new model class: first -> second
first:
This is close to what is known in the <m> machine learning literature </m> as “ learning from logged bandit feedback ” , with the distinction that we do not have access to the model generating the action .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: same cluster, base class: no relation, new model class: first -> second
first:
The term is the classic machine learning generalization - error , and in turn can be upper bounded using the empirical error and model complexity terms , applying standard <m> machine learning theory </m> . subsection : Problem setup
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: same cluster, base class: no relation, new model class: first -> second
first:
From a <m> machine learning perspective </m> , text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: same cluster, base class: no relation, new model class: first -> second
first:
While the field of <m> machine learning field </m> developed many powerful algorithms for predictive modeling , most algorithms were designed for classification tasks .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: same cluster, base class: no relation, new model class: first -> second
first:
Following , we have phrased this as a <m> machine learning problem </m> where we will learn to map from a set of input graphs , representing the state of memory , to a logical description of the data structures that have been instantiated .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: same cluster, base class: no relation, new model class: first -> second
first:
Learning top - down generative models that can explain complex data distribution is a long - standing problem in <m> machine learning research </m> .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: same cluster, base class: no relation, new model class: first -> second
first:
These reasons motivate our exploration of a <m> machine learning solution </m> that exploits a flexible , high capacity DNN while being extremely efficient .
second:
To this end , this work proposes a proactive method based on CEP and machine learning ( ML ) where historical data is exploited using <m> ML part </m> and combined with real-time flow of CEP to provide the basis for predictive event processing .


class: same cluster, base class: no relation, new model class: no relation
first:
The presented approach is expected to yield high echo suppression and fast convergence for the <m> filter weight adaptation </m> by means of multirate parallel processing of the filter bank .
second:
the <m> iterative match ide al filter bank </m> has simple structurk , convenent application and ideal frequency splitting .


class: same cluster, base class: no relation, new model class: no relation
first:
It first depicts the performance of the proposed models on different length of sentences , and then conducts a sensitivity analysis of <m> 2D filter </m> and max pooling size .
second:
Such kernels are built with a parameterized “ neural response ” function , which consists in computing the maximal response of a <m> base kernel </m> over a local neighborhood .


class: same cluster, base class: no relation, new model class: no relation
first:
It first depicts the performance of the proposed models on different length of sentences , and then conducts a sensitivity analysis of <m> 2D filter </m> and max pooling size .
second:
a <m> kernel size </m> c × f 1 ×


class: same cluster, base class: no relation, new model class: no relation
first:
It first depicts the performance of the proposed models on different length of sentences , and then conducts a sensitivity analysis of <m> 2D filter </m> and max pooling size .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: same cluster, base class: no relation, new model class: no relation
first:
It first depicts the performance of the proposed models on different length of sentences , and then conducts a sensitivity analysis of <m> 2D filter </m> and max pooling size .
second:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .


class: same cluster, base class: no relation, new model class: no relation
first:
It first depicts the performance of the proposed models on different length of sentences , and then conducts a sensitivity analysis of <m> 2D filter </m> and max pooling size .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: same cluster, base class: no relation, new model class: no relation
first:
Before the DU - Net , a <m> Conv ( ) filter </m> with stride 2 and a max pooling would produce 128 features with resolution 64 64 .
second:
Moreover , we give some model analysis on the <m> filter size configuration </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
Before the DU - Net , a <m> Conv ( ) filter </m> with stride 2 and a max pooling would produce 128 features with resolution 64 64 .
second:
This was achieved by iterating over the differences using a <m> maximum filter </m> with decreasing filter size until 20 frames have been found .


class: same cluster, base class: no relation, new model class: no relation
first:
Before the DU - Net , a <m> Conv ( ) filter </m> with stride 2 and a max pooling would produce 128 features with resolution 64 64 .
second:
. , ŷ ll , n ) is a diagonal matrix of spectral multipliers representing a <m> learnable filter </m> in the spectral domain , and ξ is a nonlinearity ( e.g. ReLU ) applied on the vertex - wise function values .


class: same cluster, base class: no relation, new model class: no relation
first:
Moreover , we give some model analysis on the <m> filter size configuration </m> .
second:
This was achieved by iterating over the differences using a <m> maximum filter </m> with decreasing filter size until 20 frames have been found .


class: same cluster, base class: no relation, new model class: no relation
first:
Moreover , we give some model analysis on the <m> filter size configuration </m> .
second:
. , ŷ ll , n ) is a diagonal matrix of spectral multipliers representing a <m> learnable filter </m> in the spectral domain , and ξ is a nonlinearity ( e.g. ReLU ) applied on the vertex - wise function values .


class: same cluster, base class: no relation, new model class: first -> second
first:
This was achieved by iterating over the differences using a <m> maximum filter </m> with decreasing filter size until 20 frames have been found .
second:
. , ŷ ll , n ) is a diagonal matrix of spectral multipliers representing a <m> learnable filter </m> in the spectral domain , and ξ is a nonlinearity ( e.g. ReLU ) applied on the vertex - wise function values .


class: same cluster, base class: no relation, new model class: no relation
first:
Iris location is a <m> kernel procession </m> in an iris recognition system .
second:
The second one is sampled from and serves as input to the generator along with a latent vector to generate an image , and it can be sampled using different approaches : Kernel density estimation : also known as Parzen window estimation , it consists in randomly sampling from a <m> kernel </m> ( e.g. Gaussian kernel with a cross - validated ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Iris location is a <m> kernel procession </m> in an iris recognition system .
second:
The structured random transformations developed in the <m> kernel literature </m> perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance .


class: same cluster, base class: no relation, new model class: no relation
first:
Iris location is a <m> kernel procession </m> in an iris recognition system .
second:
taubtabib2015template suggest a <m> kernel - based approach </m> to implicitly consider all possible feature combinations over sets of core - features .


class: same cluster, base class: no relation, new model class: no relation
first:
Iris location is a <m> kernel procession </m> in an iris recognition system .
second:
Furthermore , to deal with the non - linearity of the person ’s appearance , a <m> kernel version </m> can be developed easily to further boost the matching performance within the null space .


class: same cluster, base class: no relation, new model class: no relation
first:
Such kernels are built with a parameterized “ neural response ” function , which consists in computing the maximal response of a <m> base kernel </m> over a local neighborhood .
second:
a <m> kernel size </m> c × f 1 ×


class: same cluster, base class: no relation, new model class: second -> first
first:
Such kernels are built with a parameterized “ neural response ” function , which consists in computing the maximal response of a <m> base kernel </m> over a local neighborhood .
second:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .


class: same cluster, base class: second -> first, new model class: no relation
first:
Such kernels are built with a parameterized “ neural response ” function , which consists in computing the maximal response of a <m> base kernel </m> over a local neighborhood .
second:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .


class: same cluster, base class: second -> first, new model class: no relation
first:
Such kernels are built with a parameterized “ neural response ” function , which consists in computing the maximal response of a <m> base kernel </m> over a local neighborhood .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: same cluster, base class: no relation, new model class: no relation
first:
a <m> kernel size </m> c × f 1 ×
second:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .


class: same cluster, base class: no relation, new model class: no relation
first:
a <m> kernel size </m> c × f 1 ×
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: same cluster, base class: no relation, new model class: no relation
first:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .
second:
Experimental results are reported for a suite of multimedia image processing programs including <m> convolutions with kernels </m> , MPEG-2 decoding , and edge chain coding .


class: same cluster, base class: no relation, new model class: no relation
first:
Where the attention mechanism is a <m> kernel on </m> , then ( [ reference ] ) is akin to a kernel density estimator .
second:
However , fully connected layers can also be viewed as <m> convolutions with kernels </m> that cover their entire input regions .


class: same cluster, base class: no relation, new model class: no relation
first:
This requires that the traditional question of <m> receptive-field analysis </m> be turned on its head : instead of asking ‘ Here is a cell ; where are the points that it sees ? ’ , the question becomes , ‘
second:
Using this starting point , we present a continuous analog of the four stage edge detection breakdown by Bezdek et al. , and arrive at a framework for casting many kinds of image processing algorithms in a biologically plausible manner : as feed forward , <m> receptive field based algorithms </m> using known operations .


class: same cluster, base class: no relation, new model class: no relation
first:
This requires that the traditional question of <m> receptive-field analysis </m> be turned on its head : instead of asking ‘ Here is a cell ; where are the points that it sees ? ’ , the question becomes , ‘
second:
The <m> receptive field concept </m> is described , along with how this concept has been applied to retinal ganglion cells .


class: same cluster, base class: no relation, new model class: no relation
first:
This requires that the traditional question of <m> receptive-field analysis </m> be turned on its head : instead of asking ‘ Here is a cell ; where are the points that it sees ? ’ , the question becomes , ‘
second:
Here we use natural speech stimuli and advanced <m> receptive field characterization methods </m> to show that spectrotemporal features within speech are well organized along the posterior-to-anterior axis of the human STG .


class: same cluster, base class: no relation, new model class: no relation
first:
Using this starting point , we present a continuous analog of the four stage edge detection breakdown by Bezdek et al. , and arrive at a framework for casting many kinds of image processing algorithms in a biologically plausible manner : as feed forward , <m> receptive field based algorithms </m> using known operations .
second:
The <m> receptive field concept </m> is described , along with how this concept has been applied to retinal ganglion cells .


class: same cluster, base class: no relation, new model class: no relation
first:
Using this starting point , we present a continuous analog of the four stage edge detection breakdown by Bezdek et al. , and arrive at a framework for casting many kinds of image processing algorithms in a biologically plausible manner : as feed forward , <m> receptive field based algorithms </m> using known operations .
second:
Here we use natural speech stimuli and advanced <m> receptive field characterization methods </m> to show that spectrotemporal features within speech are well organized along the posterior-to-anterior axis of the human STG .


class: same cluster, base class: no relation, new model class: no relation
first:
The <m> receptive field concept </m> is described , along with how this concept has been applied to retinal ganglion cells .
second:
Here we use natural speech stimuli and advanced <m> receptive field characterization methods </m> to show that spectrotemporal features within speech are well organized along the posterior-to-anterior axis of the human STG .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Moreover , since it does not need <m> word segmentation </m> , it can be applied to Eastern languages where they do not put clear spacing between words .
second:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .


class: same cluster, base class: second -> first, new model class: second -> first
first:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .
second:
Therefore , in this paper , we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit <m> word segmentation </m> .


class: same cluster, base class: second -> first, new model class: second -> first
first:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .
second:
This <m> word segmentation procedure </m> can be as simple as tokenization followed by some punctuation normalization , but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance .


class: same cluster, base class: second -> first, new model class: second -> first
first:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .
second:
paragraph : Why not Word - Level Translation ? The most pressing issue with word - level processing is that we do not have a perfect <m> word segmentation algorithm </m> for any one language .


class: same cluster, base class: second -> first, new model class: second -> first
first:
The news public opinion monitoring system comprises a news information gathering module , a news data pre-processing module , a news public opinion analyzing module and a news public opinion result display module ; the news data pre-processing module comprises a primary filtering sub-module , a text extracting sub-module , a <m> word segmentation sub-module </m> , a feature phrase filtering sub-module , a text emotion tendency analyzing sub-module , a picture analyzing sub-module and a public opinion popularity acquiring sub-module .
second:
We discuss the suitability of different <m> word segmentation techniques </m> , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back - off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU , respectively .


class: same cluster, base class: no relation, new model class: no relation
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .


class: same cluster, base class: no relation, new model class: no relation
first:
Our systems are based on an attentional encoder - decoder , using <m> BPE subword segmentation </m> for open - vocabulary translation with a fixed vocabulary .
second:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .


class: same cluster, base class: no relation, new model class: no relation
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .


class: same cluster, base class: no relation, new model class: no relation
first:
We propose a novel neural machine translation technique using word-embedding along with <m> Byte-Pair-Encoding ( BPE </m> ) to develop an efficient translation system that overcomes the OOV ( Out Of Vocabulary ) problem for languages which do not have much translations available online .
second:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .
second:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .
second:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .
second:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .
second:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .
second:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .
second:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: same cluster, base class: no relation, new model class: no relation
first:
Furthermore , we also utilize the existing common semantic patterns and <m> BPE patterns </m> to construct a new Rich-BPE-PCFGs password generator .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: same cluster, base class: no relation, new model class: no relation
first:
• We adapt byte pair encoding ( <m> BPE </m> ) [ reference ] , a compression algorithm , to the task of word segmentation .
second:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .


class: same cluster, base class: no relation, new model class: no relation
first:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .
second:
Table 1 shows that our fully convolutional sequence to sequence model ( ConvS2S ) outperforms the WMT'16 winning entry for English - Romanian by 1.9 BLEU with a <m> BPE encoding </m> and by 1.3 BLEU with a word factored vocabulary .


class: same cluster, base class: no relation, new model class: no relation
first:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .
second:
We thus follow the approach described in , first mapping the Russian text into Latin characters via ISO - 9 transliteration , then learning the <m> BPE operations </m> on the concatenation of the English and latinized Russian training data , then mapping the BPE operations back into Cyrillic alphabet .


class: same cluster, base class: no relation, new model class: no relation
first:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .
second:
Grammar based compression , where one replaces a long string by a small contextfree grammar that generates the string , is a simple and powerful paradigm that captures many of the popular compression schemes , including the Lempel-Ziv family , Run-Length Encoding , <m> Byte-Pair Encoding </m> , Sequitur and Re-Pair .


class: same cluster, base class: no relation, new model class: no relation
first:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .
second:
The dominant approach to subword segmentation is <m> Byte Pair Encoding ( BPE ) </m> , which keeps the most frequent words intact while splitting the rare ones into multiple tokens .


class: same cluster, base class: no relation, new model class: no relation
first:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .
second:
Different from the methods which generate regular expressions from character level , we first utilize <m> byte pair encoder ( BPE ) </m> to extract some frequent items , which are then used to construct regular expressions .


class: same cluster, base class: no relation, new model class: no relation
first:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .
second:
The submitted system is based on Neural Machine Translation using <m> byte-pair encoding segmentation </m> on both source and target languages for open-vocabulary translations .


class: same cluster, base class: no relation, new model class: no relation
first:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .
second:
Using a <m> byte - pair - encoding </m> ( BPE ) of various sized we notice that a 32 , 000 word - piece vocabulary achieves a better bits per character ( BPC ) loss over one epoch of the Amazon Reviews dataset than a small vocabulary .


class: same cluster, base class: no relation, new model class: no relation
first:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .
second:
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece wu2016google and <m> byte - pair </m> sennrich2015neural representations .


class: same cluster, base class: no relation, new model class: no relation
first:
We show performance gains over the baseline with both <m> BPE segmentation </m> , and a simple character bigram segmentation .
second:
We also consider a joint <m> source and target byte - pair encoding </m> ( BPE ) with 40 K types [ reference ][ reference ] .


class: same cluster, base class: first -> second, new model class: first -> second
first:
The results show that combining unigrams and PoS-filtered skipgrams leads to a significant improvement in classification scores over the <m> unigram baseline </m> .
second:
One may also use a <m> unigram distribution </m> which has been shown to work better on some tasks pereyra2017regularize .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Traditional approaches used <m> unigram based models </m> for text classification .
second:
One may also use a <m> unigram distribution </m> which has been shown to work better on some tasks pereyra2017regularize .


class: same cluster, base class: first -> second, new model class: first -> second
first:
A <m> unigram analysis </m> of our annotated data shows that student learning correlates both with the tutor 's dialogue acts and with the student 's dialogue acts .
second:
One may also use a <m> unigram distribution </m> which has been shown to work better on some tasks pereyra2017regularize .


class: same cluster, base class: first -> second, new model class: first -> second
first:
By augmenting conventional techniques of topic modeling with <m> unigram analysis </m> and community detection , we establish an automated method that generates a comprehensive and meaningful summary of forum conversations over time that also sheds light on patterns of user behavior .
second:
One may also use a <m> unigram distribution </m> which has been shown to work better on some tasks pereyra2017regularize .


class: same cluster, base class: first -> second, new model class: first -> second
first:
Unbiased tweets were extracted from Twitter related to this specific campaign , and on comparing with manual tagging we were able to achieve 84.47 % accuracy using <m> unigram machine learning approach </m> .
second:
One may also use a <m> unigram distribution </m> which has been shown to work better on some tasks pereyra2017regularize .


class: same cluster, base class: no relation, new model class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
In the first part , the scope and features of <m> digital advertising </m> as well as the tools deployed are presented and defined .


class: same cluster, base class: no relation, new model class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
While predictive modeling has been thoroughly researched in recent years in the <m> digital advertising domain </m> , the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification .


class: same cluster, base class: no relation, new model class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
This paper reports a multi-phase study to address the following questions in the <m> digital advertising context </m> : ( 1 ) to what extent did consumers use ads in the past ? ( 2 ) What are consumers ’ perceived values toward ads ?


class: same cluster, base class: no relation, new model class: no relation
first:
For supporting sales for a company , they can use a technology for <m> online marketing </m> using e-commerce .
second:
What is the simplicity and complexity of the digital advertising design structure?The purpose of the research is to focus on the discovery of the rug and the complexity of the structure of the design of digital advertising , while the importance of research by contributing to enrich the knowledge and application of the specialists and specialists to study the simplicity and complexity in the structure of <m> digital advertising design </m> and the introduction of specialized designers working on the subject of simplicity and complexity in the design of digital advertising And the terminology in it , and the researcher has developed spatial boundaries represented by a number of Iraqi digital ad designs , and time limits ( limited in the period of 2017 ) As for the substantive boundaries represented in ( study of simplicity and complexity in the structure of design Digital advertising.while the second section chapter which consist the theoretical frame which interest in three researcher , the first research turning to ( Simplicity in the digital advertising design)while the the second research interest in ( complexity in the digital advertising design)and the third research turning into about the(structure digital advertising design ) .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
In the first part , the scope and features of <m> digital advertising </m> as well as the tools deployed are presented and defined .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
While predictive modeling has been thoroughly researched in recent years in the <m> digital advertising domain </m> , the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification .


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
This paper reports a multi-phase study to address the following questions in the <m> digital advertising context </m> : ( 1 ) to what extent did consumers use ads in the past ? ( 2 ) What are consumers ’ perceived values toward ads ?


class: same cluster, base class: no relation, new model class: no relation
first:
<m> Online-based marketing technology </m> is a solution in terms of promotion and marketing .
second:
What is the simplicity and complexity of the digital advertising design structure?The purpose of the research is to focus on the discovery of the rug and the complexity of the structure of the design of digital advertising , while the importance of research by contributing to enrich the knowledge and application of the specialists and specialists to study the simplicity and complexity in the structure of <m> digital advertising design </m> and the introduction of specialized designers working on the subject of simplicity and complexity in the design of digital advertising And the terminology in it , and the researcher has developed spatial boundaries represented by a number of Iraqi digital ad designs , and time limits ( limited in the period of 2017 ) As for the substantive boundaries represented in ( study of simplicity and complexity in the structure of design Digital advertising.while the second section chapter which consist the theoretical frame which interest in three researcher , the first research turning to ( Simplicity in the digital advertising design)while the the second research interest in ( complexity in the digital advertising design)and the third research turning into about the(structure digital advertising design ) .


class: same cluster, base class: no relation, new model class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
In the first part , the scope and features of <m> digital advertising </m> as well as the tools deployed are presented and defined .


class: same cluster, base class: no relation, new model class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
While predictive modeling has been thoroughly researched in recent years in the <m> digital advertising domain </m> , the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification .


class: same cluster, base class: no relation, new model class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
This paper reports a multi-phase study to address the following questions in the <m> digital advertising context </m> : ( 1 ) to what extent did consumers use ads in the past ? ( 2 ) What are consumers ’ perceived values toward ads ?


class: same cluster, base class: no relation, new model class: no relation
first:
E-commerce companies are now reducing expenses and investment in <m> online marketing and technology </m> [ Aldridge et al. 1997:162 , Bonanos 2001 , Mollison 2002 , Joseph & Poon 2001 ] .
second:
What is the simplicity and complexity of the digital advertising design structure?The purpose of the research is to focus on the discovery of the rug and the complexity of the structure of the design of digital advertising , while the importance of research by contributing to enrich the knowledge and application of the specialists and specialists to study the simplicity and complexity in the structure of <m> digital advertising design </m> and the introduction of specialized designers working on the subject of simplicity and complexity in the design of digital advertising And the terminology in it , and the researcher has developed spatial boundaries represented by a number of Iraqi digital ad designs , and time limits ( limited in the period of 2017 ) As for the substantive boundaries represented in ( study of simplicity and complexity in the structure of design Digital advertising.while the second section chapter which consist the theoretical frame which interest in three researcher , the first research turning to ( Simplicity in the digital advertising design)while the the second research interest in ( complexity in the digital advertising design)and the third research turning into about the(structure digital advertising design ) .


class: same cluster, base class: no relation, new model class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
In the first part , the scope and features of <m> digital advertising </m> as well as the tools deployed are presented and defined .


class: same cluster, base class: no relation, new model class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
While predictive modeling has been thoroughly researched in recent years in the <m> digital advertising domain </m> , the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification .


class: same cluster, base class: no relation, new model class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
This paper reports a multi-phase study to address the following questions in the <m> digital advertising context </m> : ( 1 ) to what extent did consumers use ads in the past ? ( 2 ) What are consumers ’ perceived values toward ads ?


class: same cluster, base class: no relation, new model class: no relation
first:
The findings suggest that the traditional advertising hierarchy of effects model is relevant in the <m> online marketing environment </m> , and that investment in online marketing communication can be evaluated using this stable and reliable method .
second:
What is the simplicity and complexity of the digital advertising design structure?The purpose of the research is to focus on the discovery of the rug and the complexity of the structure of the design of digital advertising , while the importance of research by contributing to enrich the knowledge and application of the specialists and specialists to study the simplicity and complexity in the structure of <m> digital advertising design </m> and the introduction of specialized designers working on the subject of simplicity and complexity in the design of digital advertising And the terminology in it , and the researcher has developed spatial boundaries represented by a number of Iraqi digital ad designs , and time limits ( limited in the period of 2017 ) As for the substantive boundaries represented in ( study of simplicity and complexity in the structure of design Digital advertising.while the second section chapter which consist the theoretical frame which interest in three researcher , the first research turning to ( Simplicity in the digital advertising design)while the the second research interest in ( complexity in the digital advertising design)and the third research turning into about the(structure digital advertising design ) .


class: same cluster, base class: no relation, new model class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
In the first part , the scope and features of <m> digital advertising </m> as well as the tools deployed are presented and defined .


class: same cluster, base class: no relation, new model class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
While predictive modeling has been thoroughly researched in recent years in the <m> digital advertising domain </m> , the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification .


class: same cluster, base class: no relation, new model class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
This paper reports a multi-phase study to address the following questions in the <m> digital advertising context </m> : ( 1 ) to what extent did consumers use ads in the past ? ( 2 ) What are consumers ’ perceived values toward ads ?


class: same cluster, base class: no relation, new model class: no relation
first:
The conclusions of this study would provide helpful suggestions on the <m> online marketing operation </m> .
second:
What is the simplicity and complexity of the digital advertising design structure?The purpose of the research is to focus on the discovery of the rug and the complexity of the structure of the design of digital advertising , while the importance of research by contributing to enrich the knowledge and application of the specialists and specialists to study the simplicity and complexity in the structure of <m> digital advertising design </m> and the introduction of specialized designers working on the subject of simplicity and complexity in the design of digital advertising And the terminology in it , and the researcher has developed spatial boundaries represented by a number of Iraqi digital ad designs , and time limits ( limited in the period of 2017 ) As for the substantive boundaries represented in ( study of simplicity and complexity in the structure of design Digital advertising.while the second section chapter which consist the theoretical frame which interest in three researcher , the first research turning to ( Simplicity in the digital advertising design)while the the second research interest in ( complexity in the digital advertising design)and the third research turning into about the(structure digital advertising design ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
In the first part , the scope and features of <m> digital advertising </m> as well as the tools deployed are presented and defined .


class: same cluster, base class: no relation, new model class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
While predictive modeling has been thoroughly researched in recent years in the <m> digital advertising domain </m> , the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification .


class: same cluster, base class: no relation, new model class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
This paper reports a multi-phase study to address the following questions in the <m> digital advertising context </m> : ( 1 ) to what extent did consumers use ads in the past ? ( 2 ) What are consumers ’ perceived values toward ads ?


class: same cluster, base class: no relation, new model class: no relation
first:
Cloud computing as an exciting development in a educational Institute and <m> online marketing perspective </m> .
second:
What is the simplicity and complexity of the digital advertising design structure?The purpose of the research is to focus on the discovery of the rug and the complexity of the structure of the design of digital advertising , while the importance of research by contributing to enrich the knowledge and application of the specialists and specialists to study the simplicity and complexity in the structure of <m> digital advertising design </m> and the introduction of specialized designers working on the subject of simplicity and complexity in the design of digital advertising And the terminology in it , and the researcher has developed spatial boundaries represented by a number of Iraqi digital ad designs , and time limits ( limited in the period of 2017 ) As for the substantive boundaries represented in ( study of simplicity and complexity in the structure of design Digital advertising.while the second section chapter which consist the theoretical frame which interest in three researcher , the first research turning to ( Simplicity in the digital advertising design)while the the second research interest in ( complexity in the digital advertising design)and the third research turning into about the(structure digital advertising design ) .


class: same cluster, base class: no relation, new model class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
In the first part , the scope and features of <m> digital advertising </m> as well as the tools deployed are presented and defined .


class: same cluster, base class: no relation, new model class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
While predictive modeling has been thoroughly researched in recent years in the <m> digital advertising domain </m> , the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification .


class: same cluster, base class: no relation, new model class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
This paper reports a multi-phase study to address the following questions in the <m> digital advertising context </m> : ( 1 ) to what extent did consumers use ads in the past ? ( 2 ) What are consumers ’ perceived values toward ads ?


class: same cluster, base class: no relation, new model class: no relation
first:
We believe that our approach could be used in industry for identifying high-value users for <m> online marketing purpose </m> .
second:
What is the simplicity and complexity of the digital advertising design structure?The purpose of the research is to focus on the discovery of the rug and the complexity of the structure of the design of digital advertising , while the importance of research by contributing to enrich the knowledge and application of the specialists and specialists to study the simplicity and complexity in the structure of <m> digital advertising design </m> and the introduction of specialized designers working on the subject of simplicity and complexity in the design of digital advertising And the terminology in it , and the researcher has developed spatial boundaries represented by a number of Iraqi digital ad designs , and time limits ( limited in the period of 2017 ) As for the substantive boundaries represented in ( study of simplicity and complexity in the structure of design Digital advertising.while the second section chapter which consist the theoretical frame which interest in three researcher , the first research turning to ( Simplicity in the digital advertising design)while the the second research interest in ( complexity in the digital advertising design)and the third research turning into about the(structure digital advertising design ) .


class: same cluster, base class: no relation, new model class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
In the first part , the scope and features of <m> digital advertising </m> as well as the tools deployed are presented and defined .


class: same cluster, base class: no relation, new model class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
While predictive modeling has been thoroughly researched in recent years in the <m> digital advertising domain </m> , the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification .


class: same cluster, base class: no relation, new model class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
This paper reports a multi-phase study to address the following questions in the <m> digital advertising context </m> : ( 1 ) to what extent did consumers use ads in the past ? ( 2 ) What are consumers ’ perceived values toward ads ?


class: same cluster, base class: no relation, new model class: no relation
first:
system for <m> online marketing site </m> plays a key role for the e-marketing or purchase made online by consumers .
second:
What is the simplicity and complexity of the digital advertising design structure?The purpose of the research is to focus on the discovery of the rug and the complexity of the structure of the design of digital advertising , while the importance of research by contributing to enrich the knowledge and application of the specialists and specialists to study the simplicity and complexity in the structure of <m> digital advertising design </m> and the introduction of specialized designers working on the subject of simplicity and complexity in the design of digital advertising And the terminology in it , and the researcher has developed spatial boundaries represented by a number of Iraqi digital ad designs , and time limits ( limited in the period of 2017 ) As for the substantive boundaries represented in ( study of simplicity and complexity in the structure of design Digital advertising.while the second section chapter which consist the theoretical frame which interest in three researcher , the first research turning to ( Simplicity in the digital advertising design)while the the second research interest in ( complexity in the digital advertising design)and the third research turning into about the(structure digital advertising design ) .


class: same cluster, base class: no relation, new model class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
In the first part , the scope and features of <m> digital advertising </m> as well as the tools deployed are presented and defined .


class: same cluster, base class: no relation, new model class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
While predictive modeling has been thoroughly researched in recent years in the <m> digital advertising domain </m> , the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification .


class: same cluster, base class: no relation, new model class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
This paper reports a multi-phase study to address the following questions in the <m> digital advertising context </m> : ( 1 ) to what extent did consumers use ads in the past ? ( 2 ) What are consumers ’ perceived values toward ads ?


class: same cluster, base class: no relation, new model class: no relation
first:
Real-time response and interactive narrative provide a game-like experience in two systems : Boom Chameleon for evaluating virtual models and StyleCam for developing <m> online product marketing </m> and advertising .
second:
What is the simplicity and complexity of the digital advertising design structure?The purpose of the research is to focus on the discovery of the rug and the complexity of the structure of the design of digital advertising , while the importance of research by contributing to enrich the knowledge and application of the specialists and specialists to study the simplicity and complexity in the structure of <m> digital advertising design </m> and the introduction of specialized designers working on the subject of simplicity and complexity in the design of digital advertising And the terminology in it , and the researcher has developed spatial boundaries represented by a number of Iraqi digital ad designs , and time limits ( limited in the period of 2017 ) As for the substantive boundaries represented in ( study of simplicity and complexity in the structure of design Digital advertising.while the second section chapter which consist the theoretical frame which interest in three researcher , the first research turning to ( Simplicity in the digital advertising design)while the the second research interest in ( complexity in the digital advertising design)and the third research turning into about the(structure digital advertising design ) .


class: same cluster, base class: no relation, new model class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
In the first part , the scope and features of <m> digital advertising </m> as well as the tools deployed are presented and defined .


class: same cluster, base class: no relation, new model class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
While predictive modeling has been thoroughly researched in recent years in the <m> digital advertising domain </m> , the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification .


class: same cluster, base class: no relation, new model class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
This paper reports a multi-phase study to address the following questions in the <m> digital advertising context </m> : ( 1 ) to what extent did consumers use ads in the past ? ( 2 ) What are consumers ’ perceived values toward ads ?


class: same cluster, base class: no relation, new model class: no relation
first:
To address this issue , we develop the Visual Perception Model ( VP ) for <m> Online Target marketing </m> and then conducted an experiment by manipulating the independent variables ( i.e. perceived security of online banner ads and match between web user ’s need and banner ad contents ) .
second:
What is the simplicity and complexity of the digital advertising design structure?The purpose of the research is to focus on the discovery of the rug and the complexity of the structure of the design of digital advertising , while the importance of research by contributing to enrich the knowledge and application of the specialists and specialists to study the simplicity and complexity in the structure of <m> digital advertising design </m> and the introduction of specialized designers working on the subject of simplicity and complexity in the design of digital advertising And the terminology in it , and the researcher has developed spatial boundaries represented by a number of Iraqi digital ad designs , and time limits ( limited in the period of 2017 ) As for the substantive boundaries represented in ( study of simplicity and complexity in the structure of design Digital advertising.while the second section chapter which consist the theoretical frame which interest in three researcher , the first research turning to ( Simplicity in the digital advertising design)while the the second research interest in ( complexity in the digital advertising design)and the third research turning into about the(structure digital advertising design ) .


class: same cluster, base class: no relation, new model class: no relation
first:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .
second:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .
second:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .
second:
By analyzing the unstructured characteristics of online ads and search engine user behavior data , proposed a kind of <m> personalized ads recommendation method </m> based on User-Interest-Behavior model , which can extract the user ’s interest preferences by the topic model and generate the recommended list of ads based on the nearest neighbor and user behavior .


class: same cluster, base class: no relation, new model class: first -> second
first:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .
second:
With the popularity of social networks such as Facebook and twitter , social recommendations have become possible , which rely on individual 's social connections in order to make <m> personalized recommendations of ads </m> , content , products , and people .


class: same cluster, base class: no relation, new model class: first -> second
first:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .
second:
In the first paper , Adany , Kraus and Ordonez investigate the area of <m> personalized TV ads </m> to improve the effectiveness of ads distribution to targeted viewers .


class: same cluster, base class: no relation, new model class: first -> second
first:
In this age of massive data gathering for purposes of personalization , <m> targeted ads </m> , etc .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: same cluster, base class: no relation, new model class: no relation
first:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .
second:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .
second:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .
second:
By analyzing the unstructured characteristics of online ads and search engine user behavior data , proposed a kind of <m> personalized ads recommendation method </m> based on User-Interest-Behavior model , which can extract the user ’s interest preferences by the topic model and generate the recommended list of ads based on the nearest neighbor and user behavior .


class: same cluster, base class: no relation, new model class: first -> second
first:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .
second:
With the popularity of social networks such as Facebook and twitter , social recommendations have become possible , which rely on individual 's social connections in order to make <m> personalized recommendations of ads </m> , content , products , and people .


class: same cluster, base class: no relation, new model class: first -> second
first:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .
second:
In the first paper , Adany , Kraus and Ordonez investigate the area of <m> personalized TV ads </m> to improve the effectiveness of ads distribution to targeted viewers .


class: same cluster, base class: no relation, new model class: first -> second
first:
However , the collection and processing of user information required for <m> targeting of ads </m> may lead to privacy concerns .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: same cluster, base class: first -> second, new model class: no relation
first:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .
second:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .


class: same cluster, base class: first -> second, new model class: no relation
first:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .
second:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .


class: same cluster, base class: first -> second, new model class: no relation
first:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .
second:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .
second:
Here we investigate the network properties of the third party referral structures that facilitate gathering of user information for the <m> delivery of personalized ads </m> .


class: same cluster, base class: first -> second, new model class: no relation
first:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .
second:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .


class: same cluster, base class: no relation, new model class: no relation
first:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: same cluster, base class: no relation, new model class: no relation
first:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .
second:
In many e-commerce applications , ranging from dynamic Web content presentation , to <m> personalized ad targeting </m> , to individual recommendations to the customers , it is important to build personalized profiles of individual users from their transactional histories .


class: same cluster, base class: no relation, new model class: no relation
first:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .
second:
Driven by the rapid growth of large scale real-time data mining applications for <m> personalized ads </m> and content recommendations , distributed stream processing systems are widely applied in modern big-data architectures .


class: same cluster, base class: no relation, new model class: no relation
first:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .
second:
By analyzing the unstructured characteristics of online ads and search engine user behavior data , proposed a kind of <m> personalized ads recommendation method </m> based on User-Interest-Behavior model , which can extract the user ’s interest preferences by the topic model and generate the recommended list of ads based on the nearest neighbor and user behavior .


class: same cluster, base class: no relation, new model class: no relation
first:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .
second:
With the popularity of social networks such as Facebook and twitter , social recommendations have become possible , which rely on individual 's social connections in order to make <m> personalized recommendations of ads </m> , content , products , and people .


class: same cluster, base class: no relation, new model class: no relation
first:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .
second:
In this paper we present an approach for the presentation of <m> personalized advertisements </m> on mobile devices , which is based on a user model that takes into account user ’s interests over time .


class: same cluster, base class: no relation, new model class: no relation
first:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .
second:
This work takes the first step toward the <m> personalized positioning of ads </m> on article pages .


class: same cluster, base class: no relation, new model class: no relation
first:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .
second:
In the first paper , Adany , Kraus and Ordonez investigate the area of <m> personalized TV ads </m> to improve the effectiveness of ads distribution to targeted viewers .


class: same cluster, base class: no relation, new model class: no relation
first:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .
second:
Mobile devices provide a continuous data stream of contextual behavioral information which can be leveraged for a variety of user services , such as in <m> personalizing ads </m> and customizing home screens .


class: same cluster, base class: no relation, new model class: no relation
first:
By the above-described embodiment , the present application is possible in the case of user authorization , by converting the line identification of the user IP address corresponding to the user ID , user ID and can be used for accurate <m> delivery of ads </m> .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: same cluster, base class: no relation, new model class: first -> second
first:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .
second:
By analyzing the unstructured characteristics of online ads and search engine user behavior data , proposed a kind of <m> personalized ads recommendation method </m> based on User-Interest-Behavior model , which can extract the user ’s interest preferences by the topic model and generate the recommended list of ads based on the nearest neighbor and user behavior .


class: same cluster, base class: no relation, new model class: first -> second
first:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .
second:
With the popularity of social networks such as Facebook and twitter , social recommendations have become possible , which rely on individual 's social connections in order to make <m> personalized recommendations of ads </m> , content , products , and people .


class: same cluster, base class: no relation, new model class: first -> second
first:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .
second:
In the first paper , Adany , Kraus and Ordonez investigate the area of <m> personalized TV ads </m> to improve the effectiveness of ads distribution to targeted viewers .


class: same cluster, base class: no relation, new model class: first -> second
first:
We illustrate our approach in three scenarios : the <m> delivery of targeted ads </m> specific to a user ’s home location , the estimation of traffic speed , and location prediction .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: same cluster, base class: no relation, new model class: first -> second
first:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .
second:
By analyzing the unstructured characteristics of online ads and search engine user behavior data , proposed a kind of <m> personalized ads recommendation method </m> based on User-Interest-Behavior model , which can extract the user ’s interest preferences by the topic model and generate the recommended list of ads based on the nearest neighbor and user behavior .


class: same cluster, base class: no relation, new model class: first -> second
first:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .
second:
With the popularity of social networks such as Facebook and twitter , social recommendations have become possible , which rely on individual 's social connections in order to make <m> personalized recommendations of ads </m> , content , products , and people .


class: same cluster, base class: no relation, new model class: first -> second
first:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .
second:
In the first paper , Adany , Kraus and Ordonez investigate the area of <m> personalized TV ads </m> to improve the effectiveness of ads distribution to targeted viewers .


class: same cluster, base class: no relation, new model class: first -> second
first:
Social networks offer various services such as recommendations of social events , or <m> delivery of targeted advertising material </m> to certain users .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: same cluster, base class: no relation, new model class: first -> second
first:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .
second:
By analyzing the unstructured characteristics of online ads and search engine user behavior data , proposed a kind of <m> personalized ads recommendation method </m> based on User-Interest-Behavior model , which can extract the user ’s interest preferences by the topic model and generate the recommended list of ads based on the nearest neighbor and user behavior .


class: same cluster, base class: no relation, new model class: first -> second
first:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .
second:
With the popularity of social networks such as Facebook and twitter , social recommendations have become possible , which rely on individual 's social connections in order to make <m> personalized recommendations of ads </m> , content , products , and people .


class: same cluster, base class: no relation, new model class: first -> second
first:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .
second:
In the first paper , Adany , Kraus and Ordonez investigate the area of <m> personalized TV ads </m> to improve the effectiveness of ads distribution to targeted viewers .


class: same cluster, base class: no relation, new model class: first -> second
first:
This data is being used for a variety of purposes apart from <m> delivering targeted advertisements </m> .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: same cluster, base class: no relation, new model class: no relation
first:
Here we investigate the network properties of the third party referral structures that facilitate gathering of user information for the <m> delivery of personalized ads </m> .
second:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .


class: same cluster, base class: no relation, new model class: first -> second
first:
Here we investigate the network properties of the third party referral structures that facilitate gathering of user information for the <m> delivery of personalized ads </m> .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: same cluster, base class: no relation, new model class: no relation
first:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .
second:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .


class: same cluster, base class: no relation, new model class: no relation
first:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .
second:
In many e-commerce applications , ranging from dynamic Web content presentation , to <m> personalized ad targeting </m> , to individual recommendations to the customers , it is important to build personalized profiles of individual users from their transactional histories .


class: same cluster, base class: no relation, new model class: no relation
first:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .
second:
Driven by the rapid growth of large scale real-time data mining applications for <m> personalized ads </m> and content recommendations , distributed stream processing systems are widely applied in modern big-data architectures .


class: same cluster, base class: no relation, new model class: no relation
first:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .
second:
By analyzing the unstructured characteristics of online ads and search engine user behavior data , proposed a kind of <m> personalized ads recommendation method </m> based on User-Interest-Behavior model , which can extract the user ’s interest preferences by the topic model and generate the recommended list of ads based on the nearest neighbor and user behavior .


class: same cluster, base class: no relation, new model class: no relation
first:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .
second:
With the popularity of social networks such as Facebook and twitter , social recommendations have become possible , which rely on individual 's social connections in order to make <m> personalized recommendations of ads </m> , content , products , and people .


class: same cluster, base class: no relation, new model class: no relation
first:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .
second:
In this paper we present an approach for the presentation of <m> personalized advertisements </m> on mobile devices , which is based on a user model that takes into account user ’s interests over time .


class: same cluster, base class: no relation, new model class: no relation
first:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .
second:
This work takes the first step toward the <m> personalized positioning of ads </m> on article pages .


class: same cluster, base class: no relation, new model class: no relation
first:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .
second:
In the first paper , Adany , Kraus and Ordonez investigate the area of <m> personalized TV ads </m> to improve the effectiveness of ads distribution to targeted viewers .


class: same cluster, base class: no relation, new model class: no relation
first:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .
second:
Mobile devices provide a continuous data stream of contextual behavioral information which can be leveraged for a variety of user services , such as in <m> personalizing ads </m> and customizing home screens .


class: same cluster, base class: no relation, new model class: no relation
first:
Ontologies are used as domain knowledge in the <m> delivery of relevant ads </m> .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .
second:
By analyzing the unstructured characteristics of online ads and search engine user behavior data , proposed a kind of <m> personalized ads recommendation method </m> based on User-Interest-Behavior model , which can extract the user ’s interest preferences by the topic model and generate the recommended list of ads based on the nearest neighbor and user behavior .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .
second:
With the popularity of social networks such as Facebook and twitter , social recommendations have become possible , which rely on individual 's social connections in order to make <m> personalized recommendations of ads </m> , content , products , and people .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .
second:
In the first paper , Adany , Kraus and Ordonez investigate the area of <m> personalized TV ads </m> to improve the effectiveness of ads distribution to targeted viewers .


class: same cluster, base class: no relation, new model class: first -> second
first:
<m> Targeting display ads </m> can be improved substantially with machine learning methods , but building many models on massive data becomes prohibitively expensive computationally .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: same cluster, base class: no relation, new model class: first -> second
first:
In many e-commerce applications , ranging from dynamic Web content presentation , to <m> personalized ad targeting </m> , to individual recommendations to the customers , it is important to build personalized profiles of individual users from their transactional histories .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: same cluster, base class: no relation, new model class: first -> second
first:
Driven by the rapid growth of large scale real-time data mining applications for <m> personalized ads </m> and content recommendations , distributed stream processing systems are widely applied in modern big-data architectures .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: same cluster, base class: no relation, new model class: no relation
first:
By analyzing the unstructured characteristics of online ads and search engine user behavior data , proposed a kind of <m> personalized ads recommendation method </m> based on User-Interest-Behavior model , which can extract the user ’s interest preferences by the topic model and generate the recommended list of ads based on the nearest neighbor and user behavior .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: same cluster, base class: no relation, new model class: no relation
first:
With the popularity of social networks such as Facebook and twitter , social recommendations have become possible , which rely on individual 's social connections in order to make <m> personalized recommendations of ads </m> , content , products , and people .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: same cluster, base class: no relation, new model class: first -> second
first:
In this paper we present an approach for the presentation of <m> personalized advertisements </m> on mobile devices , which is based on a user model that takes into account user ’s interests over time .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: same cluster, base class: no relation, new model class: first -> second
first:
This work takes the first step toward the <m> personalized positioning of ads </m> on article pages .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: same cluster, base class: no relation, new model class: first -> second
first:
Mobile devices provide a continuous data stream of contextual behavioral information which can be leveraged for a variety of user services , such as in <m> personalizing ads </m> and customizing home screens .
second:
The technique could be employed in <m> personalizing video ads </m> that are presented to people whilst they view programming over the Internet or in copy testing of ads to unobtrusively quantify effectiveness .


class: same cluster, base class: no relation, new model class: no relation
first:
Text information extraction comprises of <m> text image classification </m> , text detection , localization , segmentation , enhancement and recognition .
second:
The RPN is used for proposing text regions and the Fast R - CNN model [ reference ] is modified to do <m> text region classification </m> , refinement and inclined box prediction .


class: same cluster, base class: no relation, new model class: no relation
first:
KeywoRDS Confidence Computation , Document Image , Fuzzy Matching , Handwritten Documents , Performance Analysis , Printed Documents , SVM , <m> Text Image Classification </m> , Word Association
second:
The RPN is used for proposing text regions and the Fast R - CNN model [ reference ] is modified to do <m> text region classification </m> , refinement and inclined box prediction .


class: same cluster, base class: no relation, new model class: first -> second
first:
Proposes a new evaluation of <m> text classification vector </m> , and uses it to discuss two models : the optimization model and the collection of the optimization model .
second:
Authorship attribution may be considered as a <m> text categorization problem </m> .


