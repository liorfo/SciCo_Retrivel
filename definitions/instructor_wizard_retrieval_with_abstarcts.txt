Processing term: unsupervised automatic facial point detection from normal terms


Unsupervised automatic facial point detection refers to a process where computer algorithms are used to
identify and locate specific points on a person's face without any prior knowledge or training data about
those points. It involves analyzing images of faces and identifying patterns or landmarks that can be used to
track changes in facial expressions and movements.


From:
1. Facial emotion expressions in human-robot interaction: A survey
Abstract: Facial expressions are an ideal means of communicating one's emotions or
intentions to others. This overview will focus on human facial expression
recognition as well as robotic facial expression generation. In the case of
human facial expression recognition, both facial expression recognition on
predefined datasets as well as in real-time will be covered. For robotic facial
expression generation, hand-coded and automated methods i.e., facial
expressions of a robot are generated by moving the features (eyes, mouth) of
the robot by hand-coding or automatically using machine learning techniques,
will also be covered. There are already plenty of studies that achieve high
accuracy for emotion expression recognition on predefined datasets, but the
accuracy for facial expression recognition in real-time is comparatively lower.
In the case of expression generation in robots, while most of the robots are
capable of making basic facial expressions, there are not many studies that

2. CoNFies: Controllable Neural Face Avatars
Abstract: use automated facial action recognition (AFAR) to characterize facial
expressions as a combination of action units (AU) and their intensities. AUs
provide both the semantic locations and control labels for the system. CoNFies
outperformed competing methods for novel view and expression synthesis in terms
of visual and anatomic fidelity of expressions.

3. Attention Based Relation Network for Facial Action Units Recognition
Abstract: Facial action unit (AU) recognition is essential to facial expression
analysis. Since there are highly positive or negative correlations between AUs,
some existing AU recognition works have focused on modeling AU relations.
However, previous relationship-based approaches typically embed predefined
rules into their models and ignore the impact of various AU relations in
different crowds. In this paper, we propose a novel Attention Based Relation
Network (ABRNet) for AU recognition, which can automatically capture AU
relations without unnecessary or even disturbing predefined rules. ABRNet uses
several relation learning layers to automatically capture different AU
relations. The learned AU relation features are then fed into a self-attention
fusion module, which aims to refine individual AU features with attention
weights to enhance the feature robustness. Furthermore, we propose an AU
relation dropout strategy and AU relation loss (AUR-Loss) to better model AU



Processing term: ASR from normal terms


 Automatic Speech Recognition (ASR) is a technology that enables machines to recognize and understand human
speech. It involves converting spoken words into text or commands that can be used by computers or other
devices. ASR systems use various algorithms and models to analyze and interpret the acoustic signals produced
by the human voice. These signals are typically captured using microphones and then processed through a series
of steps before being converted into text or commands.


From:
1. Visualizing Automatic Speech Recognition -- Means for a Better
  Understanding?
Abstract: Automatic speech recognition (ASR) is improving ever more at mimicking human
speech processing. The functioning of ASR, however, remains to a large extent
obfuscated by the complex structure of the deep neural networks (DNNs) they are
based on. In this paper, we show how so-called attribution methods, that we
import from image recognition and suitably adapt to handle audio data, can help
to clarify the working of ASR. Taking DeepSpeech, an end-to-end model for ASR,
as a case study, we show how these techniques help to visualize which features
of the input are the most influential in determining the output. We focus on
three visualization techniques: Layer-wise Relevance Propagation (LRP),
Saliency Maps, and Shapley Additive Explanations (SHAP). We compare these
methods and discuss potential further applications, such as in the detection of
adversarial examples.

2. Evolutionary optimization of contexts for phonetic correction in speech
  recognition systems
Abstract: Automatic Speech Recognition (ASR) is an area of growing academic and
commercial interest due to the high demand for applications that use it to
provide a natural communication method. It is common for general purpose ASR
systems to fail in applications that use a domain-specific language. Various
strategies have been used to reduce the error, such as providing a context that
modifies the language model and post-processing correction methods. This
article explores the use of an evolutionary process to generate an optimized
context for a specific application domain, as well as different correction
techniques based on phonetic distance metrics. The results show the viability
of a genetic algorithm as a tool for context optimization, which, added to a
post-processing correction based on phonetic representations, can reduce the
errors on the recognized speech.

3. Speech Recognition by Simply Fine-tuning BERT
Abstract: We propose a simple method for automatic speech recognition (ASR) by
fine-tuning BERT, which is a language model (LM) trained on large-scale
unlabeled text data and can generate rich contextual representations. Our
assumption is that given a history context sequence, a powerful LM can narrow
the range of possible choices and the speech signal can be used as a simple
clue. Hence, comparing to conventional ASR systems that train a powerful
acoustic model (AM) from scratch, we believe that speech recognition is
possible by simply fine-tuning a BERT model. As an initial study, we
demonstrate the effectiveness of the proposed idea on the AISHELL dataset and
show that stacking a very simple AM on top of BERT can yield reasonable
performance.



Processing term: Feature Pyramid Network from normal terms


 Feature Pyramid Network (FPN) is a deep learning model architecture used for image classification, object
detection, and semantic segmentation. It consists of a series of convolutional layers that take in input
images at different scales and produces feature maps at each level. These feature maps are then combined using
concatenations and upscaling operations to produce final predictions. FPN has become a popular choice for many
state-of-the-art models because it can learn half of the preceding feature maps and has fewer parameters
compared to traditional architectures that use separate convolutional layers for each scale.


From:
1. SFPN: Synthetic FPN for Object Detection
Abstract: FPN (Feature Pyramid Network) has become a basic component of most SoTA one
stage object detectors. Many previous studies have repeatedly proved that FPN
can caputre better multi-scale feature maps to more precisely describe objects
if they are with different sizes. However, for most backbones such VGG, ResNet,
or DenseNet, the feature maps at each layer are downsized to their quarters due
to the pooling operation or convolutions with stride 2. The gap of
down-scaling-by-2 is large and makes its FPN not fuse the features smoothly.
This paper proposes a new SFPN (Synthetic Fusion Pyramid Network) arichtecture
which creates various synthetic layers between layers of the original FPN to
enhance the accuracy of light-weight CNN backones to extract objects' visual
features more accurately. Finally, experiments prove the SFPN architecture
outperforms either the large backbone VGG16, ResNet50 or light-weight backbones
such as MobilenetV2 based on AP score.

2. Pyramid Point: A Multi-Level Focusing Network for Revisiting Feature
  Layers
Abstract: We present a method to learn a diverse group of object categories from an
unordered point set. We propose our Pyramid Point network, which uses a dense
pyramid structure instead of the traditional 'U' shape, typically seen in
semantic segmentation networks. This pyramid structure gives a second look,
allowing the network to revisit different layers simultaneously, increasing the
contextual information by creating additional layers with less noise. We
introduce a Focused Kernel Point convolution (FKP Conv), which expands on the
traditional point convolutions by adding an attention mechanism to the kernel
outputs. This FKP Conv increases our feature quality and allows us to weigh the
kernel outputs dynamically. These FKP Convs are the central part of our
Recurrent FKP Bottleneck block, which makes up the backbone of our encoder.
With this distinct network, we demonstrate competitive performance on three
benchmark data sets. We also perform an ablation study to show the positive

3. SPFNet:Subspace Pyramid Fusion Network for Semantic Segmentation
Abstract: The encoder-decoder structure has significantly improved performance in many
vision tasks by fusing low-level and high-level feature maps. However, this
approach can hardly extract sufficient context information for pixel-wise
segmentation. In addition, extracting similar low-level features at multiple
scales could lead to redundant information. To tackle these issues, we propose
Subspace Pyramid Fusion Network (SPFNet). Specifically, we combine pyramidal
module and context aggregation module to exploit the impact of
multi-scale/global context information. At first, we construct a Subspace
Pyramid Fusion Module (SPFM) based on Reduced Pyramid Pooling (RPP). Then, we
propose the Efficient Global Context Aggregation (EGCA) module to capture
discriminative features by fusing multi-level global context features. Finally,
we add decoder-based subpixel convolution to retrieve the high-resolution
feature maps, which can help select category localization details. SPFM learns



Processing term: spatial attention mechanisms from normal terms


Spatial attention mechanisms refer to a type of attention mechanism that focuses on specific regions of an
image or video frame while ignoring other regions. These mechanisms are designed to improve the accuracy and
efficiency of object detection, segmentation, and tracking tasks. They do this by learning the importance of
different regions of an image or frame and using this information to adjust the weights of the neural network
during training.


From:
1. Looking Beyond Two Frames: End-to-End Multi-Object Tracking Using
  Spatial and Temporal Transformers
Abstract: spatial attention mechanism enables our framework to learn implicit
representations between all the objects and the objects to the measurements,
while the temporal attention mechanism focuses on specific parts of past
information, allowing our approach to resolve occlusions over multiple frames.
Our experiments demonstrate the potential of this new approach, achieving
results on par with or better than the current state-of-the-art on multiple MOT
metrics for several popular multi-object tracking benchmarks.

2. ConAM: Confidence Attention Module for Convolutional Neural Networks
Abstract: The so-called "attention" is an efficient mechanism to improve the
performance of convolutional neural networks. It uses contextual information to
recalibrate the input to strengthen the propagation of informative features.
However, the majority of the attention mechanisms only consider either local or
global contextual information, which is singular to extract features. Moreover,
many existing mechanisms directly use the contextual information to recalibrate
the input, which unilaterally enhances the propagation of the informative
features, but does not suppress the useless ones. This paper proposes a new
attention mechanism module based on the correlation between local and global
contextual information and we name this correlation as confidence. The novel
attention mechanism extracts the local and global contextual information
simultaneously, and calculates the confidence between them, then uses this
confidence to recalibrate the input pixels. The extraction of local and global

3. SA-Net: Shuffle Attention for Deep Convolutional Neural Networks
Abstract: Attention mechanisms, which enable a neural network to accurately focus on
all the relevant elements of the input, have become an essential component to
improve the performance of deep neural networks. There are mainly two attention
mechanisms widely used in computer vision studies, \textit{spatial attention}
and \textit{channel attention}, which aim to capture the pixel-level pairwise
relationship and channel dependency, respectively. Although fusing them
together may achieve better performance than their individual implementations,
it will inevitably increase the computational overhead. In this paper, we
propose an efficient Shuffle Attention (SA) module to address this issue, which
adopts Shuffle Units to combine two types of attention mechanisms effectively.
Specifically, SA first groups channel dimensions into multiple sub-features
before processing them in parallel. Then, for each sub-feature, SA utilizes a
Shuffle Unit to depict feature dependencies in both spatial and channel



Processing term: JPEG method from normal terms


The term "JPEG method" refers to the process of compressing and decompressing digital images using the Joint
Photographic Experts Group (JPEG) algorithm. It is a widely used standard for lossy compression of
photographic images, which reduces the size of the image while retaining most of its visual quality.


From:
1. Discrete Cosine Transform in JPEG Compression
Abstract: Image Compression has become an absolute necessity in today's day and age.
With the advent of the Internet era, compressing files to share among other
users is quintessential. Several efforts have been made to reduce file sizes
while still maintain image quality in order to transmit files even on limited
bandwidth connections. This paper discusses the need for Discrete Cosine
Transform or DCT in the compression of images in Joint Photographic Experts
Group or JPEG file format. Via an intensive literature study, this paper first
introduces DCT and JPEG Compression. The section preceding it discusses how
JPEG compression is implemented by DCT. The last section concludes with further
real world applications of DCT in image processing.

2. Learning to Immunize Images for Tamper Localization and Self-Recovery
Abstract: and benign image post-processing, where a novel distillation-based JPEG
simulator is proposed for improved JPEG robustness. Our method achieves
promising results in real-world tests where experiments show accurate tamper
localization as well as high-fidelity content recovery. Additionally, we show
superior performance on tamper localization compared to state-of-the-art
schemes based on passive forensics.

3. Neural JPEG: End-to-End Image Compression Leveraging a Standard JPEG
  Encoder-Decoder
Abstract: Recent advances in deep learning have led to superhuman performance across a
variety of applications. Recently, these methods have been successfully
employed to improve the rate-distortion performance in the task of image
compression. However, current methods either use additional post-processing
blocks on the decoder end to improve compression or propose an end-to-end
compression scheme based on heuristics. For the majority of these, the trained
deep neural networks (DNNs) are not compatible with standard encoders and would
be difficult to deply on personal computers and cellphones. In light of this,
we propose a system that learns to improve the encoding performance by
enhancing its internal neural representations on both the encoder and decoder
ends, an approach we call Neural JPEG. We propose frequency domain pre-editing
and post-editing methods to optimize the distribution of the DCT coefficients
at both encoder and decoder ends in order to improve the standard compression



Processing term: video data generation from normal terms


 Video data generation refers to the process of creating new digital video content or modifying existing
content to create new versions of the same video. This may involve adding special effects, changing camera
angles, adjusting lighting, or altering other parameters to create variations of the original video. The goal
of video data generation is often to create new content that can be used for various purposes, such as
training machine learning models, creating virtual reality experiences, or enhancing entertainment content.


From:
1. Video Content Swapping Using GAN
Abstract: Video generation is an interesting problem in computer vision. It is quite
popular for data augmentation, special effect in move, AR/VR and so on. With
the advances of deep learning, many deep generative models have been proposed
to solve this task. These deep generative models provide away to utilize all
the unlabeled images and videos online, since it can learn deep feature
representations with unsupervised manner. These models can also generate
different kinds of images, which have great value for visual application.
However generating a video would be much more challenging since we need to
model not only the appearances of objects in the video but also their temporal
motion. In this work, we will break down any frame in the video into content
and pose. We first extract the pose information from a video using a
pre-trained human pose detection and use a generative model to synthesize the
video based on the content code and pose code.

2. GPT2MVS: Generative Pre-trained Transformer-2 for Multi-modal Video
  Summarization
Abstract: Traditional video summarization methods generate fixed video representations
regardless of user interest. Therefore such methods limit users' expectations
in content search and exploration scenarios. Multi-modal video summarization is
one of the methods utilized to address this problem. When multi-modal video
summarization is used to help video exploration, a text-based query is
considered as one of the main drivers of video summary generation, as it is
user-defined. Thus, encoding the text-based query and the video effectively are
both important for the task of multi-modal video summarization. In this work, a
new method is proposed that uses a specialized attention network and
contextualized word representations to tackle this task. The proposed model
consists of a contextualized video summary controller, multi-modal attention
mechanisms, an interactive attention network, and a video summary generator.
Based on the evaluation of the existing multi-modal video summarization

3. Comprehensive Video Understanding: Video summarization with
  content-based video recommender design
Abstract: Video summarization aims to extract keyframes/shots from a long video.
Previous methods mainly take diversity and representativeness of generated
summaries as prior knowledge in algorithm design. In this paper, we formulate
video summarization as a content-based recommender problem, which should
distill the most useful content from a long video for users who suffer from
information overload. A scalable deep neural network is proposed on predicting
if one video segment is a useful segment for users by explicitly modelling both
segment and video. Moreover, we accomplish scene and action recognition in
untrimmed videos in order to find more correlations among different aspects of
video understanding tasks. Also, our paper will discuss the effect of audio and
visual features in summarization task. We also extend our work by data
augmentation and multi-task learning for preventing the model from early-stage
overfitting. The final results of our model win the first place in ICCV 2019



Processing term: weight update from hard 20 terms


Weight update refers to the process of adjusting the weights of a neural network based on the errors
encountered during training. This is done in order to improve the accuracy of the network's predictions.
During each iteration of the training process, the weights are updated using an optimization algorithm such as
stochastic gradient descent. The goal is to find the optimal values of the weights that minimize the loss
function and lead to the best possible predictions.


From:
1. Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting
Abstract: Current deep neural networks (DNNs) can easily overfit to biased training
data with corrupted labels or class imbalance. Sample re-weighting strategy is
commonly used to alleviate this issue by designing a weighting function mapping
from training loss to sample weight, and then iterating between weight
recalculating and classifier updating. Current approaches, however, need
manually pre-specify the weighting function as well as its additional
hyper-parameters. It makes them fairly hard to be generally applied in practice
due to the significant variation of proper weighting schemes relying on the
investigated problem and training data. To address this issue, we propose a
method capable of adaptively learning an explicit weighting function directly
from data. The weighting function is an MLP with one hidden layer, constituting
a universal approximator to almost any continuous functions, making the method
able to fit a wide range of weighting functions including those assumed in

2. The activity-weight duality in feed forward neural networks: The
  geometric determinants of generalization
Abstract: One of the fundamental problems in machine learning is generalization. In
neural network models with a large number of weights (parameters), many
solutions can be found to fit the training data equally well. The key question
is which solution can describe testing data not in the training set. Here, we
report the discovery of an exact duality (equivalence) between changes in
activities in a given layer of neurons and changes in weights that connect to
the next layer of neurons in a densely connected layer in any feed forward
neural network. The activity-weight (A-W) duality allows us to map variations
in inputs (data) to variations of the corresponding dual weights. By using this
mapping, we show that the generalization loss can be decomposed into a sum of
contributions from different eigen-directions of the Hessian matrix of the loss
function at the solution in weight space. The contribution from a given
eigen-direction is the product of two geometric factors (determinants): the

3. A Modern Self-Referential Weight Matrix That Learns to Modify Itself
Abstract: The weight matrix (WM) of a neural network (NN) is its program. The programs
of many traditional NNs are learned through gradient descent in some error
function, then remain fixed. The WM of a self-referential NN, however, can keep
rapidly modifying all of itself during runtime. In principle, such NNs can
meta-learn to learn, and meta-meta-learn to meta-learn to learn, and so on, in
the sense of recursive self-improvement. While NN architectures potentially
capable of implementing such behaviour have been proposed since the '90s, there
have been few if any practical studies. Here we revisit such NNs, building upon
recent successes of fast weight programmers and closely related linear
Transformers. We propose a scalable self-referential WM (SRWM) that learns to
use outer products and the delta update rule to modify itself. We evaluate our
SRWM in supervised few-shot learning and in multi-task reinforcement learning



Processing term: Understanding Behaviors of Neurons from hard 20 terms


Understanding Behaviors of Neurons refers to the study of how neurons in the brain behave and interact with
each other during different tasks or activities. It involves analyzing the electrical signals generated by
neurons and how they communicate with each other to perform specific functions. The term is often used in the
field of neuroscience and is important for understanding how the brain processes information and performs
complex tasks.


From:
1. Account for Neuronal Representations from the Perspective of Neurons
Abstract: synchronization as their strategies and demonstrate how the execution of these
strategies during turn them into specialized neurons that selectively but
strongly respond to familiar entities. In the macro-level, I further discuss
how these specialized neurons underlie various cognitive functions and
phenomena. Significantly, this paper, through defending neuronal
representation, introduces a novel way to understand the relationship between
brain and cognition.

2. Capturing cross-session neural population variability through
  self-supervised identification of consistent neuron ensembles
Abstract: Decoding stimuli or behaviour from recorded neural activity is a common
approach to interrogate brain function in research, and an essential part of
brain-computer and brain-machine interfaces. Reliable decoding even from small
neural populations is possible because high dimensional neural population
activity typically occupies low dimensional manifolds that are discoverable
with suitable latent variable models. Over time however, drifts in activity of
individual neurons and instabilities in neural recording devices can be
substantial, making stable decoding over days and weeks impractical. While this
drift cannot be predicted on an individual neuron level, population level
variations over consecutive recording sessions such as differing sets of
neurons and varying permutations of consistent neurons in recorded data may be
learnable when the underlying manifold is stable over time. Classification of
consistent versus unfamiliar neurons across sessions and accounting for

3. Neuron-level Interpretation of Deep NLP Models: A Survey
Abstract: The proliferation of deep neural networks in various domains has seen an
increased need for interpretability of these models. Preliminary work done
along this line and papers that surveyed such, are focused on high-level
representation analysis. However, a recent branch of work has concentrated on
interpretability at a more granular level of analyzing neurons within these
models. In this paper, we survey the work done on neuron analysis including: i)
methods to discover and understand neurons in a network, ii) evaluation
methods, iii) major findings including cross architectural comparisons that
neuron analysis has unraveled, iv) applications of neuron probing such as:
controlling the model, domain adaptation etc., and v) a discussion on open
issues and future research directions.



Processing term: policy gradient methods from hard 20 terms


 Policy gradient methods are a type of reinforcement learning algorithm that uses the gradient descent method
to optimize a policy that maps states to actions. They are based on the idea that the gradient of the expected
return with respect to the policy parameters can be estimated using samples of experience tuples (states,
actions, rewards). By iteratively updating the policy parameters using these estimates, policy gradient
methods can learn an optimal policy that maximizes the expected return.


From:
1. Semi-On-Policy Training for Sample Efficient Multi-Agent Policy
  Gradients
Abstract: Policy gradient methods are an attractive approach to multi-agent
reinforcement learning problems due to their convergence properties and
robustness in partially observable scenarios. However, there is a significant
performance gap between state-of-the-art policy gradient and value-based
methods on the popular StarCraft Multi-Agent Challenge (SMAC) benchmark. In
this paper, we introduce semi-on-policy (SOP) training as an effective and
computationally efficient way to address the sample inefficiency of on-policy
policy gradient methods. We enhance two state-of-the-art policy gradient
algorithms with SOP training, demonstrating significant performance
improvements. Furthermore, we show that our methods perform as well or better
than state-of-the-art value-based methods on a variety of SMAC tasks.

2. Stein Variational Policy Gradient
Abstract: Policy gradient methods have been successfully applied to many complex
reinforcement learning problems. However, policy gradient methods suffer from
high variance, slow convergence, and inefficient exploration. In this work, we
introduce a maximum entropy policy optimization framework which explicitly
encourages parameter exploration, and show that this framework can be reduced
to a Bayesian inference problem. We then propose a novel Stein variational
policy gradient method (SVPG) which combines existing policy gradient methods
and a repulsive functional to generate a set of diverse but well-behaved
policies. SVPG is robust to initialization and can easily be implemented in a
parallel manner. On continuous control problems, we find that implementing SVPG
on top of REINFORCE and advantage actor-critic algorithms improves both average
return and data efficiency.

3. On Policy Gradients
Abstract: The goal of policy gradient approaches is to find a policy in a given class
of policies which maximizes the expected return. Given a differentiable model
of the policy, we want to apply a gradient-ascent technique to reach a local
optimum. We mainly use gradient ascent, because it is theoretically well
researched. The main issue is that the policy gradient with respect to the
expected return is not available, thus we need to estimate it. As policy
gradient algorithms also tend to require on-policy data for the gradient
estimate, their biggest weakness is sample efficiency. For this reason, most
research is focused on finding algorithms with improved sample efficiency. This
paper provides a formal introduction to policy gradient that shows the
development of policy gradient approaches, and should enable the reader to
follow current research on the topic.



Processing term: satellite navigation module from hard 20 terms


 A satellite navigation module is a component of an unmanned aerial vehicle (UAV) or drone that uses satellite
signals to determine the position and orientation of the UAV. It may also use other sensors such as GPS,
GLONASS, or BeiDou to improve accuracy. The module typically includes a GPS or other satellite antenna, a GPS
or other satellite signal processor, and a microcontroller or other processing hardware to interpret the
signals and calculate the UAV's position and orientation.


From:
1. Customizable Stochastic High Fidelity Model of the Sensors and Camera
  onboard a Low SWaP Fixed Wing Autonomous Aircraft
Abstract: The navigation systems of autonomous aircraft rely on the readings provided
by a suite of onboard sensors to estimate the aircraft state. In the case of
fixed wing vehicles, the sensor suite is composed by triads of accelerometers,
gyroscopes, and magnetometers, a Global Navigation Satellite System (GNSS)
receiver, and an air data system (Pitot tube, air vanes, thermometer, and
barometer), and is often complemented by one or more digital cameras. An
accurate representation of the behavior and error sources of each of these
sensors, together with the images generated by the cameras, in indispensable
for flight simulation and the evaluation of novel inertial or visual navigation
algorithms, and more so in the case of low SWaP (size, weight, and power)
aircraft, in which the quality and price of the sensors is limited. This
article presents realistic and customizable models for each of these sensors,
which have been implemented as an open-source C ++ simulation. Provided with

2. Stochastic High Fidelity Simulation and Scenarios for Testing of Fixed
  Wing Autonomous GNSS-Denied Navigation Algorithms
Abstract: tube, an air data system, a GNSS receiver, and a digital camera, so the
simulation is valid for inertial, visual, and visual inertial navigation
systems. Two scenarios involving the loss of GNSS signals are considered: the
first represents the challenges involved in aborting the mission and heading
towards a remote recovery location while experiencing varying weather, and the
second models the continuation of the mission based on a series of closely
spaced bearing changes.
  All simulation modules have been modeled with as few simplifications as
possible to increase the realism of the results. While the implementation of
the aircraft performances and its control system is deterministic, that of all
other modules, including the mission, sensors, weather, wind, turbulence, and
initial estimations, is fully stochastic. This enables a robust evaluation of
each proposed navigation system by means of Monte-Carlo simulations that rely
on a high number of executions of both scenarios.

3. Integrated guidance and control framework for the waypoint navigation of
  a miniature aircraft with highly coupled longitudinal and lateral dynamics
Abstract: A solution to the waypoint navigation problem for fixed wing micro air
vehicles (MAV) is addressed in this paper, in the framework of integrated
guidance and control (IGC). IGC yields a single step solution to the waypoint
navigation problem, unlike conventional multiple loop design. The pure
proportional navigation (PPN) guidance law is integrated with the MAV dynamics.
A multivariable static output feedback (SOF) controller is designed for the
linear state space model formulated in the IGC framework. The waypoint
navigation algorithm handles the minimum turn radius constraint of the MAV. The
algorithm also evaluates the feasibility of reaching a waypoint. Extensive
non-linear simulations are performed on high fidelity 150 mm wingspan MAV model
to demonstrate the potential advantages of the proposed waypoint navigation
algorithm.

/cs/labs/tomhope/forer11/vnev_2/lib/python3.9/site-packages/transformers/pipelines/base.py:1081: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
  warnings.warn(


Processing term: GRU from hard 20 terms


GRU stands for Gated Recurrent Unit. It is a type of recurrent neural network (RNN) cell that is commonly used
in sequence-to-sequence models like NMT. The GRU cell consists of three gates: an input gate, a forget gate,
and an output gate. These gates help the model remember relevant information while forgetting irrelevant
information during the processing of each input sequence.


From:
1. Neural Machine Translation: A Review of Methods, Resources, and Tools
Abstract: Machine translation (MT) is an important sub-field of natural language
processing that aims to translate natural languages using computers. In recent
years, end-to-end neural machine translation (NMT) has achieved great success
and has become the new mainstream method in practical MT systems. In this
article, we first provide a broad review of the methods for NMT and focus on
methods relating to architectures, decoding, and data augmentation. Then we
summarize the resources and tools that are useful for researchers. Finally, we
conclude with a discussion of possible future research directions.

2. A Survey of Domain Adaptation for Neural Machine Translation
Abstract: Neural machine translation (NMT) is a deep learning based approach for
machine translation, which yields the state-of-the-art translation performance
in scenarios where large-scale parallel corpora are available. Although the
high-quality and domain-specific translation is crucial in the real world,
domain-specific corpora are usually scarce or nonexistent, and thus vanilla NMT
performs poorly in such scenarios. Domain adaptation that leverages both
out-of-domain parallel corpora as well as monolingual corpora for in-domain
translation, is very important for domain-specific translation. In this paper,
we give a comprehensive survey of the state-of-the-art domain adaptation
techniques for NMT.

3. English-Japanese Neural Machine Translation with
  Encoder-Decoder-Reconstructor
Abstract: Neural machine translation (NMT) has recently become popular in the field of
machine translation. However, NMT suffers from the problem of repeating or
missing words in the translation. To address this problem, Tu et al. (2017)
proposed an encoder-decoder-reconstructor framework for NMT using
back-translation. In this method, they selected the best forward translation
model in the same manner as Bahdanau et al. (2015), and then trained a
bi-directional translation model as fine-tuning. Their experiments show that it
offers significant improvement in BLEU scores in Chinese-English translation
task. We confirm that our re-implementation also shows the same tendency and
alleviates the problem of repeating and missing words in the translation on a
English-Japanese task too. In addition, we evaluate the effectiveness of
pre-training by comparing it with a jointly-trained model of forward
translation and back-translation.



Processing term: ReLU from hard 20 terms


ReLU stands for Rectified Linear Unit, which is a commonly used activation function in neural networks. It is
defined as f(x) = max(0, x), which means that it returns zero if the input is negative and the input itself if
the input is positive. While ReLU is simple and easy to implement, it has some limitations such as being non-
differentiable at the origin, which makes it difficult to train certain deep neural networks.


From:
1. SAU: Smooth activation function using convolution with approximate
  identities
Abstract: Well-known activation functions like ReLU or Leaky ReLU are
non-differentiable at the origin. Over the years, many smooth approximations of
ReLU have been proposed using various smoothing techniques. We propose new
smooth approximations of a non-differentiable activation function by convolving
it with approximate identities. In particular, we present smooth approximations
of Leaky ReLU and show that they outperform several well-known activation
functions in various datasets and models. We call this function Smooth
Activation Unit (SAU). Replacing ReLU by SAU, we get 5.12% improvement with
ShuffleNet V2 (2.0x) model on CIFAR100 dataset.

2. Deep Nonparametric Regression on Approximately Low-dimensional Manifolds
Abstract: structure of neural networks and propose a notion of network relative
efficiency between two types of neural networks, which provides a quantitative
measure for evaluating the relative merits of different network structures. To
establish these results, we derive a novel approximation error bound for the
H\"older smooth functions with a positive smoothness index using ReLU activated
neural networks, which may be of independent interest. Our results are derived
under weaker assumptions on the data distribution and the neural network
structure than those in the existing literature.

3. SMU: smooth activation function for deep networks using smoothing
  maximum technique
Abstract: Deep learning researchers have a keen interest in proposing two new novel
activation functions which can boost network performance. A good choice of
activation function can have significant consequences in improving network
performance. A handcrafted activation is the most common choice in neural
network models. ReLU is the most common choice in the deep learning community
due to its simplicity though ReLU has some serious drawbacks. In this paper, we
have proposed a new novel activation function based on approximation of known
activation functions like Leaky ReLU, and we call this function Smooth Maximum
Unit (SMU). Replacing ReLU by SMU, we have got 6.22% improvement in the
CIFAR100 dataset with the ShuffleNet V2 model.



Processing term: L2 from hard 20 terms


L2 refers to the Hilbert space of square-integrable functions on a certain domain, often denoted by $\cal
L^2(\Omega)$. It is commonly used in quantum mechanics and signal processing, among other fields. In the
context of control theory, L2 is sometimes used to refer to the norm of a vector or matrix in a specific inner
product space. Specifically, if $X$ is a matrix whose columns span a control input space, then the L2 norm of
$X$ is defined as $\|X\|_2 = \sqrt{\sum_{i=1}^{n} x_i^2}$, where $x_i$ represents the $i$-th column of $X$.
This norm is


From:
1. SOS Methods for Multi-Delay Systems: A Dual Form of Lyapanov-Krasovskii
  Functional
Abstract: We present a dual form of Lyapunov-Krasovskii functional which allows the
problem of controller synthesis of multi-delay systems to be formulated and
solved in a convex manner. First, we give a general form of dual stability
condition formulated in terms of Lyapunov operators which are positive,
self-adjoint and preserve the structure of the state-space. Second, we provide
a class of such operators and express the stability conditions as positivity
and negativity of quadratic Lyapunov-Krasovskii functional forms. Next, we
adapt the SOS methodology to express positivity and negativity of these forms
as LMIs, describing a new set of polynomial manipulation tools designed for
this purpose. Finally, we apply the resulting LMIs to a battery of numerical
examples and demonstrate that the stability conditions are not conservative.
The results of this paper are significant in that they open the way for dynamic
output H-infinity optimal control of systems with multiple time-delays.

2. On Linear Time-Invariant Systems Analysis via A Single Trajectory: A
  Linear Programming Approach
Abstract: In this note, a novel methodology that can extract a number of analysis
results for linear time-invariant systems (LTI) given only a single trajectory
of the considered system is proposed. The superiority of the proposed technique
relies on the fact that it provides an automatic and formal way to obtain
valuable information about the controlled system by only having access to a
single trajectory over a finite period of time (i.e., the system dynamics is
assumed to be unknown). At first, we characterize the stability region of LTI
systems given only a single trajectory dataset by constructing the associated
Lyapunov function of the system. The Lyapunov function is found by formulating
and solving a linear programming (LP) problem. Then, we extend the same
methodology to a variety of essential analysis results for LTI systems such as
deriving bounds on the output energy, deriving bounds on output peak, deriving
$\mathbf{L}_2$ and RMS gains. To illustrate the efficacy of the proposed

3. $H_{\infty}$-optimal control of coupled ODE-PDE systems using PIE
  framework and LPIs
Abstract: In this paper, we present a dual formulation for Lyapunov's stability test
and Bounded-real Lemma for Partial Integral Equations (PIEs). Then, we use this
formulation to solve the $H_{\infty}$-optimal controller synthesis problem for
ODE-PDEs that can be converted to a PIE. First, we provide a general dual
criterion for the Lyapunov stability and the Bounded-real Lemma for PIEs using
quadratic Lyapunov functionals. Then, we use a class of operators called
Partial Integral (PI) operators to parametrize the said Lyapunov functional and
express the dual criterion as an operator-valued convex optimization problem
with sign-definite constraints on PI operators -- called Linear PI Inequalities
(LPIs). Consequently, the optimal controller synthesis problem for PIEs, and
thus, for PDEs, is formulated as LPIs. Finally, the LPI optimization problems
are solved using a computational toolbox called PIETOOLS and tested on various
examples to demonstrate the utility of the proposed method.



Processing term: robust speech or speaker recognition from hard 20 terms


Robust speech or speaker recognition refers to speech technology that can accurately recognize speech even in
challenging conditions such as background noise, accents, and dialects. It involves using advanced algorithms
and machine learning techniques to analyze and understand spoken language.


From:
1. Unsupervised Voice Activity Detection by Modeling Source and System
  Information using Zero Frequency Filtering
Abstract: Voice activity detection (VAD) is an important pre-processing step for speech
technology applications. The task consists of deriving segment boundaries of
audio signals which contain voicing information. In recent years, it has been
shown that voice source and vocal tract system information can be extracted
using zero-frequency filtering (ZFF) without making any explicit model
assumptions about the speech signal. This paper investigates the potential of
zero-frequency filtering for jointly modeling voice source and vocal tract
system information, and proposes two approaches for VAD. The first approach
demarcates voiced regions using a composite signal composed of different
zero-frequency filtered signals. The second approach feeds the composite signal
as input to the rVAD algorithm. These approaches are compared with other
supervised and unsupervised VAD methods in the literature, and are evaluated on
the Aurora-2 database, across a range of SNRs (20 to -5 dB). Our studies show

2. SG-VAD: Stochastic Gates Based Speech Activity Detection
Abstract: We propose a novel voice activity detection (VAD) model in a low-resource
environment. Our key idea is to model VAD as a denoising task, and construct a
network that is designed to identify nuisance features for a speech
classification task. We train the model to simultaneously identify irrelevant
features while predicting the type of speech event. Our model contains only
7.8K parameters, outperforms the previously proposed methods on the AVA-Speech
evaluation set, and provides comparative results on the HAVIC dataset. We
present its architecture, experimental results, and ablation study on the
model's components. We publish the code and the models here
https://www.github.com/jsvir/vad.

3. Improvement of Noise-Robust Single-Channel Voice Activity Detection with
  Spatial Pre-processing
Abstract: Voice activity detection (VAD) remains a challenge in noisy environments.
With access to multiple microphones, prior studies have attempted to improve
the noise robustness of VAD by creating multi-channel VAD (MVAD) methods.
However, MVAD is relatively new compared to single-channel VAD (SVAD), which
has been thoroughly developed in the past. It might therefore be advantageous
to improve SVAD methods with pre-processing to obtain superior VAD, which is
under-explored. This paper improves SVAD through two pre-processing methods, a
beamformer and a spatial target speaker detector. The spatial detector sets
signal frames to zero when no potential speaker is present within a target
direction. The detector may be implemented as a filter, meaning the input
signal for the SVAD is filtered according to the detector's output; or it may
be implemented as a spatial VAD to be combined with the SVAD output. The
evaluation is made on a noisy reverberant speech database, with clean speech



Processing term: end-to-end deep neural network from hard 10 terms


An "end-to-end" deep neural network refers to a type of neural network architecture where all the layers from
the input layer to the output layer are connected together in a single chain without any intermediate layers.
This means that there are no separate pre-processing or feature extraction layers before the
classification/regression layers. The term "deep" refers to the number of hidden layers in the network, which
can range from one to hundreds or more. End-to-end deep neural networks are commonly used for tasks such as
image recognition, natural language processing, speech recognition, and autonomous driving.


From:
1. StereoSpike: Depth Learning with a Spiking Neural Network
Abstract: Depth estimation is an important computer vision task, useful in particular
for navigation in autonomous vehicles, or for object manipulation in robotics.
Here we solved it using an end-to-end neuromorphic approach, combining two
event-based cameras and a Spiking Neural Network (SNN) with a slightly modified
U-Net-like encoder-decoder architecture, that we named StereoSpike. More
specifically, we used the Multi Vehicle Stereo Event Camera Dataset (MVSEC). It
provides a depth ground-truth, which was used to train StereoSpike in a
supervised manner, using surrogate gradient descent. We propose a novel readout
paradigm to obtain a dense analog prediction -- the depth of each pixel -- from
the spikes of the decoder. We demonstrate that this architecture generalizes
very well, even better than its non-spiking counterparts, leading to
state-of-the-art test accuracy. To the best of our knowledge, it is the first
time that such a large-scale regression problem is solved by a fully spiking

2. Dilated Fully Convolutional Neural Network for Depth Estimation from a
  Single Image
Abstract: Depth prediction plays a key role in understanding a 3D scene. Several
techniques have been developed throughout the years, among which Convolutional
Neural Network has recently achieved state-of-the-art performance on estimating
depth from a single image. However, traditional CNNs suffer from the lower
resolution and information loss caused by the pooling layers. And oversized
parameters generated from fully connected layers often lead to a exploded
memory usage problem. In this paper, we present an advanced Dilated Fully
Convolutional Neural Network to address the deficiencies. Taking advantages of
the exponential expansion of the receptive field in dilated convolutions, our
model can minimize the loss of resolution. It also reduces the amount of
parameters significantly by replacing the fully connected layers with the fully
convolutional layers. We show experimentally on NYU Depth V2 datasets that the
depth prediction obtained from our model is considerably closer to ground truth

3. Interpretability in Contact-Rich Manipulation via Kinodynamic Images
Abstract: Deep Neural Networks (NNs) have been widely utilized in contact-rich
manipulation tasks to model the complicated contact dynamics. However, NN-based
models are often difficult to decipher which can lead to seemingly inexplicable
behaviors and unidentifiable failure cases. In this work, we address the
interpretability of NN-based models by introducing the kinodynamic images. We
propose a methodology that creates images from the kinematic and dynamic data
of a contact-rich manipulation task. Our formulation visually reflects the
task's state by encoding its kinodynamic variations and temporal evolution. By
using images as the state representation, we enable the application of
interpretability modules that were previously limited to vision-based tasks. We
use this representation to train Convolution-based Networks and we extract
interpretations of the model's decisions with Grad-CAM, a technique that
produces visual explanations. Our method is versatile and can be applied to any



Processing term: Conv ( ) filter from hard 10 terms


 A conv ( ) filter refers to a convolutional filter used in computer vision and machine learning. It is a
mathematical operator that performs a specific operation on two-dimensional arrays, known as images. Conv
filters are commonly used in deep learning architectures such as convolutional neural networks (CNNs). They
are designed to extract features from images by applying a set of weights to the input pixels. The output of a
conv filter is a new image that highlights certain features within the original image.


From:
1. Resolution learning in deep convolutional networks using scale-space
  theory
Abstract: Resolution in deep convolutional neural networks (CNNs) is typically bounded
by the receptive field size through filter sizes, and subsampling layers or
strided convolutions on feature maps. The optimal resolution may vary
significantly depending on the dataset. Modern CNNs hard-code their resolution
hyper-parameters in the network architecture which makes tuning such
hyper-parameters cumbersome. We propose to do away with hard-coded resolution
hyper-parameters and aim to learn the appropriate resolution from data. We use
scale-space theory to obtain a self-similar parametrization of filters and make
use of the N-Jet: a truncated Taylor series to approximate a filter by a
learned combination of Gaussian derivative filters. The parameter sigma of the
Gaussian basis controls both the amount of detail the filter encodes and the
spatial extent of the filter. Since sigma is a continuous parameter, we can
optimize it with respect to the loss. The proposed N-Jet layer achieves

2. Gabor filter incorporated CNN for compression
Abstract: Convolutional neural networks (CNNs) are remarkably successful in many
computer vision tasks. However, the high cost of inference is problematic for
embedded and real-time systems, so there are many studies on compressing the
networks. On the other hand, recent advances in self-attention models showed
that convolution filters are preferable to self-attention in the earlier
layers, which indicates that stronger inductive biases are better in the
earlier layers. As shown in convolutional filters, strong biases can train
specific filters and construct unnecessarily filters to zero. This is analogous
to classical image processing tasks, where choosing the suitable filters makes
a compact dictionary to represent features. We follow this idea and incorporate
Gabor filters in the earlier layers of CNNs for compression. The parameters of
Gabor filters are learned through backpropagation, so the features are
restricted to Gabor filters. We show that the first layer of VGG-16 for

3. Multidimensional Projection Filters via Automatic Differentiation and
  Sparse-Grid Integration
Abstract: The projection filter is a technique for approximating the solutions of
optimal filtering problems. In projection filters, the Kushner--Stratonovich
stochastic partial differential equation that governs the propagation of the
optimal filtering density is projected to a manifold of parametric densities,
resulting in a finite-dimensional stochastic differential equation. Despite the
fact that projection filters are capable of representing complicated
probability densities, their current implementations are limited to Gaussian
family or unidimensional filtering applications. This work considers a
combination of numerical integration and automatic differentiation to construct
projection filter algorithms for more generic problems. Specifically, we
provide a detailed exposition of this combination for the manifold of the
exponential family, and show how to apply the projection filter to
multidimensional cases. We demonstrate numerically that based on comparison to



Processing term: Time Series Analysis from hard 10 terms


Time Series Analysis (TSA) is a statistical technique used to analyze data that is collected over time. It
involves analyzing patterns and trends in the data over multiple time periods to identify any changes or
anomalies that may occur. TSA is often used in fields such as finance, economics, and meteorology to make
predictions about future events based on past data.


From:
1. A Review of Open Source Software Tools for Time Series Analysis
Abstract: Time series data is used in a wide range of real world applications. In a
variety of domains , detailed analysis of time series data (via Forecasting and
Anomaly Detection) leads to a better understanding of how events associated
with a specific time instance behave. Time Series Analysis (TSA) is commonly
performed with plots and traditional models. Machine Learning (ML) approaches ,
on the other hand , have seen an increase in the state of the art for
Forecasting and Anomaly Detection because they provide comparable results when
time and data constraints are met. A number of time series toolboxes are
available that offer rich interfaces to specific model classes (ARIMA/filters ,
neural networks) or framework interfaces to isolated time series modelling
tasks (forecasting , feature extraction , annotation , classification).
Nonetheless , open source machine learning capabilities for time series remain
limited , and existing libraries are frequently incompatible with one another.

2. Foundations of Sequence-to-Sequence Modeling for Time Series
Abstract: The availability of large amounts of time series data, paired with the
performance of deep-learning algorithms on a broad class of problems, has
recently led to significant interest in the use of sequence-to-sequence models
for time series forecasting. We provide the first theoretical analysis of this
time series forecasting framework. We include a comparison of
sequence-to-sequence modeling to classical time series models, and as such our
theory can serve as a quantitative guide for practitioners choosing between
different modeling methodologies.

3. A Survey of Estimation Methods for Sparse High-dimensional Time Series
  Models
Abstract: High-dimensional time series datasets are becoming increasingly common in
many areas of biological and social sciences. Some important applications
include gene regulatory network reconstruction using time course gene
expression data, brain connectivity analysis from neuroimaging data, structural
analysis of a large panel of macroeconomic indicators, and studying linkages
among financial firms for more robust financial regulation. These applications
have led to renewed interest in developing principled statistical methods and
theory for estimating large time series models given only a relatively small
number of temporally dependent samples. Sparse modeling approaches have gained
popularity over the last two decades in statistics and machine learning for
their interpretability and predictive accuracy. Although there is a rich
literature on several sparsity inducing methods when samples are independent,
research on the statistical properties of these methods for estimating time



Processing term: LSTM from hard 10 terms


 LSTM stands for Long Short-Term Memory, which is a type of recurrent neural network (RNN) that is designed to
handle long-term dependencies in sequential data. It is particularly useful for tasks like language modeling,
speech recognition, and sentiment analysis. The LSTM cell consists of a memory gate and an update gate, which
allow it to remember important features of the input sequence while also updating its internal state based on
new information.


From:
1. Word Sense Disambiguation with LSTM: Do We Really Need 100 Billion
  Words?
Abstract: Recently, Yuan et al. (2016) have shown the effectiveness of using Long
Short-Term Memory (LSTM) for performing Word Sense Disambiguation (WSD). Their
proposed technique outperformed the previous state-of-the-art with several
benchmarks, but neither the training data nor the source code was released.
This paper presents the results of a reproduction study of this technique using
only openly available datasets (GigaWord, SemCore, OMSTI) and software
(TensorFlow). From them, it emerged that state-of-the-art results can be
obtained with much less data than hinted by Yuan et al. All code and trained
models are made freely available.

2. A Comprehensive Survey on Word Representation Models: From Classical to
  State-Of-The-Art Word Representation Language Models
Abstract: Word representation has always been an important research area in the history
of natural language processing (NLP). Understanding such complex text data is
imperative, given that it is rich in information and can be used widely across
various applications. In this survey, we explore different word representation
models and its power of expression, from the classical to modern-day
state-of-the-art word representation language models (LMS). We describe a
variety of text representation methods, and model designs have blossomed in the
context of NLP, including SOTA LMs. These models can transform large volumes of
text into effective vector representations capturing the same semantic
information. Further, such representations can be utilized by various machine
learning (ML) algorithms for a variety of NLP related tasks. In the end, this
survey briefly discusses the commonly used ML and DL based classifiers,
evaluation metrics and the applications of these word embeddings in different

3. LSTM based models stability in the context of Sentiment Analysis for
  social media
Abstract: Deep learning techniques have proven their effectiveness for Sentiment
Analysis (SA) related tasks. Recurrent neural networks (RNN), especially Long
Short-Term Memory (LSTM) and Bidirectional LSTM, have become a reference for
building accurate predictive models. However, the models complexity and the
number of hyperparameters to configure raises several questions related to
their stability. In this paper, we present various LSTM models and their key
parameters, and we perform experiments to test the stability of these models in
the context of Sentiment Analysis.



Processing term: MLP from hard 10 terms


 MLP stands for Multi-Layer Perceptron, which is a type of neural network architecture commonly used for
supervised learning tasks like classification and regression. It consists of multiple layers of nodes, each of
which performs a nonlinear transformation of the input data before passing it to the next layer.


From:
1. ECG Heartbeat Classification Using Multimodal Fusion
Abstract: Electrocardiogram (ECG) is an authoritative source to diagnose and counter
critical cardiovascular syndromes such as arrhythmia and myocardial infarction
(MI). Current machine learning techniques either depend on manually extracted
features or large and complex deep learning networks which merely utilize the
1D ECG signal directly. Since intelligent multimodal fusion can perform at the
stateof-the-art level with an efficient deep network, therefore, in this paper,
we propose two computationally efficient multimodal fusion frameworks for ECG
heart beat classification called Multimodal Image Fusion (MIF) and Multimodal
Feature Fusion (MFF). At the input of these frameworks, we convert the raw ECG
data into three different images using Gramian Angular Field (GAF), Recurrence
Plot (RP) and Markov Transition Field (MTF). In MIF, we first perform image
fusion by combining three imaging modalities to create a single image modality

2. ECG Heartbeat Classification Using Multimodal Fusion
Abstract: Plot (RP) and Markov Transition Field (MTF). In MIF, we first perform image
fusion by combining three imaging modalities to create a single image modality
which serves as input to the Convolutional Neural Network (CNN). In MFF, we
extracted features from penultimate layer of CNNs and fused them to get unique
and interdependent information necessary for better performance of classifier.
These informational features are finally used to train a Support Vector Machine
(SVM) classifier for ECG heart-beat classification. We demonstrate the
superiority of the proposed fusion models by performing experiments on
PhysioNets MIT-BIH dataset for five distinct conditions of arrhythmias which
are consistent with the AAMI EC57 protocols and on PTB diagnostics dataset for
Myocardial Infarction (MI) classification. We achieved classification accuracy
of 99.7% and 99.2% on arrhythmia and MI classification, respectively.

3. A Transfer-Learning Based Ensemble Architecture for ECG Signal
  Classification
Abstract: Manual interpretation and classification of ECG signals lack both accuracy
and reliability. These continuous time-series signals are more effective when
represented as an image for CNN-based classification. A continuous Wavelet
transform filter is used here to get corresponding images. In achieving the
best result generic CNN architectures lack sufficient accuracy and also have a
higher run-time. To address this issue, we propose an ensemble method of
transfer learning-based models to classify ECG signals. In our work, two
modified VGG-16 models and one InceptionResNetV2 model with added feature
extracting layers and ImageNet weights are working as the backbone. After
ensemble, we report an increase of 6.36% accuracy than previous MLP-based
algorithms. After 5-fold cross-validation with the Physionet dataset, our model
reaches an accuracy of 99.98%.



Processing term: categorical data analysis technique from hard 10 terms


Categorical data analysis technique refers to any statistical method used to analyze data that has been
categorized or grouped into categories or classes. Hildebrand's del is a specific technique used in
epidemiology to identify risk factors associated with a particular disease or condition. It involves dividing
the population into subsets based on certain characteristics, such as age or gender, and then comparing the
incidence rates of the disease within each subgroup.


From:
1. Classification at the Accuracy Limit -- Facing the Problem of Data
  Ambiguity
Abstract: Data classification, the process of analyzing data and organizing it into
categories, is a fundamental computing problem of natural and artificial
information processing systems. Ideally, the performance of classifier models
would be evaluated using unambiguous data sets, where the 'correct' assignment
of category labels to the input data vectors is unequivocal. In real-world
problems, however, a significant fraction of actually occurring data vectors
will be located in a boundary zone between or outside of all categories, so
that perfect classification cannot even in principle be achieved. We derive the
theoretical limit for classification accuracy that arises from the overlap of
data categories. By using a surrogate data generation model with adjustable
statistical properties, we show that sufficiently powerful classifiers based on
completely different principles, such as perceptrons and Bayesian models, all

2. Unbiased Top-k Learning to Rank with Causal Likelihood Decomposition
Abstract: Decomposition (CLD), a unified approach to simultaneously mitigating these two
biases in top-k learning to rank. By decomposing the log-likelihood of the
biased data as an unbiased term that only related to relevance, plus other
terms related to biases, CLD successfully detaches the relevance from position
bias and sample selection bias. An unbiased ranking model can be obtained from
the unbiased term, via maximizing the whole likelihood. An extension to the
pairwise neural ranking is also developed. Advantages of CLD include
theoretical soundness and a unified framework for pointwise and pairwise
unbiased top-k learning to rank. Extensive experimental results verified that
CLD, including its pairwise neural extension, outperformed the baselines by
mitigating both the position bias and the sample selection bias. Empirical
studies also showed that CLD is robust to the variation of bias severity and
the click noise.

3. Benchmarking distance-based partitioning methods for mixed-type data
Abstract: Clustering mixed-type data, that is, observation by variable data that
consist of both continuous and categorical variables poses novel challenges.
Foremost among these challenges is the choice of the most appropriate
clustering method for the data. This paper presents a benchmarking study
comparing eight distance-based partitioning methods for mixed-type data in
terms of cluster recovery performance. A series of simulations carried out by a
full factorial design are presented that examined the effect of a variety of
factors on cluster recovery. The amount of cluster overlap, the percentage of
categorical variables in the data set, the number of clusters and the number of
observations had the largest effects on cluster recovery and in most of the
tested scenarios. KAMILA, K-Prototypes and sequential Factor Analysis and
K-Means clustering typically performed better than other methods. The study can
be a useful reference for practitioners in the choice of the most appropriate
method.